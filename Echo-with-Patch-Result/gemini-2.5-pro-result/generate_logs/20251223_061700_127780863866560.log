2025-12-23 06:17:00,455 - thread-127780863866560.app.docker.base_container - INFO - Created temporary project directory: /tmp/tmpjnpypqcr/xarray
2025-12-23 06:17:00,455 - thread-127780863866560.app.docker.base_container - DEBUG - Using workdir: /testbed
2025-12-23 06:17:00,455 - thread-127780863866560.app.docker.base_container - INFO - Pulling docker image: swebench/sweb.eval.x86_64.pydata_1776_xarray-4356:v1
2025-12-23 06:17:08,454 - thread-127780863866560.app.docker.base_container - INFO - Tagging pulled image as: prometheus_user_defined_container_1afe38a694
2025-12-23 06:17:08,533 - thread-127780863866560.app.docker.base_container - INFO - Starting container from image prometheus_user_defined_container_1afe38a694
2025-12-23 06:17:11,485 - thread-127780863866560.app.docker.base_container - INFO - Starting persistent shell for interactive mode...
2025-12-23 06:17:15,402 - thread-127780863866560.app.docker.base_container - INFO - Persistent shell is ready
2025-12-23 06:17:15,402 - thread-127780863866560.app.docker.base_container - DEBUG - Executing command: . /opt/miniconda3/bin/activate
2025-12-23 06:17:15,670 - thread-127780863866560.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-23 06:17:15,670 - thread-127780863866560.app.docker.base_container - DEBUG - Command output:


2025-12-23 06:17:15,670 - thread-127780863866560.app.docker.base_container - DEBUG - Executing command: conda activate testbed
2025-12-23 06:17:15,909 - thread-127780863866560.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-23 06:17:15,909 - thread-127780863866560.app.docker.base_container - DEBUG - Command output:


2025-12-23 06:17:15,910 - thread-127780863866560.app.docker.base_container - DEBUG - Executing command: python -m pip install -e .
2025-12-23 06:17:22,478 - thread-127780863866560.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-23 06:17:22,478 - thread-127780863866560.app.docker.base_container - DEBUG - Command output:

Obtaining file:///testbed
  Preparing metadata (setup.py) ... [?25l- \ | / done
[?25hRequirement already satisfied: numpy>=1.15 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from xarray==0.16.1.dev50+ge05fddea) (1.23.0)
Requirement already satisfied: pandas>=0.25 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from xarray==0.16.1.dev50+ge05fddea) (1.5.3)
Requirement already satisfied: setuptools>=38.4 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from xarray==0.16.1.dev50+ge05fddea) (68.0.0)
Requirement already satisfied: python-dateutil>=2.8.1 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from pandas>=0.25->xarray==0.16.1.dev50+ge05fddea) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from pandas>=0.25->xarray==0.16.1.dev50+ge05fddea) (2023.3)
Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=0.25->xarray==0.16.1.dev50+ge05fddea) (1.16.0)
Installing collected packages: xarray
  Attempting uninstall: xarray
    Found existing installation: xarray 0.16.1.dev50+ge05fddea
    Uninstalling xarray-0.16.1.dev50+ge05fddea:
      Successfully uninstalled xarray-0.16.1.dev50+ge05fddea
[33m  DEPRECATION: Legacy editable install of xarray==0.16.1.dev50+ge05fddea from file:///testbed (setup.py develop) is deprecated. pip 25.0 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457[0m[33m
[0m  Running setup.py develop for xarray
Successfully installed xarray
[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.[0m[33m

2025-12-23 06:17:22,504 - thread-127780863866560.app.lang_graph.nodes.focal_code_retrieval_node - INFO - Retrieving focal code context via external CRA
2025-12-23 06:25:43,020 - thread-127780863866560.app.lang_graph.nodes.focal_code_retrieval_node - INFO - Focal code context retrieved: 6 items
2025-12-23 06:25:43,021 - thread-127780863866560.app.lang_graph.nodes.test_case_retrieval_node - INFO - Retrieving test case context via external CRA
2025-12-23 06:31:08,659 - thread-127780863866560.app.lang_graph.nodes.test_case_retrieval_node - INFO - Test case context retrieved: 3 items
2025-12-23 06:31:08,670 - thread-127780863866560.app.lang_graph.nodes.bug_reproducing_write_message_node - DEBUG - Sending message to BugReproducingWriteNode:
content='Issue title:\nsum: min_count is not available for reduction with more than one dimensions\n\nIssue description: \n**Is your feature request related to a problem? Please describe.**\n\n`sum` with `min_count` errors when passing more than one dim:\n\n```python\nimport xarray as xr\nda = xr.DataArray([[1., 2, 3], [4, 5, 6]])\nda.sum(["dim_0", "dim_1"], min_count=1)\n```\n\n**Describe the solution you\'d like**\nThe logic to calculate the number of valid elements is here:\nhttps://github.com/pydata/xarray/blob/1be777fe725a85b8cc0f65a2bc41f4bc2ba18043/xarray/core/nanops.py#L35\n\nI *think* this can be fixed by replacing\n\n`mask.shape[axis]` with `np.take(a.shape, axis).prod()`\n\n**Additional context**\nPotentially relevant for #4351\n\n\nIssue comments:\n\n\nPatch Information:\n```diff\ndiff --git a/xarray/core/nanops.py b/xarray/core/nanops.py\nindex 41c8d258..a37b19b4 100644\n--- a/xarray/core/nanops.py\n+++ b/xarray/core/nanops.py\n@@ -26,13 +26,14 @@ def _maybe_null_out(result, axis, mask, min_count=1):\n     """\n     xarray version of pandas.core.nanops._maybe_null_out\n     """\n-    if hasattr(axis, "__len__"):  # if tuple or list\n-        raise ValueError(\n-            "min_count is not available for reduction with more than one dimensions."\n-        )\n-\n     if axis is not None and getattr(result, "ndim", False):\n-        null_mask = (mask.shape[axis] - mask.sum(axis) - min_count) < 0\n+        if hasattr(axis, "__len__"):  # if tuple or list\n+            # Calculate the product of the dimensions being reduced\n+            n = np.take(mask.shape, axis).prod()\n+            null_mask = (n - mask.sum(axis=axis) - min_count) < 0\n+        else:\n+            null_mask = (mask.shape[axis] - mask.sum(axis=axis) - min_count) < 0\n+        \n         if null_mask.any():\n             dtype, fill_value = dtypes.maybe_promote(result.dtype)\n             result = result.astype(dtype)\n\n```\n\nFocal Code Context (relevant code that may need fixing):\n{\'relative_path\': \'xarray/core/computation.py\', \'content\': \'1. """\\n2. Functions for applying functions that act on arrays to xarray\\\'s labeled data.\\n3. """\\n4. import functools\\n5. import itertools\\n6. import operator\\n7. import warnings\\n8. from collections import Counter\\n9. from distutils.version import LooseVersion\\n10. from typing import (\\n11.     TYPE_CHECKING,\\n12.     AbstractSet,\\n13.     Any,\\n14.     Callable,\\n15.     Dict,\\n16.     Hashable,\\n17.     Iterable,\\n18.     List,\\n19.     Mapping,\\n20.     Optional,\\n21.     Sequence,\\n22.     Tuple,\\n23.     Union,\\n24. )\\n25. \\n26. import numpy as np\\n27. \\n28. from . import dtypes, duck_array_ops, utils\\n29. from .alignment import align, deep_align\\n30. from .merge import merge_coordinates_without_align\\n31. from .options import OPTIONS\\n32. from .pycompat import dask_array_type\\n33. from .utils import is_dict_like\\n34. from .variable import Variable\\n35. \\n36. if TYPE_CHECKING:\\n37.     from .coordinates import Coordinates  # noqa\\n38.     from .dataset import Dataset\\n39. \\n40. _NO_FILL_VALUE = utils.ReprObject("<no-fill-value>")\\n41. _DEFAULT_NAME = utils.ReprObject("<default-name>")\\n42. _JOINS_WITHOUT_FILL_VALUES = frozenset({"inner", "exact"})\\n43. \\n44. \\n45. class _UFuncSignature:\\n46.     """Core dimensions signature for a given function.\\n47. \\n48.     Based on the signature provided by generalized ufuncs in NumPy.\\n49. \\n50.     Attributes\\n51.     ----------\\n52.     input_core_dims : tuple[tuple]\\n53.         Core dimension names on each input variable.\\n54.     output_core_dims : tuple[tuple]\\n55.         Core dimension names on each output variable.\\n56.     """\\n57. \\n58.     __slots__ = (\\n59.         "input_core_dims",\\n60.         "output_core_dims",\\n61.         "_all_input_core_dims",\\n62.         "_all_output_core_dims",\\n63.         "_all_core_dims",\\n64.     )\\n65. \\n66.     def __init__(self, input_core_dims, output_core_dims=((),)):\\n67.         self.input_core_dims = tuple(tuple(a) for a in input_core_dims)\\n68.         self.output_core_dims = tuple(tuple(a) for a in output_core_dims)\\n69.         self._all_input_core_dims = None\\n70.         self._all_output_core_dims = None\\n71.         self._all_core_dims = None\\n72. \\n73.     @property\\n74.     def all_input_core_dims(self):\\n75.         if self._all_input_core_dims is None:\\n76.             self._all_input_core_dims = frozenset(\\n77.                 dim for dims in self.input_core_dims for dim in dims\\n78.             )\\n79.         return self._all_input_core_dims\\n80. \\n81.     @property\\n82.     def all_output_core_dims(self):\\n83.         if self._all_output_core_dims is None:\\n84.             self._all_output_core_dims = frozenset(\\n85.                 dim for dims in self.output_core_dims for dim in dims\\n86.             )\\n87.         return self._all_output_core_dims\\n88. \\n89.     @property\\n90.     def all_core_dims(self):\\n91.         if self._all_core_dims is None:\\n92.             self._all_core_dims = self.all_input_core_dims | self.all_output_core_dims\\n93.         return self._all_core_dims\\n94. \\n95.     @property\\n96.     def dims_map(self):\\n97.         return {\\n98.             core_dim: f"dim{n}" for n, core_dim in enumerate(sorted(self.all_core_dims))\\n99.         }\\n100. \\n101.     @property\\n102.     def num_inputs(self):\\n103.         return len(self.input_core_dims)\\n104. \\n105.     @property\\n106.     def num_outputs(self):\\n107.         return len(self.output_core_dims)\\n108. \\n109.     def __eq__(self, other):\\n110.         try:\\n111.             return (\\n112.                 self.input_core_dims == other.input_core_dims\\n113.                 and self.output_core_dims == other.output_core_dims\\n114.             )\\n115.         except AttributeError:\\n116.             return False\\n117. \\n118.     def __ne__(self, other):\\n119.         return not self == other\\n120. \\n121.     def __repr__(self):\\n122.         return "{}({!r}, {!r})".format(\\n123.             type(self).__name__, list(self.input_core_dims), list(self.output_core_dims)\\n124.         )\\n125. \\n126.     def __str__(self):\\n127.         lhs = ",".join("({})".format(",".join(dims)) for dims in self.input_core_dims)\\n128.         rhs = ",".join("({})".format(",".join(dims)) for dims in self.output_core_dims)\\n129.         return f"{lhs}->{rhs}"\\n130. \\n131.     def to_gufunc_string(self):\\n132.         """Create an equivalent signature string for a NumPy gufunc.\\n133. \\n134.         Unlike __str__, handles dimensions that don\\\'t map to Python\\n135.         identifiers.\\n136.         """\\n137.         input_core_dims = [\\n138.             [self.dims_map[dim] for dim in core_dims]\\n139.             for core_dims in self.input_core_dims\\n140.         ]\\n141.         output_core_dims = [\\n142.             [self.dims_map[dim] for dim in core_dims]\\n143.             for core_dims in self.output_core_dims\\n144.         ]\\n145.         alt_signature = type(self)(input_core_dims, output_core_dims)\\n146.         return str(alt_signature)\\n147. \\n148. \\n149. def result_name(objects: list) -> Any:\\n150.     # use the same naming heuristics as pandas:\\n151.     # https://github.com/blaze/blaze/issues/458#issuecomment-51936356\\n152.     names = {getattr(obj, "name", _DEFAULT_NAME) for obj in objects}\\n153.     names.discard(_DEFAULT_NAME)\\n154.     if len(names) == 1:\\n155.         (name,) = names\\n156.     else:\\n157.         name = None\\n158.     return name\\n159. \\n160. \\n161. def _get_coords_list(args) -> List["Coordinates"]:\\n162.     coords_list = []\\n163.     for arg in args:\\n164.         try:\\n165.             coords = arg.coords\\n166.         except AttributeError:\\n167.             pass  # skip this argument\\n168.         else:\\n169.             coords_list.append(coords)\\n170.     return coords_list\\n171. \\n172. \\n173. def build_output_coords(\\n174.     args: list, signature: _UFuncSignature, exclude_dims: AbstractSet = frozenset()\\n175. ) -> "List[Dict[Any, Variable]]":\\n176.     """Build output coordinates for an operation.\\n177. \\n178.     Parameters\\n179.     ----------\\n180.     args : list\\n181.         List of raw operation arguments. Any valid types for xarray operations\\n182.         are OK, e.g., scalars, Variable, DataArray, Dataset.\\n183.     signature : _UfuncSignature\\n184.         Core dimensions signature for the operation.\\n185.     exclude_dims : set, optional\\n186.         Dimensions excluded from the operation. Coordinates along these\\n187.         dimensions are dropped.\\n188. \\n189.     Returns\\n190.     -------\\n191.     Dictionary of Variable objects with merged coordinates.\\n192.     """\\n193.     coords_list = _get_coords_list(args)\\n194. \\n195.     if len(coords_list) == 1 and not exclude_dims:\\n196.         # we can skip the expensive merge\\n197.         (unpacked_coords,) = coords_list\\n198.         merged_vars = dict(unpacked_coords.variables)\\n199.     else:\\n200.         # TODO: save these merged indexes, instead of re-computing them later\\n201.         merged_vars, unused_indexes = merge_coordinates_without_align(\\n202.             coords_list, exclude_dims=exclude_dims\\n203.         )\\n204. \\n205.     output_coords = []\\n206.     for output_dims in signature.output_core_dims:\\n207.         dropped_dims = signature.all_input_core_dims - set(output_dims)\\n208.         if dropped_dims:\\n209.             filtered = {\\n210.                 k: v for k, v in merged_vars.items() if dropped_dims.isdisjoint(v.dims)\\n211.             }\\n212.         else:\\n213.             filtered = merged_vars\\n214.         output_coords.append(filtered)\\n215. \\n216.     return output_coords\\n217. \\n218. \\n219. def apply_dataarray_vfunc(\\n220.     func, *args, signature, join="inner", exclude_dims=frozenset(), keep_attrs=False\\n221. ):\\n222.     """Apply a variable level function over DataArray, Variable and/or ndarray\\n223.     objects.\\n224.     """\\n225.     from .dataarray import DataArray\\n226. \\n227.     if len(args) > 1:\\n228.         args = deep_align(\\n229.             args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False\\n230.         )\\n231. \\n232.     if keep_attrs and hasattr(args[0], "name"):\\n233.         name = args[0].name\\n234.     else:\\n235.         name = result_name(args)\\n236.     result_coords = build_output_coords(args, signature, exclude_dims)\\n237. \\n238.     data_vars = [getattr(a, "variable", a) for a in args]\\n239.     result_var = func(*data_vars)\\n240. \\n241.     if signature.num_outputs > 1:\\n242.         out = tuple(\\n243.             DataArray(variable, coords, name=name, fastpath=True)\\n244.             for variable, coords in zip(result_var, result_coords)\\n245.         )\\n246.     else:\\n247.         (coords,) = result_coords\\n248.         out = DataArray(result_var, coords, name=name, fastpath=True)\\n249. \\n250.     return out\\n251. \\n252. \\n253. def ordered_set_union(all_keys: List[Iterable]) -> Iterable:\\n254.     return {key: None for keys in all_keys for key in keys}.keys()\\n255. \\n256. \\n257. def ordered_set_intersection(all_keys: List[Iterable]) -> Iterable:\\n258.     intersection = set(all_keys[0])\\n259.     for keys in all_keys[1:]:\\n260.         intersection.intersection_update(keys)\\n261.     return [key for key in all_keys[0] if key in intersection]\\n262. \\n263. \\n264. def assert_and_return_exact_match(all_keys):\\n265.     first_keys = all_keys[0]\\n266.     for keys in all_keys[1:]:\\n267.         if keys != first_keys:\\n268.             raise ValueError(\\n269.                 "exact match required for all data variable names, "\\n270.                 "but %r != %r" % (keys, first_keys)\\n271.             )\\n272.     return first_keys\\n273. \\n274. \\n275. _JOINERS = {\\n276.     "inner": ordered_set_intersection,\\n277.     "outer": ordered_set_union,\\n278.     "left": operator.itemgetter(0),\\n279.     "right": operator.itemgetter(-1),\\n280.     "exact": assert_and_return_exact_match,\\n281. }\\n282. \\n283. \\n284. def join_dict_keys(\\n285.     objects: Iterable[Union[Mapping, Any]], how: str = "inner"\\n286. ) -> Iterable:\\n287.     joiner = _JOINERS[how]\\n288.     all_keys = [obj.keys() for obj in objects if hasattr(obj, "keys")]\\n289.     return joiner(all_keys)\\n290. \\n291. \\n292. def collect_dict_values(\\n293.     objects: Iterable[Union[Mapping, Any]], keys: Iterable, fill_value: object = None\\n294. ) -> List[list]:\\n295.     return [\\n296.         [obj.get(key, fill_value) if is_dict_like(obj) else obj for obj in objects]\\n297.         for key in keys\\n298.     ]\\n299. \\n300. \\n301. def _as_variables_or_variable(arg):\\n302.     try:\\n303.         return arg.variables\\n304.     except AttributeError:\\n305.         try:\\n306.             return arg.variable\\n307.         except AttributeError:\\n308.             return arg\\n309. \\n310. \\n311. def _unpack_dict_tuples(\\n312.     result_vars: Mapping[Hashable, Tuple[Variable, ...]], num_outputs: int\\n313. ) -> Tuple[Dict[Hashable, Variable], ...]:\\n314.     out: Tuple[Dict[Hashable, Variable], ...] = tuple({} for _ in range(num_outputs))\\n315.     for name, values in result_vars.items():\\n316.         for value, results_dict in zip(values, out):\\n317.             results_dict[name] = value\\n318.     return out\\n319. \\n320. \\n321. def apply_dict_of_variables_vfunc(\\n322.     func, *args, signature, join="inner", fill_value=None\\n323. ):\\n324.     """Apply a variable level function over dicts of DataArray, DataArray,\\n325.     Variable and ndarray objects.\\n326.     """\\n327.     args = [_as_variables_or_variable(arg) for arg in args]\\n328.     names = join_dict_keys(args, how=join)\\n329.     grouped_by_name = collect_dict_values(args, names, fill_value)\\n330. \\n331.     result_vars = {}\\n332.     for name, variable_args in zip(names, grouped_by_name):\\n333.         result_vars[name] = func(*variable_args)\\n334. \\n335.     if signature.num_outputs > 1:\\n336.         return _unpack_dict_tuples(result_vars, signature.num_outputs)\\n337.     else:\\n338.         return result_vars\\n339. \\n340. \\n341. def _fast_dataset(\\n342.     variables: Dict[Hashable, Variable], coord_variables: Mapping[Hashable, Variable]\\n343. ) -> "Dataset":\\n344.     """Create a dataset as quickly as possible.\\n345. \\n346.     Beware: the `variables` dict is modified INPLACE.\\n347.     """\\n348.     from .dataset import Dataset\\n349. \\n350.     variables.update(coord_variables)\\n351.     coord_names = set(coord_variables)\\n352.     return Dataset._construct_direct(variables, coord_names)\\n353. \\n354. \\n355. def apply_dataset_vfunc(\\n356.     func,\\n357.     *args,\\n358.     signature,\\n359.     join="inner",\\n360.     dataset_join="exact",\\n361.     fill_value=_NO_FILL_VALUE,\\n362.     exclude_dims=frozenset(),\\n363.     keep_attrs=False,\\n364. ):\\n365.     """Apply a variable level function over Dataset, dict of DataArray,\\n366.     DataArray, Variable and/or ndarray objects.\\n367.     """\\n368.     from .dataset import Dataset\\n369. \\n370.     first_obj = args[0]  # we\\\'ll copy attrs from this in case keep_attrs=True\\n371. \\n372.     if dataset_join not in _JOINS_WITHOUT_FILL_VALUES and fill_value is _NO_FILL_VALUE:\\n373.         raise TypeError(\\n374.             "to apply an operation to datasets with different "\\n375.             "data variables with apply_ufunc, you must supply the "\\n376.             "dataset_fill_value argument."\\n377.         )\\n378. \\n379.     if len(args) > 1:\\n380.         args = deep_align(\\n381.             args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False\\n382.         )\\n383. \\n384.     list_of_coords = build_output_coords(args, signature, exclude_dims)\\n385.     args = [getattr(arg, "data_vars", arg) for arg in args]\\n386. \\n387.     result_vars = apply_dict_of_variables_vfunc(\\n388.         func, *args, signature=signature, join=dataset_join, fill_value=fill_value\\n389.     )\\n390. \\n391.     if signature.num_outputs > 1:\\n392.         out = tuple(_fast_dataset(*args) for args in zip(result_vars, list_of_coords))\\n393.     else:\\n394.         (coord_vars,) = list_of_coords\\n395.         out = _fast_dataset(result_vars, coord_vars)\\n396. \\n397.     if keep_attrs and isinstance(first_obj, Dataset):\\n398.         if isinstance(out, tuple):\\n399.             out = tuple(ds._copy_attrs_from(first_obj) for ds in out)\\n400.         else:\\n401.             out._copy_attrs_from(first_obj)\\n402.     return out\\n403. \\n404. \\n405. def _iter_over_selections(obj, dim, values):\\n406.     """Iterate over selections of an xarray object in the provided order."""\\n407.     from .groupby import _dummy_copy\\n408. \\n409.     dummy = None\\n410.     for value in values:\\n411.         try:\\n412.             obj_sel = obj.sel(**{dim: value})\\n413.         except (KeyError, IndexError):\\n414.             if dummy is None:\\n415.                 dummy = _dummy_copy(obj)\\n416.             obj_sel = dummy\\n417.         yield obj_sel\\n418. \\n419. \\n420. def apply_groupby_func(func, *args):\\n421.     """Apply a dataset or datarray level function over GroupBy, Dataset,\\n422.     DataArray, Variable and/or ndarray objects.\\n423.     """\\n424.     from .groupby import GroupBy, peek_at\\n425.     from .variable import Variable\\n426. \\n427.     groupbys = [arg for arg in args if isinstance(arg, GroupBy)]\\n428.     assert groupbys, "must have at least one groupby to iterate over"\\n429.     first_groupby = groupbys[0]\\n430.     if any(not first_groupby._group.equals(gb._group) for gb in groupbys[1:]):\\n431.         raise ValueError(\\n432.             "apply_ufunc can only perform operations over "\\n433.             "multiple GroupBy objects at once if they are all "\\n434.             "grouped the same way"\\n435.         )\\n436. \\n437.     grouped_dim = first_groupby._group.name\\n438.     unique_values = first_groupby._unique_coord.values\\n439. \\n440.     iterators = []\\n441.     for arg in args:\\n442.         if isinstance(arg, GroupBy):\\n443.             iterator = (value for _, value in arg)\\n444.         elif hasattr(arg, "dims") and grouped_dim in arg.dims:\\n445.             if isinstance(arg, Variable):\\n446.                 raise ValueError(\\n447.                     "groupby operations cannot be performed with "\\n448.                     "xarray.Variable objects that share a dimension with "\\n449.                     "the grouped dimension"\\n450.                 )\\n451.             iterator = _iter_over_selections(arg, grouped_dim, unique_values)\\n452.         else:\\n453.             iterator = itertools.repeat(arg)\\n454.         iterators.append(iterator)\\n455. \\n456.     applied = (func(*zipped_args) for zipped_args in zip(*iterators))\\n457.     applied_example, applied = peek_at(applied)\\n458.     combine = first_groupby._combine\\n459.     if isinstance(applied_example, tuple):\\n460.         combined = tuple(combine(output) for output in zip(*applied))\\n461.     else:\\n462.         combined = combine(applied)\\n463.     return combined\\n464. \\n465. \\n466. def unified_dim_sizes(\\n467.     variables: Iterable[Variable], exclude_dims: AbstractSet = frozenset()\\n468. ) -> Dict[Hashable, int]:\\n469. \\n470.     dim_sizes: Dict[Hashable, int] = {}\\n471. \\n472.     for var in variables:\\n473.         if len(set(var.dims)) < len(var.dims):\\n474.             raise ValueError(\\n475.                 "broadcasting cannot handle duplicate "\\n476.                 "dimensions on a variable: %r" % list(var.dims)\\n477.             )\\n478.         for dim, size in zip(var.dims, var.shape):\\n479.             if dim not in exclude_dims:\\n480.                 if dim not in dim_sizes:\\n481.                     dim_sizes[dim] = size\\n482.                 elif dim_sizes[dim] != size:\\n483.                     raise ValueError(\\n484.                         "operands cannot be broadcast together "\\n485.                         "with mismatched lengths for dimension "\\n486.                         "%r: %s vs %s" % (dim, dim_sizes[dim], size)\\n487.                     )\\n488.     return dim_sizes\\n489. \\n490. \\n491. SLICE_NONE = slice(None)\\n492. \\n493. \\n494. def broadcast_compat_data(\\n495.     variable: Variable,\\n496.     broadcast_dims: Tuple[Hashable, ...],\\n497.     core_dims: Tuple[Hashable, ...],\\n498. ) -> Any:\\n499.     data = variable.data\\n500. \\n501.     old_dims = variable.dims\\n502.     new_dims = broadcast_dims + core_dims\\n503. \\n504.     if new_dims == old_dims:\\n505.         # optimize for the typical case\\n506.         return data\\n507. \\n508.     set_old_dims = set(old_dims)\\n509.     missing_core_dims = [d for d in core_dims if d not in set_old_dims]\\n510.     if missing_core_dims:\\n511.         raise ValueError(\\n512.             "operand to apply_ufunc has required core dimensions {}, but "\\n513.             "some of these dimensions are absent on an input variable: {}".format(\\n514.                 list(core_dims), missing_core_dims\\n515.             )\\n516.         )\\n517. \\n518.     set_new_dims = set(new_dims)\\n519.     unexpected_dims = [d for d in old_dims if d not in set_new_dims]\\n520.     if unexpected_dims:\\n521.         raise ValueError(\\n522.             "operand to apply_ufunc encountered unexpected "\\n523.             "dimensions %r on an input variable: these are core "\\n524.             "dimensions on other input or output variables" % unexpected_dims\\n525.         )\\n526. \\n527.     # for consistency with numpy, keep broadcast dimensions to the left\\n528.     old_broadcast_dims = tuple(d for d in broadcast_dims if d in set_old_dims)\\n529.     reordered_dims = old_broadcast_dims + core_dims\\n530.     if reordered_dims != old_dims:\\n531.         order = tuple(old_dims.index(d) for d in reordered_dims)\\n532.         data = duck_array_ops.transpose(data, order)\\n533. \\n534.     if new_dims != reordered_dims:\\n535.         key_parts = []\\n536.         for dim in new_dims:\\n537.             if dim in set_old_dims:\\n538.                 key_parts.append(SLICE_NONE)\\n539.             elif key_parts:\\n540.                 # no need to insert new axes at the beginning that are already\\n541.                 # handled by broadcasting\\n542.                 key_parts.append(np.newaxis)\\n543.         data = data[tuple(key_parts)]\\n544. \\n545.     return data\\n546. \\n547. \\n548. def _vectorize(func, signature, output_dtypes):\\n549.     if signature.all_core_dims:\\n550.         func = np.vectorize(\\n551.             func, otypes=output_dtypes, signature=signature.to_gufunc_string()\\n552.         )\\n553.     else:\\n554.         func = np.vectorize(func, otypes=output_dtypes)\\n555. \\n556.     return func\\n557. \\n558. \\n559. def apply_variable_ufunc(\\n560.     func,\\n561.     *args,\\n562.     signature,\\n563.     exclude_dims=frozenset(),\\n564.     dask="forbidden",\\n565.     output_dtypes=None,\\n566.     vectorize=False,\\n567.     keep_attrs=False,\\n568.     dask_gufunc_kwargs=None,\\n569. ):\\n570.     """Apply a ndarray level function over Variable and/or ndarray objects.\\n571.     """\\n572.     from .variable import Variable, as_compatible_data\\n573. \\n574.     dim_sizes = unified_dim_sizes(\\n575.         (a for a in args if hasattr(a, "dims")), exclude_dims=exclude_dims\\n576.     )\\n577.     broadcast_dims = tuple(\\n578.         dim for dim in dim_sizes if dim not in signature.all_core_dims\\n579.     )\\n580.     output_dims = [broadcast_dims + out for out in signature.output_core_dims]\\n581. \\n582.     input_data = [\\n583.         broadcast_compat_data(arg, broadcast_dims, core_dims)\\n584.         if isinstance(arg, Variable)\\n585.         else arg\\n586.         for arg, core_dims in zip(args, signature.input_core_dims)\\n587.     ]\\n588. \\n589.     if any(isinstance(array, dask_array_type) for array in input_data):\\n590.         if dask == "forbidden":\\n591.             raise ValueError(\\n592.                 "apply_ufunc encountered a dask array on an "\\n593.                 "argument, but handling for dask arrays has not "\\n594.                 "been enabled. Either set the ``dask`` argument "\\n595.                 "or load your data into memory first with "\\n596.                 "``.load()`` or ``.compute()``"\\n597.             )\\n598.         elif dask == "parallelized":\\n599.             numpy_func = func\\n600. \\n601.             if dask_gufunc_kwargs is None:\\n602.                 dask_gufunc_kwargs = {}\\n603. \\n604.             output_sizes = dask_gufunc_kwargs.pop("output_sizes", {})\\n605.             if output_sizes:\\n606.                 output_sizes_renamed = {}\\n607.                 for key, value in output_sizes.items():\\n608.                     if key not in signature.all_output_core_dims:\\n609.                         raise ValueError(\\n610.                             f"dimension \\\'{key}\\\' in \\\'output_sizes\\\' must correspond to output_core_dims"\\n611.                         )\\n612.                     output_sizes_renamed[signature.dims_map[key]] = value\\n613.                 dask_gufunc_kwargs["output_sizes"] = output_sizes_renamed\\n614. \\n615.             for key in signature.all_output_core_dims:\\n616.                 if key not in signature.all_input_core_dims and key not in output_sizes:\\n617.                     raise ValueError(\\n618.                         f"dimension \\\'{key}\\\' in \\\'output_core_dims\\\' needs corresponding (dim, size) in \\\'output_sizes\\\'"\\n619.                     )\\n620. \\n621.             def func(*arrays):\\n622.                 import dask.array as da\\n623. \\n624.                 res = da.apply_gufunc(\\n625.                     numpy_func,\\n626.                     signature.to_gufunc_string(),\\n627.                     *arrays,\\n628.                     vectorize=vectorize,\\n629.                     output_dtypes=output_dtypes,\\n630.                     **dask_gufunc_kwargs,\\n631.                 )\\n632. \\n633.                 # todo: covers for https://github.com/dask/dask/pull/6207\\n634.                 #  remove when minimal dask version >= 2.17.0\\n635.                 from dask import __version__ as dask_version\\n636. \\n637.                 if LooseVersion(dask_version) < LooseVersion("2.17.0"):\\n638.                     if signature.num_outputs > 1:\\n639.                         res = tuple(res)\\n640. \\n641.                 return res\\n642. \\n643.         elif dask == "allowed":\\n644.             pass\\n645.         else:\\n646.             raise ValueError(\\n647.                 "unknown setting for dask array handling in "\\n648.                 "apply_ufunc: {}".format(dask)\\n649.             )\\n650.     else:\\n651.         if vectorize:\\n652.             func = _vectorize(func, signature, output_dtypes=output_dtypes)\\n653. \\n654.     result_data = func(*input_data)\\n655. \\n656.     if signature.num_outputs == 1:\\n657.         result_data = (result_data,)\\n658.     elif (\\n659.         not isinstance(result_data, tuple) or len(result_data) != signature.num_outputs\\n660.     ):\\n661.         raise ValueError(\\n662.             "applied function does not have the number of "\\n663.             "outputs specified in the ufunc signature. "\\n664.             "Result is not a tuple of {} elements: {!r}".format(\\n665.                 signature.num_outputs, result_data\\n666.             )\\n667.         )\\n668. \\n669.     output = []\\n670.     for dims, data in zip(output_dims, result_data):\\n671.         data = as_compatible_data(data)\\n672.         if data.ndim != len(dims):\\n673.             raise ValueError(\\n674.                 "applied function returned data with unexpected "\\n675.                 f"number of dimensions. Received {data.ndim} dimension(s) but "\\n676.                 f"expected {len(dims)} dimensions with names: {dims!r}"\\n677.             )\\n678. \\n679.         var = Variable(dims, data, fastpath=True)\\n680.         for dim, new_size in var.sizes.items():\\n681.             if dim in dim_sizes and new_size != dim_sizes[dim]:\\n682.                 raise ValueError(\\n683.                     "size of dimension {!r} on inputs was unexpectedly "\\n684.                     "changed by applied function from {} to {}. Only "\\n685.                     "dimensions specified in ``exclude_dims`` with "\\n686.                     "xarray.apply_ufunc are allowed to change size.".format(\\n687.                         dim, dim_sizes[dim], new_size\\n688.                     )\\n689.                 )\\n690. \\n691.         if keep_attrs and isinstance(args[0], Variable):\\n692.             var.attrs.update(args[0].attrs)\\n693.         output.append(var)\\n694. \\n695.     if signature.num_outputs == 1:\\n696.         return output[0]\\n697.     else:\\n698.         return tuple(output)\\n699. \\n700. \\n701. def apply_array_ufunc(func, *args, dask="forbidden"):\\n702.     """Apply a ndarray level function over ndarray objects."""\\n703.     if any(isinstance(arg, dask_array_type) for arg in args):\\n704.         if dask == "forbidden":\\n705.             raise ValueError(\\n706.                 "apply_ufunc encountered a dask array on an "\\n707.                 "argument, but handling for dask arrays has not "\\n708.                 "been enabled. Either set the ``dask`` argument "\\n709.                 "or load your data into memory first with "\\n710.                 "``.load()`` or ``.compute()``"\\n711.             )\\n712.         elif dask == "parallelized":\\n713.             raise ValueError(\\n714.                 "cannot use dask=\\\'parallelized\\\' for apply_ufunc "\\n715.                 "unless at least one input is an xarray object"\\n716.             )\\n717.         elif dask == "allowed":\\n718.             pass\\n719.         else:\\n720.             raise ValueError(f"unknown setting for dask array handling: {dask}")\\n721.     return func(*args)\\n722. \\n723. \\n724. def apply_ufunc(\\n725.     func: Callable,\\n726.     *args: Any,\\n727.     input_core_dims: Sequence[Sequence] = None,\\n728.     output_core_dims: Optional[Sequence[Sequence]] = ((),),\\n729.     exclude_dims: AbstractSet = frozenset(),\\n730.     vectorize: bool = False,\\n731.     join: str = "exact",\\n732.     dataset_join: str = "exact",\\n733.     dataset_fill_value: object = _NO_FILL_VALUE,\\n734.     keep_attrs: bool = False,\\n735.     kwargs: Mapping = None,\\n736.     dask: str = "forbidden",\\n737.     output_dtypes: Sequence = None,\\n738.     output_sizes: Mapping[Any, int] = None,\\n739.     meta: Any = None,\\n740.     dask_gufunc_kwargs: Dict[str, Any] = None,\\n741. ) -> Any:\\n742.     """Apply a vectorized function for unlabeled arrays on xarray objects.\\n743. \\n744.     The function will be mapped over the data variable(s) of the input\\n745.     arguments using xarray\\\'s standard rules for labeled computation, including\\n746.     alignment, broadcasting, looping over GroupBy/Dataset variables, and\\n747.     merging of coordinates.\\n748. \\n749.     Parameters\\n750.     ----------\\n751.     func : callable\\n752.         Function to call like ``func(*args, **kwargs)`` on unlabeled arrays\\n753.         (``.data``) that returns an array or tuple of arrays. If multiple\\n754.         arguments with non-matching dimensions are supplied, this function is\\n755.         expected to vectorize (broadcast) over axes of positional arguments in\\n756.         the style of NumPy universal functions [1]_ (if this is not the case,\\n757.         set ``vectorize=True``). If this function returns multiple outputs, you\\n758.         must set ``output_core_dims`` as well.\\n759.     *args : Dataset, DataArray, GroupBy, Variable, numpy.ndarray, dask.array.Array or scalar\\n760.         Mix of labeled and/or unlabeled arrays to which to apply the function.\\n761.     input_core_dims : sequence of sequence, optional\\n762.         List of the same length as ``args`` giving the list of core dimensions\\n763.         on each input argument that should not be broadcast. By default, we\\n764.         assume there are no core dimensions on any input arguments.\\n765. \\n766.         For example, ``input_core_dims=[[], [\\\'time\\\']]`` indicates that all\\n767.         dimensions on the first argument and all dimensions other than \\\'time\\\'\\n768.         on the second argument should be broadcast.\\n769. \\n770.         Core dimensions are automatically moved to the last axes of input\\n771.         variables before applying ``func``, which facilitates using NumPy style\\n772.         generalized ufuncs [2]_.\\n773.     output_core_dims : list of tuple, optional\\n774.         List of the same length as the number of output arguments from\\n775.         ``func``, giving the list of core dimensions on each output that were\\n776.         not broadcast on the inputs. By default, we assume that ``func``\\n777.         outputs exactly one array, with axes corresponding to each broadcast\\n778.         dimension.\\n779. \\n780.         Core dimensions are assumed to appear as the last dimensions of each\\n781.         output in the provided order.\\n782.     exclude_dims : set, optional\\n783.         Core dimensions on the inputs to exclude from alignment and\\n784.         broadcasting entirely. Any input coordinates along these dimensions\\n785.         will be dropped. Each excluded dimension must also appear in\\n786.         ``input_core_dims`` for at least one argument. Only dimensions listed\\n787.         here are allowed to change size between input and output objects.\\n788.     vectorize : bool, optional\\n789.         If True, then assume ``func`` only takes arrays defined over core\\n790.         dimensions as input and vectorize it automatically with\\n791.         :py:func:`numpy.vectorize`. This option exists for convenience, but is\\n792.         almost always slower than supplying a pre-vectorized function.\\n793.         Using this option requires NumPy version 1.12 or newer.\\n794.     join : {"outer", "inner", "left", "right", "exact"}, default: "exact"\\n795.         Method for joining the indexes of the passed objects along each\\n796.         dimension, and the variables of Dataset objects with mismatched\\n797.         data variables:\\n798. \\n799.         - \\\'outer\\\': use the union of object indexes\\n800.         - \\\'inner\\\': use the intersection of object indexes\\n801.         - \\\'left\\\': use indexes from the first object with each dimension\\n802.         - \\\'right\\\': use indexes from the last object with each dimension\\n803.         - \\\'exact\\\': raise `ValueError` instead of aligning when indexes to be\\n804.           aligned are not equal\\n805.     dataset_join : {"outer", "inner", "left", "right", "exact"}, default: "exact"\\n806.         Method for joining variables of Dataset objects with mismatched\\n807.         data variables.\\n808. \\n809.         - \\\'outer\\\': take variables from both Dataset objects\\n810.         - \\\'inner\\\': take only overlapped variables\\n811.         - \\\'left\\\': take only variables from the first object\\n812.         - \\\'right\\\': take only variables from the last object\\n813.         - \\\'exact\\\': data variables on all Dataset objects must match exactly\\n814.     dataset_fill_value : optional\\n815.         Value used in place of missing variables on Dataset inputs when the\\n816.         datasets do not share the exact same ``data_vars``. Required if\\n817.         ``dataset_join not in {\\\'inner\\\', \\\'exact\\\'}``, otherwise ignored.\\n818.     keep_attrs: bool, optional\\n819.         Whether to copy attributes from the first argument to the output.\\n820.     kwargs: dict, optional\\n821.         Optional keyword arguments passed directly on to call ``func``.\\n822.     dask: {"forbidden", "allowed", "parallelized"}, default: "forbidden"\\n823.         How to handle applying to objects containing lazy data in the form of\\n824.         dask arrays:\\n825. \\n826.         - \\\'forbidden\\\' (default): raise an error if a dask array is encountered.\\n827.         - \\\'allowed\\\': pass dask arrays directly on to ``func``. Prefer this option if\\n828.           ``func`` natively supports dask arrays.\\n829.         - \\\'parallelized\\\': automatically parallelize ``func`` if any of the\\n830.           inputs are a dask array by using `dask.array.apply_gufunc`. Multiple output\\n831.           arguments are supported. Only use this option if ``func`` does not natively\\n832.           support dask arrays (e.g. converts them to numpy arrays).\\n833.     dask_gufunc_kwargs : dict, optional\\n834.         Optional keyword arguments passed to ``dask.array.apply_gufunc`` if\\n835.         dask=\\\'parallelized\\\'. Possible keywords are ``output_sizes``, ``allow_rechunk``\\n836.         and ``meta``.\\n837.     output_dtypes : list of dtype, optional\\n838.         Optional list of output dtypes. Only used if ``dask=\\\'parallelized\\\'`` or\\n839.         vectorize=True.\\n840.     output_sizes : dict, optional\\n841.         Optional mapping from dimension names to sizes for outputs. Only used\\n842.         if dask=\\\'parallelized\\\' and new dimensions (not found on inputs) appear\\n843.         on outputs. ``output_sizes`` should be given in the ``dask_gufunc_kwargs``\\n844.         parameter. It will be removed as direct parameter in a future version.\\n845.     meta : optional\\n846.         Size-0 object representing the type of array wrapped by dask array. Passed on to\\n847.         ``dask.array.apply_gufunc``. ``meta`` should be given in the\\n848.         ``dask_gufunc_kwargs`` parameter . It will be removed as direct parameter\\n849.         a future version.\\n850. \\n851.     Returns\\n852.     -------\\n853.     Single value or tuple of Dataset, DataArray, Variable, dask.array.Array or\\n854.     numpy.ndarray, the first type on that list to appear on an input.\\n855. \\n856.     Examples\\n857.     --------\\n858. \\n859.     Calculate the vector magnitude of two arguments:\\n860. \\n861.     >>> def magnitude(a, b):\\n862.     ...     func = lambda x, y: np.sqrt(x ** 2 + y ** 2)\\n863.     ...     return xr.apply_ufunc(func, a, b)\\n864. \\n865.     You can now apply ``magnitude()`` to ``xr.DataArray`` and ``xr.Dataset``\\n866.     objects, with automatically preserved dimensions and coordinates, e.g.,\\n867. \\n868.     >>> array = xr.DataArray([1, 2, 3], coords=[("x", [0.1, 0.2, 0.3])])\\n869.     >>> magnitude(array, -array)\\n870.     <xarray.DataArray (x: 3)>\\n871.     array([1.414214, 2.828427, 4.242641])\\n872.     Coordinates:\\n873.       * x        (x) float64 0.1 0.2 0.3\\n874. \\n875.     Plain scalars, numpy arrays and a mix of these with xarray objects is also\\n876.     supported:\\n877. \\n878.     >>> magnitude(3, 4)\\n879.     5.0\\n880.     >>> magnitude(3, np.array([0, 4]))\\n881.     array([3., 5.])\\n882.     >>> magnitude(array, 0)\\n883.     <xarray.DataArray (x: 3)>\\n884.     array([1., 2., 3.])\\n885.     Coordinates:\\n886.       * x        (x) float64 0.1 0.2 0.3\\n887. \\n888.     Other examples of how you could use ``apply_ufunc`` to write functions to\\n889.     (very nearly) replicate existing xarray functionality:\\n890. \\n891.     Compute the mean (``.mean``) over one dimension::\\n892. \\n893.         def mean(obj, dim):\\n894.             # note: apply always moves core dimensions to the end\\n895.             return apply_ufunc(np.mean, obj,\\n896.                                input_core_dims=[[dim]],\\n897.                                kwargs={\\\'axis\\\': -1})\\n898. \\n899.     Inner product over a specific dimension (like ``xr.dot``)::\\n900. \\n901.         def _inner(x, y):\\n902.             result = np.matmul(x[..., np.newaxis, :], y[..., :, np.newaxis])\\n903.             return result[..., 0, 0]\\n904. \\n905.         def inner_product(a, b, dim):\\n906.             return apply_ufunc(_inner, a, b, input_core_dims=[[dim], [dim]])\\n907. \\n908.     Stack objects along a new dimension (like ``xr.concat``)::\\n909. \\n910.         def stack(objects, dim, new_coord):\\n911.             # note: this version does not stack coordinates\\n912.             func = lambda *x: np.stack(x, axis=-1)\\n913.             result = apply_ufunc(func, *objects,\\n914.                                  output_core_dims=[[dim]],\\n915.                                  join=\\\'outer\\\',\\n916.                                  dataset_fill_value=np.nan)\\n917.             result[dim] = new_coord\\n918.             return result\\n919. \\n920.     If your function is not vectorized but can be applied only to core\\n921.     dimensions, you can use ``vectorize=True`` to turn into a vectorized\\n922.     function. This wraps :py:func:`numpy.vectorize`, so the operation isn\\\'t\\n923.     terribly fast. Here we\\\'ll use it to calculate the distance between\\n924.     empirical samples from two probability distributions, using a scipy\\n925.     function that needs to be applied to vectors::\\n926. \\n927.         import scipy.stats\\n928. \\n929.         def earth_mover_distance(first_samples,\\n930.                                  second_samples,\\n931.                                  dim=\\\'ensemble\\\'):\\n932.             return apply_ufunc(scipy.stats.wasserstein_distance,\\n933.                                first_samples, second_samples,\\n934.                                input_core_dims=[[dim], [dim]],\\n935.                                vectorize=True)\\n936. \\n937.     Most of NumPy\\\'s builtin functions already broadcast their inputs\\n938.     appropriately for use in `apply`. You may find helper functions such as\\n939.     numpy.broadcast_arrays helpful in writing your function. `apply_ufunc` also\\n940.     works well with numba\\\'s vectorize and guvectorize. Further explanation with\\n941.     examples are provided in the xarray documentation [3]_.\\n942. \\n943.     See also\\n944.     --------\\n945.     numpy.broadcast_arrays\\n946.     numba.vectorize\\n947.     numba.guvectorize\\n948. \\n949.     References\\n950.     ----------\\n951.     .. [1] http://docs.scipy.org/doc/numpy/reference/ufuncs.html\\n952.     .. [2] http://docs.scipy.org/doc/numpy/reference/c-api.generalized-ufuncs.html\\n953.     .. [3] http://xarray.pydata.org/en/stable/computation.html#wrapping-custom-computation\\n954.     """\\n955.     from .dataarray import DataArray\\n956.     from .groupby import GroupBy\\n957.     from .variable import Variable\\n958. \\n959.     if input_core_dims is None:\\n960.         input_core_dims = ((),) * (len(args))\\n961.     elif len(input_core_dims) != len(args):\\n962.         raise ValueError(\\n963.             f"input_core_dims must be None or a tuple with the length same to "\\n964.             f"the number of arguments. "\\n965.             f"Given {len(input_core_dims)} input_core_dims: {input_core_dims}, "\\n966.             f" but number of args is {len(args)}."\\n967.         )\\n968. \\n969.     if kwargs is None:\\n970.         kwargs = {}\\n971. \\n972.     signature = _UFuncSignature(input_core_dims, output_core_dims)\\n973. \\n974.     if exclude_dims:\\n975.         if not isinstance(exclude_dims, set):\\n976.             raise TypeError(\\n977.                 f"Expected exclude_dims to be a \\\'set\\\'. Received \\\'{type(exclude_dims).__name__}\\\' instead."\\n978.             )\\n979.         if not exclude_dims <= signature.all_core_dims:\\n980.             raise ValueError(\\n981.                 f"each dimension in `exclude_dims` must also be a "\\n982.                 f"core dimension in the function signature. "\\n983.                 f"Please make {(exclude_dims - signature.all_core_dims)} a core dimension"\\n984.             )\\n985. \\n986.     # handle dask_gufunc_kwargs\\n987.     if dask == "parallelized":\\n988.         if dask_gufunc_kwargs is None:\\n989.             dask_gufunc_kwargs = {}\\n990.         # todo: remove warnings after deprecation cycle\\n991.         if meta is not None:\\n992.             warnings.warn(\\n993.                 "``meta`` should be given in the ``dask_gufunc_kwargs`` parameter."\\n994.                 " It will be removed as direct parameter in a future version.",\\n995.                 FutureWarning,\\n996.                 stacklevel=2,\\n997.             )\\n998.             dask_gufunc_kwargs.setdefault("meta", meta)\\n999.         if output_sizes is not None:\\n1000.             warnings.warn(\\n1001.                 "``output_sizes`` should be given in the ``dask_gufunc_kwargs`` "\\n1002.                 "parameter. It will be removed as direct parameter in a future "\\n1003.                 "version.",\\n1004.                 FutureWarning,\\n1005.                 stacklevel=2,\\n1006.             )\\n1007.             dask_gufunc_kwargs.setdefault("output_sizes", output_sizes)\\n1008. \\n1009.     if kwargs:\\n1010.         func = functools.partial(func, **kwargs)\\n1011. \\n1012.     variables_vfunc = functools.partial(\\n1013.         apply_variable_ufunc,\\n1014.         func,\\n1015.         signature=signature,\\n1016.         exclude_dims=exclude_dims,\\n1017.         keep_attrs=keep_attrs,\\n1018.         dask=dask,\\n1019.         vectorize=vectorize,\\n1020.         output_dtypes=output_dtypes,\\n1021.         dask_gufunc_kwargs=dask_gufunc_kwargs,\\n1022.     )\\n1023. \\n1024.     # feed groupby-apply_ufunc through apply_groupby_func\\n1025.     if any(isinstance(a, GroupBy) for a in args):\\n1026.         this_apply = functools.partial(\\n1027.             apply_ufunc,\\n1028.             func,\\n1029.             input_core_dims=input_core_dims,\\n1030.             output_core_dims=output_core_dims,\\n1031.             exclude_dims=exclude_dims,\\n1032.             join=join,\\n1033.             dataset_join=dataset_join,\\n1034.             dataset_fill_value=dataset_fill_value,\\n1035.             keep_attrs=keep_attrs,\\n1036.             dask=dask,\\n1037.             vectorize=vectorize,\\n1038.             output_dtypes=output_dtypes,\\n1039.             dask_gufunc_kwargs=dask_gufunc_kwargs,\\n1040.         )\\n1041.         return apply_groupby_func(this_apply, *args)\\n1042.     # feed datasets apply_variable_ufunc through apply_dataset_vfunc\\n1043.     elif any(is_dict_like(a) for a in args):\\n1044.         return apply_dataset_vfunc(\\n1045.             variables_vfunc,\\n1046.             *args,\\n1047.             signature=signature,\\n1048.             join=join,\\n1049.             exclude_dims=exclude_dims,\\n1050.             dataset_join=dataset_join,\\n1051.             fill_value=dataset_fill_value,\\n1052.             keep_attrs=keep_attrs,\\n1053.         )\\n1054.     # feed DataArray apply_variable_ufunc through apply_dataarray_vfunc\\n1055.     elif any(isinstance(a, DataArray) for a in args):\\n1056.         return apply_dataarray_vfunc(\\n1057.             variables_vfunc,\\n1058.             *args,\\n1059.             signature=signature,\\n1060.             join=join,\\n1061.             exclude_dims=exclude_dims,\\n1062.             keep_attrs=keep_attrs,\\n1063.         )\\n1064.     # feed Variables directly through apply_variable_ufunc\\n1065.     elif any(isinstance(a, Variable) for a in args):\\n1066.         return variables_vfunc(*args)\\n1067.     else:\\n1068.         # feed anything else through apply_array_ufunc\\n1069.         return apply_array_ufunc(func, *args, dask=dask)\\n1070. \\n1071. \\n1072. def cov(da_a, da_b, dim=None, ddof=1):\\n1073.     """\\n1074.     Compute covariance between two DataArray objects along a shared dimension.\\n1075. \\n1076.     Parameters\\n1077.     ----------\\n1078.     da_a: DataArray\\n1079.         Array to compute.\\n1080.     da_b: DataArray\\n1081.         Array to compute.\\n1082.     dim : str, optional\\n1083.         The dimension along which the covariance will be computed\\n1084.     ddof: int, optional\\n1085.         If ddof=1, covariance is normalized by N-1, giving an unbiased estimate,\\n1086.         else normalization is by N.\\n1087. \\n1088.     Returns\\n1089.     -------\\n1090.     covariance: DataArray\\n1091. \\n1092.     See also\\n1093.     --------\\n1094.     pandas.Series.cov: corresponding pandas function\\n1095.     xr.corr: respective function to calculate correlation\\n1096. \\n1097.     Examples\\n1098.     --------\\n1099.     >>> da_a = DataArray(\\n1100.     ...     np.array([[1, 2, 3], [0.1, 0.2, 0.3], [3.2, 0.6, 1.8]]),\\n1101.     ...     dims=("space", "time"),\\n1102.     ...     coords=[\\n1103.     ...         ("space", ["IA", "IL", "IN"]),\\n1104.     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),\\n1105.     ...     ],\\n1106.     ... )\\n1107.     >>> da_a\\n1108.     <xarray.DataArray (space: 3, time: 3)>\\n1109.     array([[1. , 2. , 3. ],\\n1110.            [0.1, 0.2, 0.3],\\n1111.            [3.2, 0.6, 1.8]])\\n1112.     Coordinates:\\n1113.       * space    (space) <U2 \\\'IA\\\' \\\'IL\\\' \\\'IN\\\'\\n1114.       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03\\n1115.     >>> da_b = DataArray(\\n1116.     ...     np.array([[0.2, 0.4, 0.6], [15, 10, 5], [3.2, 0.6, 1.8]]),\\n1117.     ...     dims=("space", "time"),\\n1118.     ...     coords=[\\n1119.     ...         ("space", ["IA", "IL", "IN"]),\\n1120.     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),\\n1121.     ...     ],\\n1122.     ... )\\n1123.     >>> da_b\\n1124.     <xarray.DataArray (space: 3, time: 3)>\\n1125.     array([[ 0.2,  0.4,  0.6],\\n1126.            [15. , 10. ,  5. ],\\n1127.            [ 3.2,  0.6,  1.8]])\\n1128.     Coordinates:\\n1129.       * space    (space) <U2 \\\'IA\\\' \\\'IL\\\' \\\'IN\\\'\\n1130.       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03\\n1131.     >>> xr.cov(da_a, da_b)\\n1132.     <xarray.DataArray ()>\\n1133.     array(-3.53055556)\\n1134.     >>> xr.cov(da_a, da_b, dim="time")\\n1135.     <xarray.DataArray (space: 3)>\\n1136.     array([ 0.2, -0.5,  1.69333333])\\n1137.     Coordinates:\\n1138.       * space    (space) <U2 \\\'IA\\\' \\\'IL\\\' \\\'IN\\\'\\n1139.     """\\n1140.     from .dataarray import DataArray\\n1141. \\n1142.     if any(not isinstance(arr, DataArray) for arr in [da_a, da_b]):\\n1143.         raise TypeError(\\n1144.             "Only xr.DataArray is supported."\\n1145.             "Given {}.".format([type(arr) for arr in [da_a, da_b]])\\n1146.         )\\n1147. \\n1148.     return _cov_corr(da_a, da_b, dim=dim, ddof=ddof, method="cov")\\n1149. \\n1150. \\n1151. def corr(da_a, da_b, dim=None):\\n1152.     """\\n1153.     Compute the Pearson correlation coefficient between\\n1154.     two DataArray objects along a shared dimension.\\n1155. \\n1156.     Parameters\\n1157.     ----------\\n1158.     da_a: DataArray\\n1159.         Array to compute.\\n1160.     da_b: DataArray\\n1161.         Array to compute.\\n1162.     dim: str, optional\\n1163.         The dimension along which the correlation will be computed\\n1164. \\n1165.     Returns\\n1166.     -------\\n1167.     correlation: DataArray\\n1168. \\n1169.     See also\\n1170.     --------\\n1171.     pandas.Series.corr: corresponding pandas function\\n1172.     xr.cov: underlying covariance function\\n1173. \\n1174.     Examples\\n1175.     --------\\n1176.     >>> da_a = DataArray(\\n1177.     ...     np.array([[1, 2, 3], [0.1, 0.2, 0.3], [3.2, 0.6, 1.8]]),\\n1178.     ...     dims=("space", "time"),\\n1179.     ...     coords=[\\n1180.     ...         ("space", ["IA", "IL", "IN"]),\\n1181.     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),\\n1182.     ...     ],\\n1183.     ... )\\n1184.     >>> da_a\\n1185.     <xarray.DataArray (space: 3, time: 3)>\\n1186.     array([[1. , 2. , 3. ],\\n1187.            [0.1, 0.2, 0.3],\\n1188.            [3.2, 0.6, 1.8]])\\n1189.     Coordinates:\\n1190.       * space    (space) <U2 \\\'IA\\\' \\\'IL\\\' \\\'IN\\\'\\n1191.       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03\\n1192.     >>> da_b = DataArray(\\n1193.     ...     np.array([[0.2, 0.4, 0.6], [15, 10, 5], [3.2, 0.6, 1.8]]),\\n1194.     ...     dims=("space", "time"),\\n1195.     ...     coords=[\\n1196.     ...         ("space", ["IA", "IL", "IN"]),\\n1197.     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),\\n1198.     ...     ],\\n1199.     ... )\\n1200.     >>> da_b\\n1201.     <xarray.DataArray (space: 3, time: 3)>\\n1202.     array([[ 0.2,  0.4,  0.6],\\n1203.            [15. , 10. ,  5. ],\\n1204.            [ 3.2,  0.6,  1.8]])\\n1205.     Coordinates:\\n1206.       * space    (space) <U2 \\\'IA\\\' \\\'IL\\\' \\\'IN\\\'\\n1207.       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03\\n1208.     >>> xr.corr(da_a, da_b)\\n1209.     <xarray.DataArray ()>\\n1210.     array(-0.57087777)\\n1211.     >>> xr.corr(da_a, da_b, dim="time")\\n1212.     <xarray.DataArray (space: 3)>\\n1213.     array([ 1., -1.,  1.])\\n1214.     Coordinates:\\n1215.       * space    (space) <U2 \\\'IA\\\' \\\'IL\\\' \\\'IN\\\'\\n1216.     """\\n1217.     from .dataarray import DataArray\\n1218. \\n1219.     if any(not isinstance(arr, DataArray) for arr in [da_a, da_b]):\\n1220.         raise TypeError(\\n1221.             "Only xr.DataArray is supported."\\n1222.             "Given {}.".format([type(arr) for arr in [da_a, da_b]])\\n1223.         )\\n1224. \\n1225.     return _cov_corr(da_a, da_b, dim=dim, method="corr")\\n1226. \\n1227. \\n1228. def _cov_corr(da_a, da_b, dim=None, ddof=0, method=None):\\n1229.     """\\n1230.     Internal method for xr.cov() and xr.corr() so only have to\\n1231.     sanitize the input arrays once and we don\\\'t repeat code.\\n1232.     """\\n1233.     # 1. Broadcast the two arrays\\n1234.     da_a, da_b = align(da_a, da_b, join="inner", copy=False)\\n1235. \\n1236.     # 2. Ignore the nans\\n1237.     valid_values = da_a.notnull() & da_b.notnull()\\n1238. \\n1239.     if not valid_values.all():\\n1240.         da_a = da_a.where(valid_values)\\n1241.         da_b = da_b.where(valid_values)\\n1242. \\n1243.     valid_count = valid_values.sum(dim) - ddof\\n1244. \\n1245.     # 3. Detrend along the given dim\\n1246.     demeaned_da_a = da_a - da_a.mean(dim=dim)\\n1247.     demeaned_da_b = da_b - da_b.mean(dim=dim)\\n1248. \\n1249.     # 4. Compute covariance along the given dim\\n1250.     # N.B. `skipna=False` is required or there is a bug when computing\\n1251.     # auto-covariance. E.g. Try xr.cov(da,da) for\\n1252.     # da = xr.DataArray([[1, 2], [1, np.nan]], dims=["x", "time"])\\n1253.     cov = (demeaned_da_a * demeaned_da_b).sum(dim=dim, skipna=False) / (valid_count)\\n1254. \\n1255.     if method == "cov":\\n1256.         return cov\\n1257. \\n1258.     else:\\n1259.         # compute std + corr\\n1260.         da_a_std = da_a.std(dim=dim)\\n1261.         da_b_std = da_b.std(dim=dim)\\n1262.         corr = cov / (da_a_std * da_b_std)\\n1263.         return corr\\n1264. \\n1265. \\n1266. def dot(*arrays, dims=None, **kwargs):\\n1267.     """Generalized dot product for xarray objects. Like np.einsum, but\\n1268.     provides a simpler interface based on array dimensions.\\n1269. \\n1270.     Parameters\\n1271.     ----------\\n1272.     arrays : DataArray or Variable\\n1273.         Arrays to compute.\\n1274.     dims : ..., str or tuple of str, optional\\n1275.         Which dimensions to sum over. Ellipsis (\\\'...\\\') sums over all dimensions.\\n1276.         If not specified, then all the common dimensions are summed over.\\n1277.     **kwargs : dict\\n1278.         Additional keyword arguments passed to numpy.einsum or\\n1279.         dask.array.einsum\\n1280. \\n1281.     Returns\\n1282.     -------\\n1283.     DataArray\\n1284. \\n1285.     Examples\\n1286.     --------\\n1287. \\n1288.     >>> import numpy as np\\n1289.     >>> import xarray as xr\\n1290.     >>> da_a = xr.DataArray(np.arange(3 * 2).reshape(3, 2), dims=["a", "b"])\\n1291.     >>> da_b = xr.DataArray(np.arange(3 * 2 * 2).reshape(3, 2, 2), dims=["a", "b", "c"])\\n1292.     >>> da_c = xr.DataArray(np.arange(2 * 3).reshape(2, 3), dims=["c", "d"])\\n1293. \\n1294.     >>> da_a\\n1295.     <xarray.DataArray (a: 3, b: 2)>\\n1296.     array([[0, 1],\\n1297.            [2, 3],\\n1298.            [4, 5]])\\n1299.     Dimensions without coordinates: a, b\\n1300. \\n1301.     >>> da_b\\n1302.     <xarray.DataArray (a: 3, b: 2, c: 2)>\\n1303.     array([[[ 0,  1],\\n1304.             [ 2,  3]],\\n1305.            [[ 4,  5],\\n1306.             [ 6,  7]],\\n1307.            [[ 8,  9],\\n1308.             [10, 11]]])\\n1309.     Dimensions without coordinates: a, b, c\\n1310. \\n1311.     >>> da_c\\n1312.     <xarray.DataArray (c: 2, d: 3)>\\n1313.     array([[0, 1, 2],\\n1314.            [3, 4, 5]])\\n1315.     Dimensions without coordinates: c, d\\n1316. \\n1317.     >>> xr.dot(da_a, da_b, dims=["a", "b"])\\n1318.     <xarray.DataArray (c: 2)>\\n1319.     array([110, 125])\\n1320.     Dimensions without coordinates: c\\n1321. \\n1322.     >>> xr.dot(da_a, da_b, dims=["a"])\\n1323.     <xarray.DataArray (b: 2, c: 2)>\\n1324.     array([[40, 46],\\n1325.            [70, 79]])\\n1326.     Dimensions without coordinates: b, c\\n1327. \\n1328.     >>> xr.dot(da_a, da_b, da_c, dims=["b", "c"])\\n1329.     <xarray.DataArray (a: 3, d: 3)>\\n1330.     array([[  9,  14,  19],\\n1331.            [ 93, 150, 207],\\n1332.            [273, 446, 619]])\\n1333.     Dimensions without coordinates: a, d\\n1334. \\n1335.     >>> xr.dot(da_a, da_b)\\n1336.     <xarray.DataArray (c: 2)>\\n1337.     array([110, 125])\\n1338.     Dimensions without coordinates: c\\n1339. \\n1340.     >>> xr.dot(da_a, da_b, dims=...)\\n1341.     <xarray.DataArray ()>\\n1342.     array(235)\\n1343.     """\\n1344.     from .dataarray import DataArray\\n1345.     from .variable import Variable\\n1346. \\n1347.     if any(not isinstance(arr, (Variable, DataArray)) for arr in arrays):\\n1348.         raise TypeError(\\n1349.             "Only xr.DataArray and xr.Variable are supported."\\n1350.             "Given {}.".format([type(arr) for arr in arrays])\\n1351.         )\\n1352. \\n1353.     if len(arrays) == 0:\\n1354.         raise TypeError("At least one array should be given.")\\n1355. \\n1356.     if isinstance(dims, str):\\n1357.         dims = (dims,)\\n1358. \\n1359.     common_dims = set.intersection(*[set(arr.dims) for arr in arrays])\\n1360.     all_dims = []\\n1361.     for arr in arrays:\\n1362.         all_dims += [d for d in arr.dims if d not in all_dims]\\n1363. \\n1364.     einsum_axes = "abcdefghijklmnopqrstuvwxyz"\\n1365.     dim_map = {d: einsum_axes[i] for i, d in enumerate(all_dims)}\\n1366. \\n1367.     if dims is ...:\\n1368.         dims = all_dims\\n1369.     elif dims is None:\\n1370.         # find dimensions that occur more than one times\\n1371.         dim_counts = Counter()\\n1372.         for arr in arrays:\\n1373.             dim_counts.update(arr.dims)\\n1374.         dims = tuple(d for d, c in dim_counts.items() if c > 1)\\n1375. \\n1376.     dims = tuple(dims)  # make dims a tuple\\n1377. \\n1378.     # dimensions to be parallelized\\n1379.     broadcast_dims = tuple(d for d in all_dims if d in common_dims and d not in dims)\\n1380.     input_core_dims = [\\n1381.         [d for d in arr.dims if d not in broadcast_dims] for arr in arrays\\n1382.     ]\\n1383.     output_core_dims = [tuple(d for d in all_dims if d not in dims + broadcast_dims)]\\n1384. \\n1385.     # construct einsum subscripts, such as \\\'...abc,...ab->...c\\\'\\n1386.     # Note: input_core_dims are always moved to the last position\\n1387.     subscripts_list = [\\n1388.         "..." + "".join(dim_map[d] for d in ds) for ds in input_core_dims\\n1389.     ]\\n1390.     subscripts = ",".join(subscripts_list)\\n1391.     subscripts += "->..." + "".join(dim_map[d] for d in output_core_dims[0])\\n1392. \\n1393.     join = OPTIONS["arithmetic_join"]\\n1394.     # using "inner" emulates `(a * b).sum()` for all joins (except "exact")\\n1395.     if join != "exact":\\n1396.         join = "inner"\\n1397. \\n1398.     # subscripts should be passed to np.einsum as arg, not as kwargs. We need\\n1399.     # to construct a partial function for apply_ufunc to work.\\n1400.     func = functools.partial(duck_array_ops.einsum, subscripts, **kwargs)\\n1401.     result = apply_ufunc(\\n1402.         func,\\n1403.         *arrays,\\n1404.         input_core_dims=input_core_dims,\\n1405.         output_core_dims=output_core_dims,\\n1406.         join=join,\\n1407.         dask="allowed",\\n1408.     )\\n1409.     return result.transpose(*[d for d in all_dims if d in result.dims])\\n1410. \\n1411. \\n1412. def where(cond, x, y):\\n1413.     """Return elements from `x` or `y` depending on `cond`.\\n1414. \\n1415.     Performs xarray-like broadcasting across input arguments.\\n1416. \\n1417.     All dimension coordinates on `x` and `y`  must be aligned with each\\n1418.     other and with `cond`.\\n1419. \\n1420. \\n1421.     Parameters\\n1422.     ----------\\n1423.     cond : scalar, array, Variable, DataArray or Dataset\\n1424.         When True, return values from `x`, otherwise returns values from `y`.\\n1425.     x : scalar, array, Variable, DataArray or Dataset\\n1426.         values to choose from where `cond` is True\\n1427.     y : scalar, array, Variable, DataArray or Dataset\\n1428.         values to choose from where `cond` is False\\n1429. \\n1430.     Returns\\n1431.     -------\\n1432.     Dataset, DataArray, Variable or array\\n1433.         In priority order: Dataset, DataArray, Variable or array, whichever\\n1434.         type appears as an input argument.\\n1435. \\n1436.     Examples\\n1437.     --------\\n1438.     >>> import xarray as xr\\n1439.     >>> import numpy as np\\n1440.     >>> x = xr.DataArray(\\n1441.     ...     0.1 * np.arange(10),\\n1442.     ...     dims=["lat"],\\n1443.     ...     coords={"lat": np.arange(10)},\\n1444.     ...     name="sst",\\n1445.     ... )\\n1446.     >>> x\\n1447.     <xarray.DataArray \\\'sst\\\' (lat: 10)>\\n1448.     array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\\n1449.     Coordinates:\\n1450.     * lat      (lat) int64 0 1 2 3 4 5 6 7 8 9\\n1451. \\n1452.     >>> xr.where(x < 0.5, x, x * 100)\\n1453.     <xarray.DataArray \\\'sst\\\' (lat: 10)>\\n1454.     array([ 0. ,  0.1,  0.2,  0.3,  0.4, 50. , 60. , 70. , 80. , 90. ])\\n1455.     Coordinates:\\n1456.     * lat      (lat) int64 0 1 2 3 4 5 6 7 8 9\\n1457. \\n1458.     >>> y = xr.DataArray(\\n1459.     ...     0.1 * np.arange(9).reshape(3, 3),\\n1460.     ...     dims=["lat", "lon"],\\n1461.     ...     coords={"lat": np.arange(3), "lon": 10 + np.arange(3)},\\n1462.     ...     name="sst",\\n1463.     ... )\\n1464.     >>> y\\n1465.     <xarray.DataArray \\\'sst\\\' (lat: 3, lon: 3)>\\n1466.     array([[0. , 0.1, 0.2],\\n1467.            [0.3, 0.4, 0.5],\\n1468.            [0.6, 0.7, 0.8]])\\n1469.     Coordinates:\\n1470.     * lat      (lat) int64 0 1 2\\n1471.     * lon      (lon) int64 10 11 12\\n1472. \\n1473.     >>> xr.where(y.lat < 1, y, -1)\\n1474.     <xarray.DataArray (lat: 3, lon: 3)>\\n1475.     array([[ 0. ,  0.1,  0.2],\\n1476.            [-1. , -1. , -1. ],\\n1477.            [-1. , -1. , -1. ]])\\n1478.     Coordinates:\\n1479.     * lat      (lat) int64 0 1 2\\n1480.     * lon      (lon) int64 10 11 12\\n1481. \\n1482.     >>> cond = xr.DataArray([True, False], dims=["x"])\\n1483.     >>> x = xr.DataArray([1, 2], dims=["y"])\\n1484.     >>> xr.where(cond, x, 0)\\n1485.     <xarray.DataArray (x: 2, y: 2)>\\n1486.     array([[1, 2],\\n1487.            [0, 0]])\\n1488.     Dimensions without coordinates: x, y\\n1489. \\n1490.     See also\\n1491.     --------\\n1492.     numpy.where : corresponding numpy function\\n1493.     Dataset.where, DataArray.where : equivalent methods\', \'start_line_number\': 1, \'end_line_number\': 1493}\n\n{\'relative_path\': \'xarray/core/dataarray.py\', \'content\': \'1. import datetime\\n2. import functools\\n3. from numbers import Number\\n4. from typing import (\\n5.     TYPE_CHECKING,\\n6.     Any,\\n7.     Callable,\\n8.     Dict,\\n9.     Hashable,\\n10.     Iterable,\\n11.     List,\\n12.     Mapping,\\n13.     Optional,\\n14.     Sequence,\\n15.     Tuple,\\n16.     TypeVar,\\n17.     Union,\\n18.     cast,\\n19. )\\n20. \\n21. import numpy as np\\n22. import pandas as pd\\n23. \\n24. from ..plot.plot import _PlotMethods\\n25. from . import (\\n26.     computation,\\n27.     dtypes,\\n28.     groupby,\\n29.     indexing,\\n30.     ops,\\n31.     pdcompat,\\n32.     resample,\\n33.     rolling,\\n34.     utils,\\n35.     weighted,\\n36. )\\n37. from .accessor_dt import CombinedDatetimelikeAccessor\\n38. from .accessor_str import StringAccessor\\n39. from .alignment import (\\n40.     _broadcast_helper,\\n41.     _get_broadcast_dims_map_common_coords,\\n42.     align,\\n43.     reindex_like_indexers,\\n44. )\\n45. from .common import AbstractArray, DataWithCoords\\n46. from .coordinates import (\\n47.     DataArrayCoordinates,\\n48.     LevelCoordinatesSource,\\n49.     assert_coordinate_consistent,\\n50.     remap_label_indexers,\\n51. )\\n52. from .dataset import Dataset, split_indexes\\n53. from .formatting import format_item\\n54. from .indexes import Indexes, default_indexes, propagate_indexes\\n55. from .indexing import is_fancy_indexer\\n56. from .merge import PANDAS_TYPES, MergeError, _extract_indexes_from_coords\\n57. from .options import OPTIONS\\n58. from .utils import Default, ReprObject, _check_inplace, _default, either_dict_or_kwargs\\n59. from .variable import (\\n60.     IndexVariable,\\n61.     Variable,\\n62.     as_compatible_data,\\n63.     as_variable,\\n64.     assert_unique_multiindex_level_names,\\n65. )\\n66. \\n67. if TYPE_CHECKING:\\n68.     T_DSorDA = TypeVar("T_DSorDA", "DataArray", Dataset)\\n69. \\n70.     try:\\n71.         from dask.delayed import Delayed\\n72.     except ImportError:\\n73.         Delayed = None\\n74.     try:\\n75.         from cdms2 import Variable as cdms2_Variable\\n76.     except ImportError:\\n77.         cdms2_Variable = None\\n78.     try:\\n79.         from iris.cube import Cube as iris_Cube\\n80.     except ImportError:\\n81.         iris_Cube = None\\n82. \\n83. \\n84. def _infer_coords_and_dims(\\n85.     shape, coords, dims\\n86. ) -> "Tuple[Dict[Any, Variable], Tuple[Hashable, ...]]":\\n87.     """All the logic for creating a new DataArray"""\\n88. \\n89.     if (\\n90.         coords is not None\\n91.         and not utils.is_dict_like(coords)\\n92.         and len(coords) != len(shape)\\n93.     ):\\n94.         raise ValueError(\\n95.             "coords is not dict-like, but it has %s items, "\\n96.             "which does not match the %s dimensions of the "\\n97.             "data" % (len(coords), len(shape))\\n98.         )\\n99. \\n100.     if isinstance(dims, str):\\n101.         dims = (dims,)\\n102. \\n103.     if dims is None:\\n104.         dims = ["dim_%s" % n for n in range(len(shape))]\\n105.         if coords is not None and len(coords) == len(shape):\\n106.             # try to infer dimensions from coords\\n107.             if utils.is_dict_like(coords):\\n108.                 # deprecated in GH993, removed in GH1539\\n109.                 raise ValueError(\\n110.                     "inferring DataArray dimensions from "\\n111.                     "dictionary like ``coords`` is no longer "\\n112.                     "supported. Use an explicit list of "\\n113.                     "``dims`` instead."\\n114.                 )\\n115.             for n, (dim, coord) in enumerate(zip(dims, coords)):\\n116.                 coord = as_variable(coord, name=dims[n]).to_index_variable()\\n117.                 dims[n] = coord.name\\n118.         dims = tuple(dims)\\n119.     elif len(dims) != len(shape):\\n120.         raise ValueError(\\n121.             "different number of dimensions on data "\\n122.             "and dims: %s vs %s" % (len(shape), len(dims))\\n123.         )\\n124.     else:\\n125.         for d in dims:\\n126.             if not isinstance(d, str):\\n127.                 raise TypeError("dimension %s is not a string" % d)\\n128. \\n129.     new_coords: Dict[Any, Variable] = {}\\n130. \\n131.     if utils.is_dict_like(coords):\\n132.         for k, v in coords.items():\\n133.             new_coords[k] = as_variable(v, name=k)\\n134.     elif coords is not None:\\n135.         for dim, coord in zip(dims, coords):\\n136.             var = as_variable(coord, name=dim)\\n137.             var.dims = (dim,)\\n138.             new_coords[dim] = var.to_index_variable()\\n139. \\n140.     sizes = dict(zip(dims, shape))\\n141.     for k, v in new_coords.items():\\n142.         if any(d not in dims for d in v.dims):\\n143.             raise ValueError(\\n144.                 "coordinate %s has dimensions %s, but these "\\n145.                 "are not a subset of the DataArray "\\n146.                 "dimensions %s" % (k, v.dims, dims)\\n147.             )\\n148. \\n149.         for d, s in zip(v.dims, v.shape):\\n150.             if s != sizes[d]:\\n151.                 raise ValueError(\\n152.                     "conflicting sizes for dimension %r: "\\n153.                     "length %s on the data but length %s on "\\n154.                     "coordinate %r" % (d, sizes[d], s, k)\\n155.                 )\\n156. \\n157.         if k in sizes and v.shape != (sizes[k],):\\n158.             raise ValueError(\\n159.                 "coordinate %r is a DataArray dimension, but "\\n160.                 "it has shape %r rather than expected shape %r "\\n161.                 "matching the dimension size" % (k, v.shape, (sizes[k],))\\n162.             )\\n163. \\n164.     assert_unique_multiindex_level_names(new_coords)\\n165. \\n166.     return new_coords, dims\\n167. \\n168. \\n169. def _check_data_shape(data, coords, dims):\\n170.     if data is dtypes.NA:\\n171.         data = np.nan\\n172.     if coords is not None and utils.is_scalar(data, include_0d=False):\\n173.         if utils.is_dict_like(coords):\\n174.             if dims is None:\\n175.                 return data\\n176.             else:\\n177.                 data_shape = tuple(\\n178.                     as_variable(coords[k], k).size if k in coords.keys() else 1\\n179.                     for k in dims\\n180.                 )\\n181.         else:\\n182.             data_shape = tuple(as_variable(coord, "foo").size for coord in coords)\\n183.         data = np.full(data_shape, data)\\n184.     return data\\n185. \\n186. \\n187. class _LocIndexer:\\n188.     __slots__ = ("data_array",)\\n189. \\n190.     def __init__(self, data_array: "DataArray"):\\n191.         self.data_array = data_array\\n192. \\n193.     def __getitem__(self, key) -> "DataArray":\\n194.         if not utils.is_dict_like(key):\\n195.             # expand the indexer so we can handle Ellipsis\\n196.             labels = indexing.expanded_indexer(key, self.data_array.ndim)\\n197.             key = dict(zip(self.data_array.dims, labels))\\n198.         return self.data_array.sel(**key)\\n199. \\n200.     def __setitem__(self, key, value) -> None:\\n201.         if not utils.is_dict_like(key):\\n202.             # expand the indexer so we can handle Ellipsis\\n203.             labels = indexing.expanded_indexer(key, self.data_array.ndim)\\n204.             key = dict(zip(self.data_array.dims, labels))\\n205. \\n206.         pos_indexers, _ = remap_label_indexers(self.data_array, key)\\n207.         self.data_array[pos_indexers] = value\\n208. \\n209. \\n210. # Used as the key corresponding to a DataArray\\\'s variable when converting\\n211. # arbitrary DataArray objects to datasets\\n212. _THIS_ARRAY = ReprObject("<this-array>")\\n213. \\n214. \\n215. class DataArray(AbstractArray, DataWithCoords):\\n216.     """N-dimensional array with labeled coordinates and dimensions.\\n217. \\n218.     DataArray provides a wrapper around numpy ndarrays that uses labeled\\n219.     dimensions and coordinates to support metadata aware operations. The API is\\n220.     similar to that for the pandas Series or DataFrame, but DataArray objects\\n221.     can have any number of dimensions, and their contents have fixed data\\n222.     types.\\n223. \\n224.     Additional features over raw numpy arrays:\\n225. \\n226.     - Apply operations over dimensions by name: ``x.sum(\\\'time\\\')``.\\n227.     - Select or assign values by integer location (like numpy): ``x[:10]``\\n228.       or by label (like pandas): ``x.loc[\\\'2014-01-01\\\']`` or\\n229.       ``x.sel(time=\\\'2014-01-01\\\')``.\\n230.     - Mathematical operations (e.g., ``x - y``) vectorize across multiple\\n231.       dimensions (known in numpy as "broadcasting") based on dimension names,\\n232.       regardless of their original order.\\n233.     - Keep track of arbitrary metadata in the form of a Python dictionary:\\n234.       ``x.attrs``\\n235.     - Convert to a pandas Series: ``x.to_series()``.\\n236. \\n237.     Getting items from or doing mathematical operations with a DataArray\\n238.     always returns another DataArray.\\n239.     """\\n240. \\n241.     _cache: Dict[str, Any]\\n242.     _coords: Dict[Any, Variable]\\n243.     _indexes: Optional[Dict[Hashable, pd.Index]]\\n244.     _name: Optional[Hashable]\\n245.     _variable: Variable\\n246. \\n247.     __slots__ = (\\n248.         "_cache",\\n249.         "_coords",\\n250.         "_file_obj",\\n251.         "_indexes",\\n252.         "_name",\\n253.         "_variable",\\n254.         "__weakref__",\\n255.     )\\n256. \\n257.     _groupby_cls = groupby.DataArrayGroupBy\\n258.     _rolling_cls = rolling.DataArrayRolling\\n259.     _coarsen_cls = rolling.DataArrayCoarsen\\n260.     _resample_cls = resample.DataArrayResample\\n261.     _weighted_cls = weighted.DataArrayWeighted\\n262. \\n263.     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor)\\n264. \\n265.     def __init__(\\n266.         self,\\n267.         data: Any = dtypes.NA,\\n268.         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,\\n269.         dims: Union[Hashable, Sequence[Hashable], None] = None,\\n270.         name: Hashable = None,\\n271.         attrs: Mapping = None,\\n272.         # internal parameters\\n273.         indexes: Dict[Hashable, pd.Index] = None,\\n274.         fastpath: bool = False,\\n275.     ):\\n276.         """\\n277.         Parameters\\n278.         ----------\\n279.         data : array_like\\n280.             Values for this array. Must be an ``numpy.ndarray``, ndarray like,\\n281.             or castable to an ``ndarray``. If a self-described xarray or pandas\\n282.             object, attempts are made to use this array\\\'s metadata to fill in\\n283.             other unspecified arguments. A view of the array\\\'s data is used\\n284.             instead of a copy if possible.\\n285.         coords : sequence or dict of array_like, optional\\n286.             Coordinates (tick labels) to use for indexing along each dimension.\\n287.             The following notations are accepted:\\n288. \\n289.             - mapping {dimension name: array-like}\\n290.             - sequence of tuples that are valid arguments for xarray.Variable()\\n291.               - (dims, data)\\n292.               - (dims, data, attrs)\\n293.               - (dims, data, attrs, encoding)\\n294. \\n295.             Additionally, it is possible to define a coord whose name\\n296.             does not match the dimension name, or a coord based on multiple\\n297.             dimensions, with one of the following notations:\\n298. \\n299.             - mapping {coord name: DataArray}\\n300.             - mapping {coord name: Variable}\\n301.             - mapping {coord name: (dimension name, array-like)}\\n302.             - mapping {coord name: (tuple of dimension names, array-like)}\\n303. \\n304.         dims : hashable or sequence of hashable, optional\\n305.             Name(s) of the data dimension(s). Must be either a hashable (only\\n306.             for 1D data) or a sequence of hashables with length equal to the\\n307.             number of dimensions. If this argument is omitted, dimension names\\n308.             default to ``[\\\'dim_0\\\', ... \\\'dim_n\\\']``.\\n309.         name : str or None, optional\\n310.             Name of this array.\\n311.         attrs : dict_like or None, optional\\n312.             Attributes to assign to the new instance. By default, an empty\\n313.             attribute dictionary is initialized.\\n314.         """\\n315.         if fastpath:\\n316.             variable = data\\n317.             assert dims is None\\n318.             assert attrs is None\\n319.         else:\\n320.             # try to fill in arguments from data if they weren\\\'t supplied\\n321.             if coords is None:\\n322. \\n323.                 if isinstance(data, DataArray):\\n324.                     coords = data.coords\\n325.                 elif isinstance(data, pd.Series):\\n326.                     coords = [data.index]\\n327.                 elif isinstance(data, pd.DataFrame):\\n328.                     coords = [data.index, data.columns]\\n329.                 elif isinstance(data, (pd.Index, IndexVariable)):\\n330.                     coords = [data]\\n331.                 elif isinstance(data, pdcompat.Panel):\\n332.                     coords = [data.items, data.major_axis, data.minor_axis]\\n333. \\n334.             if dims is None:\\n335.                 dims = getattr(data, "dims", getattr(coords, "dims", None))\\n336.             if name is None:\\n337.                 name = getattr(data, "name", None)\\n338.             if attrs is None and not isinstance(data, PANDAS_TYPES):\\n339.                 attrs = getattr(data, "attrs", None)\\n340. \\n341.             data = _check_data_shape(data, coords, dims)\\n342.             data = as_compatible_data(data)\\n343.             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)\\n344.             variable = Variable(dims, data, attrs, fastpath=True)\\n345.             indexes = dict(\\n346.                 _extract_indexes_from_coords(coords)\\n347.             )  # needed for to_dataset\\n348. \\n349.         # These fully describe a DataArray\\n350.         self._variable = variable\\n351.         assert isinstance(coords, dict)\\n352.         self._coords = coords\\n353.         self._name = name\\n354. \\n355.         # TODO(shoyer): document this argument, once it becomes part of the\\n356.         # public interface.\\n357.         self._indexes = indexes\\n358. \\n359.         self._file_obj = None\\n360. \\n361.     def _replace(\\n362.         self,\\n363.         variable: Variable = None,\\n364.         coords=None,\\n365.         name: Union[Hashable, None, Default] = _default,\\n366.         indexes=None,\\n367.     ) -> "DataArray":\\n368.         if variable is None:\\n369.             variable = self.variable\\n370.         if coords is None:\\n371.             coords = self._coords\\n372.         if name is _default:\\n373.             name = self.name\\n374.         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)\\n375. \\n376.     def _replace_maybe_drop_dims(\\n377.         self, variable: Variable, name: Union[Hashable, None, Default] = _default\\n378.     ) -> "DataArray":\\n379.         if variable.dims == self.dims and variable.shape == self.shape:\\n380.             coords = self._coords.copy()\\n381.             indexes = self._indexes\\n382.         elif variable.dims == self.dims:\\n383.             # Shape has changed (e.g. from reduce(..., keepdims=True)\\n384.             new_sizes = dict(zip(self.dims, variable.shape))\\n385.             coords = {\\n386.                 k: v\\n387.                 for k, v in self._coords.items()\\n388.                 if v.shape == tuple(new_sizes[d] for d in v.dims)\\n389.             }\\n390.             changed_dims = [\\n391.                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]\\n392.             ]\\n393.             indexes = propagate_indexes(self._indexes, exclude=changed_dims)\\n394.         else:\\n395.             allowed_dims = set(variable.dims)\\n396.             coords = {\\n397.                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims\\n398.             }\\n399.             indexes = propagate_indexes(\\n400.                 self._indexes, exclude=(set(self.dims) - allowed_dims)\\n401.             )\\n402.         return self._replace(variable, coords, name, indexes=indexes)\\n403. \\n404.     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> "DataArray":\\n405.         if not len(indexes):\\n406.             return self\\n407.         coords = self._coords.copy()\\n408.         for name, idx in indexes.items():\\n409.             coords[name] = IndexVariable(name, idx)\\n410.         obj = self._replace(coords=coords)\\n411. \\n412.         # switch from dimension to level names, if necessary\\n413.         dim_names: Dict[Any, str] = {}\\n414.         for dim, idx in indexes.items():\\n415.             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:\\n416.                 dim_names[dim] = idx.name\\n417.         if dim_names:\\n418.             obj = obj.rename(dim_names)\\n419.         return obj\\n420. \\n421.     def _to_temp_dataset(self) -> Dataset:\\n422.         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)\\n423. \\n424.     def _from_temp_dataset(\\n425.         self, dataset: Dataset, name: Union[Hashable, None, Default] = _default\\n426.     ) -> "DataArray":\\n427.         variable = dataset._variables.pop(_THIS_ARRAY)\\n428.         coords = dataset._variables\\n429.         indexes = dataset._indexes\\n430.         return self._replace(variable, coords, name, indexes=indexes)\\n431. \\n432.     def _to_dataset_split(self, dim: Hashable) -> Dataset:\\n433.         """ splits dataarray along dimension \\\'dim\\\' """\\n434. \\n435.         def subset(dim, label):\\n436.             array = self.loc[{dim: label}]\\n437.             array.attrs = {}\\n438.             return as_variable(array)\\n439. \\n440.         variables = {label: subset(dim, label) for label in self.get_index(dim)}\\n441.         variables.update({k: v for k, v in self._coords.items() if k != dim})\\n442.         indexes = propagate_indexes(self._indexes, exclude=dim)\\n443.         coord_names = set(self._coords) - set([dim])\\n444.         dataset = Dataset._construct_direct(\\n445.             variables, coord_names, indexes=indexes, attrs=self.attrs\\n446.         )\\n447.         return dataset\\n448. \\n449.     def _to_dataset_whole(\\n450.         self, name: Hashable = None, shallow_copy: bool = True\\n451.     ) -> Dataset:\\n452.         if name is None:\\n453.             name = self.name\\n454.         if name is None:\\n455.             raise ValueError(\\n456.                 "unable to convert unnamed DataArray to a "\\n457.                 "Dataset without providing an explicit name"\\n458.             )\\n459.         if name in self.coords:\\n460.             raise ValueError(\\n461.                 "cannot create a Dataset from a DataArray with "\\n462.                 "the same name as one of its coordinates"\\n463.             )\\n464.         # use private APIs for speed: this is called by _to_temp_dataset(),\\n465.         # which is used in the guts of a lot of operations (e.g., reindex)\\n466.         variables = self._coords.copy()\\n467.         variables[name] = self.variable\\n468.         if shallow_copy:\\n469.             for k in variables:\\n470.                 variables[k] = variables[k].copy(deep=False)\\n471.         indexes = self._indexes\\n472. \\n473.         coord_names = set(self._coords)\\n474.         dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)\\n475.         return dataset\\n476. \\n477.     def to_dataset(\\n478.         self,\\n479.         dim: Hashable = None,\\n480.         *,\\n481.         name: Hashable = None,\\n482.         promote_attrs: bool = False,\\n483.     ) -> Dataset:\\n484.         """Convert a DataArray to a Dataset.\\n485. \\n486.         Parameters\\n487.         ----------\\n488.         dim : hashable, optional\\n489.             Name of the dimension on this array along which to split this array\\n490.             into separate variables. If not provided, this array is converted\\n491.             into a Dataset of one variable.\\n492.         name : hashable, optional\\n493.             Name to substitute for this array\\\'s name. Only valid if ``dim`` is\\n494.             not provided.\\n495.         promote_attrs : bool, default: False\\n496.             Set to True to shallow copy attrs of DataArray to returned Dataset.\\n497. \\n498.         Returns\\n499.         -------\\n500.         dataset : Dataset\\n501.         """\\n502.         if dim is not None and dim not in self.dims:\\n503.             raise TypeError(\\n504.                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."\\n505.             )\\n506. \\n507.         if dim is not None:\\n508.             if name is not None:\\n509.                 raise TypeError("cannot supply both dim and name arguments")\\n510.             result = self._to_dataset_split(dim)\\n511.         else:\\n512.             result = self._to_dataset_whole(name)\\n513. \\n514.         if promote_attrs:\\n515.             result.attrs = dict(self.attrs)\\n516. \\n517.         return result\\n518. \\n519.     @property\\n520.     def name(self) -> Optional[Hashable]:\\n521.         """The name of this array.\\n522.         """\\n523.         return self._name\\n524. \\n525.     @name.setter\\n526.     def name(self, value: Optional[Hashable]) -> None:\\n527.         self._name = value\\n528. \\n529.     @property\\n530.     def variable(self) -> Variable:\\n531.         """Low level interface to the Variable object for this DataArray."""\\n532.         return self._variable\\n533. \\n534.     @property\\n535.     def dtype(self) -> np.dtype:\\n536.         return self.variable.dtype\\n537. \\n538.     @property\\n539.     def shape(self) -> Tuple[int, ...]:\\n540.         return self.variable.shape\\n541. \\n542.     @property\\n543.     def size(self) -> int:\\n544.         return self.variable.size\\n545. \\n546.     @property\\n547.     def nbytes(self) -> int:\\n548.         return self.variable.nbytes\\n549. \\n550.     @property\\n551.     def ndim(self) -> int:\\n552.         return self.variable.ndim\\n553. \\n554.     def __len__(self) -> int:\\n555.         return len(self.variable)\\n556. \\n557.     @property\\n558.     def data(self) -> Any:\\n559.         """The array\\\'s data as a dask or numpy array\\n560.         """\\n561.         return self.variable.data\\n562. \\n563.     @data.setter\\n564.     def data(self, value: Any) -> None:\\n565.         self.variable.data = value\\n566. \\n567.     @property\\n568.     def values(self) -> np.ndarray:\\n569.         """The array\\\'s data as a numpy.ndarray"""\\n570.         return self.variable.values\\n571. \\n572.     @values.setter\\n573.     def values(self, value: Any) -> None:\\n574.         self.variable.values = value\\n575. \\n576.     @property\\n577.     def _in_memory(self) -> bool:\\n578.         return self.variable._in_memory\\n579. \\n580.     def to_index(self) -> pd.Index:\\n581.         """Convert this variable to a pandas.Index. Only possible for 1D\\n582.         arrays.\\n583.         """\\n584.         return self.variable.to_index()\\n585. \\n586.     @property\\n587.     def dims(self) -> Tuple[Hashable, ...]:\\n588.         """Tuple of dimension names associated with this array.\\n589. \\n590.         Note that the type of this property is inconsistent with\\n591.         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for\\n592.         consistently named properties.\\n593.         """\\n594.         return self.variable.dims\\n595. \\n596.     @dims.setter\\n597.     def dims(self, value):\\n598.         raise AttributeError(\\n599.             "you cannot assign dims on a DataArray. Use "\\n600.             ".rename() or .swap_dims() instead."\\n601.         )\\n602. \\n603.     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:\\n604.         if utils.is_dict_like(key):\\n605.             return key\\n606.         else:\\n607.             key = indexing.expanded_indexer(key, self.ndim)\\n608.             return dict(zip(self.dims, key))\\n609. \\n610.     @property\\n611.     def _level_coords(self) -> Dict[Hashable, Hashable]:\\n612.         """Return a mapping of all MultiIndex levels and their corresponding\\n613.         coordinate name.\\n614.         """\\n615.         level_coords: Dict[Hashable, Hashable] = {}\\n616. \\n617.         for cname, var in self._coords.items():\\n618.             if var.ndim == 1 and isinstance(var, IndexVariable):\\n619.                 level_names = var.level_names\\n620.                 if level_names is not None:\\n621.                     (dim,) = var.dims\\n622.                     level_coords.update({lname: dim for lname in level_names})\\n623.         return level_coords\\n624. \\n625.     def _getitem_coord(self, key):\\n626.         from .dataset import _get_virtual_variable\\n627. \\n628.         try:\\n629.             var = self._coords[key]\\n630.         except KeyError:\\n631.             dim_sizes = dict(zip(self.dims, self.shape))\\n632.             _, key, var = _get_virtual_variable(\\n633.                 self._coords, key, self._level_coords, dim_sizes\\n634.             )\\n635. \\n636.         return self._replace_maybe_drop_dims(var, name=key)\\n637. \\n638.     def __getitem__(self, key: Any) -> "DataArray":\\n639.         if isinstance(key, str):\\n640.             return self._getitem_coord(key)\\n641.         else:\\n642.             # xarray-style array indexing\\n643.             return self.isel(indexers=self._item_key_to_dict(key))\\n644. \\n645.     def __setitem__(self, key: Any, value: Any) -> None:\\n646.         if isinstance(key, str):\\n647.             self.coords[key] = value\\n648.         else:\\n649.             # Coordinates in key, value and self[key] should be consistent.\\n650.             # TODO Coordinate consistency in key is checked here, but it\\n651.             # causes unnecessary indexing. It should be optimized.\\n652.             obj = self[key]\\n653.             if isinstance(value, DataArray):\\n654.                 assert_coordinate_consistent(value, obj.coords.variables)\\n655.             # DataArray key -> Variable key\\n656.             key = {\\n657.                 k: v.variable if isinstance(v, DataArray) else v\\n658.                 for k, v in self._item_key_to_dict(key).items()\\n659.             }\\n660.             self.variable[key] = value\\n661. \\n662.     def __delitem__(self, key: Any) -> None:\\n663.         del self.coords[key]\\n664. \\n665.     @property\\n666.     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:\\n667.         """List of places to look-up items for attribute-style access\\n668.         """\\n669.         return self._item_sources + [self.attrs]\\n670. \\n671.     @property\\n672.     def _item_sources(self) -> List[Mapping[Hashable, Any]]:\\n673.         """List of places to look-up items for key-completion\\n674.         """\\n675.         return [\\n676.             self.coords,\\n677.             {d: self.coords[d] for d in self.dims},\\n678.             LevelCoordinatesSource(self),\\n679.         ]\\n680. \\n681.     def __contains__(self, key: Any) -> bool:\\n682.         return key in self.data\\n683. \\n684.     @property\\n685.     def loc(self) -> _LocIndexer:\\n686.         """Attribute for location based indexing like pandas.\\n687.         """\\n688.         return _LocIndexer(self)\\n689. \\n690.     @property\\n691.     def attrs(self) -> Dict[Hashable, Any]:\\n692.         """Dictionary storing arbitrary metadata with this array."""\\n693.         return self.variable.attrs\\n694. \\n695.     @attrs.setter\\n696.     def attrs(self, value: Mapping[Hashable, Any]) -> None:\\n697.         # Disable type checking to work around mypy bug - see mypy#4167\\n698.         self.variable.attrs = value  # type: ignore\\n699. \\n700.     @property\\n701.     def encoding(self) -> Dict[Hashable, Any]:\\n702.         """Dictionary of format-specific settings for how this array should be\\n703.         serialized."""\\n704.         return self.variable.encoding\\n705. \\n706.     @encoding.setter\\n707.     def encoding(self, value: Mapping[Hashable, Any]) -> None:\\n708.         self.variable.encoding = value\\n709. \\n710.     @property\\n711.     def indexes(self) -> Indexes:\\n712.         """Mapping of pandas.Index objects used for label based indexing\\n713.         """\\n714.         if self._indexes is None:\\n715.             self._indexes = default_indexes(self._coords, self.dims)\\n716.         return Indexes(self._indexes)\\n717. \\n718.     @property\\n719.     def coords(self) -> DataArrayCoordinates:\\n720.         """Dictionary-like container of coordinate arrays.\\n721.         """\\n722.         return DataArrayCoordinates(self)\\n723. \\n724.     def reset_coords(\\n725.         self,\\n726.         names: Union[Iterable[Hashable], Hashable, None] = None,\\n727.         drop: bool = False,\\n728.         inplace: bool = None,\\n729.     ) -> Union[None, "DataArray", Dataset]:\\n730.         """Given names of coordinates, reset them to become variables.\\n731. \\n732.         Parameters\\n733.         ----------\\n734.         names : hashable or iterable of hashable, optional\\n735.             Name(s) of non-index coordinates in this dataset to reset into\\n736.             variables. By default, all non-index coordinates are reset.\\n737.         drop : bool, optional\\n738.             If True, remove coordinates instead of converting them into\\n739.             variables.\\n740. \\n741.         Returns\\n742.         -------\\n743.         Dataset, or DataArray if ``drop == True``\\n744.         """\\n745.         _check_inplace(inplace)\\n746.         if names is None:\\n747.             names = set(self.coords) - set(self.dims)\\n748.         dataset = self.coords.to_dataset().reset_coords(names, drop)\\n749.         if drop:\\n750.             return self._replace(coords=dataset._variables)\\n751.         else:\\n752.             if self.name is None:\\n753.                 raise ValueError(\\n754.                     "cannot reset_coords with drop=False on an unnamed DataArrray"\\n755.                 )\\n756.             dataset[self.name] = self.variable\\n757.             return dataset\\n758. \\n759.     def __dask_tokenize__(self):\\n760.         from dask.base import normalize_token\\n761. \\n762.         return normalize_token((type(self), self._variable, self._coords, self._name))\\n763. \\n764.     def __dask_graph__(self):\\n765.         return self._to_temp_dataset().__dask_graph__()\\n766. \\n767.     def __dask_keys__(self):\\n768.         return self._to_temp_dataset().__dask_keys__()\\n769. \\n770.     def __dask_layers__(self):\\n771.         return self._to_temp_dataset().__dask_layers__()\\n772. \\n773.     @property\\n774.     def __dask_optimize__(self):\\n775.         return self._to_temp_dataset().__dask_optimize__\\n776. \\n777.     @property\\n778.     def __dask_scheduler__(self):\\n779.         return self._to_temp_dataset().__dask_scheduler__\\n780. \\n781.     def __dask_postcompute__(self):\\n782.         func, args = self._to_temp_dataset().__dask_postcompute__()\\n783.         return self._dask_finalize, (func, args, self.name)\\n784. \\n785.     def __dask_postpersist__(self):\\n786.         func, args = self._to_temp_dataset().__dask_postpersist__()\\n787.         return self._dask_finalize, (func, args, self.name)\\n788. \\n789.     @staticmethod\\n790.     def _dask_finalize(results, func, args, name):\\n791.         ds = func(results, *args)\\n792.         variable = ds._variables.pop(_THIS_ARRAY)\\n793.         coords = ds._variables\\n794.         return DataArray(variable, coords, name=name, fastpath=True)\\n795. \\n796.     def load(self, **kwargs) -> "DataArray":\\n797.         """Manually trigger loading of this array\\\'s data from disk or a\\n798.         remote source into memory and return this array.\\n799. \\n800.         Normally, it should not be necessary to call this method in user code,\\n801.         because all xarray functions should either work on deferred data or\\n802.         load data automatically. However, this method can be necessary when\\n803.         working with many file objects on disk.\\n804. \\n805.         Parameters\\n806.         ----------\\n807.         **kwargs : dict\\n808.             Additional keyword arguments passed on to ``dask.array.compute``.\\n809. \\n810.         See Also\\n811.         --------\\n812.         dask.array.compute\\n813.         """\\n814.         ds = self._to_temp_dataset().load(**kwargs)\\n815.         new = self._from_temp_dataset(ds)\\n816.         self._variable = new._variable\\n817.         self._coords = new._coords\\n818.         return self\\n819. \\n820.     def compute(self, **kwargs) -> "DataArray":\\n821.         """Manually trigger loading of this array\\\'s data from disk or a\\n822.         remote source into memory and return a new array. The original is\\n823.         left unaltered.\\n824. \\n825.         Normally, it should not be necessary to call this method in user code,\\n826.         because all xarray functions should either work on deferred data or\\n827.         load data automatically. However, this method can be necessary when\\n828.         working with many file objects on disk.\\n829. \\n830.         Parameters\\n831.         ----------\\n832.         **kwargs : dict\\n833.             Additional keyword arguments passed on to ``dask.array.compute``.\\n834. \\n835.         See Also\\n836.         --------\\n837.         dask.array.compute\\n838.         """\\n839.         new = self.copy(deep=False)\\n840.         return new.load(**kwargs)\\n841. \\n842.     def persist(self, **kwargs) -> "DataArray":\\n843.         """ Trigger computation in constituent dask arrays\\n844. \\n845.         This keeps them as dask arrays but encourages them to keep data in\\n846.         memory.  This is particularly useful when on a distributed machine.\\n847.         When on a single machine consider using ``.compute()`` instead.\\n848. \\n849.         Parameters\\n850.         ----------\\n851.         **kwargs : dict\\n852.             Additional keyword arguments passed on to ``dask.persist``.\\n853. \\n854.         See Also\\n855.         --------\\n856.         dask.persist\\n857.         """\\n858.         ds = self._to_temp_dataset().persist(**kwargs)\\n859.         return self._from_temp_dataset(ds)\\n860. \\n861.     def copy(self, deep: bool = True, data: Any = None) -> "DataArray":\\n862.         """Returns a copy of this array.\\n863. \\n864.         If `deep=True`, a deep copy is made of the data array.\\n865.         Otherwise, a shallow copy is made, and the returned data array\\\'s\\n866.         values are a new view of this data array\\\'s values.\\n867. \\n868.         Use `data` to create a new object with the same structure as\\n869.         original but entirely new data.\\n870. \\n871.         Parameters\\n872.         ----------\\n873.         deep : bool, optional\\n874.             Whether the data array and its coordinates are loaded into memory\\n875.             and copied onto the new object. Default is True.\\n876.         data : array_like, optional\\n877.             Data to use in the new object. Must have same shape as original.\\n878.             When `data` is used, `deep` is ignored for all data variables,\\n879.             and only used for coords.\\n880. \\n881.         Returns\\n882.         -------\\n883.         object : DataArray\\n884.             New object with dimensions, attributes, coordinates, name,\\n885.             encoding, and optionally data copied from original.\\n886. \\n887.         Examples\\n888.         --------\\n889. \\n890.         Shallow versus deep copy\\n891. \\n892.         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})\\n893.         >>> array.copy()\\n894.         <xarray.DataArray (x: 3)>\\n895.         array([1, 2, 3])\\n896.         Coordinates:\\n897.         * x        (x) <U1 \\\'a\\\' \\\'b\\\' \\\'c\\\'\\n898.         >>> array_0 = array.copy(deep=False)\\n899.         >>> array_0[0] = 7\\n900.         >>> array_0\\n901.         <xarray.DataArray (x: 3)>\\n902.         array([7, 2, 3])\\n903.         Coordinates:\\n904.         * x        (x) <U1 \\\'a\\\' \\\'b\\\' \\\'c\\\'\\n905.         >>> array\\n906.         <xarray.DataArray (x: 3)>\\n907.         array([7, 2, 3])\\n908.         Coordinates:\\n909.         * x        (x) <U1 \\\'a\\\' \\\'b\\\' \\\'c\\\'\\n910. \\n911.         Changing the data using the ``data`` argument maintains the\\n912.         structure of the original object, but with the new data. Original\\n913.         object is unaffected.\\n914. \\n915.         >>> array.copy(data=[0.1, 0.2, 0.3])\\n916.         <xarray.DataArray (x: 3)>\\n917.         array([ 0.1,  0.2,  0.3])\\n918.         Coordinates:\\n919.         * x        (x) <U1 \\\'a\\\' \\\'b\\\' \\\'c\\\'\\n920.         >>> array\\n921.         <xarray.DataArray (x: 3)>\\n922.         array([1, 2, 3])\\n923.         Coordinates:\\n924.         * x        (x) <U1 \\\'a\\\' \\\'b\\\' \\\'c\\\'\\n925. \\n926.         See Also\\n927.         --------\\n928.         pandas.DataFrame.copy\\n929.         """\\n930.         variable = self.variable.copy(deep=deep, data=data)\\n931.         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}\\n932.         if self._indexes is None:\\n933.             indexes = self._indexes\\n934.         else:\\n935.             indexes = {k: v.copy(deep=deep) for k, v in self._indexes.items()}\\n936.         return self._replace(variable, coords, indexes=indexes)\\n937. \\n938.     def __copy__(self) -> "DataArray":\\n939.         return self.copy(deep=False)\\n940. \\n941.     def __deepcopy__(self, memo=None) -> "DataArray":\\n942.         # memo does nothing but is required for compatibility with\\n943.         # copy.deepcopy\\n944.         return self.copy(deep=True)\\n945. \\n946.     # mutable objects should not be hashable\\n947.     # https://github.com/python/mypy/issues/4266\\n948.     __hash__ = None  # type: ignore\\n949. \\n950.     @property\\n951.     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:\\n952.         """Block dimensions for this array\\\'s data or None if it\\\'s not a dask\\n953.         array.\\n954.         """\\n955.         return self.variable.chunks\\n956. \\n957.     def chunk(\\n958.         self,\\n959.         chunks: Union[\\n960.             None,\\n961.             Number,\\n962.             Tuple[Number, ...],\\n963.             Tuple[Tuple[Number, ...], ...],\\n964.             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],\\n965.         ] = None,\\n966.         name_prefix: str = "xarray-",\\n967.         token: str = None,\\n968.         lock: bool = False,\\n969.     ) -> "DataArray":\\n970.         """Coerce this array\\\'s data into a dask arrays with the given chunks.\\n971. \\n972.         If this variable is a non-dask array, it will be converted to dask\\n973.         array. If it\\\'s a dask array, it will be rechunked to the given chunk\\n974.         sizes.\\n975. \\n976.         If neither chunks is not provided for one or more dimensions, chunk\\n977.         sizes along that dimension will not be updated; non-dask arrays will be\\n978.         converted into dask arrays with a single block.\\n979. \\n980.         Parameters\\n981.         ----------\\n982.         chunks : int, tuple of int or mapping of hashable to int, optional\\n983.             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\\n984.             ``{\\\'x\\\': 5, \\\'y\\\': 5}``.\\n985.         name_prefix : str, optional\\n986.             Prefix for the name of the new dask array.\\n987.         token : str, optional\\n988.             Token uniquely identifying this array.\\n989.         lock : optional\\n990.             Passed on to :py:func:`dask.array.from_array`, if the array is not\\n991.             already as dask array.\\n992. \\n993.         Returns\\n994.         -------\\n995.         chunked : xarray.DataArray\\n996.         """\\n997.         if isinstance(chunks, (tuple, list)):\\n998.             chunks = dict(zip(self.dims, chunks))\\n999. \\n1000.         ds = self._to_temp_dataset().chunk(\\n1001.             chunks, name_prefix=name_prefix, token=token, lock=lock\\n1002.         )\\n1003.         return self._from_temp_dataset(ds)\\n1004. \\n1005.     def isel(\\n1006.         self,\\n1007.         indexers: Mapping[Hashable, Any] = None,\\n1008.         drop: bool = False,\\n1009.         missing_dims: str = "raise",\\n1010.         **indexers_kwargs: Any,\\n1011.     ) -> "DataArray":\\n1012.         """Return a new DataArray whose data is given by integer indexing\\n1013.         along the specified dimension(s).\\n1014. \\n1015.         Parameters\\n1016.         ----------\\n1017.         indexers : dict, optional\\n1018.             A dict with keys matching dimensions and values given\\n1019.             by integers, slice objects or arrays.\\n1020.             indexer can be a integer, slice, array-like or DataArray.\\n1021.             If DataArrays are passed as indexers, xarray-style indexing will be\\n1022.             carried out. See :ref:`indexing` for the details.\\n1023.             One of indexers or indexers_kwargs must be provided.\\n1024.         drop : bool, optional\\n1025.             If ``drop=True``, drop coordinates variables indexed by integers\\n1026.             instead of making them scalar.\\n1027.         missing_dims : {"raise", "warn", "ignore"}, default: "raise"\\n1028.             What to do if dimensions that should be selected from are not present in the\\n1029.             DataArray:\\n1030.             - "raise": raise an exception\\n1031.             - "warning": raise a warning, and ignore the missing dimensions\\n1032.             - "ignore": ignore the missing dimensions\\n1033.         **indexers_kwargs : {dim: indexer, ...}, optional\\n1034.             The keyword arguments form of ``indexers``.\\n1035. \\n1036.         See Also\\n1037.         --------\\n1038.         Dataset.isel\\n1039.         DataArray.sel\\n1040.         """\\n1041. \\n1042.         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")\\n1043. \\n1044.         if any(is_fancy_indexer(idx) for idx in indexers.values()):\\n1045.             ds = self._to_temp_dataset()._isel_fancy(\\n1046.                 indexers, drop=drop, missing_dims=missing_dims\\n1047.             )\\n1048.             return self._from_temp_dataset(ds)\\n1049. \\n1050.         # Much faster algorithm for when all indexers are ints, slices, one-dimensional\\n1051.         # lists, or zero or one-dimensional np.ndarray\\\'s\\n1052. \\n1053.         variable = self._variable.isel(indexers, missing_dims=missing_dims)\\n1054. \\n1055.         coords = {}\\n1056.         for coord_name, coord_value in self._coords.items():\\n1057.             coord_indexers = {\\n1058.                 k: v for k, v in indexers.items() if k in coord_value.dims\\n1059.             }\\n1060.             if coord_indexers:\\n1061.                 coord_value = coord_value.isel(coord_indexers)\\n1062.                 if drop and coord_value.ndim == 0:\\n1063.                     continue\\n1064.             coords[coord_name] = coord_value\\n1065. \\n1066.         return self._replace(variable=variable, coords=coords)\\n1067. \\n1068.     def sel(\\n1069.         self,\\n1070.         indexers: Mapping[Hashable, Any] = None,\\n1071.         method: str = None,\\n1072.         tolerance=None,\\n1073.         drop: bool = False,\\n1074.         **indexers_kwargs: Any,\\n1075.     ) -> "DataArray":\\n1076.         """Return a new DataArray whose data is given by selecting index\\n1077.         labels along the specified dimension(s).\\n1078. \\n1079.         In contrast to `DataArray.isel`, indexers for this method should use\\n1080.         labels instead of integers.\\n1081. \\n1082.         Under the hood, this method is powered by using pandas\\\'s powerful Index\\n1083.         objects. This makes label based indexing essentially just as fast as\\n1084.         using integer indexing.\\n1085. \\n1086.         It also means this method uses pandas\\\'s (well documented) logic for\\n1087.         indexing. This means you can use string shortcuts for datetime indexes\\n1088.         (e.g., \\\'2000-01\\\' to select all values in January 2000). It also means\\n1089.         that slices are treated as inclusive of both the start and stop values,\\n1090.         unlike normal Python indexing.\\n1091. \\n1092.         .. warning::\\n1093. \\n1094.           Do not try to assign values when using any of the indexing methods\\n1095.           ``isel`` or ``sel``::\\n1096. \\n1097.             da = xr.DataArray([0, 1, 2, 3], dims=[\\\'x\\\'])\\n1098.             # DO NOT do this\\n1099.             da.isel(x=[0, 1, 2])[1] = -1\\n1100. \\n1101.           Assigning values with the chained indexing using ``.sel`` or\\n1102.           ``.isel`` fails silently.\\n1103. \\n1104.         Parameters\\n1105.         ----------\\n1106.         indexers : dict, optional\\n1107.             A dict with keys matching dimensions and values given\\n1108.             by scalars, slices or arrays of tick labels. For dimensions with\\n1109.             multi-index, the indexer may also be a dict-like object with keys\\n1110.             matching index level names.\\n1111.             If DataArrays are passed as indexers, xarray-style indexing will be\\n1112.             carried out. See :ref:`indexing` for the details.\\n1113.             One of indexers or indexers_kwargs must be provided.\\n1114.         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional\\n1115.             Method to use for inexact matches:\\n1116. \\n1117.             * None (default): only exact matches\\n1118.             * pad / ffill: propagate last valid index value forward\\n1119.             * backfill / bfill: propagate next valid index value backward\\n1120.             * nearest: use nearest valid index value\\n1121.         tolerance : optional\\n1122.             Maximum distance between original and new labels for inexact\\n1123.             matches. The values of the index at the matching locations must\\n1124.             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\\n1125.         drop : bool, optional\\n1126.             If ``drop=True``, drop coordinates variables in `indexers` instead\\n1127.             of making them scalar.\\n1128.         **indexers_kwargs : {dim: indexer, ...}, optional\\n1129.             The keyword arguments form of ``indexers``.\\n1130.             One of indexers or indexers_kwargs must be provided.\\n1131. \\n1132.         Returns\\n1133.         -------\\n1134.         obj : DataArray\\n1135.             A new DataArray with the same contents as this DataArray, except the\\n1136.             data and each dimension is indexed by the appropriate indexers.\\n1137.             If indexer DataArrays have coordinates that do not conflict with\\n1138.             this object, then these coordinates will be attached.\\n1139.             In general, each array\\\'s data will be a view of the array\\\'s data\\n1140.             in this DataArray, unless vectorized indexing was triggered by using\\n1141.             an array indexer, in which case the data will be a copy.\\n1142. \\n1143.         See Also\\n1144.         --------\\n1145.         Dataset.sel\\n1146.         DataArray.isel\\n1147. \\n1148.         """\\n1149.         ds = self._to_temp_dataset().sel(\\n1150.             indexers=indexers,\\n1151.             drop=drop,\\n1152.             method=method,\\n1153.             tolerance=tolerance,\\n1154.             **indexers_kwargs,\\n1155.         )\\n1156.         return self._from_temp_dataset(ds)\\n1157. \\n1158.     def head(\\n1159.         self,\\n1160.         indexers: Union[Mapping[Hashable, int], int] = None,\\n1161.         **indexers_kwargs: Any,\\n1162.     ) -> "DataArray":\\n1163.         """Return a new DataArray whose data is given by the the first `n`\\n1164.         values along the specified dimension(s). Default `n` = 5\\n1165. \\n1166.         See Also\\n1167.         --------\\n1168.         Dataset.head\\n1169.         DataArray.tail\\n1170.         DataArray.thin\\n1171.         """\\n1172.         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)\\n1173.         return self._from_temp_dataset(ds)\\n1174. \\n1175.     def tail(\\n1176.         self,\\n1177.         indexers: Union[Mapping[Hashable, int], int] = None,\\n1178.         **indexers_kwargs: Any,\\n1179.     ) -> "DataArray":\\n1180.         """Return a new DataArray whose data is given by the the last `n`\\n1181.         values along the specified dimension(s). Default `n` = 5\\n1182. \\n1183.         See Also\\n1184.         --------\\n1185.         Dataset.tail\\n1186.         DataArray.head\\n1187.         DataArray.thin\\n1188.         """\\n1189.         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)\\n1190.         return self._from_temp_dataset(ds)\\n1191. \\n1192.     def thin(\\n1193.         self,\\n1194.         indexers: Union[Mapping[Hashable, int], int] = None,\\n1195.         **indexers_kwargs: Any,\\n1196.     ) -> "DataArray":\\n1197.         """Return a new DataArray whose data is given by each `n` value\\n1198.         along the specified dimension(s).\\n1199. \\n1200.         See Also\\n1201.         --------\\n1202.         Dataset.thin\\n1203.         DataArray.head\\n1204.         DataArray.tail\\n1205.         """\\n1206.         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)\\n1207.         return self._from_temp_dataset(ds)\\n1208. \\n1209.     def broadcast_like(\\n1210.         self, other: Union["DataArray", Dataset], exclude: Iterable[Hashable] = None\\n1211.     ) -> "DataArray":\\n1212.         """Broadcast this DataArray against another Dataset or DataArray.\\n1213. \\n1214.         This is equivalent to xr.broadcast(other, self)[1]\\n1215. \\n1216.         xarray objects are broadcast against each other in arithmetic\\n1217.         operations, so this method is not be necessary for most uses.\\n1218. \\n1219.         If no change is needed, the input data is returned to the output\\n1220.         without being copied.\\n1221. \\n1222.         If new coords are added by the broadcast, their values are\\n1223.         NaN filled.\\n1224. \\n1225.         Parameters\\n1226.         ----------\\n1227.         other : Dataset or DataArray\\n1228.             Object against which to broadcast this array.\\n1229.         exclude : iterable of hashable, optional\\n1230.             Dimensions that must not be broadcasted\\n1231. \\n1232.         Returns\\n1233.         -------\\n1234.         new_da : DataArray\\n1235.             The caller broadcasted against ``other``.\\n1236. \\n1237.         Examples\\n1238.         --------\\n1239. \\n1240.         >>> arr1\\n1241.         <xarray.DataArray (x: 2, y: 3)>\\n1242.         array([[0.840235, 0.215216, 0.77917 ],\\n1243.                [0.726351, 0.543824, 0.875115]])\\n1244.         Coordinates:\\n1245.           * x        (x) <U1 \\\'a\\\' \\\'b\\\'\\n1246.           * y        (y) <U1 \\\'a\\\' \\\'b\\\' \\\'c\\\'\\n1247.         >>> arr2\\n1248.         <xarray.DataArray (x: 3, y: 2)>\\n1249.         array([[0.612611, 0.125753],\\n1250.                [0.853181, 0.948818],\\n1251.                [0.180885, 0.33363 ]])\\n1252.         Coordinates:\\n1253.           * x        (x) <U1 \\\'a\\\' \\\'b\\\' \\\'c\\\'\\n1254.           * y        (y) <U1 \\\'a\\\' \\\'b\\\'\\n1255.         >>> arr1.broadcast_like(arr2)\\n1256.         <xarray.DataArray (x: 3, y: 3)>\\n1257.         array([[0.840235, 0.215216, 0.77917 ],\\n1258.                [0.726351, 0.543824, 0.875115],\\n1259.                [     nan,      nan,      nan]])\\n1260.         Coordinates:\\n1261.           * x        (x) object \\\'a\\\' \\\'b\\\' \\\'c\\\'\\n1262.           * y        (y) object \\\'a\\\' \\\'b\\\' \\\'c\\\'\\n1263.         """\\n1264.         if exclude is None:\\n1265.             exclude = set()\\n1266.         else:\\n1267.             exclude = set(exclude)\\n1268.         args = align(other, self, join="outer", copy=False, exclude=exclude)\\n1269. \\n1270.         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\\n1271. \\n1272.         return _broadcast_helper(args[1], exclude, dims_map, common_coords)\\n1273. \\n1274.     def reindex_like(\\n1275.         self,\\n1276.         other: Union["DataArray", Dataset],\\n1277.         method: str = None,\\n1278.         tolerance=None,\\n1279.         copy: bool = True,\\n1280.         fill_value=dtypes.NA,\\n1281.     ) -> "DataArray":\\n1282.         """Conform this object onto the indexes of another object, filling in\\n1283.         missing values with ``fill_value``. The default fill value is NaN.\\n1284. \\n1285.         Parameters\\n1286.         ----------\\n1287.         other : Dataset or DataArray\\n1288.             Object with an \\\'indexes\\\' attribute giving a mapping from dimension\\n1289.             names to pandas.Index objects, which provides coordinates upon\\n1290.             which to index the variables in this dataset. The indexes on this\\n1291.             other object need not be the same as the indexes on this\\n1292.             dataset. Any mis-matched index values will be filled in with\\n1293.             NaN, and any mis-matched dimension names will simply be ignored.\\n1294.         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional\\n1295.             Method to use for filling index values from other not found on this\\n1296.             data array:\\n1297. \\n1298.             * None (default): don\\\'t fill gaps\\n1299.             * pad / ffill: propagate last valid index value forward\\n1300.             * backfill / bfill: propagate next valid index value backward\\n1301.             * nearest: use nearest valid index value\\n1302.         tolerance : optional\\n1303.             Maximum distance between original and new labels for inexact\\n1304.             matches. The values of the index at the matching locations must\\n1305.             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\\n1306.         copy : bool, optional\\n1307.             If ``copy=True``, data in the return value is always copied. If\\n1308.             ``copy=False`` and reindexing is unnecessary, or can be performed\\n1309.             with only slice operations, then the output may share memory with\\n1310.             the input. In either case, a new xarray object is always returned.\\n1311.         fill_value : scalar, optional\\n1312.             Value to use for newly missing values\\n1313. \\n1314.         Returns\\n1315.         -------\\n1316.         reindexed : DataArray\\n1317.             Another dataset array, with this array\\\'s data but coordinates from\\n1318.             the other object.\\n1319. \\n1320.         See Also\\n1321.         --------\\n1322.         DataArray.reindex\\n1323.         align\\n1324.         """\\n1325.         indexers = reindex_like_indexers(self, other)\\n1326.         return self.reindex(\\n1327.             indexers=indexers,\\n1328.             method=method,\\n1329.             tolerance=tolerance,\\n1330.             copy=copy,\\n1331.             fill_value=fill_value,\\n1332.         )\\n1333. \\n1334.     def reindex(\\n1335.         self,\\n1336.         indexers: Mapping[Hashable, Any] = None,\\n1337.         method: str = None,\\n1338.         tolerance=None,\\n1339.         copy: bool = True,\\n1340.         fill_value=dtypes.NA,\\n1341.         **indexers_kwargs: Any,\\n1342.     ) -> "DataArray":\\n1343.         """Conform this object onto the indexes of another object, filling in\\n1344.         missing values with ``fill_value``. The default fill value is NaN.\\n1345. \\n1346.         Parameters\\n1347.         ----------\\n1348.         indexers : dict, optional\\n1349.             Dictionary with keys given by dimension names and values given by\\n1350.             arrays of coordinates tick labels. Any mis-matched coordinate\\n1351.             values will be filled in with NaN, and any mis-matched dimension\\n1352.             names will simply be ignored.\\n1353.             One of indexers or indexers_kwargs must be provided.\\n1354.         copy : bool, optional\\n1355.             If ``copy=True``, data in the return value is always copied. If\\n1356.             ``copy=False`` and reindexing is unnecessary, or can be performed\\n1357.             with only slice operations, then the output may share memory with\\n1358.             the input. In either case, a new xarray object is always returned.\\n1359.         method : {None, \\\'nearest\\\', \\\'pad\\\'/\\\'ffill\\\', \\\'backfill\\\'/\\\'bfill\\\'}, optional\\n1360.             Method to use for filling index values in ``indexers`` not found on\\n1361.             this data array:\\n1362. \\n1363.             * None (default): don\\\'t fill gaps\\n1364.             * pad / ffill: propagate last valid index value forward\\n1365.             * backfill / bfill: propagate next valid index value backward\\n1366.             * nearest: use nearest valid index value\\n1367.         tolerance : optional\\n1368.             Maximum distance between original and new labels for inexact\\n1369.             matches. The values of the index at the matching locations must\\n1370.             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\\n1371.         fill_value : scalar, optional\\n1372.             Value to use for newly missing values\\n1373.         **indexers_kwargs : {dim: indexer, ...}, optional\\n1374.             The keyword arguments form of ``indexers``.\\n1375.             One of indexers or indexers_kwargs must be provided.\\n1376. \\n1377.         Returns\\n1378.         -------\\n1379.         reindexed : DataArray\\n1380.             Another dataset array, with this array\\\'s data but replaced\\n1381.             coordinates.\\n1382. \\n1383.         See Also\\n1384.         --------\\n1385.         DataArray.reindex_like\\n1386.         align\\n1387.         """\\n1388.         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")\\n1389.         ds = self._to_temp_dataset().reindex(\\n1390.             indexers=indexers,\\n1391.             method=method,\\n1392.             tolerance=tolerance,\\n1393.             copy=copy,\\n1394.             fill_value=fill_value,\\n1395.         )\\n1396.         return self._from_temp_dataset(ds)\\n1397. \\n1398.     def interp(\\n1399.         self,\\n1400.         coords: Mapping[Hashable, Any] = None,\\n1401.         method: str = "linear",\\n1402.         assume_sorted: bool = False,\\n1403.         kwargs: Mapping[str, Any] = None,\\n1404.         **coords_kwargs: Any,\\n1405.     ) -> "DataArray":\\n1406.         """ Multidimensional interpolation of variables.\\n1407. \\n1408.         Parameters\\n1409.         ----------\\n1410.         coords : dict, optional\\n1411.             Mapping from dimension names to the new coordinates.\\n1412.             new coordinate can be an scalar, array-like or DataArray.\\n1413.             If DataArrays are passed as new coordates, their dimensions are\\n1414.             used for the broadcasting.\\n1415.         method : str, default: "linear"\\n1416.             The method used to interpolate. Choose from\\n1417. \\n1418.             - {"linear", "nearest"} for multidimensional array,\\n1419.             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"} for 1-dimensional array.\\n1420.         assume_sorted : bool, optional\\n1421.             If False, values of x can be in any order and they are sorted\\n1422.             first. If True, x has to be an array of monotonically increasing\\n1423.             values.\\n1424.         kwargs : dict\\n1425.             Additional keyword arguments passed to scipy\\\'s interpolator. Valid\\n1426.             options and their behavior depend on if 1-dimensional or\\n1427.             multi-dimensional interpolation is used.\\n1428.         **coords_kwargs : {dim: coordinate, ...}, optional\\n1429.             The keyword arguments form of ``coords``.\\n1430.             One of coords or coords_kwargs must be provided.\\n1431. \\n1432.         Returns\\n1433.         -------\\n1434.         interpolated : DataArray\\n1435.             New dataarray on the new coordinates.\\n1436. \\n1437.         Notes\\n1438.         -----\\n1439.         scipy is required.\\n1440. \\n1441.         See Also\\n1442.         --------\\n1443.         scipy.interpolate.interp1d\\n1444.         scipy.interpolate.interpn\\n1445. \\n1446.         Examples\\n1447.         --------\\n1448.         >>> da = xr.DataArray([1, 3], [("x", np.arange(2))])\\n1449.         >>> da.interp(x=0.5)\\n1450.         <xarray.DataArray ()>\\n1451.         array(2.0)\\n1452.         Coordinates:\\n1453.             x        float64 0.5\\n1454.         """\\n1455.         if self.dtype.kind not in "uifc":\\n1456.             raise TypeError(\\n1457.                 "interp only works for a numeric type array. "\\n1458.                 "Given {}.".format(self.dtype)\\n1459.             )\\n1460.         ds = self._to_temp_dataset().interp(\\n1461.             coords,\\n1462.             method=method,\\n1463.             kwargs=kwargs,\\n1464.             assume_sorted=assume_sorted,\\n1465.             **coords_kwargs,\\n1466.         )\\n1467.         return self._from_temp_dataset(ds)\\n1468. \\n1469.     def interp_like(\\n1470.         self,\\n1471.         other: Union["DataArray", Dataset],\\n1472.         method: str = "linear",\\n1473.         assume_sorted: bool = False,\\n1474.         kwargs: Mapping[str, Any] = None,\\n1475.     ) -> "DataArray":\\n1476.         """Interpolate this object onto the coordinates of another object,\\n1477.         filling out of range values with NaN.\\n1478. \\n1479.         Parameters\\n1480.         ----------\\n1481.         other : Dataset or DataArray\\n1482.             Object with an \\\'indexes\\\' attribute giving a mapping from dimension\\n1483.             names to an 1d array-like, which provides coordinates upon\\n1484.             which to index the variables in this dataset.\\n1485.         method : str, default: "linear"\\n1486.             The method used to interpolate. Choose from\\n1487. \\n1488.             - {"linear", "nearest"} for multidimensional array,\\n1489.             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"} for 1-dimensional array.\\n1490.         assume_sorted : bool, optional\\n1491.             If False, values of coordinates that are interpolated over can be\\n1492.             in any order and they are sorted first. If True, interpolated\\n1493.             coordinates are assumed to be an array of monotonically increasing\\n1494.             values.\\n1495.         kwargs : dict, optional\\n1496.             Additional keyword passed to scipy\\\'s interpolator.\\n1497. \\n1498.         Returns\\n1499.         -------\\n1500.         interpolated : DataArray\\n1501.             Another dataarray by interpolating this dataarray\\\'s data along the\\n1502.             coordinates of the other object.\\n1503. \\n1504.         Notes\\n1505.         -----\\n1506.         scipy is required.\\n1507.         If the dataarray has object-type coordinates, reindex is used for these\\n1508.         coordinates instead of the interpolation.\\n1509. \\n1510.         See Also\\n1511.         --------\\n1512.         DataArray.interp\\n1513.         DataArray.reindex_like\\n1514.         """\\n1515.         if self.dtype.kind not in "uifc":\\n1516.             raise TypeError(\\n1517.                 "interp only works for a numeric type array. "\\n1518.                 "Given {}.".format(self.dtype)\\n1519.             )\\n1520.         ds = self._to_temp_dataset().interp_like(\\n1521.             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted\\n1522.         )\\n1523.         return self._from_temp_dataset(ds)\\n1524. \\n1525.     def rename(\\n1526.         self,\\n1527.         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,\\n1528.         **names: Hashable,\\n1529.     ) -> "DataArray":\\n1530.         """Returns a new DataArray with renamed coordinates or a new name.\\n1531. \\n1532.         Parameters\\n1533.         ----------\\n1534.         new_name_or_name_dict : str or dict-like, optional\\n1535.             If the argument is dict-like, it used as a mapping from old\\n1536.             names to new names for coordinates. Otherwise, use the argument\\n1537.             as the new name for this array.\\n1538.         **names : hashable, optional\\n1539.             The keyword arguments form of a mapping from old names to\\n1540.             new names for coordinates.\\n1541.             One of new_name_or_name_dict or names must be provided.\\n1542. \\n1543.         Returns\\n1544.         -------\\n1545.         renamed : DataArray\\n1546.             Renamed array or array with renamed coordinates.\\n1547. \\n1548.         See Also\\n1549.         --------\\n1550.         Dataset.rename\\n1551.         DataArray.swap_dims\\n1552.         """\\n1553.         if names or utils.is_dict_like(new_name_or_name_dict):\\n1554.             new_name_or_name_dict = cast(\\n1555.                 Mapping[Hashable, Hashable], new_name_or_name_dict\\n1556.             )\\n1557.             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")\\n1558.             dataset = self._to_temp_dataset().rename(name_dict)\\n1559.             return self._from_temp_dataset(dataset)\\n1560.         else:\\n1561.             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)\\n1562.             return self._replace(name=new_name_or_name_dict)\\n1563. \\n1564.     def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> "DataArray":\\n1565.         """Returns a new DataArray with swapped dimensions.\\n1566. \\n1567.         Parameters\\n1568.         ----------\\n1569.         dims_dict : dict-like\\n1570.             Dictionary whose keys are current dimension names and whose values\\n1571.             are new names.\\n1572. \\n1573.         Returns\\n1574.         -------\\n1575.         swapped : DataArray\\n1576.             DataArray with swapped dimensions.\\n1577. \\n1578.         Examples\\n1579.         --------\\n1580. \\n1581.         >>> arr = xr.DataArray(\\n1582.         ...     data=[0, 1], dims="x", coords={"x": ["a", "b"], "y": ("x", [0, 1])},\\n1583.         ... )\\n1584.         >>> arr\\n1585.         <xarray.DataArray (x: 2)>\\n1586.         array([0, 1])\\n1587.         Coordinates:\\n1588.           * x        (x) <U1 \\\'a\\\' \\\'b\\\'\\n1589.             y        (x) int64 0 1\\n1590. \\n1591.         >>> arr.swap_dims({"x": "y"})\\n1592.         <xarray.DataArray (y: 2)>\\n1593.         array([0, 1])\\n1594.         Coordinates:\\n1595.             x        (y) <U1 \\\'a\\\' \\\'b\\\'\\n1596.           * y        (y) int64 0 1\\n1597. \\n1598.         >>> arr.swap_dims({"x": "z"})\\n1599.         <xarray.DataArray (z: 2)>\\n1600.         array([0, 1])\\n1601.         Coordinates:\\n1602.             x        (z) <U1 \\\'a\\\' \\\'b\\\'\\n1603.             y        (z) int64 0 1\\n1604.         Dimensions without coordinates: z\\n1605. \\n1606.         See Also\\n1607.         --------\\n1608. \\n1609.         DataArray.rename\\n1610.         Dataset.swap_dims\\n1611.         """\\n1612.         ds = self._to_temp_dataset().swap_dims(dims_dict)\\n1613.         return self._from_temp_dataset(ds)\\n1614. \\n1615.     def expand_dims(\\n1616.         self,\\n1617.         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,\\n1618.         axis=None,\\n1619.         **dim_kwargs: Any,\\n1620.     ) -> "DataArray":\\n1621.         """Return a new object with an additional axis (or axes) inserted at\\n1622.         the corresponding position in the array shape. The new object is a\\n1623.         view into the underlying array, not a copy.\\n1624. \\n1625. \\n1626.         If dim is already a scalar coordinate, it will be promoted to a 1D\\n1627.         coordinate consisting of a single value.\\n1628. \\n1629.         Parameters\\n1630.         ----------\\n1631.         dim : hashable, sequence of hashable, dict, or None, optional\\n1632.             Dimensions to include on the new variable.\\n1633.             If provided as str or sequence of str, then dimensions are inserted\\n1634.             with length 1. If provided as a dict, then the keys are the new\\n1635.             dimensions and the values are either integers (giving the length of\\n1636.             the new dimensions) or sequence/ndarray (giving the coordinates of\\n1637.             the new dimensions).\\n1638.         axis : int, list of int or tuple of int, or None, default: None\\n1639.             Axis position(s) where new axis is to be inserted (position(s) on\\n1640.             the result array). If a list (or tuple) of integers is passed,\\n1641.             multiple axes are inserted. In this case, dim arguments should be\\n1642.             same length list. If axis=None is passed, all the axes will be\\n1643.             inserted to the start of the result array.\\n1644.         **dim_kwargs : int or sequence or ndarray\\n1645.             The keywords are arbitrary dimensions being inserted and the values\\n1646.             are either the lengths of the new dims (if int is given), or their\\n1647.             coordinates. Note, this is an alternative to passing a dict to the\\n1648.             dim kwarg and will only be used if dim is None.\\n1649. \\n1650.         Returns\\n1651.         -------\\n1652.         expanded : same type as caller\\n1653.             This object, but with an additional dimension(s).\\n1654.         """\\n1655.         if isinstance(dim, int):\\n1656.             raise TypeError("dim should be hashable or sequence/mapping of hashables")\\n1657.         elif isinstance(dim, Sequence) and not isinstance(dim, str):\\n1658.             if len(dim) != len(set(dim)):\\n1659.                 raise ValueError("dims should not contain duplicate values.")\\n1660.             dim = dict.fromkeys(dim, 1)\\n1661.         elif dim is not None and not isinstance(dim, Mapping):\\n1662.             dim = {cast(Hashable, dim): 1}\\n1663. \\n1664.         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")\\n1665.         ds = self._to_temp_dataset().expand_dims(dim, axis)\\n1666.         return self._from_temp_dataset(ds)\\n1667. \\n1668.     def set_index(\\n1669.         self,\\n1670.         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,\\n1671.         append: bool = False,\\n1672.         inplace: bool = None,\\n1673.         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],\\n1674.     ) -> Optional["DataArray"]:\\n1675.         """Set DataArray (multi-)indexes using one or more existing\\n1676.         coordinates.\\n1677. \\n1678.         Parameters\\n1679.         ----------\\n1680.         indexes : {dim: index, ...}\\n1681.             Mapping from names matching dimensions and values given\\n1682.             by (lists of) the names of existing coordinates or variables to set\\n1683.             as new (multi-)index.\\n1684.         append : bool, optional\\n1685.             If True, append the supplied index(es) to the existing index(es).\\n1686.             Otherwise replace the existing index(es) (default).\\n1687.         **indexes_kwargs : optional\\n1688.             The keyword arguments form of ``indexes``.\\n1689.             One of indexes or indexes_kwargs must be provided.\\n1690. \\n1691.         Returns\\n1692.         -------\\n1693.         obj : DataArray\\n1694.             Another DataArray, with this data but replaced coordinates.\\n1695. \\n1696.         Examples\\n1697.         --------\\n1698.         >>> arr = xr.DataArray(\\n1699.         ...     data=np.ones((2, 3)),\\n1700.         ...     dims=["x", "y"],\\n1701.         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},\\n1702.         ... )\\n1703.         >>> arr\\n1704.         <xarray.DataArray (x: 2, y: 3)>\\n1705.         array([[1., 1., 1.],\\n1706.                [1., 1., 1.]])\\n1707.         Coordinates:\\n1708.           * x        (x) int64 0 1\\n1709.           * y        (y) int64 0 1 2\\n1710.             a        (x) int64 3 4\\n1711.         >>> arr.set_index(x="a")\\n1712.         <xarray.DataArray (x: 2, y: 3)>\\n1713.         array([[1., 1., 1.],\\n1714.                [1., 1., 1.]])\\n1715.         Coordinates:\\n1716.           * x        (x) int64 3 4\\n1717.           * y        (y) int64 0 1 2\\n1718. \\n1719.         See Also\\n1720.         --------\\n1721.         DataArray.reset_index\\n1722.         """\\n1723.         ds = self._to_temp_dataset().set_index(\\n1724.             indexes, append=append, inplace=inplace, **indexes_kwargs\\n1725.         )\\n1726.         return self._from_temp_dataset(ds)\\n1727. \\n1728.     def reset_index(\\n1729.         self,\\n1730.         dims_or_levels: Union[Hashable, Sequence[Hashable]],\\n1731.         drop: bool = False,\\n1732.         inplace: bool = None,\\n1733.     ) -> Optional["DataArray"]:\\n1734.         """Reset the specified index(es) or multi-index level(s).\\n1735. \\n1736.         Parameters\\n1737.         ----------\\n1738.         dims_or_levels : hashable or sequence of hashable\\n1739.             Name(s) of the dimension(s) and/or multi-index level(s) that will\\n1740.             be reset.\\n1741.         drop : bool, optional\\n1742.             If True, remove the specified indexes and/or multi-index levels\\n1743.             instead of extracting them as new coordinates (default: False).\\n1744. \\n1745.         Returns\\n1746.         -------\\n1747.         obj : DataArray\\n1748.             Another dataarray, with this dataarray\\\'s data but replaced\\n1749.             coordinates.\\n1750. \\n1751.         See Also\\n1752.         --------\\n1753.         DataArray.set_index\\n1754.         """\\n1755.         _check_inplace(inplace)\\n1756.         coords, _ = split_indexes(\\n1757.             dims_or_levels, self._coords, set(), self._level_coords, drop=drop\\n1758.         )\\n1759.         return self._replace(coords=coords)\\n1760. \\n1761.     def reorder_levels(\\n1762.         self,\\n1763.         dim_order: Mapping[Hashable, Sequence[int]] = None,\\n1764.         inplace: bool = None,\\n1765.         **dim_order_kwargs: Sequence[int],\\n1766.     ) -> "DataArray":\\n1767.         """Rearrange index levels using input order.\\n1768. \\n1769.         Parameters\\n1770.         ----------\\n1771.         dim_order : optional\\n1772.             Mapping from names matching dimensions and values given\\n1773.             by lists representing new level orders. Every given dimension\\n1774.             must have a multi-index.\\n1775.         **dim_order_kwargs : optional\\n1776.             The keyword arguments form of ``dim_order``.\\n1777.             One of dim_order or dim_order_kwargs must be provided.\\n1778. \\n1779.         Returns\\n1780.         -------\\n1781.         obj : DataArray\\n1782.             Another dataarray, with this dataarray\\\'s data but replaced\\n1783.             coordinates.\\n1784.         """\\n1785.         _check_inplace(inplace)\\n1786.         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")\\n1787.         replace_coords = {}\\n1788.         for dim, order in dim_order.items():\\n1789.             coord = self._coords[dim]\\n1790.             index = coord.to_index()\\n1791.             if not isinstance(index, pd.MultiIndex):\\n1792.                 raise ValueError("coordinate %r has no MultiIndex" % dim)\\n1793.             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))\\n1794.         coords = self._coords.copy()\\n1795.         coords.update(replace_coords)\\n1796.         return self._replace(coords=coords)\\n1797. \\n1798.     def stack(\\n1799.         self,\\n1800.         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,\\n1801.         **dimensions_kwargs: Sequence[Hashable],\\n1802.     ) -> "DataArray":\\n1803.         """\\n1804.         Stack any number of existing dimensions into a single new dimension.\\n1805. \\n1806.         New dimensions will be added at the end, and the corresponding\\n1807.         coordinate variables will be combined into a MultiIndex.\\n1808. \\n1809.         Parameters\\n1810.         ----------\\n1811.         dimensions : mapping of hashable to sequence of hashable\\n1812.             Mapping of the form `new_name=(dim1, dim2, ...)`.\\n1813.             Names of new dimensions, and the existing dimensions that they\\n1814.             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.\\n1815.             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over\\n1816.             all dimensions.\\n1817.         **dimensions_kwargs\\n1818.             The keyword arguments form of ``dimensions``.\\n1819.             One of dimensions or dimensions_kwargs must be provided.\\n1820. \\n1821.         Returns\\n1822.         -------\\n1823.         stacked : DataArray\\n1824.             DataArray with stacked data.\\n1825. \\n1826.         Examples\\n1827.         --------\\n1828. \\n1829.         >>> arr = xr.DataArray(\\n1830.         ...     np.arange(6).reshape(2, 3),\\n1831.         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],\\n1832.         ... )\\n1833.         >>> arr\\n1834.         <xarray.DataArray (x: 2, y: 3)>\\n1835.         array([[0, 1, 2],\\n1836.                [3, 4, 5]])\\n1837.         Coordinates:\\n1838.           * x        (x) |S1 \\\'a\\\' \\\'b\\\'\\n1839.           * y        (y) int64 0 1 2\\n1840.         >>> stacked = arr.stack(z=("x", "y"))\\n1841.         >>> stacked.indexes["z"]\\n1842.         MultiIndex(levels=[[\\\'a\\\', \\\'b\\\'], [0, 1, 2]],\\n1843.                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\\n1844.                    names=[\\\'x\\\', \\\'y\\\'])\\n1845. \\n1846.         See Also\\n1847.         --------\\n1848.         DataArray.unstack\\n1849.         """\\n1850.         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)\\n1851.         return self._from_temp_dataset(ds)\\n1852. \\n1853.     def unstack(\\n1854.         self,\\n1855.         dim: Union[Hashable, Sequence[Hashable], None] = None,\\n1856.         fill_value: Any = dtypes.NA,\\n1857.         sparse: bool = False,\\n1858.     ) -> "DataArray":\\n1859.         """\\n1860.         Unstack existing dimensions corresponding to MultiIndexes into\\n1861.         multiple new dimensions.\\n1862. \\n1863.         New dimensions will be added at the end.\\n1864. \\n1865.         Parameters\\n1866.         ----------\\n1867.         dim : hashable or sequence of hashable, optional\\n1868.             Dimension(s) over which to unstack. By default unstacks all\\n1869.             MultiIndexes.\\n1870.         fill_value : scalar, default: nan\\n1871.             value to be filled.\\n1872.         sparse : bool, default: False\\n1873.             use sparse-array if True\\n1874. \\n1875.         Returns\\n1876.         -------\\n1877.         unstacked : DataArray\\n1878.             Array with unstacked data.\\n1879. \\n1880.         Examples\\n1881.         --------\\n1882. \\n1883.         >>> arr = xr.DataArray(\\n1884.         ...     np.arange(6).reshape(2, 3),\\n1885.         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],\\n1886.         ... )\\n1887.         >>> arr\\n1888.         <xarray.DataArray (x: 2, y: 3)>\\n1889.         array([[0, 1, 2],\\n1890.                [3, 4, 5]])\\n1891.         Coordinates:\\n1892.           * x        (x) |S1 \\\'a\\\' \\\'b\\\'\\n1893.           * y        (y) int64 0 1 2\\n1894.         >>> stacked = arr.stack(z=("x", "y"))\\n1895.         >>> stacked.indexes["z"]\\n1896.         MultiIndex(levels=[[\\\'a\\\', \\\'b\\\'], [0, 1, 2]],\\n1897.                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\\n1898.                    names=[\\\'x\\\', \\\'y\\\'])\\n1899.         >>> roundtripped = stacked.unstack()\\n1900.         >>> arr.identical(roundtripped)\\n1901.         True\\n1902. \\n1903.         See Also\\n1904.         --------\\n1905.         DataArray.stack\\n1906.         """\\n1907.         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)\\n1908.         return self._from_temp_dataset(ds)\\n1909. \\n1910.     def to_unstacked_dataset(self, dim, level=0):\\n1911.         """Unstack DataArray expanding to Dataset along a given level of a\\n1912.         stacked coordinate.\\n1913. \\n1914.         This is the inverse operation of Dataset.to_stacked_array.\\n1915. \\n1916.         Parameters\\n1917.         ----------\\n1918.         dim : str\\n1919.             Name of existing dimension to unstack\\n1920.         level : int or str\\n1921.             The MultiIndex level to expand to a dataset along. Can either be\\n1922.             the integer index of the level or its name.\\n1923.         label : int, default: 0\\n1924.             Label of the level to expand dataset along. Overrides the label\\n1925.             argument if given.\\n1926. \\n1927.         Returns\\n1928.         -------\\n1929.         unstacked: Dataset\\n1930. \\n1931.         Examples\\n1932.         --------\\n1933.         >>> import xarray as xr\\n1934.         >>> arr = xr.DataArray(\\n1935.         ...     np.arange(6).reshape(2, 3),\\n1936.         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],\\n1937.         ... )\\n1938.         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})\\n1939.         >>> data\\n1940.         <xarray.Dataset>\\n1941.         Dimensions:  (x: 2, y: 3)\\n1942.         Coordinates:\\n1943.           * x        (x) <U1 \\\'a\\\' \\\'b\\\'\\n1944.           * y        (y) int64 0 1 2\\n1945.         Data variables:\\n1946.             a        (x, y) int64 0 1 2 3 4 5\\n1947.             b        (x) int64 0 3\\n1948.         >>> stacked = data.to_stacked_array("z", ["y"])\\n1949.         >>> stacked.indexes["z"]\\n1950.         MultiIndex(levels=[[\\\'a\\\', \\\'b\\\'], [0, 1, 2]],\\n1951.                 labels=[[0, 0, 0, 1], [0, 1, 2, -1]],\\n1952.                 names=[\\\'variable\\\', \\\'y\\\'])\\n1953.         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")\\n1954.         >>> data.identical(roundtripped)\\n1955.         True\\n1956. \\n1957.         See Also\\n1958.         --------\\n1959.         Dataset.to_stacked_array\\n1960.         """\\n1961. \\n1962.         idx = self.indexes[dim]\\n1963.         if not isinstance(idx, pd.MultiIndex):\\n1964.             raise ValueError(f"\\\'{dim}\\\' is not a stacked coordinate")\\n1965. \\n1966.         level_number = idx._get_level_number(level)\\n1967.         variables = idx.levels[level_number]\\n1968.         variable_dim = idx.names[level_number]\\n1969. \\n1970.         # pull variables out of datarray\\n1971.         data_dict = {}\\n1972.         for k in variables:\\n1973.             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)\\n1974. \\n1975.         # unstacked dataset\\n1976.         return Dataset(data_dict)\\n1977. \\n1978.     def transpose(self, *dims: Hashable, transpose_coords: bool = True) -> "DataArray":\\n1979.         """Return a new DataArray object with transposed dimensions.\\n1980. \\n1981.         Parameters\\n1982.         ----------\\n1983.         *dims : hashable, optional\\n1984.             By default, reverse the dimensions. Otherwise, reorder the\\n1985.             dimensions to this order.\\n1986.         transpose_coords : bool, default: True\\n1987.             If True, also transpose the coordinates of this DataArray.\\n1988. \\n1989.         Returns\\n1990.         -------\\n1991.         transposed : DataArray\\n1992.             The returned DataArray\\\'s array is transposed.\\n1993. \\n1994.         Notes\\n1995.         -----\\n1996.         This operation returns a view of this array\\\'s data. It is\\n1997.         lazy for dask-backed DataArrays but not for numpy-backed DataArrays\\n1998.         -- the data will be fully loaded.\\n1999. \\n2000.         See Also\\n2001.         --------\\n2002.         numpy.transpose\\n2003.         Dataset.transpose\\n2004.         """\\n2005.         if dims:\\n2006.             dims = tuple(utils.infix_dims(dims, self.dims))\\n2007.         variable = self.variable.transpose(*dims)\\n2008.         if transpose_coords:\\n2009.             coords: Dict[Hashable, Variable] = {}\\n2010.             for name, coord in self.coords.items():\\n2011.                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)\\n2012.                 coords[name] = coord.variable.transpose(*coord_dims)\\n2013.             return self._replace(variable, coords)\\n2014.         else:\\n2015.             return self._replace(variable)\\n2016. \\n2017.     @property\\n2018.     def T(self) -> "DataArray":\\n2019.         return self.transpose()\\n2020. \\n2021.     def drop_vars(\\n2022.         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"\\n2023.     ) -> "DataArray":\\n2024.         """Returns an array with dropped variables.\\n2025. \\n2026.         Parameters\\n2027.         ----------\\n2028.         names : hashable or iterable of hashable\\n2029.             Name(s) of variables to drop.\\n2030.         errors: {"raise", "ignore"}, optional\\n2031.             If \\\'raise\\\' (default), raises a ValueError error if any of the variable\\n2032.             passed are not in the dataset. If \\\'ignore\\\', any given names that are in the\\n2033.             DataArray are dropped and no error is raised.\\n2034. \\n2035.         Returns\\n2036.         -------\\n2037.         dropped : Dataset\\n2038.             New Dataset copied from `self` with variables removed.\\n2039.         """\\n2040.         ds = self._to_temp_dataset().drop_vars(names, errors=errors)\\n2041.         return self._from_temp_dataset(ds)\\n2042. \\n2043.     def drop(\\n2044.         self,\\n2045.         labels: Mapping = None,\\n2046.         dim: Hashable = None,\\n2047.         *,\\n2048.         errors: str = "raise",\\n2049.         **labels_kwargs,\\n2050.     ) -> "DataArray":\\n2051.         """Backward compatible method based on `drop_vars` and `drop_sel`\\n2052. \\n2053.         Using either `drop_vars` or `drop_sel` is encouraged\\n2054. \\n2055.         See Also\\n2056.         --------\\n2057.         DataArray.drop_vars\\n2058.         DataArray.drop_sel\\n2059.         """\\n2060.         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)\\n2061.         return self._from_temp_dataset(ds)\\n2062. \\n2063.     def drop_sel(\\n2064.         self,\\n2065.         labels: Mapping[Hashable, Any] = None,\\n2066.         *,\\n2067.         errors: str = "raise",\\n2068.         **labels_kwargs,\\n2069.     ) -> "DataArray":\\n2070.         """Drop index labels from this DataArray.\\n2071. \\n2072.         Parameters\\n2073.         ----------\\n2074.         labels : mapping of hashable to Any\\n2075.             Index labels to drop\\n2076.         errors : {"raise", "ignore"}, optional\\n2077.             If \\\'raise\\\' (default), raises a ValueError error if\\n2078.             any of the index labels passed are not\\n2079.             in the dataset. If \\\'ignore\\\', any given labels that are in the\\n2080.             dataset are dropped and no error is raised.\\n2081.         **labels_kwargs : {dim: label, ...}, optional\\n2082.             The keyword arguments form of ``dim`` and ``labels``\\n2083. \\n2084.         Returns\\n2085.         -------\\n2086.         dropped : DataArray\\n2087.         """\\n2088.         if labels_kwargs or isinstance(labels, dict):\\n2089.             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")\\n2090. \\n2091.         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)\\n2092.         return self._from_temp_dataset(ds)\\n2093. \\n2094.     def dropna(\\n2095.         self, dim: Hashable, how: str = "any", thresh: int = None\\n2096.     ) -> "DataArray":\\n2097.         """Returns a new array with dropped labels for missing values along\\n2098.         the provided dimension.\\n2099. \\n2100.         Parameters\\n2101.         ----------\\n2102.         dim : hashable\\n2103.             Dimension along which to drop missing values. Dropping along\\n2104.             multiple dimensions simultaneously is not yet supported.\\n2105.         how : {"any", "all"}, optional\\n2106.             * any : if any NA values are present, drop that label\\n2107.             * all : if all values are NA, drop that label\\n2108.         thresh : int, default: None\\n2109.             If supplied, require this many non-NA values.\\n2110. \\n2111.         Returns\\n2112.         -------\\n2113.         DataArray\\n2114.         """\\n2115.         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)\\n2116.         return self._from_temp_dataset(ds)\\n2117. \\n2118.     def fillna(self, value: Any) -> "DataArray":\\n2119.         """Fill missing values in this object.\\n2120. \\n2121.         This operation follows the normal broadcasting and alignment rules that\\n2122.         xarray uses for binary arithmetic, except the result is aligned to this\\n2123.         object (``join=\\\'left\\\'``) instead of aligned to the intersection of\\n2124.         index coordinates (``join=\\\'inner\\\'``).\\n2125. \\n2126.         Parameters\\n2127.         ----------\\n2128.         value : scalar, ndarray or DataArray\\n2129.             Used to fill all matching missing values in this array. If the\\n2130.             argument is a DataArray, it is first aligned with (reindexed to)\\n2131.             this array.\\n2132. \\n2133.         Returns\\n2134.         -------\\n2135.         DataArray\\n2136.         """\\n2137.         if utils.is_dict_like(value):\\n2138.             raise TypeError(\\n2139.                 "cannot provide fill value as a dictionary with "\\n2140.                 "fillna on a DataArray"\\n2141.             )\\n2142.         out = ops.fillna(self, value)\\n2143.         return out\\n2144. \\n2145.     def interpolate_na(\\n2146.         self,\\n2147.         dim: Hashable = None,\\n2148.         method: str = "linear",\\n2149.         limit: int = None,\\n2150.         use_coordinate: Union[bool, str] = True,\\n2151.         max_gap: Union[\\n2152.             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta\\n2153.         ] = None,\\n2154.         keep_attrs: bool = None,\\n2155.         **kwargs: Any,\\n2156.     ) -> "DataArray":\\n2157.         """Fill in NaNs by interpolating according to different methods.\\n2158. \\n2159.         Parameters\\n2160.         ----------\\n2161.         dim : str\\n2162.             Specifies the dimension along which to interpolate.\\n2163.         method : str, optional\\n2164.             String indicating which method to use for interpolation:\\n2165. \\n2166.             - \\\'linear\\\': linear interpolation (Default). Additional keyword\\n2167.               arguments are passed to :py:func:`numpy.interp`\\n2168.             - \\\'nearest\\\', \\\'zero\\\', \\\'slinear\\\', \\\'quadratic\\\', \\\'cubic\\\', \\\'polynomial\\\':\\n2169.               are passed to :py:func:`scipy.interpolate.interp1d`. If\\n2170.               ``method=\\\'polynomial\\\'``, the ``order`` keyword argument must also be\\n2171.               provided.\\n2172.             - \\\'barycentric\\\', \\\'krog\\\', \\\'pchip\\\', \\\'spline\\\', \\\'akima\\\': use their\\n2173.               respective :py:class:`scipy.interpolate` classes.\\n2174. \\n2175.         use_coordinate : bool or str, default: True\\n2176.             Specifies which index to use as the x values in the interpolation\\n2177.             formulated as `y = f(x)`. If False, values are treated as if\\n2178.             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is\\n2179.             used. If ``use_coordinate`` is a string, it specifies the name of a\\n2180.             coordinate variariable to use as the index.\\n2181.         limit : int, default: None\\n2182.             Maximum number of consecutive NaNs to fill. Must be greater than 0\\n2183.             or None for no limit. This filling is done regardless of the size of\\n2184.             the gap in the data. To only interpolate over gaps less than a given length,\\n2185.             see ``max_gap``.\\n2186.         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None\\n2187.             Maximum size of gap, a continuous sequence of NaNs, that will be filled.\\n2188.             Use None for no limit. When interpolating along a datetime64 dimension\\n2189.             and ``use_coordinate=True``, ``max_gap`` can be one of the following:\\n2190. \\n2191.             - a string that is valid input for pandas.to_timedelta\\n2192.             - a :py:class:`numpy.timedelta64` object\\n2193.             - a :py:class:`pandas.Timedelta` object\\n2194.             - a :py:class:`datetime.timedelta` object\\n2195. \\n2196.             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled\\n2197.             dimensions has not been implemented yet. Gap length is defined as the difference\\n2198.             between coordinate values at the first data point after a gap and the last value\\n2199.             before a gap. For gaps at the beginning (end), gap length is defined as the difference\\n2200.             between coordinate values at the first (last) valid data point and the first (last) NaN.\\n2201.             For example, consider::\\n2202. \\n2203.                 <xarray.DataArray (x: 9)>\\n2204.                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])\\n2205.                 Coordinates:\\n2206.                   * x        (x) int64 0 1 2 3 4 5 6 7 8\\n2207. \\n2208.             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively\\n2209.         keep_attrs : bool, default: True\\n2210.             If True, the dataarray\\\'s attributes (`attrs`) will be copied from\\n2211.             the original object to the new one.  If False, the new\\n2212.             object will be returned without attributes.\\n2213.         kwargs : dict, optional\\n2214.             parameters passed verbatim to the underlying interpolation function\\n2215. \\n2216.         Returns\\n2217.         -------\\n2218.         interpolated: DataArray\\n2219.             Filled in DataArray.\\n2220. \\n2221.         See also\\n2222.         --------\\n2223.         numpy.interp\\n2224.         scipy.interpolate\\n2225.         """\\n2226.         from .missing import interp_na\\n2227. \\n2228.         return interp_na(\\n2229.             self,\\n2230.             dim=dim,\\n2231.             method=method,\\n2232.             limit=limit,\\n2233.             use_coordinate=use_coordinate,\\n2234.             max_gap=max_gap,\\n2235.             keep_attrs=keep_attrs,\\n2236.             **kwargs,\\n2237.         )\\n2238. \\n2239.     def ffill(self, dim: Hashable, limit: int = None) -> "DataArray":\\n2240.         """Fill NaN values by propogating values forward\\n2241. \\n2242.         *Requires bottleneck.*\\n2243. \\n2244.         Parameters\\n2245.         ----------\\n2246.         dim : hashable\\n2247.             Specifies the dimension along which to propagate values when\\n2248.             filling.\\n2249.         limit : int, default: None\\n2250.             The maximum number of consecutive NaN values to forward fill. In\\n2251.             other words, if there is a gap with more than this number of\\n2252.             consecutive NaNs, it will only be partially filled. Must be greater\\n2253.             than 0 or None for no limit.\\n2254. \\n2255.         Returns\\n2256.         -------\\n2257.         DataArray\\n2258.         """\\n2259.         from .missing import ffill\\n2260. \\n2261.         return ffill(self, dim, limit=limit)\\n2262. \\n2263.     def bfill(self, dim: Hashable, limit: int = None) -> "DataArray":\\n2264.         """Fill NaN values by propogating values backward\\n2265. \\n2266.         *Requires bottleneck.*\\n2267. \\n2268.         Parameters\\n2269.         ----------\\n2270.         dim : str\\n2271.             Specifies the dimension along which to propagate values when\\n2272.             filling.\\n2273.         limit : int, default: None\\n2274.             The maximum number of consecutive NaN values to backward fill. In\\n2275.             other words, if there is a gap with more than this number of\\n2276.             consecutive NaNs, it will only be partially filled. Must be greater\\n2277.             than 0 or None for no limit.\\n2278. \\n2279.         Returns\\n2280.         -------\\n2281.         DataArray\\n2282.         """\\n2283.         from .missing import bfill\\n2284. \\n2285.         return bfill(self, dim, limit=limit)\\n2286. \\n2287.     def combine_first(self, other: "DataArray") -> "DataArray":\\n2288.         """Combine two DataArray objects, with union of coordinates.\\n2289. \\n2290.         This operation follows the normal broadcasting and alignment rules of\\n2291.         ``join=\\\'outer\\\'``.  Default to non-null values of array calling the\\n2292.         method.  Use np.nan to fill in vacant cells after alignment.\\n2293. \\n2294.         Parameters\\n2295.         ----------\\n2296.         other : DataArray\\n2297.             Used to fill all matching missing values in this array.\\n2298. \\n2299.         Returns\\n2300.         -------\\n2301.         DataArray\\n2302.         """\\n2303.         return ops.fillna(self, other, join="outer")\\n2304. \\n2305.     def reduce(\\n2306.         self,\\n2307.         func: Callable[..., Any],\\n2308.         dim: Union[None, Hashable, Sequence[Hashable]] = None,\\n2309.         axis: Union[None, int, Sequence[int]] = None,\\n2310.         keep_attrs: bool = None,\\n2311.         keepdims: bool = False,\\n2312.         **kwargs: Any,\\n2313.     ) -> "DataArray":\\n2314.         """Reduce this array by applying `func` along some dimension(s).\\n2315. \\n2316.         Parameters\\n2317.         ----------\\n2318.         func : callable\\n2319.             Function which can be called in the form\\n2320.             `f(x, axis=axis, **kwargs)` to return the result of reducing an\\n2321.             np.ndarray over an integer valued axis.\\n2322.         dim : hashable or sequence of hashable, optional\\n2323.             Dimension(s) over which to apply `func`.\\n2324.         axis : int or sequence of int, optional\\n2325.             Axis(es) over which to repeatedly apply `func`. Only one of the\\n2326.             \\\'dim\\\' and \\\'axis\\\' arguments can be supplied. If neither are\\n2327.             supplied, then the reduction is calculated over the flattened array\\n2328.             (by calling `f(x)` without an axis argument).\\n2329.         keep_attrs : bool, optional\\n2330.             If True, the variable\\\'s attributes (`attrs`) will be copied from\\n2331.             the original object to the new one.  If False (default), the new\\n2332.             object will be returned without attributes.\\n2333.         keepdims : bool, default: False\\n2334.             If True, the dimensions which are reduced are left in the result\\n2335.             as dimensions of size one. Coordinates that use these dimensions\\n2336.             are removed.\\n2337.         **kwargs : dict\\n2338.             Additional keyword arguments passed on to `func`.\\n2339. \\n2340.         Returns\\n2341.         -------\\n2342.         reduced : DataArray\\n2343.             DataArray with this object\\\'s array replaced with an array with\\n2344.             summarized data and the indicated dimension(s) removed.\\n2345.         """\\n2346. \\n2347.         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)\\n2348.         return self._replace_maybe_drop_dims(var)\\n2349. \\n2350.     def to_pandas(self) -> Union["DataArray", pd.Series, pd.DataFrame]:\\n2351.         """Convert this array into a pandas object with the same shape.\\n2352. \\n2353.         The type of the returned object depends on the number of DataArray\\n2354.         dimensions:\\n2355. \\n2356.         * 0D -> `xarray.DataArray`\\n2357.         * 1D -> `pandas.Series`\\n2358.         * 2D -> `pandas.DataFrame`\\n2359. \\n2360.         Only works for arrays with 2 or fewer dimensions.\\n2361. \\n2362.         The DataArray constructor performs the inverse transformation.\\n2363.         """\\n2364.         # TODO: consolidate the info about pandas constructors and the\\n2365.         # attributes that correspond to their indexes into a separate module?\\n2366.         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}\\n2367.         try:\\n2368.             constructor = constructors[self.ndim]\\n2369.         except KeyError:\\n2370.             raise ValueError(\\n2371.                 "cannot convert arrays with %s dimensions into "\\n2372.                 "pandas objects" % self.ndim\\n2373.             )\\n2374.         indexes = [self.get_index(dim) for dim in self.dims]\\n2375.         return constructor(self.values, *indexes)\\n2376. \\n2377.     def to_dataframe(\\n2378.         self, name: Hashable = None, dim_order: List[Hashable] = None\\n2379.     ) -> pd.DataFrame:\\n2380.         """Convert this array and its coordinates into a tidy pandas.DataFrame.\\n2381. \\n2382.         The DataFrame is indexed by the Cartesian product of index coordinates\\n2383.         (in the form of a :py:class:`pandas.MultiIndex`).\\n2384. \\n2385.         Other coordinates are included as columns in the DataFrame.\\n2386. \\n2387.         Parameters\\n2388.         ----------\\n2389.         name\\n2390.             Name to give to this array (required if unnamed).\\n2391.         dim_order\\n2392.             Hierarchical dimension order for the resulting dataframe.\\n2393.             Array content is transposed to this order and then written out as flat\\n2394.             vectors in contiguous order, so the last dimension in this list\\n2395.             will be contiguous in the resulting DataFrame. This has a major\\n2396.             influence on which operations are efficient on the resulting\\n2397.             dataframe.\\n2398. \\n2399.             If provided, must include all dimensions of this DataArray. By default,\\n2400.             dimensions are sorted according to the DataArray dimensions order.\\n2401. \\n2402.         Returns\\n2403.         -------\\n2404.         result\\n2405.             DataArray as a pandas DataFrame.\\n2406. \\n2407.         """\\n2408.         if name is None:\\n2409.             name = self.name\\n2410.         if name is None:\\n2411.             raise ValueError(\\n2412.                 "cannot convert an unnamed DataArray to a "\\n2413.                 "DataFrame: use the ``name`` parameter"\\n2414.             )\\n2415. \\n2416.         # By using a unique name, we can convert a DataArray into a DataFrame\\n2417.         # even if it shares a name with one of its coordinates.\\n2418.         # I would normally use unique_name = object() but that results in a\\n2419.         # dataframe with columns in the wrong order, for reasons I have not\\n2420.         # been able to debug (possibly a pandas bug?).\\n2421.         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"\\n2422.         ds = self._to_dataset_whole(name=unique_name)\\n2423. \\n2424.         if dim_order is None:\\n2425.             ordered_dims = dict(zip(self.dims, self.shape))\\n2426.         else:\\n2427.             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)\\n2428. \\n2429.         df = ds._to_dataframe(ordered_dims)\\n2430.         df.columns = [name if c == unique_name else c for c in df.columns]\\n2431.         return df\\n2432. \\n2433.     def to_series(self) -> pd.Series:\\n2434.         """Convert this array into a pandas.Series.\\n2435. \\n2436.         The Series is indexed by the Cartesian product of index coordinates\\n2437.         (in the form of a :py:class:`pandas.MultiIndex`).\\n2438.         """\\n2439.         index = self.coords.to_index()\\n2440.         return pd.Series(self.values.reshape(-1), index=index, name=self.name)\\n2441. \\n2442.     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:\\n2443.         """Convert this array into a numpy.ma.MaskedArray\\n2444. \\n2445.         Parameters\\n2446.         ----------\\n2447.         copy : bool, default: True\\n2448.             If True make a copy of the array in the result. If False,\\n2449.             a MaskedArray view of DataArray.values is returned.\\n2450. \\n2451.         Returns\\n2452.         -------\\n2453.         result : MaskedArray\\n2454.             Masked where invalid values (nan or inf) occur.\\n2455.         """\\n2456.         values = self.values  # only compute lazy arrays once\\n2457.         isnull = pd.isnull(values)\\n2458.         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)\\n2459. \\n2460.     def to_netcdf(self, *args, **kwargs) -> Union[bytes, "Delayed", None]:\\n2461.         """Write DataArray contents to a netCDF file.\\n2462. \\n2463.         All parameters are passed directly to `xarray.Dataset.to_netcdf`.\\n2464. \\n2465.         Notes\\n2466.         -----\\n2467.         Only xarray.Dataset objects can be written to netCDF files, so\\n2468.         the xarray.DataArray is converted to a xarray.Dataset object\\n2469.         containing a single variable. If the DataArray has no name, or if the\\n2470.         name is the same as a co-ordinate name, then it is given the name\\n2471.         \\\'__xarray_dataarray_variable__\\\'.\\n2472.         """\\n2473.         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE\\n2474. \\n2475.         if self.name is None:\\n2476.             # If no name is set then use a generic xarray name\\n2477.             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\\n2478.         elif self.name in self.coords or self.name in self.dims:\\n2479.             # The name is the same as one of the coords names, which netCDF\\n2480.             # doesn\\\'t support, so rename it but keep track of the old name\\n2481.             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\\n2482.             dataset.attrs[DATAARRAY_NAME] = self.name\\n2483.         else:\\n2484.             # No problems with the name - so we\\\'re fine!\\n2485.             dataset = self.to_dataset()\\n2486. \\n2487.         return dataset.to_netcdf(*args, **kwargs)\\n2488. \\n2489.     def to_dict(self, data: bool = True) -> dict:\\n2490.         """\\n2491.         Convert this xarray.DataArray into a dictionary following xarray\\n2492.         naming conventions.\\n2493. \\n2494.         Converts all variables and attributes to native Python objects.\\n2495.         Useful for converting to json. To avoid datetime incompatibility\\n2496.         use decode_times=False kwarg in xarrray.open_dataset.\\n2497. \\n2498.         Parameters\\n2499.         ----------\\n2500.         data : bool, optional\\n2501.             Whether to include the actual data in the dictionary. When set to\\n2502.             False, returns just the schema.\\n2503. \\n2504.         See also\\n2505.         --------\\n2506.         DataArray.from_dict\\n2507.         """\\n2508.         d = self.variable.to_dict(data=data)\\n2509.         d.update({"coords": {}, "name": self.name})\\n2510.         for k in self.coords:\\n2511.             d["coords"][k] = self.coords[k].variable.to_dict(data=data)\\n2512.         return d\\n2513. \\n2514.     @classmethod\\n2515.     def from_dict(cls, d: dict) -> "DataArray":\\n2516.         """\\n2517.         Convert a dictionary into an xarray.DataArray\\n2518. \\n2519.         Input dict can take several forms:\\n2520. \\n2521.         .. code:: python\\n2522. \\n2523.             d = {"dims": ("t"), "data": x}\\n2524. \\n2525.             d = {\\n2526.                 "coords": {"t": {"dims": "t", "data": t, "attrs": {"units": "s"}}},\\n2527.                 "attrs": {"title": "air temperature"},\\n2528.                 "dims": "t",\\n2529.                 "data": x,\\n2530.                 "name": "a",\\n2531.             }\\n2532. \\n2533.         where "t" is the name of the dimesion, "a" is the name of the array,\\n2534.         and  x and t are lists, numpy.arrays, or pandas objects.\\n2535. \\n2536.         Parameters\\n2537.         ----------\\n2538.         d : dict\\n2539.             Mapping with a minimum structure of {"dims": [...], "data": [...]}\\n2540. \\n2541.         Returns\\n2542.         -------\\n2543.         obj : xarray.DataArray\\n2544. \\n2545.         See also\\n2546.         --------\\n2547.         DataArray.to_dict\\n2548.         Dataset.from_dict\\n2549.         """\\n2550.         coords = None\\n2551.         if "coords" in d:\\n2552.             try:\\n2553.                 coords = {\\n2554.                     k: (v["dims"], v["data"], v.get("attrs"))\\n2555.                     for k, v in d["coords"].items()\\n2556.                 }\\n2557.             except KeyError as e:\\n2558.                 raise ValueError(\\n2559.                     "cannot convert dict when coords are missing the key "\\n2560.                     "\\\'{dims_data}\\\'".format(dims_data=str(e.args[0]))\\n2561.                 )\\n2562.         try:\\n2563.             data = d["data"]\\n2564.         except KeyError:\\n2565.             raise ValueError("cannot convert dict without the key \\\'data\\\'\\\'")\\n2566.         else:\\n2567.             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))\\n2568.         return obj\\n2569. \\n2570.     @classmethod\\n2571.     def from_series(cls, series: pd.Series, sparse: bool = False) -> "DataArray":\\n2572.         """Convert a pandas.Series into an xarray.DataArray.\\n2573. \\n2574.         If the series\\\'s index is a MultiIndex, it will be expanded into a\\n2575.         tensor product of one-dimensional coordinates (filling in missing\\n2576.         values with NaN). Thus this operation should be the inverse of the\\n2577.         `to_series` method.\\n2578. \\n2579.         If sparse=True, creates a sparse array instead of a dense NumPy array.\\n2580.         Requires the pydata/sparse package.\\n2581. \\n2582.         See also\\n2583.         --------\\n2584.         xarray.Dataset.from_dataframe\\n2585.         """\\n2586.         temp_name = "__temporary_name"\\n2587.         df = pd.DataFrame({temp_name: series})\\n2588.         ds = Dataset.from_dataframe(df, sparse=sparse)\\n2589.         result = cast(DataArray, ds[temp_name])\\n2590.         result.name = series.name\\n2591.         return result\\n2592. \\n2593.     def to_cdms2(self) -> "cdms2_Variable":\\n2594.         """Convert this array into a cdms2.Variable\\n2595.         """\\n2596.         from ..convert import to_cdms2\\n2597. \\n2598.         return to_cdms2(self)\\n2599. \\n2600.     @classmethod\\n2601.     def from_cdms2(cls, variable: "cdms2_Variable") -> "DataArray":\\n2602.         """Convert a cdms2.Variable into an xarray.DataArray\\n2603.         """\\n2604.         from ..convert import from_cdms2\\n2605. \\n2606.         return from_cdms2(variable)\\n2607. \\n2608.     def to_iris(self) -> "iris_Cube":\\n2609.         """Convert this array into a iris.cube.Cube\\n2610.         """\\n2611.         from ..convert import to_iris\\n2612. \\n2613.         return to_iris(self)\\n2614. \\n2615.     @classmethod\\n2616.     def from_iris(cls, cube: "iris_Cube") -> "DataArray":\\n2617.         """Convert a iris.cube.Cube into an xarray.DataArray\\n2618.         """\\n2619.         from ..convert import from_iris\\n2620. \\n2621.         return from_iris(cube)\\n2622. \\n2623.     def _all_compat(self, other: "DataArray", compat_str: str) -> bool:\\n2624.         """Helper function for equals, broadcast_equals, and identical\\n2625.         """\\n2626. \\n2627.         def compat(x, y):\\n2628.             return getattr(x.variable, compat_str)(y.variable)\\n2629. \\n2630.         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(\\n2631.             self, other\\n2632.         )\\n2633. \\n2634.     def broadcast_equals(self, other: "DataArray") -> bool:\\n2635.         """Two DataArrays are broadcast equal if they are equal after\\n2636.         broadcasting them against each other such that they have the same\\n2637.         dimensions.\\n2638. \\n2639.         See Also\\n2640.         --------\\n2641.         DataArray.equals\\n2642.         DataArray.identical\\n2643.         """\\n2644.         try:\\n2645.             return self._all_compat(other, "broadcast_equals")\\n2646.         except (TypeError, AttributeError):\\n2647.             return False\\n2648. \\n2649.     def equals(self, other: "DataArray") -> bool:\\n2650.         """True if two DataArrays have the same dimensions, coordinates and\\n2651.         values; otherwise False.\\n2652. \\n2653.         DataArrays can still be equal (like pandas objects) if they have NaN\\n2654.         values in the same locations.\\n2655. \\n2656.         This method is necessary because `v1 == v2` for ``DataArray``\\n2657.         does element-wise comparisons (like numpy.ndarrays).\\n2658. \\n2659.         See Also\\n2660.         --------\\n2661.         DataArray.broadcast_equals\\n2662.         DataArray.identical\\n2663.         """\\n2664.         try:\\n2665.             return self._all_compat(other, "equals")\\n2666.         except (TypeError, AttributeError):\\n2667.             return False\\n2668. \\n2669.     def identical(self, other: "DataArray") -> bool:\\n2670.         """Like equals, but also checks the array name and attributes, and\\n2671.         attributes on all coordinates.\\n2672. \\n2673.         See Also\\n2674.         --------\\n2675.         DataArray.broadcast_equals\\n2676.         DataArray.equals\\n2677.         """\\n2678.         try:\\n2679.             return self.name == other.name and self._all_compat(other, "identical")\\n2680.         except (TypeError, AttributeError):\\n2681.             return False\\n2682. \\n2683.     def _result_name(self, other: Any = None) -> Optional[Hashable]:\\n2684.         # use the same naming heuristics as pandas:\\n2685.         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356\\n2686.         other_name = getattr(other, "name", _default)\\n2687.         if other_name is _default or other_name == self.name:\\n2688.             return self.name\\n2689.         else:\\n2690.             return None\\n2691. \\n2692.     def __array_wrap__(self, obj, context=None) -> "DataArray":\\n2693.         new_var = self.variable.__array_wrap__(obj, context)\\n2694.         return self._replace(new_var)\\n2695. \\n2696.     def __matmul__(self, obj):\\n2697.         return self.dot(obj)\\n2698. \\n2699.     def __rmatmul__(self, other):\\n2700.         # currently somewhat duplicative, as only other DataArrays are\\n2701.         # compatible with matmul\\n2702.         return computation.dot(other, self)\\n2703. \\n2704.     @staticmethod\\n2705.     def _unary_op(f: Callable[..., Any]) -> Callable[..., "DataArray"]:\\n2706.         @functools.wraps(f)\\n2707.         def func(self, *args, **kwargs):\\n2708.             with np.errstate(all="ignore"):\\n2709.                 return self.__array_wrap__(f(self.variable.data, *args, **kwargs))\\n2710. \\n2711.         return func\\n2712. \\n2713.     @staticmethod\\n2714.     def _binary_op(\\n2715.         f: Callable[..., Any],\\n2716.         reflexive: bool = False,\\n2717.         join: str = None,  # see xarray.align\\n2718.         **ignored_kwargs,\\n2719.     ) -> Callable[..., "DataArray"]:\\n2720.         @functools.wraps(f)\\n2721.         def func(self, other):\\n2722.             if isinstance(other, (Dataset, groupby.GroupBy)):\\n2723.                 return NotImplemented\\n2724.             if isinstance(other, DataArray):\\n2725.                 align_type = OPTIONS["arithmetic_join"] if join is None else join\\n2726.                 self, other = align(self, other, join=align_type, copy=False)\\n2727.             other_variable = getattr(other, "variable", other)\\n2728.             other_coords = getattr(other, "coords", None)\\n2729. \\n2730.             variable = (\\n2731.                 f(self.variable, other_variable)\\n2732.                 if not reflexive\\n2733.                 else f(other_variable, self.variable)\\n2734.             )\\n2735.             coords, indexes = self.coords._merge_raw(other_coords)\\n2736.             name = self._result_name(other)\\n2737. \\n2738.             return self._replace(variable, coords, name, indexes=indexes)\\n2739. \\n2740.         return func\\n2741. \\n2742.     @staticmethod\\n2743.     def _inplace_binary_op(f: Callable) -> Callable[..., "DataArray"]:\\n2744.         @functools.wraps(f)\\n2745.         def func(self, other):\\n2746.             if isinstance(other, groupby.GroupBy):\\n2747.                 raise TypeError(\\n2748.                     "in-place operations between a DataArray and "\\n2749.                     "a grouped object are not permitted"\\n2750.                 )\\n2751.             # n.b. we can\\\'t align other to self (with other.reindex_like(self))\\n2752.             # because `other` may be converted into floats, which would cause\\n2753.             # in-place arithmetic to fail unpredictably. Instead, we simply\\n2754.             # don\\\'t support automatic alignment with in-place arithmetic.\\n2755.             other_coords = getattr(other, "coords", None)\\n2756.             other_variable = getattr(other, "variable", other)\\n2757.             try:\\n2758.                 with self.coords._merge_inplace(other_coords):\\n2759.                     f(self.variable, other_variable)\\n2760.             except MergeError as exc:\\n2761.                 raise MergeError(\\n2762.                     "Automatic alignment is not supported for in-place operations.\\\\n"\\n2763.                     "Consider aligning the indices manually or using a not-in-place operation.\\\\n"\\n2764.                     "See https://github.com/pydata/xarray/issues/3910 for more explanations."\\n2765.                 ) from exc\\n2766.             return self\\n2767. \\n2768.         return func\\n2769. \\n2770.     def _copy_attrs_from(self, other: Union["DataArray", Dataset, Variable]) -> None:\\n2771.         self.attrs = other.attrs\\n2772. \\n2773.     plot = utils.UncachedAccessor(_PlotMethods)\\n2774. \\n2775.     def _title_for_slice(self, truncate: int = 50) -> str:\\n2776.         """\\n2777.         If the dataarray has 1 dimensional coordinates or comes from a slice\\n2778.         we can show that info in the title\\n2779. \\n2780.         Parameters\\n2781.         ----------\\n2782.         truncate : int, default: 50\\n2783.             maximum number of characters for title\\n2784. \\n2785.         Returns\\n2786.         -------\\n2787.         title : string\\n2788.             Can be used for plot titles\\n2789. \\n2790.         """\\n2791.         one_dims = []\\n2792.         for dim, coord in self.coords.items():\\n2793.             if coord.size == 1:\\n2794.                 one_dims.append(\\n2795.                     "{dim} = {v}".format(dim=dim, v=format_item(coord.values))\\n2796.                 )\\n2797. \\n2798.         title = ", ".join(one_dims)\\n2799.         if len(title) > truncate:\\n2800.             title = title[: (truncate - 3)] + "..."\\n2801. \\n2802.         return title\\n2803. \\n2804.     def diff(self, dim: Hashable, n: int = 1, label: Hashable = "upper") -> "DataArray":\\n2805.         """Calculate the n-th order discrete difference along given axis.\\n2806. \\n2807.         Parameters\\n2808.         ----------\\n2809.         dim : hashable\\n2810.             Dimension over which to calculate the finite difference.\\n2811.         n : int, optional\\n2812.             The number of times values are differenced.\\n2813.         label : hashable, optional\\n2814.             The new coordinate in dimension ``dim`` will have the\\n2815.             values of either the minuend\\\'s or subtrahend\\\'s coordinate\\n2816.             for values \\\'upper\\\' and \\\'lower\\\', respectively.  Other\\n2817.             values are not supported.\\n2818. \\n2819.         Returns\\n2820.         -------\\n2821.         difference : same type as caller\\n2822.             The n-th order finite difference of this object.\\n2823. \\n2824.         .. note::\\n2825. \\n2826.             `n` matches numpy\\\'s behavior and is different from pandas\\\' first\\n2827.             argument named `periods`.\\n2828. \\n2829. \\n2830.         Examples\\n2831.         --------\\n2832.         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])\\n2833.         >>> arr.diff("x")\\n2834.         <xarray.DataArray (x: 3)>\\n2835.         array([0, 1, 0])\\n2836.         Coordinates:\\n2837.         * x        (x) int64 2 3 4\\n2838.         >>> arr.diff("x", 2)\\n2839.         <xarray.DataArray (x: 2)>\\n2840.         array([ 1, -1])\\n2841.         Coordinates:\\n2842.         * x        (x) int64 3 4\\n2843. \\n2844.         See Also\\n2845.         --------\\n2846.         DataArray.differentiate\\n2847.         """\\n2848.         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)\\n2849.         return self._from_temp_dataset(ds)\\n2850. \\n2851.     def shift(\\n2852.         self,\\n2853.         shifts: Mapping[Hashable, int] = None,\\n2854.         fill_value: Any = dtypes.NA,\\n2855.         **shifts_kwargs: int,\\n2856.     ) -> "DataArray":\\n2857.         """Shift this array by an offset along one or more dimensions.\\n2858. \\n2859.         Only the data is moved; coordinates stay in place. Values shifted from\\n2860.         beyond array bounds are replaced by NaN. This is consistent with the\\n2861.         behavior of ``shift`` in pandas.\\n2862. \\n2863.         Parameters\\n2864.         ----------\\n2865.         shifts : mapping of hashable to int, optional\\n2866.             Integer offset to shift along each of the given dimensions.\\n2867.             Positive offsets shift to the right; negative offsets shift to the\\n2868.             left.\\n2869.         fill_value: scalar, optional\\n2870.             Value to use for newly missing values\\n2871.         **shifts_kwargs\\n2872.             The keyword arguments form of ``shifts``.\\n2873.             One of shifts or shifts_kwargs must be provided.\\n2874. \\n2875.         Returns\\n2876.         -------\\n2877.         shifted : DataArray\\n2878.             DataArray with the same coordinates and attributes but shifted\\n2879.             data.\\n2880. \\n2881.         See also\\n2882.         --------\\n2883.         roll\\n2884. \\n2885.         Examples\\n2886.         --------\\n2887. \\n2888.         >>> arr = xr.DataArray([5, 6, 7], dims="x")\\n2889.         >>> arr.shift(x=1)\\n2890.         <xarray.DataArray (x: 3)>\\n2891.         array([ nan,   5.,   6.])\\n2892.         Coordinates:\\n2893.           * x        (x) int64 0 1 2\\n2894.         """\\n2895.         variable = self.variable.shift(\\n2896.             shifts=shifts, fill_value=fill_value, **shifts_kwargs\\n2897.         )\\n2898.         return self._replace(variable=variable)\\n2899. \\n2900.     def roll(\\n2901.         self,\\n2902.         shifts: Mapping[Hashable, int] = None,\\n2903.         roll_coords: bool = None,\\n2904.         **shifts_kwargs: int,\\n2905.     ) -> "DataArray":\\n2906.         """Roll this array by an offset along one or more dimensions.\\n2907. \\n2908.         Unlike shift, roll may rotate all variables, including coordinates\\n2909.         if specified. The direction of rotation is consistent with\\n2910.         :py:func:`numpy.roll`.\\n2911. \\n2912.         Parameters\\n2913.         ----------\\n2914.         shifts : mapping of hashable to int, optional\\n2915.             Integer offset to rotate each of the given dimensions.\\n2916.             Positive offsets roll to the right; negative offsets roll to the\\n2917.             left.\\n2918.         roll_coords : bool\\n2919.             Indicates whether to  roll the coordinates by the offset\\n2920.             The current default of roll_coords (None, equivalent to True) is\\n2921.             deprecated and will change to False in a future version.\\n2922.             Explicitly pass roll_coords to silence the warning.\\n2923.         **shifts_kwargs\\n2924.             The keyword arguments form of ``shifts``.\\n2925.             One of shifts or shifts_kwargs must be provided.\\n2926. \\n2927.         Returns\\n2928.         -------\\n2929.         rolled : DataArray\\n2930.             DataArray with the same attributes but rolled data and coordinates.\\n2931. \\n2932.         See also\\n2933.         --------\\n2934.         shift\\n2935. \\n2936.         Examples\\n2937.         --------\\n2938. \\n2939.         >>> arr = xr.DataArray([5, 6, 7], dims="x")\\n2940.         >>> arr.roll(x=1)\\n2941.         <xarray.DataArray (x: 3)>\\n2942.         array([7, 5, 6])\\n2943.         Coordinates:\\n2944.           * x        (x) int64 2 0 1\\n2945.         """\\n2946.         ds = self._to_temp_dataset().roll(\\n2947.             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs\\n2948.         )\\n2949.         return self._from_temp_dataset(ds)\\n2950. \\n2951.     @property\\n2952.     def real(self) -> "DataArray":\\n2953.         return self._replace(self.variable.real)\\n2954. \\n2955.     @property\\n2956.     def imag(self) -> "DataArray":\\n2957.         return self._replace(self.variable.imag)\\n2958. \\n2959.     def dot(\\n2960.         self, other: "DataArray", dims: Union[Hashable, Sequence[Hashable], None] = None\\n2961.     ) -> "DataArray":\\n2962.         """Perform dot product of two DataArrays along their shared dims.\\n2963. \\n2964.         Equivalent to taking taking tensordot over all shared dims.\\n2965. \\n2966.         Parameters\\n2967.         ----------\\n2968.         other : DataArray\\n2969.             The other array with which the dot product is performed.\\n2970.         dims : ..., hashable or sequence of hashable, optional\\n2971.             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.\\n2972.             If not specified, then all the common dimensions are summed over.\\n2973. \\n2974.         Returns\\n2975.         -------\\n2976.         result : DataArray\\n2977.             Array resulting from the dot product over all shared dimensions.\\n2978. \\n2979.         See also\\n2980.         --------\\n2981.         dot\\n2982.         numpy.tensordot\\n2983. \\n2984.         Examples\\n2985.         --------\\n2986. \\n2987.         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\\n2988.         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])\\n2989.         >>> dm_vals = np.arange(4)\\n2990.         >>> dm = xr.DataArray(dm_vals, dims=["z"])\\n2991. \\n2992.         >>> dm.dims\\n2993.         (\\\'z\\\')\\n2994. \\n2995.         >>> da.dims\\n2996.         (\\\'x\\\', \\\'y\\\', \\\'z\\\')\\n2997. \\n2998.         >>> dot_result = da.dot(dm)\\n2999.         >>> dot_result.dims\\n3000.         (\\\'x\\\', \\\'y\\\')\\n3001. \\n3002.         """\\n3003.         if isinstance(other, Dataset):\\n3004.             raise NotImplementedError(\\n3005.                 "dot products are not yet supported with Dataset objects."\\n3006.             )\\n3007.         if not isinstance(other, DataArray):\\n3008.             raise TypeError("dot only operates on DataArrays.")\\n3009. \\n3010.         return computation.dot(self, other, dims=dims)\\n3011. \\n3012.     def sortby(\\n3013.         self,\\n3014.         variables: Union[Hashable, "DataArray", Sequence[Union[Hashable, "DataArray"]]],\\n3015.         ascending: bool = True,\\n3016.     ) -> "DataArray":\\n3017.         """Sort object by labels or values (along an axis).\\n3018. \\n3019.         Sorts the dataarray, either along specified dimensions,\\n3020.         or according to values of 1-D dataarrays that share dimension\\n3021.         with calling object.\\n3022. \\n3023.         If the input variables are dataarrays, then the dataarrays are aligned\\n3024.         (via left-join) to the calling object prior to sorting by cell values.\\n3025.         NaNs are sorted to the end, following Numpy convention.\\n3026. \\n3027.         If multiple sorts along the same dimension is\\n3028.         given, numpy\\\'s lexsort is performed along that dimension:\\n3029.         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html\\n3030.         and the FIRST key in the sequence is used as the primary sort key,\\n3031.         followed by the 2nd key, etc.\\n3032. \\n3033.         Parameters\\n3034.         ----------\\n3035.         variables : hashable, DataArray, or sequence of hashable or DataArray\\n3036.             1D DataArray objects or name(s) of 1D variable(s) in\\n3037.             coords whose values are used to sort this array.\\n3038.         ascending : bool, optional\\n3039.             Whether to sort by ascending or descending order.\\n3040. \\n3041.         Returns\\n3042.         -------\\n3043.         sorted : DataArray\\n3044.             A new dataarray where all the specified dims are sorted by dim\\n3045.             labels.\\n3046. \\n3047.         Examples\\n3048.         --------\\n3049. \\n3050.         >>> da = xr.DataArray(\\n3051.         ...     np.random.rand(5),\\n3052.         ...     coords=[pd.date_range("1/1/2000", periods=5)],\\n3053.         ...     dims="time",\\n3054.         ... )\\n3055.         >>> da\\n3056.         <xarray.DataArray (time: 5)>\\n3057.         array([ 0.965471,  0.615637,  0.26532 ,  0.270962,  0.552878])\\n3058.         Coordinates:\\n3059.           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\\n3060. \\n3061.         >>> da.sortby(da)\\n3062.         <xarray.DataArray (time: 5)>\\n3063.         array([ 0.26532 ,  0.270962,  0.552878,  0.615637,  0.965471])\\n3064.         Coordinates:\\n3065.           * time     (time) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05 ...\\n3066.         """\\n3067.         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)\\n3068.         return self._from_temp_dataset(ds)\\n3069. \\n3070.     def quantile(\\n3071.         self,\\n3072.         q: Any,\\n3073.         dim: Union[Hashable, Sequence[Hashable], None] = None,\\n3074.         interpolation: str = "linear",\\n3075.         keep_attrs: bool = None,\\n3076.         skipna: bool = True,\\n3077.     ) -> "DataArray":\\n3078.         """Compute the qth quantile of the data along the specified dimension.\\n3079. \\n3080.         Returns the qth quantiles(s) of the array elements.\\n3081. \\n3082.         Parameters\\n3083.         ----------\\n3084.         q : float or array-like of float\\n3085.             Quantile to compute, which must be between 0 and 1 inclusive.\\n3086.         dim : hashable or sequence of hashable, optional\\n3087.             Dimension(s) over which to apply quantile.\\n3088.         interpolation : {"linear", "lower", "higher", "midpoint", "nearest"}, default: "linear"\\n3089.             This optional parameter specifies the interpolation method to\\n3090.             use when the desired quantile lies between two data points\\n3091.             ``i < j``:\\n3092. \\n3093.                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is\\n3094.                   the fractional part of the index surrounded by ``i`` and\\n3095.                   ``j``.\\n3096.                 - lower: ``i``.\\n3097.                 - higher: ``j``.\\n3098.                 - nearest: ``i`` or ``j``, whichever is nearest.\\n3099.                 - midpoint: ``(i + j) / 2``.\\n3100.         keep_attrs : bool, optional\\n3101.             If True, the dataset\\\'s attributes (`attrs`) will be copied from\\n3102.             the original object to the new one.  If False (default), the new\\n3103.             object will be returned without attributes.\\n3104.         skipna : bool, optional\\n3105.             Whether to skip missing values when aggregating.\\n3106. \\n3107.         Returns\\n3108.         -------\\n3109.         quantiles : DataArray\\n3110.             If `q` is a single quantile, then the result\\n3111.             is a scalar. If multiple percentiles are given, first axis of\\n3112.             the result corresponds to the quantile and a quantile dimension\\n3113.             is added to the return array. The other dimensions are the\\n3114.             dimensions that remain after the reduction of the array.\\n3115. \\n3116.         See Also\\n3117.         --------\\n3118.         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile\\n3119. \\n3120.         Examples\\n3121.         --------\\n3122. \\n3123.         >>> da = xr.DataArray(\\n3124.         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],\\n3125.         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},\\n3126.         ...     dims=("x", "y"),\\n3127.         ... )\\n3128.         >>> da.quantile(0)  # or da.quantile(0, dim=...)\\n3129.         <xarray.DataArray ()>\\n3130.         array(0.7)\\n3131.         Coordinates:\\n3132.             quantile  float64 0.0\\n3133.         >>> da.quantile(0, dim="x")\\n3134.         <xarray.DataArray (y: 4)>\\n3135.         array([0.7, 4.2, 2.6, 1.5])\\n3136.         Coordinates:\\n3137.           * y         (y) float64 1.0 1.5 2.0 2.5\\n3138.             quantile  float64 0.0\\n3139.         >>> da.quantile([0, 0.5, 1])\\n3140.         <xarray.DataArray (quantile: 3)>\\n3141.         array([0.7, 3.4, 9.4])\\n3142.         Coordinates:\\n3143.           * quantile  (quantile) float64 0.0 0.5 1.0\\n3144.         >>> da.quantile([0, 0.5, 1], dim="x")\\n3145.         <xarray.DataArray (quantile: 3, y: 4)>\\n3146.         array([[0.7 , 4.2 , 2.6 , 1.5 ],\\n3147.                [3.6 , 5.75, 6.  , 1.7 ],\\n3148.                [6.5 , 7.3 , 9.4 , 1.9 ]])\\n3149.         Coordinates:\\n3150.           * y         (y) float64 1.0 1.5 2.0 2.5\\n3151.           * quantile  (quantile) float64 0.0 0.5 1.0\\n3152.         """\\n3153. \\n3154.         ds = self._to_temp_dataset().quantile(\\n3155.             q,\\n3156.             dim=dim,\\n3157.             keep_attrs=keep_attrs,\\n3158.             interpolation=interpolation,\\n3159.             skipna=skipna,\\n3160.         )\\n3161.         return self._from_temp_dataset(ds)\\n3162. \\n3163.     def rank(\\n3164.         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None\\n3165.     ) -> "DataArray":\\n3166.         """Ranks the data.\\n3167. \\n3168.         Equal values are assigned a rank that is the average of the ranks that\\n3169.         would have been otherwise assigned to all of the values within that\\n3170.         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.\\n3171. \\n3172.         NaNs in the input array are returned as NaNs.\\n3173. \\n3174.         The `bottleneck` library is required.\\n3175. \\n3176.         Parameters\\n3177.         ----------\\n3178.         dim : hashable\\n3179.             Dimension over which to compute rank.\\n3180.         pct : bool, optional\\n3181.             If True, compute percentage ranks, otherwise compute integer ranks.\\n3182.         keep_attrs : bool, optional\\n3183.             If True, the dataset\\\'s attributes (`attrs`) will be copied from\\n3184.             the original object to the new one.  If False (default), the new\\n3185.             object will be returned without attributes.\\n3186. \\n3187.         Returns\\n3188.         -------\\n3189.         ranked : DataArray\\n3190.             DataArray with the same coordinates and dtype \\\'float64\\\'.\\n3191. \\n3192.         Examples\\n3193.         --------\\n3194. \\n3195.         >>> arr = xr.DataArray([5, 6, 7], dims="x")\\n3196.         >>> arr.rank("x")\\n3197.         <xarray.DataArray (x: 3)>\\n3198.         array([ 1.,   2.,   3.])\\n3199.         Dimensions without coordinates: x\\n3200.         """\\n3201. \\n3202.         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)\\n3203.         return self._from_temp_dataset(ds)\\n3204. \\n3205.     def differentiate(\\n3206.         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None\\n3207.     ) -> "DataArray":\\n3208.         """ Differentiate the array with the second order accurate central\\n3209.         differences.\\n3210. \\n3211.         .. note::\\n3212.             This feature is limited to simple cartesian geometry, i.e. coord\\n3213.             must be one dimensional.\\n3214. \\n3215.         Parameters\\n3216.         ----------\\n3217.         coord : hashable\\n3218.             The coordinate to be used to compute the gradient.\\n3219.         edge_order : {1, 2}, default: 1\\n3220.             N-th order accurate differences at the boundaries.\\n3221.         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \\\\\\n3222.                          "us", "ns", "ps", "fs", "as"} or None, optional\\n3223.             Unit to compute gradient. Only valid for datetime coordinate.\\n3224. \\n3225.         Returns\\n3226.         -------\\n3227.         differentiated: DataArray\\n3228. \\n3229.         See also\\n3230.         --------\\n3231.         numpy.gradient: corresponding numpy function\\n3232. \\n3233.         Examples\\n3234.         --------\\n3235. \\n3236.         >>> da = xr.DataArray(\\n3237.         ...     np.arange(12).reshape(4, 3),\\n3238.         ...     dims=["x", "y"],\\n3239.         ...     coords={"x": [0, 0.1, 1.1, 1.2]},\\n3240.         ... )\\n3241.         >>> da\\n3242.         <xarray.DataArray (x: 4, y: 3)>\\n3243.         array([[ 0,  1,  2],\\n3244.                [ 3,  4,  5],\\n3245.                [ 6,  7,  8],\\n3246.                [ 9, 10, 11]])\\n3247.         Coordinates:\\n3248.           * x        (x) float64 0.0 0.1 1.1 1.2\\n3249.         Dimensions without coordinates: y\\n3250.         >>>\\n3251.         >>> da.differentiate("x")\\n3252.         <xarray.DataArray (x: 4, y: 3)>\\n3253.         array([[30.      , 30.      , 30.      ],\\n3254.                [27.545455, 27.545455, 27.545455],\\n3255.                [27.545455, 27.545455, 27.545455],\\n3256.                [30.      , 30.      , 30.      ]])\\n3257.         Coordinates:\\n3258.           * x        (x) float64 0.0 0.1 1.1 1.2\\n3259.         Dimensions without coordinates: y\\n3260.         """\\n3261.         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)\\n3262.         return self._from_temp_dataset(ds)\\n3263. \\n3264.     def integrate(\\n3265.         self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None\\n3266.     ) -> "DataArray":\\n3267.         """ integrate the array with the trapezoidal rule.\\n3268. \\n3269.         .. note::\\n3270.             This feature is limited to simple cartesian geometry, i.e. dim\\n3271.             must be one dimensional.\\n3272. \\n3273.         Parameters\\n3274.         ----------\\n3275.         dim : hashable, or sequence of hashable\\n3276.             Coordinate(s) used for the integration.\\n3277.         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", "us", "ns", \\\\\\n3278.                          "ps", "fs", "as"}, optional\\n3279.             Can be used to specify the unit if datetime coordinate is used.\\n3280. \\n3281.         Returns\\n3282.         -------\\n3283.         integrated: DataArray\\n3284. \\n3285.         See also\\n3286.         --------\\n3287.         numpy.trapz: corresponding numpy function\\n3288. \', \'start_line_number\': 1, \'end_line_number\': 3288}\n\n{\'relative_path\': \'xarray/core/duck_array_ops.py\', \'content\': \'1. """Compatibility module defining operations on duck numpy-arrays.\\n2. \\n3. Currently, this means Dask or NumPy arrays. None of these functions should\\n4. accept or return xarray objects.\\n5. """\\n6. import contextlib\\n7. import datetime\\n8. import inspect\\n9. import warnings\\n10. from distutils.version import LooseVersion\\n11. from functools import partial\\n12. \\n13. import numpy as np\\n14. import pandas as pd\\n15. \\n16. from . import dask_array_compat, dask_array_ops, dtypes, npcompat, nputils\\n17. from .nputils import nanfirst, nanlast\\n18. from .pycompat import cupy_array_type, dask_array_type, sparse_array_type\\n19. \\n20. try:\\n21.     import dask.array as dask_array\\n22. except ImportError:\\n23.     dask_array = None  # type: ignore\\n24. \\n25. \\n26. def _dask_or_eager_func(\\n27.     name,\\n28.     eager_module=np,\\n29.     dask_module=dask_array,\\n30.     list_of_args=False,\\n31.     array_args=slice(1),\\n32.     requires_dask=None,\\n33. ):\\n34.     """Create a function that dispatches to dask for dask array inputs."""\\n35.     if dask_module is not None:\\n36. \\n37.         def f(*args, **kwargs):\\n38.             if list_of_args:\\n39.                 dispatch_args = args[0]\\n40.             else:\\n41.                 dispatch_args = args[array_args]\\n42.             if any(isinstance(a, dask_array_type) for a in dispatch_args):\\n43.                 try:\\n44.                     wrapped = getattr(dask_module, name)\\n45.                 except AttributeError as e:\\n46.                     raise AttributeError(f"{e}: requires dask >={requires_dask}")\\n47.             else:\\n48.                 wrapped = getattr(eager_module, name)\\n49.             return wrapped(*args, **kwargs)\\n50. \\n51.     else:\\n52. \\n53.         def f(*args, **kwargs):\\n54.             return getattr(eager_module, name)(*args, **kwargs)\\n55. \\n56.     return f\\n57. \\n58. \\n59. def fail_on_dask_array_input(values, msg=None, func_name=None):\\n60.     if isinstance(values, dask_array_type):\\n61.         if msg is None:\\n62.             msg = "%r is not yet a valid method on dask arrays"\\n63.         if func_name is None:\\n64.             func_name = inspect.stack()[1][3]\\n65.         raise NotImplementedError(msg % func_name)\\n66. \\n67. \\n68. # switch to use dask.array / __array_function__ version when dask supports it:\\n69. # https://github.com/dask/dask/pull/4822\\n70. moveaxis = npcompat.moveaxis\\n71. \\n72. around = _dask_or_eager_func("around")\\n73. isclose = _dask_or_eager_func("isclose")\\n74. \\n75. \\n76. isnat = np.isnat\\n77. isnan = _dask_or_eager_func("isnan")\\n78. zeros_like = _dask_or_eager_func("zeros_like")\\n79. \\n80. \\n81. pandas_isnull = _dask_or_eager_func("isnull", eager_module=pd)\\n82. \\n83. \\n84. def isnull(data):\\n85.     data = asarray(data)\\n86.     scalar_type = data.dtype.type\\n87.     if issubclass(scalar_type, (np.datetime64, np.timedelta64)):\\n88.         # datetime types use NaT for null\\n89.         # note: must check timedelta64 before integers, because currently\\n90.         # timedelta64 inherits from np.integer\\n91.         return isnat(data)\\n92.     elif issubclass(scalar_type, np.inexact):\\n93.         # float types use NaN for null\\n94.         return isnan(data)\\n95.     elif issubclass(scalar_type, (np.bool_, np.integer, np.character, np.void)):\\n96.         # these types cannot represent missing values\\n97.         return zeros_like(data, dtype=bool)\\n98.     else:\\n99.         # at this point, array should have dtype=object\\n100.         if isinstance(data, (np.ndarray, dask_array_type)):\\n101.             return pandas_isnull(data)\\n102.         else:\\n103.             # Not reachable yet, but intended for use with other duck array\\n104.             # types. For full consistency with pandas, we should accept None as\\n105.             # a null value as well as NaN, but it isn\\\'t clear how to do this\\n106.             # with duck typing.\\n107.             return data != data\\n108. \\n109. \\n110. def notnull(data):\\n111.     return ~isnull(data)\\n112. \\n113. \\n114. transpose = _dask_or_eager_func("transpose")\\n115. _where = _dask_or_eager_func("where", array_args=slice(3))\\n116. isin = _dask_or_eager_func("isin", array_args=slice(2))\\n117. take = _dask_or_eager_func("take")\\n118. broadcast_to = _dask_or_eager_func("broadcast_to")\\n119. pad = _dask_or_eager_func("pad", dask_module=dask_array_compat)\\n120. \\n121. _concatenate = _dask_or_eager_func("concatenate", list_of_args=True)\\n122. _stack = _dask_or_eager_func("stack", list_of_args=True)\\n123. \\n124. array_all = _dask_or_eager_func("all")\\n125. array_any = _dask_or_eager_func("any")\\n126. \\n127. tensordot = _dask_or_eager_func("tensordot", array_args=slice(2))\\n128. einsum = _dask_or_eager_func("einsum", array_args=slice(1, None))\\n129. \\n130. \\n131. def gradient(x, coord, axis, edge_order):\\n132.     if isinstance(x, dask_array_type):\\n133.         return dask_array.gradient(x, coord, axis=axis, edge_order=edge_order)\\n134.     return np.gradient(x, coord, axis=axis, edge_order=edge_order)\\n135. \\n136. \\n137. def trapz(y, x, axis):\\n138.     if axis < 0:\\n139.         axis = y.ndim + axis\\n140.     x_sl1 = (slice(1, None),) + (None,) * (y.ndim - axis - 1)\\n141.     x_sl2 = (slice(None, -1),) + (None,) * (y.ndim - axis - 1)\\n142.     slice1 = (slice(None),) * axis + (slice(1, None),)\\n143.     slice2 = (slice(None),) * axis + (slice(None, -1),)\\n144.     dx = x[x_sl1] - x[x_sl2]\\n145.     integrand = dx * 0.5 * (y[tuple(slice1)] + y[tuple(slice2)])\\n146.     return sum(integrand, axis=axis, skipna=False)\\n147. \\n148. \\n149. masked_invalid = _dask_or_eager_func(\\n150.     "masked_invalid", eager_module=np.ma, dask_module=getattr(dask_array, "ma", None)\\n151. )\\n152. \\n153. \\n154. def astype(data, **kwargs):\\n155.     try:\\n156.         import sparse\\n157.     except ImportError:\\n158.         sparse = None\\n159. \\n160.     if (\\n161.         sparse is not None\\n162.         and isinstance(data, sparse_array_type)\\n163.         and LooseVersion(sparse.__version__) < LooseVersion("0.11.0")\\n164.         and "casting" in kwargs\\n165.     ):\\n166.         warnings.warn(\\n167.             "The current version of sparse does not support the \\\'casting\\\' argument. It will be ignored in the call to astype().",\\n168.             RuntimeWarning,\\n169.             stacklevel=4,\\n170.         )\\n171.         kwargs.pop("casting")\\n172. \\n173.     return data.astype(**kwargs)\\n174. \\n175. \\n176. def asarray(data, xp=np):\\n177.     return (\\n178.         data\\n179.         if (isinstance(data, dask_array_type) or hasattr(data, "__array_function__"))\\n180.         else xp.asarray(data)\\n181.     )\\n182. \\n183. \\n184. def as_shared_dtype(scalars_or_arrays):\\n185.     """Cast a arrays to a shared dtype using xarray\\\'s type promotion rules."""\\n186. \\n187.     if any([isinstance(x, cupy_array_type) for x in scalars_or_arrays]):\\n188.         import cupy as cp\\n189. \\n190.         arrays = [asarray(x, xp=cp) for x in scalars_or_arrays]\\n191.     else:\\n192.         arrays = [asarray(x) for x in scalars_or_arrays]\\n193.     # Pass arrays directly instead of dtypes to result_type so scalars\\n194.     # get handled properly.\\n195.     # Note that result_type() safely gets the dtype from dask arrays without\\n196.     # evaluating them.\\n197.     out_type = dtypes.result_type(*arrays)\\n198.     return [x.astype(out_type, copy=False) for x in arrays]\\n199. \\n200. \\n201. def lazy_array_equiv(arr1, arr2):\\n202.     """Like array_equal, but doesn\\\'t actually compare values.\\n203.        Returns True when arr1, arr2 identical or their dask names are equal.\\n204.        Returns False when shapes are not equal.\\n205.        Returns None when equality cannot determined: one or both of arr1, arr2 are numpy arrays;\\n206.        or their dask names are not equal\\n207.     """\\n208.     if arr1 is arr2:\\n209.         return True\\n210.     arr1 = asarray(arr1)\\n211.     arr2 = asarray(arr2)\\n212.     if arr1.shape != arr2.shape:\\n213.         return False\\n214.     if (\\n215.         dask_array\\n216.         and isinstance(arr1, dask_array_type)\\n217.         and isinstance(arr2, dask_array_type)\\n218.     ):\\n219.         # GH3068\\n220.         if arr1.name == arr2.name:\\n221.             return True\\n222.         else:\\n223.             return None\\n224.     return None\\n225. \\n226. \\n227. def allclose_or_equiv(arr1, arr2, rtol=1e-5, atol=1e-8):\\n228.     """Like np.allclose, but also allows values to be NaN in both arrays\\n229.     """\\n230.     arr1 = asarray(arr1)\\n231.     arr2 = asarray(arr2)\\n232. \\n233.     lazy_equiv = lazy_array_equiv(arr1, arr2)\\n234.     if lazy_equiv is None:\\n235.         return bool(isclose(arr1, arr2, rtol=rtol, atol=atol, equal_nan=True).all())\\n236.     else:\\n237.         return lazy_equiv\\n238. \\n239. \\n240. def array_equiv(arr1, arr2):\\n241.     """Like np.array_equal, but also allows values to be NaN in both arrays\\n242.     """\\n243.     arr1 = asarray(arr1)\\n244.     arr2 = asarray(arr2)\\n245.     lazy_equiv = lazy_array_equiv(arr1, arr2)\\n246.     if lazy_equiv is None:\\n247.         with warnings.catch_warnings():\\n248.             warnings.filterwarnings("ignore", "In the future, \\\'NAT == x\\\'")\\n249.             flag_array = (arr1 == arr2) | (isnull(arr1) & isnull(arr2))\\n250.             return bool(flag_array.all())\\n251.     else:\\n252.         return lazy_equiv\\n253. \\n254. \\n255. def array_notnull_equiv(arr1, arr2):\\n256.     """Like np.array_equal, but also allows values to be NaN in either or both\\n257.     arrays\\n258.     """\\n259.     arr1 = asarray(arr1)\\n260.     arr2 = asarray(arr2)\\n261.     lazy_equiv = lazy_array_equiv(arr1, arr2)\\n262.     if lazy_equiv is None:\\n263.         with warnings.catch_warnings():\\n264.             warnings.filterwarnings("ignore", "In the future, \\\'NAT == x\\\'")\\n265.             flag_array = (arr1 == arr2) | isnull(arr1) | isnull(arr2)\\n266.             return bool(flag_array.all())\\n267.     else:\\n268.         return lazy_equiv\\n269. \\n270. \\n271. def count(data, axis=None):\\n272.     """Count the number of non-NA in this array along the given axis or axes\\n273.     """\\n274.     return np.sum(np.logical_not(isnull(data)), axis=axis)\\n275. \\n276. \\n277. def where(condition, x, y):\\n278.     """Three argument where() with better dtype promotion rules."""\\n279.     return _where(condition, *as_shared_dtype([x, y]))\\n280. \\n281. \\n282. def where_method(data, cond, other=dtypes.NA):\\n283.     if other is dtypes.NA:\\n284.         other = dtypes.get_fill_value(data.dtype)\\n285.     return where(cond, data, other)\\n286. \\n287. \\n288. def fillna(data, other):\\n289.     # we need to pass data first so pint has a chance of returning the\\n290.     # correct unit\\n291.     # TODO: revert after https://github.com/hgrecco/pint/issues/1019 is fixed\\n292.     return where(notnull(data), data, other)\\n293. \\n294. \\n295. def concatenate(arrays, axis=0):\\n296.     """concatenate() with better dtype promotion rules."""\\n297.     return _concatenate(as_shared_dtype(arrays), axis=axis)\\n298. \\n299. \\n300. def stack(arrays, axis=0):\\n301.     """stack() with better dtype promotion rules."""\\n302.     return _stack(as_shared_dtype(arrays), axis=axis)\\n303. \\n304. \\n305. @contextlib.contextmanager\\n306. def _ignore_warnings_if(condition):\\n307.     if condition:\\n308.         with warnings.catch_warnings():\\n309.             warnings.simplefilter("ignore")\\n310.             yield\\n311.     else:\\n312.         yield\\n313. \\n314. \\n315. def _create_nan_agg_method(name, dask_module=dask_array, coerce_strings=False):\\n316.     from . import nanops\\n317. \\n318.     def f(values, axis=None, skipna=None, **kwargs):\\n319.         if kwargs.pop("out", None) is not None:\\n320.             raise TypeError(f"`out` is not valid for {name}")\\n321. \\n322.         values = asarray(values)\\n323. \\n324.         if coerce_strings and values.dtype.kind in "SU":\\n325.             values = values.astype(object)\\n326. \\n327.         func = None\\n328.         if skipna or (skipna is None and values.dtype.kind in "cfO"):\\n329.             nanname = "nan" + name\\n330.             func = getattr(nanops, nanname)\\n331.         else:\\n332.             func = _dask_or_eager_func(name, dask_module=dask_module)\\n333. \\n334.         try:\\n335.             return func(values, axis=axis, **kwargs)\\n336.         except AttributeError:\\n337.             if not isinstance(values, dask_array_type):\\n338.                 raise\\n339.             try:  # dask/dask#3133 dask sometimes needs dtype argument\\n340.                 # if func does not accept dtype, then raises TypeError\\n341.                 return func(values, axis=axis, dtype=values.dtype, **kwargs)\\n342.             except (AttributeError, TypeError):\\n343.                 raise NotImplementedError(\\n344.                     f"{name} is not yet implemented on dask arrays"\\n345.                 )\\n346. \\n347.     f.__name__ = name\\n348.     return f\\n349. \\n350. \\n351. # Attributes `numeric_only`, `available_min_count` is used for docs.\\n352. # See ops.inject_reduce_methods\\n353. argmax = _create_nan_agg_method("argmax", coerce_strings=True)\\n354. argmin = _create_nan_agg_method("argmin", coerce_strings=True)\\n355. max = _create_nan_agg_method("max", coerce_strings=True)\\n356. min = _create_nan_agg_method("min", coerce_strings=True)\\n357. sum = _create_nan_agg_method("sum")\\n358. sum.numeric_only = True\\n359. sum.available_min_count = True\\n360. std = _create_nan_agg_method("std")\\n361. std.numeric_only = True\\n362. var = _create_nan_agg_method("var")\\n363. var.numeric_only = True\\n364. median = _create_nan_agg_method("median", dask_module=dask_array_compat)\\n365. median.numeric_only = True\\n366. prod = _create_nan_agg_method("prod")\\n367. prod.numeric_only = True\\n368. sum.available_min_count = True\\n369. cumprod_1d = _create_nan_agg_method("cumprod")\\n370. cumprod_1d.numeric_only = True\\n371. cumsum_1d = _create_nan_agg_method("cumsum")\\n372. cumsum_1d.numeric_only = True\\n373. unravel_index = _dask_or_eager_func("unravel_index")\\n374. \\n375. \\n376. _mean = _create_nan_agg_method("mean")\\n377. \\n378. \\n379. def _datetime_nanmin(array):\\n380.     """nanmin() function for datetime64.\\n381. \\n382.     Caveats that this function deals with:\\n383. \\n384.     - In numpy < 1.18, min() on datetime64 incorrectly ignores NaT\\n385.     - numpy nanmin() don\\\'t work on datetime64 (all versions at the moment of writing)\\n386.     - dask min() does not work on datetime64 (all versions at the moment of writing)\\n387.     """\\n388.     assert array.dtype.kind in "mM"\\n389.     dtype = array.dtype\\n390.     # (NaT).astype(float) does not produce NaN...\\n391.     array = where(pandas_isnull(array), np.nan, array.astype(float))\\n392.     array = min(array, skipna=True)\\n393.     if isinstance(array, float):\\n394.         array = np.array(array)\\n395.     # ...but (NaN).astype("M8") does produce NaT\\n396.     return array.astype(dtype)\\n397. \\n398. \\n399. def datetime_to_numeric(array, offset=None, datetime_unit=None, dtype=float):\\n400.     """Convert an array containing datetime-like data to numerical values.\\n401. \\n402.     Convert the datetime array to a timedelta relative to an offset.\\n403. \\n404.     Parameters\\n405.     ----------\\n406.     da : array-like\\n407.       Input data\\n408.     offset: None, datetime or cftime.datetime\\n409.       Datetime offset. If None, this is set by default to the array\\\'s minimum\\n410.       value to reduce round off errors.\\n411.     datetime_unit: {None, Y, M, W, D, h, m, s, ms, us, ns, ps, fs, as}\\n412.       If not None, convert output to a given datetime unit. Note that some\\n413.       conversions are not allowed due to non-linear relationships between units.\\n414.     dtype: dtype\\n415.       Output dtype.\\n416. \\n417.     Returns\\n418.     -------\\n419.     array\\n420.       Numerical representation of datetime object relative to an offset.\\n421. \\n422.     Notes\\n423.     -----\\n424.     Some datetime unit conversions won\\\'t work, for example from days to years, even\\n425.     though some calendars would allow for them (e.g. no_leap). This is because there\\n426.     is no `cftime.timedelta` object.\\n427.     """\\n428.     # TODO: make this function dask-compatible?\\n429.     # Set offset to minimum if not given\\n430.     if offset is None:\\n431.         if array.dtype.kind in "Mm":\\n432.             offset = _datetime_nanmin(array)\\n433.         else:\\n434.             offset = min(array)\\n435. \\n436.     # Compute timedelta object.\\n437.     # For np.datetime64, this can silently yield garbage due to overflow.\\n438.     # One option is to enforce 1970-01-01 as the universal offset.\\n439.     array = array - offset\\n440. \\n441.     # Scalar is converted to 0d-array\\n442.     if not hasattr(array, "dtype"):\\n443.         array = np.array(array)\\n444. \\n445.     # Convert timedelta objects to float by first converting to microseconds.\\n446.     if array.dtype.kind in "O":\\n447.         return py_timedelta_to_float(array, datetime_unit or "ns").astype(dtype)\\n448. \\n449.     # Convert np.NaT to np.nan\\n450.     elif array.dtype.kind in "mM":\\n451. \\n452.         # Convert to specified timedelta units.\\n453.         if datetime_unit:\\n454.             array = array / np.timedelta64(1, datetime_unit)\\n455.         return np.where(isnull(array), np.nan, array.astype(dtype))\\n456. \\n457. \\n458. def timedelta_to_numeric(value, datetime_unit="ns", dtype=float):\\n459.     """Convert a timedelta-like object to numerical values.\\n460. \\n461.     Parameters\\n462.     ----------\\n463.     value : datetime.timedelta, numpy.timedelta64, pandas.Timedelta, str\\n464.       Time delta representation.\\n465.     datetime_unit : {Y, M, W, D, h, m, s, ms, us, ns, ps, fs, as}\\n466.       The time units of the output values. Note that some conversions are not allowed due to\\n467.       non-linear relationships between units.\\n468.     dtype : type\\n469.       The output data type.\\n470. \\n471.     """\\n472.     import datetime as dt\\n473. \\n474.     if isinstance(value, dt.timedelta):\\n475.         out = py_timedelta_to_float(value, datetime_unit)\\n476.     elif isinstance(value, np.timedelta64):\\n477.         out = np_timedelta64_to_float(value, datetime_unit)\\n478.     elif isinstance(value, pd.Timedelta):\\n479.         out = pd_timedelta_to_float(value, datetime_unit)\\n480.     elif isinstance(value, str):\\n481.         try:\\n482.             a = pd.to_timedelta(value)\\n483.         except ValueError:\\n484.             raise ValueError(\\n485.                 f"Could not convert {value!r} to timedelta64 using pandas.to_timedelta"\\n486.             )\\n487.         return py_timedelta_to_float(a, datetime_unit)\\n488.     else:\\n489.         raise TypeError(\\n490.             f"Expected value of type str, pandas.Timedelta, datetime.timedelta "\\n491.             f"or numpy.timedelta64, but received {type(value).__name__}"\\n492.         )\\n493.     return out.astype(dtype)\\n494. \\n495. \\n496. def _to_pytimedelta(array, unit="us"):\\n497.     return array.astype(f"timedelta64[{unit}]").astype(datetime.timedelta)\\n498. \\n499. \\n500. def np_timedelta64_to_float(array, datetime_unit):\\n501.     """Convert numpy.timedelta64 to float.\\n502. \\n503.     Notes\\n504.     -----\\n505.     The array is first converted to microseconds, which is less likely to\\n506.     cause overflow errors.\\n507.     """\\n508.     array = array.astype("timedelta64[ns]").astype(np.float64)\\n509.     conversion_factor = np.timedelta64(1, "ns") / np.timedelta64(1, datetime_unit)\\n510.     return conversion_factor * array\\n511. \\n512. \\n513. def pd_timedelta_to_float(value, datetime_unit):\\n514.     """Convert pandas.Timedelta to float.\\n515. \\n516.     Notes\\n517.     -----\\n518.     Built on the assumption that pandas timedelta values are in nanoseconds,\\n519.     which is also the numpy default resolution.\\n520.     """\\n521.     value = value.to_timedelta64()\\n522.     return np_timedelta64_to_float(value, datetime_unit)\\n523. \\n524. \\n525. def py_timedelta_to_float(array, datetime_unit):\\n526.     """Convert a timedelta object to a float, possibly at a loss of resolution.\\n527.     """\\n528.     array = np.asarray(array)\\n529.     array = np.reshape([a.total_seconds() for a in array.ravel()], array.shape) * 1e6\\n530.     conversion_factor = np.timedelta64(1, "us") / np.timedelta64(1, datetime_unit)\\n531.     return conversion_factor * array\\n532. \\n533. \\n534. def mean(array, axis=None, skipna=None, **kwargs):\\n535.     """inhouse mean that can handle np.datetime64 or cftime.datetime\\n536.     dtypes"""\\n537.     from .common import _contains_cftime_datetimes\\n538. \\n539.     array = asarray(array)\\n540.     if array.dtype.kind in "Mm":\\n541.         offset = _datetime_nanmin(array)\\n542. \\n543.         # xarray always uses np.datetime64[ns] for np.datetime64 data\\n544.         dtype = "timedelta64[ns]"\\n545.         return (\\n546.             _mean(\\n547.                 datetime_to_numeric(array, offset), axis=axis, skipna=skipna, **kwargs\\n548.             ).astype(dtype)\\n549.             + offset\\n550.         )\\n551.     elif _contains_cftime_datetimes(array):\\n552.         if isinstance(array, dask_array_type):\\n553.             raise NotImplementedError(\\n554.                 "Computing the mean of an array containing "\\n555.                 "cftime.datetime objects is not yet implemented on "\\n556.                 "dask arrays."\\n557.             )\\n558.         offset = min(array)\\n559.         timedeltas = datetime_to_numeric(array, offset, datetime_unit="us")\\n560.         mean_timedeltas = _mean(timedeltas, axis=axis, skipna=skipna, **kwargs)\\n561.         return _to_pytimedelta(mean_timedeltas, unit="us") + offset\\n562.     else:\\n563.         return _mean(array, axis=axis, skipna=skipna, **kwargs)\\n564. \\n565. \\n566. mean.numeric_only = True  # type: ignore\\n567. \\n568. \\n569. def _nd_cum_func(cum_func, array, axis, **kwargs):\\n570.     array = asarray(array)\\n571.     if axis is None:\\n572.         axis = tuple(range(array.ndim))\\n573.     if isinstance(axis, int):\\n574.         axis = (axis,)\\n575. \\n576.     out = array\\n577.     for ax in axis:\\n578.         out = cum_func(out, axis=ax, **kwargs)\\n579.     return out\\n580. \\n581. \\n582. def cumprod(array, axis=None, **kwargs):\\n583.     """N-dimensional version of cumprod."""\\n584.     return _nd_cum_func(cumprod_1d, array, axis, **kwargs)\\n585. \\n586. \\n587. def cumsum(array, axis=None, **kwargs):\\n588.     """N-dimensional version of cumsum."""\\n589.     return _nd_cum_func(cumsum_1d, array, axis, **kwargs)\\n590. \\n591. \\n592. _fail_on_dask_array_input_skipna = partial(\\n593.     fail_on_dask_array_input,\\n594.     msg="%r with skipna=True is not yet implemented on dask arrays",\\n595. )\\n596. \\n597. \\n598. def first(values, axis, skipna=None):\\n599.     """Return the first non-NA elements in this array along the given axis\\n600.     """\\n601.     if (skipna or skipna is None) and values.dtype.kind not in "iSU":\\n602.         # only bother for dtypes that can hold NaN\\n603.         _fail_on_dask_array_input_skipna(values)\\n604.         return nanfirst(values, axis)\\n605.     return take(values, 0, axis=axis)\\n606. \\n607. \\n608. def last(values, axis, skipna=None):\\n609.     """Return the last non-NA elements in this array along the given axis\\n610.     """\\n611.     if (skipna or skipna is None) and values.dtype.kind not in "iSU":\\n612.         # only bother for dtypes that can hold NaN\\n613.         _fail_on_dask_array_input_skipna(values)\\n614.         return nanlast(values, axis)\\n615.     return take(values, -1, axis=axis)\\n616. \\n617. \\n618. def rolling_window(array, axis, window, center, fill_value):\\n619.     """\\n620.     Make an ndarray with a rolling window of axis-th dimension.\\n621.     The rolling dimension will be placed at the last dimension.\\n622.     """\\n623.     if isinstance(array, dask_array_type):\\n624.         return dask_array_ops.rolling_window(array, axis, window, center, fill_value)\\n625.     else:  # np.ndarray\\n626.         return nputils.rolling_window(array, axis, window, center, fill_value)\\n627. \\n628. \\n629. def least_squares(lhs, rhs, rcond=None, skipna=False):\\n630.     """Return the coefficients and residuals of a least-squares fit.\\n631.     """\\n632.     if isinstance(rhs, dask_array_type):\\n633.         return dask_array_ops.least_squares(lhs, rhs, rcond=rcond, skipna=skipna)\\n634.     else:\\n635.         return nputils.least_squares(lhs, rhs, rcond=rcond, skipna=skipna)\', \'start_line_number\': 1, \'end_line_number\': 635}\n\n{\'relative_path\': \'xarray/core/nanops.py\', \'content\': \'1. import numpy as np\\n2. \\n3. from . import dtypes, nputils, utils\\n4. from .duck_array_ops import _dask_or_eager_func, count, fillna, isnull, where_method\\n5. from .pycompat import dask_array_type\\n6. \\n7. try:\\n8.     import dask.array as dask_array\\n9. \\n10.     from . import dask_array_compat\\n11. except ImportError:\\n12.     dask_array = None\\n13.     dask_array_compat = None  # type: ignore\\n14. \\n15. \\n16. def _replace_nan(a, val):\\n17.     """\\n18.     replace nan in a by val, and returns the replaced array and the nan\\n19.     position\\n20.     """\\n21.     mask = isnull(a)\\n22.     return where_method(val, mask, a), mask\\n23. \\n24. \\n25. def _maybe_null_out(result, axis, mask, min_count=1):\\n26.     """\\n27.     xarray version of pandas.core.nanops._maybe_null_out\\n28.     """\\n29.     if hasattr(axis, "__len__"):  # if tuple or list\\n30.         raise ValueError(\\n31.             "min_count is not available for reduction with more than one dimensions."\\n32.         )\\n33. \\n34.     if axis is not None and getattr(result, "ndim", False):\\n35.         null_mask = (mask.shape[axis] - mask.sum(axis) - min_count) < 0\\n36.         if null_mask.any():\\n37.             dtype, fill_value = dtypes.maybe_promote(result.dtype)\\n38.             result = result.astype(dtype)\\n39.             result[null_mask] = fill_value\\n40. \\n41.     elif getattr(result, "dtype", None) not in dtypes.NAT_TYPES:\\n42.         null_mask = mask.size - mask.sum()\\n43.         if null_mask < min_count:\\n44.             result = np.nan\\n45. \\n46.     return result\\n47. \\n48. \\n49. def _nan_argminmax_object(func, fill_value, value, axis=None, **kwargs):\\n50.     """ In house nanargmin, nanargmax for object arrays. Always return integer\\n51.     type\\n52.     """\\n53.     valid_count = count(value, axis=axis)\\n54.     value = fillna(value, fill_value)\\n55.     data = _dask_or_eager_func(func)(value, axis=axis, **kwargs)\\n56. \\n57.     # TODO This will evaluate dask arrays and might be costly.\\n58.     if (valid_count == 0).any():\\n59.         raise ValueError("All-NaN slice encountered")\\n60. \\n61.     return data\\n62. \\n63. \\n64. def _nan_minmax_object(func, fill_value, value, axis=None, **kwargs):\\n65.     """ In house nanmin and nanmax for object array """\\n66.     valid_count = count(value, axis=axis)\\n67.     filled_value = fillna(value, fill_value)\\n68.     data = getattr(np, func)(filled_value, axis=axis, **kwargs)\\n69.     if not hasattr(data, "dtype"):  # scalar case\\n70.         data = fill_value if valid_count == 0 else data\\n71.         # we\\\'ve computed a single min, max value of type object.\\n72.         # don\\\'t let np.array turn a tuple back into an array\\n73.         return utils.to_0d_object_array(data)\\n74.     return where_method(data, valid_count != 0)\\n75. \\n76. \\n77. def nanmin(a, axis=None, out=None):\\n78.     if a.dtype.kind == "O":\\n79.         return _nan_minmax_object("min", dtypes.get_pos_infinity(a.dtype), a, axis)\\n80. \\n81.     module = dask_array if isinstance(a, dask_array_type) else nputils\\n82.     return module.nanmin(a, axis=axis)\\n83. \\n84. \\n85. def nanmax(a, axis=None, out=None):\\n86.     if a.dtype.kind == "O":\\n87.         return _nan_minmax_object("max", dtypes.get_neg_infinity(a.dtype), a, axis)\\n88. \\n89.     module = dask_array if isinstance(a, dask_array_type) else nputils\\n90.     return module.nanmax(a, axis=axis)\\n91. \\n92. \\n93. def nanargmin(a, axis=None):\\n94.     if a.dtype.kind == "O":\\n95.         fill_value = dtypes.get_pos_infinity(a.dtype)\\n96.         return _nan_argminmax_object("argmin", fill_value, a, axis=axis)\\n97. \\n98.     module = dask_array if isinstance(a, dask_array_type) else nputils\\n99.     return module.nanargmin(a, axis=axis)\\n100. \\n101. \\n102. def nanargmax(a, axis=None):\\n103.     if a.dtype.kind == "O":\\n104.         fill_value = dtypes.get_neg_infinity(a.dtype)\\n105.         return _nan_argminmax_object("argmax", fill_value, a, axis=axis)\\n106. \\n107.     module = dask_array if isinstance(a, dask_array_type) else nputils\\n108.     return module.nanargmax(a, axis=axis)\\n109. \\n110. \\n111. def nansum(a, axis=None, dtype=None, out=None, min_count=None):\\n112.     a, mask = _replace_nan(a, 0)\\n113.     result = _dask_or_eager_func("sum")(a, axis=axis, dtype=dtype)\\n114.     if min_count is not None:\\n115.         return _maybe_null_out(result, axis, mask, min_count)\\n116.     else:\\n117.         return result\\n118. \\n119. \\n120. def _nanmean_ddof_object(ddof, value, axis=None, dtype=None, **kwargs):\\n121.     """ In house nanmean. ddof argument will be used in _nanvar method """\\n122.     from .duck_array_ops import _dask_or_eager_func, count, fillna, where_method\\n123. \\n124.     valid_count = count(value, axis=axis)\\n125.     value = fillna(value, 0)\\n126.     # As dtype inference is impossible for object dtype, we assume float\\n127.     # https://github.com/dask/dask/issues/3162\\n128.     if dtype is None and value.dtype.kind == "O":\\n129.         dtype = value.dtype if value.dtype.kind in ["cf"] else float\\n130. \\n131.     data = _dask_or_eager_func("sum")(value, axis=axis, dtype=dtype, **kwargs)\\n132.     data = data / (valid_count - ddof)\\n133.     return where_method(data, valid_count != 0)\\n134. \\n135. \\n136. def nanmean(a, axis=None, dtype=None, out=None):\\n137.     if a.dtype.kind == "O":\\n138.         return _nanmean_ddof_object(0, a, axis=axis, dtype=dtype)\\n139. \\n140.     if isinstance(a, dask_array_type):\\n141.         return dask_array.nanmean(a, axis=axis, dtype=dtype)\\n142. \\n143.     return np.nanmean(a, axis=axis, dtype=dtype)\\n144. \\n145. \\n146. def nanmedian(a, axis=None, out=None):\\n147.     # The dask algorithm works by rechunking to one chunk along axis\\n148.     # Make sure we trigger the dask error when passing all dimensions\\n149.     # so that we don\\\'t rechunk the entire array to one chunk and\\n150.     # possibly blow memory\\n151.     if axis is not None and len(np.atleast_1d(axis)) == a.ndim:\\n152.         axis = None\\n153.     return _dask_or_eager_func(\\n154.         "nanmedian", dask_module=dask_array_compat, eager_module=nputils\\n155.     )(a, axis=axis)\\n156. \\n157. \\n158. def _nanvar_object(value, axis=None, ddof=0, keepdims=False, **kwargs):\\n159.     value_mean = _nanmean_ddof_object(\\n160.         ddof=0, value=value, axis=axis, keepdims=True, **kwargs\\n161.     )\\n162.     squared = (value.astype(value_mean.dtype) - value_mean) ** 2\\n163.     return _nanmean_ddof_object(ddof, squared, axis=axis, keepdims=keepdims, **kwargs)\\n164. \\n165. \\n166. def nanvar(a, axis=None, dtype=None, out=None, ddof=0):\\n167.     if a.dtype.kind == "O":\\n168.         return _nanvar_object(a, axis=axis, dtype=dtype, ddof=ddof)\\n169. \\n170.     return _dask_or_eager_func("nanvar", eager_module=nputils)(\\n171.         a, axis=axis, dtype=dtype, ddof=ddof\\n172.     )\\n173. \\n174. \\n175. def nanstd(a, axis=None, dtype=None, out=None, ddof=0):\\n176.     return _dask_or_eager_func("nanstd", eager_module=nputils)(\\n177.         a, axis=axis, dtype=dtype, ddof=ddof\\n178.     )\\n179. \\n180. \\n181. def nanprod(a, axis=None, dtype=None, out=None, min_count=None):\\n182.     a, mask = _replace_nan(a, 1)\\n183.     result = _dask_or_eager_func("nanprod")(a, axis=axis, dtype=dtype, out=out)\\n184.     if min_count is not None:\\n185.         return _maybe_null_out(result, axis, mask, min_count)\\n186.     else:\\n187.         return result\\n188. \\n189. \\n190. def nancumsum(a, axis=None, dtype=None, out=None):\\n191.     return _dask_or_eager_func("nancumsum", eager_module=nputils)(\\n192.         a, axis=axis, dtype=dtype\\n193.     )\\n194. \\n195. \\n196. def nancumprod(a, axis=None, dtype=None, out=None):\\n197.     return _dask_or_eager_func("nancumprod", eager_module=nputils)(\\n198.         a, axis=axis, dtype=dtype\\n199.     )\', \'start_line_number\': 1, \'end_line_number\': 199}\n\n{\'relative_path\': \'xarray/core/ops.py\', \'content\': \'1. """Define core operations for xarray objects.\\n2. \\n3. TODO(shoyer): rewrite this module, making use of xarray.core.computation,\\n4. NumPy\\\'s __array_ufunc__ and mixin classes instead of the unintuitive "inject"\\n5. functions.\\n6. """\\n7. \\n8. import operator\\n9. \\n10. import numpy as np\\n11. \\n12. from . import dtypes, duck_array_ops\\n13. from .nputils import array_eq, array_ne\\n14. \\n15. try:\\n16.     import bottleneck as bn\\n17. \\n18.     has_bottleneck = True\\n19. except ImportError:\\n20.     # use numpy methods instead\\n21.     bn = np\\n22.     has_bottleneck = False\\n23. \\n24. \\n25. UNARY_OPS = ["neg", "pos", "abs", "invert"]\\n26. CMP_BINARY_OPS = ["lt", "le", "ge", "gt"]\\n27. NUM_BINARY_OPS = [\\n28.     "add",\\n29.     "sub",\\n30.     "mul",\\n31.     "truediv",\\n32.     "floordiv",\\n33.     "mod",\\n34.     "pow",\\n35.     "and",\\n36.     "xor",\\n37.     "or",\\n38. ]\\n39. \\n40. # methods which pass on the numpy return value unchanged\\n41. # be careful not to list methods that we would want to wrap later\\n42. NUMPY_SAME_METHODS = ["item", "searchsorted"]\\n43. # methods which don\\\'t modify the data shape, so the result should still be\\n44. # wrapped in an Variable/DataArray\\n45. NUMPY_UNARY_METHODS = ["argsort", "clip", "conj", "conjugate"]\\n46. PANDAS_UNARY_FUNCTIONS = ["isnull", "notnull"]\\n47. # methods which remove an axis\\n48. REDUCE_METHODS = ["all", "any"]\\n49. NAN_REDUCE_METHODS = [\\n50.     "max",\\n51.     "min",\\n52.     "mean",\\n53.     "prod",\\n54.     "sum",\\n55.     "std",\\n56.     "var",\\n57.     "median",\\n58. ]\\n59. NAN_CUM_METHODS = ["cumsum", "cumprod"]\\n60. # TODO: wrap take, dot, sort\\n61. \\n62. \\n63. _CUM_DOCSTRING_TEMPLATE = """\\\\\\n64. Apply `{name}` along some dimension of {cls}.\\n65. \\n66. Parameters\\n67. ----------\\n68. {extra_args}\\n69. skipna : bool, optional\\n70.     If True, skip missing values (as marked by NaN). By default, only\\n71.     skips missing values for float dtypes; other dtypes either do not\\n72.     have a sentinel missing value (int) or skipna=True has not been\\n73.     implemented (object, datetime64 or timedelta64).\\n74. keep_attrs : bool, optional\\n75.     If True, the attributes (`attrs`) will be copied from the original\\n76.     object to the new one.  If False (default), the new object will be\\n77.     returned without attributes.\\n78. **kwargs : dict\\n79.     Additional keyword arguments passed on to `{name}`.\\n80. \\n81. Returns\\n82. -------\\n83. cumvalue : {cls}\\n84.     New {cls} object with `{name}` applied to its data along the\\n85.     indicated dimension.\\n86. """\\n87. \\n88. _REDUCE_DOCSTRING_TEMPLATE = """\\\\\\n89. Reduce this {cls}\\\'s data by applying `{name}` along some dimension(s).\\n90. \\n91. Parameters\\n92. ----------\\n93. {extra_args}{skip_na_docs}{min_count_docs}\\n94. keep_attrs : bool, optional\\n95.     If True, the attributes (`attrs`) will be copied from the original\\n96.     object to the new one.  If False (default), the new object will be\\n97.     returned without attributes.\\n98. **kwargs : dict\\n99.     Additional keyword arguments passed on to the appropriate array\\n100.     function for calculating `{name}` on this object\\\'s data.\\n101. \\n102. Returns\\n103. -------\\n104. reduced : {cls}\\n105.     New {cls} object with `{name}` applied to its data and the\\n106.     indicated dimension(s) removed.\\n107. """\\n108. \\n109. _SKIPNA_DOCSTRING = """\\n110. skipna : bool, optional\\n111.     If True, skip missing values (as marked by NaN). By default, only\\n112.     skips missing values for float dtypes; other dtypes either do not\\n113.     have a sentinel missing value (int) or skipna=True has not been\\n114.     implemented (object, datetime64 or timedelta64)."""\\n115. \\n116. _MINCOUNT_DOCSTRING = """\\n117. min_count : int, default: None\\n118.     The required number of valid values to perform the operation.\\n119.     If fewer than min_count non-NA values are present the result will\\n120.     be NA. New in version 0.10.8: Added with the default being None."""\\n121. \\n122. _COARSEN_REDUCE_DOCSTRING_TEMPLATE = """\\\\\\n123. Coarsen this object by applying `{name}` along its dimensions.\\n124. \\n125. Parameters\\n126. ----------\\n127. **kwargs : dict\\n128.     Additional keyword arguments passed on to `{name}`.\\n129. \\n130. Returns\\n131. -------\\n132. reduced : DataArray or Dataset\\n133.     New object with `{name}` applied along its coasen dimnensions.\\n134. """\\n135. \\n136. \\n137. def fillna(data, other, join="left", dataset_join="left"):\\n138.     """Fill missing values in this object with data from the other object.\\n139.     Follows normal broadcasting and alignment rules.\\n140. \\n141.     Parameters\\n142.     ----------\\n143.     join : {"outer", "inner", "left", "right"}, optional\\n144.         Method for joining the indexes of the passed objects along each\\n145.         dimension\\n146.         - "outer": use the union of object indexes\\n147.         - "inner": use the intersection of object indexes\\n148.         - "left": use indexes from the first object with each dimension\\n149.         - "right": use indexes from the last object with each dimension\\n150.         - "exact": raise `ValueError` instead of aligning when indexes to be\\n151.           aligned are not equal\\n152.     dataset_join : {"outer", "inner", "left", "right"}, optional\\n153.         Method for joining variables of Dataset objects with mismatched\\n154.         data variables.\\n155.         - "outer": take variables from both Dataset objects\\n156.         - "inner": take only overlapped variables\\n157.         - "left": take only variables from the first object\\n158.         - "right": take only variables from the last object\\n159.     """\\n160.     from .computation import apply_ufunc\\n161. \\n162.     return apply_ufunc(\\n163.         duck_array_ops.fillna,\\n164.         data,\\n165.         other,\\n166.         join=join,\\n167.         dask="allowed",\\n168.         dataset_join=dataset_join,\\n169.         dataset_fill_value=np.nan,\\n170.         keep_attrs=True,\\n171.     )\\n172. \\n173. \\n174. def where_method(self, cond, other=dtypes.NA):\\n175.     """Return elements from `self` or `other` depending on `cond`.\\n176. \\n177.     Parameters\\n178.     ----------\\n179.     cond : DataArray or Dataset with boolean dtype\\n180.         Locations at which to preserve this objects values.\\n181.     other : scalar, DataArray or Dataset, optional\\n182.         Value to use for locations in this object where ``cond`` is False.\\n183.         By default, inserts missing values.\\n184. \\n185.     Returns\\n186.     -------\\n187.     Same type as caller.\\n188.     """\\n189.     from .computation import apply_ufunc\\n190. \\n191.     # alignment for three arguments is complicated, so don\\\'t support it yet\\n192.     join = "inner" if other is dtypes.NA else "exact"\\n193.     return apply_ufunc(\\n194.         duck_array_ops.where_method,\\n195.         self,\\n196.         cond,\\n197.         other,\\n198.         join=join,\\n199.         dataset_join=join,\\n200.         dask="allowed",\\n201.         keep_attrs=True,\\n202.     )\\n203. \\n204. \\n205. def _call_possibly_missing_method(arg, name, args, kwargs):\\n206.     try:\\n207.         method = getattr(arg, name)\\n208.     except AttributeError:\\n209.         duck_array_ops.fail_on_dask_array_input(arg, func_name=name)\\n210.         if hasattr(arg, "data"):\\n211.             duck_array_ops.fail_on_dask_array_input(arg.data, func_name=name)\\n212.         raise\\n213.     else:\\n214.         return method(*args, **kwargs)\\n215. \\n216. \\n217. def _values_method_wrapper(name):\\n218.     def func(self, *args, **kwargs):\\n219.         return _call_possibly_missing_method(self.data, name, args, kwargs)\\n220. \\n221.     func.__name__ = name\\n222.     func.__doc__ = getattr(np.ndarray, name).__doc__\\n223.     return func\\n224. \\n225. \\n226. def _method_wrapper(name):\\n227.     def func(self, *args, **kwargs):\\n228.         return _call_possibly_missing_method(self, name, args, kwargs)\\n229. \\n230.     func.__name__ = name\\n231.     func.__doc__ = getattr(np.ndarray, name).__doc__\\n232.     return func\\n233. \\n234. \\n235. def _func_slash_method_wrapper(f, name=None):\\n236.     # try to wrap a method, but if not found use the function\\n237.     # this is useful when patching in a function as both a DataArray and\\n238.     # Dataset method\\n239.     if name is None:\\n240.         name = f.__name__\\n241. \\n242.     def func(self, *args, **kwargs):\\n243.         try:\\n244.             return getattr(self, name)(*args, **kwargs)\\n245.         except AttributeError:\\n246.             return f(self, *args, **kwargs)\\n247. \\n248.     func.__name__ = name\\n249.     func.__doc__ = f.__doc__\\n250.     return func\\n251. \\n252. \\n253. def inject_reduce_methods(cls):\\n254.     methods = (\\n255.         [\\n256.             (name, getattr(duck_array_ops, "array_%s" % name), False)\\n257.             for name in REDUCE_METHODS\\n258.         ]\\n259.         + [(name, getattr(duck_array_ops, name), True) for name in NAN_REDUCE_METHODS]\\n260.         + [("count", duck_array_ops.count, False)]\\n261.     )\\n262.     for name, f, include_skipna in methods:\\n263.         numeric_only = getattr(f, "numeric_only", False)\\n264.         available_min_count = getattr(f, "available_min_count", False)\\n265.         skip_na_docs = _SKIPNA_DOCSTRING if include_skipna else ""\\n266.         min_count_docs = _MINCOUNT_DOCSTRING if available_min_count else ""\\n267. \\n268.         func = cls._reduce_method(f, include_skipna, numeric_only)\\n269.         func.__name__ = name\\n270.         func.__doc__ = _REDUCE_DOCSTRING_TEMPLATE.format(\\n271.             name=name,\\n272.             cls=cls.__name__,\\n273.             extra_args=cls._reduce_extra_args_docstring.format(name=name),\\n274.             skip_na_docs=skip_na_docs,\\n275.             min_count_docs=min_count_docs,\\n276.         )\\n277.         setattr(cls, name, func)\\n278. \\n279. \\n280. def inject_cum_methods(cls):\\n281.     methods = [(name, getattr(duck_array_ops, name), True) for name in NAN_CUM_METHODS]\\n282.     for name, f, include_skipna in methods:\\n283.         numeric_only = getattr(f, "numeric_only", False)\\n284.         func = cls._reduce_method(f, include_skipna, numeric_only)\\n285.         func.__name__ = name\\n286.         func.__doc__ = _CUM_DOCSTRING_TEMPLATE.format(\\n287.             name=name,\\n288.             cls=cls.__name__,\\n289.             extra_args=cls._cum_extra_args_docstring.format(name=name),\\n290.         )\\n291.         setattr(cls, name, func)\\n292. \\n293. \\n294. def op_str(name):\\n295.     return "__%s__" % name\\n296. \\n297. \\n298. def get_op(name):\\n299.     return getattr(operator, op_str(name))\\n300. \\n301. \\n302. NON_INPLACE_OP = {get_op("i" + name): get_op(name) for name in NUM_BINARY_OPS}\\n303. \\n304. \\n305. def inplace_to_noninplace_op(f):\\n306.     return NON_INPLACE_OP[f]\\n307. \\n308. \\n309. def inject_binary_ops(cls, inplace=False):\\n310.     for name in CMP_BINARY_OPS + NUM_BINARY_OPS:\\n311.         setattr(cls, op_str(name), cls._binary_op(get_op(name)))\\n312. \\n313.     for name, f in [("eq", array_eq), ("ne", array_ne)]:\\n314.         setattr(cls, op_str(name), cls._binary_op(f))\\n315. \\n316.     for name in NUM_BINARY_OPS:\\n317.         # only numeric operations have in-place and reflexive variants\\n318.         setattr(cls, op_str("r" + name), cls._binary_op(get_op(name), reflexive=True))\\n319.         if inplace:\\n320.             setattr(cls, op_str("i" + name), cls._inplace_binary_op(get_op("i" + name)))\\n321. \\n322. \\n323. def inject_all_ops_and_reduce_methods(cls, priority=50, array_only=True):\\n324.     # prioritize our operations over those of numpy.ndarray (priority=1)\\n325.     # and numpy.matrix (priority=10)\\n326.     cls.__array_priority__ = priority\\n327. \\n328.     # patch in standard special operations\\n329.     for name in UNARY_OPS:\\n330.         setattr(cls, op_str(name), cls._unary_op(get_op(name)))\\n331.     inject_binary_ops(cls, inplace=True)\\n332. \\n333.     # patch in numpy/pandas methods\\n334.     for name in NUMPY_UNARY_METHODS:\\n335.         setattr(cls, name, cls._unary_op(_method_wrapper(name)))\\n336. \\n337.     for name in PANDAS_UNARY_FUNCTIONS:\\n338.         f = _func_slash_method_wrapper(getattr(duck_array_ops, name), name=name)\\n339.         setattr(cls, name, cls._unary_op(f))\\n340. \\n341.     f = _func_slash_method_wrapper(duck_array_ops.around, name="round")\\n342.     setattr(cls, "round", cls._unary_op(f))\\n343. \\n344.     if array_only:\\n345.         # these methods don\\\'t return arrays of the same shape as the input, so\\n346.         # don\\\'t try to patch these in for Dataset objects\\n347.         for name in NUMPY_SAME_METHODS:\\n348.             setattr(cls, name, _values_method_wrapper(name))\\n349. \\n350.     inject_reduce_methods(cls)\\n351.     inject_cum_methods(cls)\', \'start_line_number\': 1, \'end_line_number\': 351}\n\n{\'relative_path\': \'xarray/core/variable.py\', \'content\': \'1. import copy\\n2. import functools\\n3. import itertools\\n4. import numbers\\n5. import warnings\\n6. from collections import defaultdict\\n7. from datetime import timedelta\\n8. from distutils.version import LooseVersion\\n9. from typing import (\\n10.     Any,\\n11.     Dict,\\n12.     Hashable,\\n13.     Mapping,\\n14.     Optional,\\n15.     Sequence,\\n16.     Tuple,\\n17.     TypeVar,\\n18.     Union,\\n19. )\\n20. \\n21. import numpy as np\\n22. import pandas as pd\\n23. \\n24. import xarray as xr  # only for Dataset and DataArray\\n25. \\n26. from . import arithmetic, common, dtypes, duck_array_ops, indexing, nputils, ops, utils\\n27. from .indexing import (\\n28.     BasicIndexer,\\n29.     OuterIndexer,\\n30.     PandasIndexAdapter,\\n31.     VectorizedIndexer,\\n32.     as_indexable,\\n33. )\\n34. from .npcompat import IS_NEP18_ACTIVE\\n35. from .options import _get_keep_attrs\\n36. from .pycompat import cupy_array_type, dask_array_type, integer_types\\n37. from .utils import (\\n38.     OrderedSet,\\n39.     _default,\\n40.     decode_numpy_dict_values,\\n41.     drop_dims_from_indexers,\\n42.     either_dict_or_kwargs,\\n43.     ensure_us_time_resolution,\\n44.     infix_dims,\\n45. )\\n46. \\n47. NON_NUMPY_SUPPORTED_ARRAY_TYPES = (\\n48.     (indexing.ExplicitlyIndexed, pd.Index,) + dask_array_type + cupy_array_type\\n49. )\\n50. # https://github.com/python/mypy/issues/224\\n51. BASIC_INDEXING_TYPES = integer_types + (slice,)  # type: ignore\\n52. \\n53. VariableType = TypeVar("VariableType", bound="Variable")\\n54. """Type annotation to be used when methods of Variable return self or a copy of self.\\n55. When called from an instance of a subclass, e.g. IndexVariable, mypy identifies the\\n56. output as an instance of the subclass.\\n57. \\n58. Usage::\\n59. \\n60.    class Variable:\\n61.        def f(self: VariableType, ...) -> VariableType:\\n62.            ...\\n63. """\\n64. \\n65. \\n66. class MissingDimensionsError(ValueError):\\n67.     """Error class used when we can\\\'t safely guess a dimension name.\\n68.     """\\n69. \\n70.     # inherits from ValueError for backward compatibility\\n71.     # TODO: move this to an xarray.exceptions module?\\n72. \\n73. \\n74. def as_variable(obj, name=None) -> "Union[Variable, IndexVariable]":\\n75.     """Convert an object into a Variable.\\n76. \\n77.     Parameters\\n78.     ----------\\n79.     obj : object\\n80.         Object to convert into a Variable.\\n81. \\n82.         - If the object is already a Variable, return a shallow copy.\\n83.         - Otherwise, if the object has \\\'dims\\\' and \\\'data\\\' attributes, convert\\n84.           it into a new Variable.\\n85.         - If all else fails, attempt to convert the object into a Variable by\\n86.           unpacking it into the arguments for creating a new Variable.\\n87.     name : str, optional\\n88.         If provided:\\n89. \\n90.         - `obj` can be a 1D array, which is assumed to label coordinate values\\n91.           along a dimension of this given name.\\n92.         - Variables with name matching one of their dimensions are converted\\n93.           into `IndexVariable` objects.\\n94. \\n95.     Returns\\n96.     -------\\n97.     var : Variable\\n98.         The newly created variable.\\n99. \\n100.     """\\n101.     from .dataarray import DataArray\\n102. \\n103.     # TODO: consider extending this method to automatically handle Iris and\\n104.     if isinstance(obj, DataArray):\\n105.         # extract the primary Variable from DataArrays\\n106.         obj = obj.variable\\n107. \\n108.     if isinstance(obj, Variable):\\n109.         obj = obj.copy(deep=False)\\n110.     elif isinstance(obj, tuple):\\n111.         try:\\n112.             obj = Variable(*obj)\\n113.         except (TypeError, ValueError) as error:\\n114.             # use .format() instead of % because it handles tuples consistently\\n115.             raise error.__class__(\\n116.                 "Could not convert tuple of form "\\n117.                 "(dims, data[, attrs, encoding]): "\\n118.                 "{} to Variable.".format(obj)\\n119.             )\\n120.     elif utils.is_scalar(obj):\\n121.         obj = Variable([], obj)\\n122.     elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:\\n123.         obj = Variable(obj.name, obj)\\n124.     elif isinstance(obj, (set, dict)):\\n125.         raise TypeError("variable {!r} has invalid type {!r}".format(name, type(obj)))\\n126.     elif name is not None:\\n127.         data = as_compatible_data(obj)\\n128.         if data.ndim != 1:\\n129.             raise MissingDimensionsError(\\n130.                 "cannot set variable %r with %r-dimensional data "\\n131.                 "without explicit dimension names. Pass a tuple of "\\n132.                 "(dims, data) instead." % (name, data.ndim)\\n133.             )\\n134.         obj = Variable(name, data, fastpath=True)\\n135.     else:\\n136.         raise TypeError(\\n137.             "unable to convert object into a variable without an "\\n138.             "explicit list of dimensions: %r" % obj\\n139.         )\\n140. \\n141.     if name is not None and name in obj.dims:\\n142.         # convert the Variable into an Index\\n143.         if obj.ndim != 1:\\n144.             raise MissingDimensionsError(\\n145.                 "%r has more than 1-dimension and the same name as one of its "\\n146.                 "dimensions %r. xarray disallows such variables because they "\\n147.                 "conflict with the coordinates used to label "\\n148.                 "dimensions." % (name, obj.dims)\\n149.             )\\n150.         obj = obj.to_index_variable()\\n151. \\n152.     return obj\\n153. \\n154. \\n155. def _maybe_wrap_data(data):\\n156.     """\\n157.     Put pandas.Index and numpy.ndarray arguments in adapter objects to ensure\\n158.     they can be indexed properly.\\n159. \\n160.     NumpyArrayAdapter, PandasIndexAdapter and LazilyOuterIndexedArray should\\n161.     all pass through unmodified.\\n162.     """\\n163.     if isinstance(data, pd.Index):\\n164.         return PandasIndexAdapter(data)\\n165.     return data\\n166. \\n167. \\n168. def _possibly_convert_objects(values):\\n169.     """Convert arrays of datetime.datetime and datetime.timedelta objects into\\n170.     datetime64 and timedelta64, according to the pandas convention.\\n171.     """\\n172.     return np.asarray(pd.Series(values.ravel())).reshape(values.shape)\\n173. \\n174. \\n175. def as_compatible_data(data, fastpath=False):\\n176.     """Prepare and wrap data to put in a Variable.\\n177. \\n178.     - If data does not have the necessary attributes, convert it to ndarray.\\n179.     - If data has dtype=datetime64, ensure that it has ns precision. If it\\\'s a\\n180.       pandas.Timestamp, convert it to datetime64.\\n181.     - If data is already a pandas or xarray object (other than an Index), just\\n182.       use the values.\\n183. \\n184.     Finally, wrap it up with an adapter if necessary.\\n185.     """\\n186.     if fastpath and getattr(data, "ndim", 0) > 0:\\n187.         # can\\\'t use fastpath (yet) for scalars\\n188.         return _maybe_wrap_data(data)\\n189. \\n190.     if isinstance(data, Variable):\\n191.         return data.data\\n192. \\n193.     if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):\\n194.         return _maybe_wrap_data(data)\\n195. \\n196.     if isinstance(data, tuple):\\n197.         data = utils.to_0d_object_array(data)\\n198. \\n199.     if isinstance(data, pd.Timestamp):\\n200.         # TODO: convert, handle datetime objects, too\\n201.         data = np.datetime64(data.value, "ns")\\n202. \\n203.     if isinstance(data, timedelta):\\n204.         data = np.timedelta64(getattr(data, "value", data), "ns")\\n205. \\n206.     # we don\\\'t want nested self-described arrays\\n207.     data = getattr(data, "values", data)\\n208. \\n209.     if isinstance(data, np.ma.MaskedArray):\\n210.         mask = np.ma.getmaskarray(data)\\n211.         if mask.any():\\n212.             dtype, fill_value = dtypes.maybe_promote(data.dtype)\\n213.             data = np.asarray(data, dtype=dtype)\\n214.             data[mask] = fill_value\\n215.         else:\\n216.             data = np.asarray(data)\\n217. \\n218.     if not isinstance(data, np.ndarray):\\n219.         if hasattr(data, "__array_function__"):\\n220.             if IS_NEP18_ACTIVE:\\n221.                 return data\\n222.             else:\\n223.                 raise TypeError(\\n224.                     "Got an NumPy-like array type providing the "\\n225.                     "__array_function__ protocol but NEP18 is not enabled. "\\n226.                     "Check that numpy >= v1.16 and that the environment "\\n227.                     \\\'variable "NUMPY_EXPERIMENTAL_ARRAY_FUNCTION" is set to \\\'\\n228.                     \\\'"1"\\\'\\n229.                 )\\n230. \\n231.     # validate whether the data is valid data types\\n232.     data = np.asarray(data)\\n233. \\n234.     if isinstance(data, np.ndarray):\\n235.         if data.dtype.kind == "O":\\n236.             data = _possibly_convert_objects(data)\\n237.         elif data.dtype.kind == "M":\\n238.             data = np.asarray(data, "datetime64[ns]")\\n239.         elif data.dtype.kind == "m":\\n240.             data = np.asarray(data, "timedelta64[ns]")\\n241. \\n242.     return _maybe_wrap_data(data)\\n243. \\n244. \\n245. def _as_array_or_item(data):\\n246.     """Return the given values as a numpy array, or as an individual item if\\n247.     it\\\'s a 0d datetime64 or timedelta64 array.\\n248. \\n249.     Importantly, this function does not copy data if it is already an ndarray -\\n250.     otherwise, it will not be possible to update Variable values in place.\\n251. \\n252.     This function mostly exists because 0-dimensional ndarrays with\\n253.     dtype=datetime64 are broken :(\\n254.     https://github.com/numpy/numpy/issues/4337\\n255.     https://github.com/numpy/numpy/issues/7619\\n256. \\n257.     TODO: remove this (replace with np.asarray) once these issues are fixed\\n258.     """\\n259.     if isinstance(data, cupy_array_type):\\n260.         data = data.get()\\n261.     else:\\n262.         data = np.asarray(data)\\n263.     if data.ndim == 0:\\n264.         if data.dtype.kind == "M":\\n265.             data = np.datetime64(data, "ns")\\n266.         elif data.dtype.kind == "m":\\n267.             data = np.timedelta64(data, "ns")\\n268.     return data\\n269. \\n270. \\n271. class Variable(\\n272.     common.AbstractArray, arithmetic.SupportsArithmetic, utils.NdimSizeLenMixin\\n273. ):\\n274.     """A netcdf-like variable consisting of dimensions, data and attributes\\n275.     which describe a single Array. A single Variable object is not fully\\n276.     described outside the context of its parent Dataset (if you want such a\\n277.     fully described object, use a DataArray instead).\\n278. \\n279.     The main functional difference between Variables and numpy arrays is that\\n280.     numerical operations on Variables implement array broadcasting by dimension\\n281.     name. For example, adding an Variable with dimensions `(\\\'time\\\',)` to\\n282.     another Variable with dimensions `(\\\'space\\\',)` results in a new Variable\\n283.     with dimensions `(\\\'time\\\', \\\'space\\\')`. Furthermore, numpy reduce operations\\n284.     like ``mean`` or ``sum`` are overwritten to take a "dimension" argument\\n285.     instead of an "axis".\\n286. \\n287.     Variables are light-weight objects used as the building block for datasets.\\n288.     They are more primitive objects, so operations with them provide marginally\\n289.     higher performance than using DataArrays. However, manipulating data in the\\n290.     form of a Dataset or DataArray should almost always be preferred, because\\n291.     they can use more complete metadata in context of coordinate labels.\\n292.     """\\n293. \\n294.     __slots__ = ("_dims", "_data", "_attrs", "_encoding")\\n295. \\n296.     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\\n297.         """\\n298.         Parameters\\n299.         ----------\\n300.         dims : str or sequence of str\\n301.             Name(s) of the the data dimension(s). Must be either a string (only\\n302.             for 1D data) or a sequence of strings with length equal to the\\n303.             number of dimensions.\\n304.         data : array_like\\n305.             Data array which supports numpy-like data access.\\n306.         attrs : dict_like or None, optional\\n307.             Attributes to assign to the new variable. If None (default), an\\n308.             empty attribute dictionary is initialized.\\n309.         encoding : dict_like or None, optional\\n310.             Dictionary specifying how to encode this array\\\'s data into a\\n311.             serialized format like netCDF4. Currently used keys (for netCDF)\\n312.             include \\\'_FillValue\\\', \\\'scale_factor\\\', \\\'add_offset\\\' and \\\'dtype\\\'.\\n313.             Well-behaved code to serialize a Variable should ignore\\n314.             unrecognized encoding items.\\n315.         """\\n316.         self._data = as_compatible_data(data, fastpath=fastpath)\\n317.         self._dims = self._parse_dimensions(dims)\\n318.         self._attrs = None\\n319.         self._encoding = None\\n320.         if attrs is not None:\\n321.             self.attrs = attrs\\n322.         if encoding is not None:\\n323.             self.encoding = encoding\\n324. \\n325.     @property\\n326.     def dtype(self):\\n327.         return self._data.dtype\\n328. \\n329.     @property\\n330.     def shape(self):\\n331.         return self._data.shape\\n332. \\n333.     @property\\n334.     def nbytes(self):\\n335.         return self.size * self.dtype.itemsize\\n336. \\n337.     @property\\n338.     def _in_memory(self):\\n339.         return isinstance(self._data, (np.ndarray, np.number, PandasIndexAdapter)) or (\\n340.             isinstance(self._data, indexing.MemoryCachedArray)\\n341.             and isinstance(self._data.array, indexing.NumpyIndexingAdapter)\\n342.         )\\n343. \\n344.     @property\\n345.     def data(self):\\n346.         if hasattr(self._data, "__array_function__") or isinstance(\\n347.             self._data, dask_array_type\\n348.         ):\\n349.             return self._data\\n350.         else:\\n351.             return self.values\\n352. \\n353.     @data.setter\\n354.     def data(self, data):\\n355.         data = as_compatible_data(data)\\n356.         if data.shape != self.shape:\\n357.             raise ValueError(\\n358.                 f"replacement data must match the Variable\\\'s shape. "\\n359.                 f"replacement data has shape {data.shape}; Variable has shape {self.shape}"\\n360.             )\\n361.         self._data = data\\n362. \\n363.     def astype(self, dtype, casting="unsafe", copy=True, keep_attrs=True):\\n364.         """\\n365.         Copy of the Variable object, with data cast to a specified type.\\n366. \\n367.         Parameters\\n368.         ----------\\n369.         dtype : str or dtype\\n370.              Typecode or data-type to which the array is cast.\\n371.         casting : {\\\'no\\\', \\\'equiv\\\', \\\'safe\\\', \\\'same_kind\\\', \\\'unsafe\\\'}, optional\\n372.              Controls what kind of data casting may occur. Defaults to \\\'unsafe\\\'\\n373.              for backwards compatibility.\\n374. \\n375.              * \\\'no\\\' means the data types should not be cast at all.\\n376.              * \\\'equiv\\\' means only byte-order changes are allowed.\\n377.              * \\\'safe\\\' means only casts which can preserve values are allowed.\\n378.              * \\\'same_kind\\\' means only safe casts or casts within a kind,\\n379.                  like float64 to float32, are allowed.\\n380.              * \\\'unsafe\\\' means any data conversions may be done.\\n381.         copy : bool, optional\\n382.              By default, astype always returns a newly allocated array. If this\\n383.              is set to False and the `dtype` requirement is satisfied, the input\\n384.              array is returned instead of a copy.\\n385.         keep_attrs : bool, optional\\n386.             By default, astype keeps attributes. Set to False to remove\\n387.             attributes in the returned object.\\n388. \\n389.         Returns\\n390.         -------\\n391.         out : same as object\\n392.             New object with data cast to the specified type.\\n393. \\n394.         See also\\n395.         --------\\n396.         np.ndarray.astype\\n397.         dask.array.Array.astype\\n398.         """\\n399.         from .computation import apply_ufunc\\n400. \\n401.         return apply_ufunc(\\n402.             duck_array_ops.astype,\\n403.             self,\\n404.             kwargs=dict(dtype=dtype, casting=casting, copy=copy),\\n405.             keep_attrs=keep_attrs,\\n406.             dask="allowed",\\n407.         )\\n408. \\n409.     def load(self, **kwargs):\\n410.         """Manually trigger loading of this variable\\\'s data from disk or a\\n411.         remote source into memory and return this variable.\\n412. \\n413.         Normally, it should not be necessary to call this method in user code,\\n414.         because all xarray functions should either work on deferred data or\\n415.         load data automatically.\\n416. \\n417.         Parameters\\n418.         ----------\\n419.         **kwargs : dict\\n420.             Additional keyword arguments passed on to ``dask.array.compute``.\\n421. \\n422.         See Also\\n423.         --------\\n424.         dask.array.compute\\n425.         """\\n426.         if isinstance(self._data, dask_array_type):\\n427.             self._data = as_compatible_data(self._data.compute(**kwargs))\\n428.         elif not hasattr(self._data, "__array_function__"):\\n429.             self._data = np.asarray(self._data)\\n430.         return self\\n431. \\n432.     def compute(self, **kwargs):\\n433.         """Manually trigger loading of this variable\\\'s data from disk or a\\n434.         remote source into memory and return a new variable. The original is\\n435.         left unaltered.\\n436. \\n437.         Normally, it should not be necessary to call this method in user code,\\n438.         because all xarray functions should either work on deferred data or\\n439.         load data automatically.\\n440. \\n441.         Parameters\\n442.         ----------\\n443.         **kwargs : dict\\n444.             Additional keyword arguments passed on to ``dask.array.compute``.\\n445. \\n446.         See Also\\n447.         --------\\n448.         dask.array.compute\\n449.         """\\n450.         new = self.copy(deep=False)\\n451.         return new.load(**kwargs)\\n452. \\n453.     def __dask_tokenize__(self):\\n454.         # Use v.data, instead of v._data, in order to cope with the wrappers\\n455.         # around NetCDF and the like\\n456.         from dask.base import normalize_token\\n457. \\n458.         return normalize_token((type(self), self._dims, self.data, self._attrs))\\n459. \\n460.     def __dask_graph__(self):\\n461.         if isinstance(self._data, dask_array_type):\\n462.             return self._data.__dask_graph__()\\n463.         else:\\n464.             return None\\n465. \\n466.     def __dask_keys__(self):\\n467.         return self._data.__dask_keys__()\\n468. \\n469.     def __dask_layers__(self):\\n470.         return self._data.__dask_layers__()\\n471. \\n472.     @property\\n473.     def __dask_optimize__(self):\\n474.         return self._data.__dask_optimize__\\n475. \\n476.     @property\\n477.     def __dask_scheduler__(self):\\n478.         return self._data.__dask_scheduler__\\n479. \\n480.     def __dask_postcompute__(self):\\n481.         array_func, array_args = self._data.__dask_postcompute__()\\n482.         return (\\n483.             self._dask_finalize,\\n484.             (array_func, array_args, self._dims, self._attrs, self._encoding),\\n485.         )\\n486. \\n487.     def __dask_postpersist__(self):\\n488.         array_func, array_args = self._data.__dask_postpersist__()\\n489.         return (\\n490.             self._dask_finalize,\\n491.             (array_func, array_args, self._dims, self._attrs, self._encoding),\\n492.         )\\n493. \\n494.     @staticmethod\\n495.     def _dask_finalize(results, array_func, array_args, dims, attrs, encoding):\\n496.         if isinstance(results, dict):  # persist case\\n497.             name = array_args[0]\\n498.             results = {k: v for k, v in results.items() if k[0] == name}\\n499.         data = array_func(results, *array_args)\\n500.         return Variable(dims, data, attrs=attrs, encoding=encoding)\\n501. \\n502.     @property\\n503.     def values(self):\\n504.         """The variable\\\'s data as a numpy.ndarray"""\\n505.         return _as_array_or_item(self._data)\\n506. \\n507.     @values.setter\\n508.     def values(self, values):\\n509.         self.data = values\\n510. \\n511.     def to_base_variable(self):\\n512.         """Return this variable as a base xarray.Variable"""\\n513.         return Variable(\\n514.             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True\\n515.         )\\n516. \\n517.     to_variable = utils.alias(to_base_variable, "to_variable")\\n518. \\n519.     def to_index_variable(self):\\n520.         """Return this variable as an xarray.IndexVariable"""\\n521.         return IndexVariable(\\n522.             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True\\n523.         )\\n524. \\n525.     to_coord = utils.alias(to_index_variable, "to_coord")\\n526. \\n527.     def to_index(self):\\n528.         """Convert this variable to a pandas.Index"""\\n529.         return self.to_index_variable().to_index()\\n530. \\n531.     def to_dict(self, data=True):\\n532.         """Dictionary representation of variable."""\\n533.         item = {"dims": self.dims, "attrs": decode_numpy_dict_values(self.attrs)}\\n534.         if data:\\n535.             item["data"] = ensure_us_time_resolution(self.values).tolist()\\n536.         else:\\n537.             item.update({"dtype": str(self.dtype), "shape": self.shape})\\n538.         return item\\n539. \\n540.     @property\\n541.     def dims(self):\\n542.         """Tuple of dimension names with which this variable is associated.\\n543.         """\\n544.         return self._dims\\n545. \\n546.     @dims.setter\\n547.     def dims(self, value):\\n548.         self._dims = self._parse_dimensions(value)\\n549. \\n550.     def _parse_dimensions(self, dims):\\n551.         if isinstance(dims, str):\\n552.             dims = (dims,)\\n553.         dims = tuple(dims)\\n554.         if len(dims) != self.ndim:\\n555.             raise ValueError(\\n556.                 "dimensions %s must have the same length as the "\\n557.                 "number of data dimensions, ndim=%s" % (dims, self.ndim)\\n558.             )\\n559.         return dims\\n560. \\n561.     def _item_key_to_tuple(self, key):\\n562.         if utils.is_dict_like(key):\\n563.             return tuple(key.get(dim, slice(None)) for dim in self.dims)\\n564.         else:\\n565.             return key\\n566. \\n567.     def _broadcast_indexes(self, key):\\n568.         """Prepare an indexing key for an indexing operation.\\n569. \\n570.         Parameters\\n571.         -----------\\n572.         key: int, slice, array-like, dict or tuple of integer, slice and array-like\\n573.             Any valid input for indexing.\\n574. \\n575.         Returns\\n576.         -------\\n577.         dims : tuple\\n578.             Dimension of the resultant variable.\\n579.         indexers : IndexingTuple subclass\\n580.             Tuple of integer, array-like, or slices to use when indexing\\n581.             self._data. The type of this argument indicates the type of\\n582.             indexing to perform, either basic, outer or vectorized.\\n583.         new_order : Optional[Sequence[int]]\\n584.             Optional reordering to do on the result of indexing. If not None,\\n585.             the first len(new_order) indexing should be moved to these\\n586.             positions.\\n587.         """\\n588.         key = self._item_key_to_tuple(key)  # key is a tuple\\n589.         # key is a tuple of full size\\n590.         key = indexing.expanded_indexer(key, self.ndim)\\n591.         # Convert a scalar Variable to an integer\\n592.         key = tuple(\\n593.             k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key\\n594.         )\\n595.         # Convert a 0d-array to an integer\\n596.         key = tuple(\\n597.             k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key\\n598.         )\\n599. \\n600.         if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):\\n601.             return self._broadcast_indexes_basic(key)\\n602. \\n603.         self._validate_indexers(key)\\n604.         # Detect it can be mapped as an outer indexer\\n605.         # If all key is unlabeled, or\\n606.         # key can be mapped as an OuterIndexer.\\n607.         if all(not isinstance(k, Variable) for k in key):\\n608.             return self._broadcast_indexes_outer(key)\\n609. \\n610.         # If all key is 1-dimensional and there are no duplicate labels,\\n611.         # key can be mapped as an OuterIndexer.\\n612.         dims = []\\n613.         for k, d in zip(key, self.dims):\\n614.             if isinstance(k, Variable):\\n615.                 if len(k.dims) > 1:\\n616.                     return self._broadcast_indexes_vectorized(key)\\n617.                 dims.append(k.dims[0])\\n618.             elif not isinstance(k, integer_types):\\n619.                 dims.append(d)\\n620.         if len(set(dims)) == len(dims):\\n621.             return self._broadcast_indexes_outer(key)\\n622. \\n623.         return self._broadcast_indexes_vectorized(key)\\n624. \\n625.     def _broadcast_indexes_basic(self, key):\\n626.         dims = tuple(\\n627.             dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)\\n628.         )\\n629.         return dims, BasicIndexer(key), None\\n630. \\n631.     def _validate_indexers(self, key):\\n632.         """ Make sanity checks """\\n633.         for dim, k in zip(self.dims, key):\\n634.             if isinstance(k, BASIC_INDEXING_TYPES):\\n635.                 pass\\n636.             else:\\n637.                 if not isinstance(k, Variable):\\n638.                     k = np.asarray(k)\\n639.                     if k.ndim > 1:\\n640.                         raise IndexError(\\n641.                             "Unlabeled multi-dimensional array cannot be "\\n642.                             "used for indexing: {}".format(k)\\n643.                         )\\n644.                 if k.dtype.kind == "b":\\n645.                     if self.shape[self.get_axis_num(dim)] != len(k):\\n646.                         raise IndexError(\\n647.                             "Boolean array size {:d} is used to index array "\\n648.                             "with shape {:s}.".format(len(k), str(self.shape))\\n649.                         )\\n650.                     if k.ndim > 1:\\n651.                         raise IndexError(\\n652.                             "{}-dimensional boolean indexing is "\\n653.                             "not supported. ".format(k.ndim)\\n654.                         )\\n655.                     if getattr(k, "dims", (dim,)) != (dim,):\\n656.                         raise IndexError(\\n657.                             "Boolean indexer should be unlabeled or on the "\\n658.                             "same dimension to the indexed array. Indexer is "\\n659.                             "on {:s} but the target dimension is {:s}.".format(\\n660.                                 str(k.dims), dim\\n661.                             )\\n662.                         )\\n663. \\n664.     def _broadcast_indexes_outer(self, key):\\n665.         dims = tuple(\\n666.             k.dims[0] if isinstance(k, Variable) else dim\\n667.             for k, dim in zip(key, self.dims)\\n668.             if not isinstance(k, integer_types)\\n669.         )\\n670. \\n671.         new_key = []\\n672.         for k in key:\\n673.             if isinstance(k, Variable):\\n674.                 k = k.data\\n675.             if not isinstance(k, BASIC_INDEXING_TYPES):\\n676.                 k = np.asarray(k)\\n677.                 if k.size == 0:\\n678.                     # Slice by empty list; numpy could not infer the dtype\\n679.                     k = k.astype(int)\\n680.                 elif k.dtype.kind == "b":\\n681.                     (k,) = np.nonzero(k)\\n682.             new_key.append(k)\\n683. \\n684.         return dims, OuterIndexer(tuple(new_key)), None\\n685. \\n686.     def _nonzero(self):\\n687.         """ Equivalent numpy\\\'s nonzero but returns a tuple of Varibles. """\\n688.         # TODO we should replace dask\\\'s native nonzero\\n689.         # after https://github.com/dask/dask/issues/1076 is implemented.\\n690.         nonzeros = np.nonzero(self.data)\\n691.         return tuple(Variable((dim), nz) for nz, dim in zip(nonzeros, self.dims))\\n692. \\n693.     def _broadcast_indexes_vectorized(self, key):\\n694.         variables = []\\n695.         out_dims_set = OrderedSet()\\n696.         for dim, value in zip(self.dims, key):\\n697.             if isinstance(value, slice):\\n698.                 out_dims_set.add(dim)\\n699.             else:\\n700.                 variable = (\\n701.                     value\\n702.                     if isinstance(value, Variable)\\n703.                     else as_variable(value, name=dim)\\n704.                 )\\n705.                 if variable.dtype.kind == "b":  # boolean indexing case\\n706.                     (variable,) = variable._nonzero()\\n707. \\n708.                 variables.append(variable)\\n709.                 out_dims_set.update(variable.dims)\\n710. \\n711.         variable_dims = set()\\n712.         for variable in variables:\\n713.             variable_dims.update(variable.dims)\\n714. \\n715.         slices = []\\n716.         for i, (dim, value) in enumerate(zip(self.dims, key)):\\n717.             if isinstance(value, slice):\\n718.                 if dim in variable_dims:\\n719.                     # We only convert slice objects to variables if they share\\n720.                     # a dimension with at least one other variable. Otherwise,\\n721.                     # we can equivalently leave them as slices aknd transpose\\n722.                     # the result. This is significantly faster/more efficient\\n723.                     # for most array backends.\\n724.                     values = np.arange(*value.indices(self.sizes[dim]))\\n725.                     variables.insert(i - len(slices), Variable((dim,), values))\\n726.                 else:\\n727.                     slices.append((i, value))\\n728. \\n729.         try:\\n730.             variables = _broadcast_compat_variables(*variables)\\n731.         except ValueError:\\n732.             raise IndexError(f"Dimensions of indexers mismatch: {key}")\\n733. \\n734.         out_key = [variable.data for variable in variables]\\n735.         out_dims = tuple(out_dims_set)\\n736.         slice_positions = set()\\n737.         for i, value in slices:\\n738.             out_key.insert(i, value)\\n739.             new_position = out_dims.index(self.dims[i])\\n740.             slice_positions.add(new_position)\\n741. \\n742.         if slice_positions:\\n743.             new_order = [i for i in range(len(out_dims)) if i not in slice_positions]\\n744.         else:\\n745.             new_order = None\\n746. \\n747.         return out_dims, VectorizedIndexer(tuple(out_key)), new_order\\n748. \\n749.     def __getitem__(self: VariableType, key) -> VariableType:\\n750.         """Return a new Variable object whose contents are consistent with\\n751.         getting the provided key from the underlying data.\\n752. \\n753.         NB. __getitem__ and __setitem__ implement xarray-style indexing,\\n754.         where if keys are unlabeled arrays, we index the array orthogonally\\n755.         with them. If keys are labeled array (such as Variables), they are\\n756.         broadcasted with our usual scheme and then the array is indexed with\\n757.         the broadcasted key, like numpy\\\'s fancy indexing.\\n758. \\n759.         If you really want to do indexing like `x[x > 0]`, manipulate the numpy\\n760.         array `x.values` directly.\\n761.         """\\n762.         dims, indexer, new_order = self._broadcast_indexes(key)\\n763.         data = as_indexable(self._data)[indexer]\\n764.         if new_order:\\n765.             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\\n766.         return self._finalize_indexing_result(dims, data)\\n767. \\n768.     def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:\\n769.         """Used by IndexVariable to return IndexVariable objects when possible.\\n770.         """\\n771.         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\\n772. \\n773.     def _getitem_with_mask(self, key, fill_value=dtypes.NA):\\n774.         """Index this Variable with -1 remapped to fill_value."""\\n775.         # TODO(shoyer): expose this method in public API somewhere (isel?) and\\n776.         # use it for reindex.\\n777.         # TODO(shoyer): add a sanity check that all other integers are\\n778.         # non-negative\\n779.         # TODO(shoyer): add an optimization, remapping -1 to an adjacent value\\n780.         # that is actually indexed rather than mapping it to the last value\\n781.         # along each axis.\\n782. \\n783.         if fill_value is dtypes.NA:\\n784.             fill_value = dtypes.get_fill_value(self.dtype)\\n785. \\n786.         dims, indexer, new_order = self._broadcast_indexes(key)\\n787. \\n788.         if self.size:\\n789.             if isinstance(self._data, dask_array_type):\\n790.                 # dask\\\'s indexing is faster this way; also vindex does not\\n791.                 # support negative indices yet:\\n792.                 # https://github.com/dask/dask/pull/2967\\n793.                 actual_indexer = indexing.posify_mask_indexer(indexer)\\n794.             else:\\n795.                 actual_indexer = indexer\\n796. \\n797.             data = as_indexable(self._data)[actual_indexer]\\n798.             mask = indexing.create_mask(indexer, self.shape, data)\\n799.             # we need to invert the mask in order to pass data first. This helps\\n800.             # pint to choose the correct unit\\n801.             # TODO: revert after https://github.com/hgrecco/pint/issues/1019 is fixed\\n802.             data = duck_array_ops.where(np.logical_not(mask), data, fill_value)\\n803.         else:\\n804.             # array cannot be indexed along dimensions of size 0, so just\\n805.             # build the mask directly instead.\\n806.             mask = indexing.create_mask(indexer, self.shape)\\n807.             data = np.broadcast_to(fill_value, getattr(mask, "shape", ()))\\n808. \\n809.         if new_order:\\n810.             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\\n811.         return self._finalize_indexing_result(dims, data)\\n812. \\n813.     def __setitem__(self, key, value):\\n814.         """__setitem__ is overloaded to access the underlying numpy values with\\n815.         orthogonal indexing.\\n816. \\n817.         See __getitem__ for more details.\\n818.         """\\n819.         dims, index_tuple, new_order = self._broadcast_indexes(key)\\n820. \\n821.         if not isinstance(value, Variable):\\n822.             value = as_compatible_data(value)\\n823.             if value.ndim > len(dims):\\n824.                 raise ValueError(\\n825.                     "shape mismatch: value array of shape %s could not be "\\n826.                     "broadcast to indexing result with %s dimensions"\\n827.                     % (value.shape, len(dims))\\n828.                 )\\n829.             if value.ndim == 0:\\n830.                 value = Variable((), value)\\n831.             else:\\n832.                 value = Variable(dims[-value.ndim :], value)\\n833.         # broadcast to become assignable\\n834.         value = value.set_dims(dims).data\\n835. \\n836.         if new_order:\\n837.             value = duck_array_ops.asarray(value)\\n838.             value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]\\n839.             value = duck_array_ops.moveaxis(value, new_order, range(len(new_order)))\\n840. \\n841.         indexable = as_indexable(self._data)\\n842.         indexable[index_tuple] = value\\n843. \\n844.     @property\\n845.     def attrs(self) -> Dict[Hashable, Any]:\\n846.         """Dictionary of local attributes on this variable.\\n847.         """\\n848.         if self._attrs is None:\\n849.             self._attrs = {}\\n850.         return self._attrs\\n851. \\n852.     @attrs.setter\\n853.     def attrs(self, value: Mapping[Hashable, Any]) -> None:\\n854.         self._attrs = dict(value)\\n855. \\n856.     @property\\n857.     def encoding(self):\\n858.         """Dictionary of encodings on this variable.\\n859.         """\\n860.         if self._encoding is None:\\n861.             self._encoding = {}\\n862.         return self._encoding\\n863. \\n864.     @encoding.setter\\n865.     def encoding(self, value):\\n866.         try:\\n867.             self._encoding = dict(value)\\n868.         except ValueError:\\n869.             raise ValueError("encoding must be castable to a dictionary")\\n870. \\n871.     def copy(self, deep=True, data=None):\\n872.         """Returns a copy of this object.\\n873. \\n874.         If `deep=True`, the data array is loaded into memory and copied onto\\n875.         the new object. Dimensions, attributes and encodings are always copied.\\n876. \\n877.         Use `data` to create a new object with the same structure as\\n878.         original but entirely new data.\\n879. \\n880.         Parameters\\n881.         ----------\\n882.         deep : bool, optional\\n883.             Whether the data array is loaded into memory and copied onto\\n884.             the new object. Default is True.\\n885.         data : array_like, optional\\n886.             Data to use in the new object. Must have same shape as original.\\n887.             When `data` is used, `deep` is ignored.\\n888. \\n889.         Returns\\n890.         -------\\n891.         object : Variable\\n892.             New object with dimensions, attributes, encodings, and optionally\\n893.             data copied from original.\\n894. \\n895.         Examples\\n896.         --------\\n897. \\n898.         Shallow copy versus deep copy\\n899. \\n900.         >>> var = xr.Variable(data=[1, 2, 3], dims="x")\\n901.         >>> var.copy()\\n902.         <xarray.Variable (x: 3)>\\n903.         array([1, 2, 3])\\n904.         >>> var_0 = var.copy(deep=False)\\n905.         >>> var_0[0] = 7\\n906.         >>> var_0\\n907.         <xarray.Variable (x: 3)>\\n908.         array([7, 2, 3])\\n909.         >>> var\\n910.         <xarray.Variable (x: 3)>\\n911.         array([7, 2, 3])\\n912. \\n913.         Changing the data using the ``data`` argument maintains the\\n914.         structure of the original object, but with the new data. Original\\n915.         object is unaffected.\\n916. \\n917.         >>> var.copy(data=[0.1, 0.2, 0.3])\\n918.         <xarray.Variable (x: 3)>\\n919.         array([ 0.1,  0.2,  0.3])\\n920.         >>> var\\n921.         <xarray.Variable (x: 3)>\\n922.         array([7, 2, 3])\\n923. \\n924.         See Also\\n925.         --------\\n926.         pandas.DataFrame.copy\\n927.         """\\n928.         if data is None:\\n929.             data = self._data\\n930. \\n931.             if isinstance(data, indexing.MemoryCachedArray):\\n932.                 # don\\\'t share caching between copies\\n933.                 data = indexing.MemoryCachedArray(data.array)\\n934. \\n935.             if deep:\\n936.                 if hasattr(data, "__array_function__") or isinstance(\\n937.                     data, dask_array_type\\n938.                 ):\\n939.                     data = data.copy()\\n940.                 elif not isinstance(data, PandasIndexAdapter):\\n941.                     # pandas.Index is immutable\\n942.                     data = np.array(data)\\n943.         else:\\n944.             data = as_compatible_data(data)\\n945.             if self.shape != data.shape:\\n946.                 raise ValueError(\\n947.                     "Data shape {} must match shape of object {}".format(\\n948.                         data.shape, self.shape\\n949.                     )\\n950.                 )\\n951. \\n952.         # note:\\n953.         # dims is already an immutable tuple\\n954.         # attributes and encoding will be copied when the new Array is created\\n955.         return self._replace(data=data)\\n956. \\n957.     def _replace(\\n958.         self, dims=_default, data=_default, attrs=_default, encoding=_default\\n959.     ) -> "Variable":\\n960.         if dims is _default:\\n961.             dims = copy.copy(self._dims)\\n962.         if data is _default:\\n963.             data = copy.copy(self.data)\\n964.         if attrs is _default:\\n965.             attrs = copy.copy(self._attrs)\\n966.         if encoding is _default:\\n967.             encoding = copy.copy(self._encoding)\\n968.         return type(self)(dims, data, attrs, encoding, fastpath=True)\\n969. \\n970.     def __copy__(self):\\n971.         return self.copy(deep=False)\\n972. \\n973.     def __deepcopy__(self, memo=None):\\n974.         # memo does nothing but is required for compatibility with\\n975.         # copy.deepcopy\\n976.         return self.copy(deep=True)\\n977. \\n978.     # mutable objects should not be hashable\\n979.     # https://github.com/python/mypy/issues/4266\\n980.     __hash__ = None  # type: ignore\\n981. \\n982.     @property\\n983.     def chunks(self):\\n984.         """Block dimensions for this array\\\'s data or None if it\\\'s not a dask\\n985.         array.\\n986.         """\\n987.         return getattr(self._data, "chunks", None)\\n988. \\n989.     _array_counter = itertools.count()\\n990. \\n991.     def chunk(self, chunks=None, name=None, lock=False):\\n992.         """Coerce this array\\\'s data into a dask arrays with the given chunks.\\n993. \\n994.         If this variable is a non-dask array, it will be converted to dask\\n995.         array. If it\\\'s a dask array, it will be rechunked to the given chunk\\n996.         sizes.\\n997. \\n998.         If neither chunks is not provided for one or more dimensions, chunk\\n999.         sizes along that dimension will not be updated; non-dask arrays will be\\n1000.         converted into dask arrays with a single block.\\n1001. \\n1002.         Parameters\\n1003.         ----------\\n1004.         chunks : int, tuple or dict, optional\\n1005.             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\\n1006.             ``{\\\'x\\\': 5, \\\'y\\\': 5}``.\\n1007.         name : str, optional\\n1008.             Used to generate the name for this array in the internal dask\\n1009.             graph. Does not need not be unique.\\n1010.         lock : optional\\n1011.             Passed on to :py:func:`dask.array.from_array`, if the array is not\\n1012.             already as dask array.\\n1013. \\n1014.         Returns\\n1015.         -------\\n1016.         chunked : xarray.Variable\\n1017.         """\\n1018.         import dask\\n1019.         import dask.array as da\\n1020. \\n1021.         if utils.is_dict_like(chunks):\\n1022.             chunks = {self.get_axis_num(dim): chunk for dim, chunk in chunks.items()}\\n1023. \\n1024.         if chunks is None:\\n1025.             chunks = self.chunks or self.shape\\n1026. \\n1027.         data = self._data\\n1028.         if isinstance(data, da.Array):\\n1029.             data = data.rechunk(chunks)\\n1030.         else:\\n1031.             if isinstance(data, indexing.ExplicitlyIndexed):\\n1032.                 # Unambiguously handle array storage backends (like NetCDF4 and h5py)\\n1033.                 # that can\\\'t handle general array indexing. For example, in netCDF4 you\\n1034.                 # can do "outer" indexing along two dimensions independent, which works\\n1035.                 # differently from how NumPy handles it.\\n1036.                 # da.from_array works by using lazy indexing with a tuple of slices.\\n1037.                 # Using OuterIndexer is a pragmatic choice: dask does not yet handle\\n1038.                 # different indexing types in an explicit way:\\n1039.                 # https://github.com/dask/dask/issues/2883\\n1040.                 data = indexing.ImplicitToExplicitIndexingAdapter(\\n1041.                     data, indexing.OuterIndexer\\n1042.                 )\\n1043.                 if LooseVersion(dask.__version__) < "2.0.0":\\n1044.                     kwargs = {}\\n1045.                 else:\\n1046.                     # All of our lazily loaded backend array classes should use NumPy\\n1047.                     # array operations.\\n1048.                     kwargs = {"meta": np.ndarray}\\n1049.             else:\\n1050.                 kwargs = {}\\n1051. \\n1052.             if utils.is_dict_like(chunks):\\n1053.                 chunks = tuple(chunks.get(n, s) for n, s in enumerate(self.shape))\\n1054. \\n1055.             data = da.from_array(data, chunks, name=name, lock=lock, **kwargs)\\n1056. \\n1057.         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)\\n1058. \\n1059.     def _as_sparse(self, sparse_format=_default, fill_value=dtypes.NA):\\n1060.         """\\n1061.         use sparse-array as backend.\\n1062.         """\\n1063.         import sparse\\n1064. \\n1065.         # TODO  what to do if dask-backended?\\n1066.         if fill_value is dtypes.NA:\\n1067.             dtype, fill_value = dtypes.maybe_promote(self.dtype)\\n1068.         else:\\n1069.             dtype = dtypes.result_type(self.dtype, fill_value)\\n1070. \\n1071.         if sparse_format is _default:\\n1072.             sparse_format = "coo"\\n1073.         try:\\n1074.             as_sparse = getattr(sparse, "as_{}".format(sparse_format.lower()))\\n1075.         except AttributeError:\\n1076.             raise ValueError("{} is not a valid sparse format".format(sparse_format))\\n1077. \\n1078.         data = as_sparse(self.data.astype(dtype), fill_value=fill_value)\\n1079.         return self._replace(data=data)\\n1080. \\n1081.     def _to_dense(self):\\n1082.         """\\n1083.         Change backend from sparse to np.array\\n1084.         """\\n1085.         if hasattr(self._data, "todense"):\\n1086.             return self._replace(data=self._data.todense())\\n1087.         return self.copy(deep=False)\\n1088. \\n1089.     def isel(\\n1090.         self: VariableType,\\n1091.         indexers: Mapping[Hashable, Any] = None,\\n1092.         missing_dims: str = "raise",\\n1093.         **indexers_kwargs: Any,\\n1094.     ) -> VariableType:\\n1095.         """Return a new array indexed along the specified dimension(s).\\n1096. \\n1097.         Parameters\\n1098.         ----------\\n1099.         **indexers : {dim: indexer, ...}\\n1100.             Keyword arguments with names matching dimensions and values given\\n1101.             by integers, slice objects or arrays.\\n1102.         missing_dims : {"raise", "warn", "ignore"}, default: "raise"\\n1103.             What to do if dimensions that should be selected from are not present in the\\n1104.             DataArray:\\n1105.             - "raise": raise an exception\\n1106.             - "warning": raise a warning, and ignore the missing dimensions\\n1107.             - "ignore": ignore the missing dimensions\\n1108. \\n1109.         Returns\\n1110.         -------\\n1111.         obj : Array object\\n1112.             A new Array with the selected data and dimensions. In general,\\n1113.             the new variable\\\'s data will be a view of this variable\\\'s data,\\n1114.             unless numpy fancy indexing was triggered by using an array\\n1115.             indexer, in which case the data will be a copy.\\n1116.         """\\n1117.         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")\\n1118. \\n1119.         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\\n1120. \\n1121.         key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)\\n1122.         return self[key]\\n1123. \\n1124.     def squeeze(self, dim=None):\\n1125.         """Return a new object with squeezed data.\\n1126. \\n1127.         Parameters\\n1128.         ----------\\n1129.         dim : None or str or tuple of str, optional\\n1130.             Selects a subset of the length one dimensions. If a dimension is\\n1131.             selected with length greater than one, an error is raised. If\\n1132.             None, all length one dimensions are squeezed.\\n1133. \\n1134.         Returns\\n1135.         -------\\n1136.         squeezed : same type as caller\\n1137.             This object, but with with all or a subset of the dimensions of\\n1138.             length 1 removed.\\n1139. \\n1140.         See Also\\n1141.         --------\\n1142.         numpy.squeeze\\n1143.         """\\n1144.         dims = common.get_squeeze_dims(self, dim)\\n1145.         return self.isel({d: 0 for d in dims})\\n1146. \\n1147.     def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):\\n1148.         axis = self.get_axis_num(dim)\\n1149. \\n1150.         if count > 0:\\n1151.             keep = slice(None, -count)\\n1152.         elif count < 0:\\n1153.             keep = slice(-count, None)\\n1154.         else:\\n1155.             keep = slice(None)\\n1156. \\n1157.         trimmed_data = self[(slice(None),) * axis + (keep,)].data\\n1158. \\n1159.         if fill_value is dtypes.NA:\\n1160.             dtype, fill_value = dtypes.maybe_promote(self.dtype)\\n1161.         else:\\n1162.             dtype = self.dtype\\n1163. \\n1164.         width = min(abs(count), self.shape[axis])\\n1165.         dim_pad = (width, 0) if count >= 0 else (0, width)\\n1166.         pads = [(0, 0) if d != dim else dim_pad for d in self.dims]\\n1167. \\n1168.         data = duck_array_ops.pad(\\n1169.             trimmed_data.astype(dtype),\\n1170.             pads,\\n1171.             mode="constant",\\n1172.             constant_values=fill_value,\\n1173.         )\\n1174. \\n1175.         if isinstance(data, dask_array_type):\\n1176.             # chunked data should come out with the same chunks; this makes\\n1177.             # it feasible to combine shifted and unshifted data\\n1178.             # TODO: remove this once dask.array automatically aligns chunks\\n1179.             data = data.rechunk(self.data.chunks)\\n1180. \\n1181.         return type(self)(self.dims, data, self._attrs, fastpath=True)\\n1182. \\n1183.     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):\\n1184.         """\\n1185.         Return a new Variable with shifted data.\\n1186. \\n1187.         Parameters\\n1188.         ----------\\n1189.         shifts : mapping of the form {dim: offset}\\n1190.             Integer offset to shift along each of the given dimensions.\\n1191.             Positive offsets shift to the right; negative offsets shift to the\\n1192.             left.\\n1193.         fill_value: scalar, optional\\n1194.             Value to use for newly missing values\\n1195.         **shifts_kwargs\\n1196.             The keyword arguments form of ``shifts``.\\n1197.             One of shifts or shifts_kwargs must be provided.\\n1198. \\n1199.         Returns\\n1200.         -------\', \'start_line_number\': 1, \'end_line_number\': 1200}\n\nTest Case Context (similar existing tests for reference):\n{\'relative_path\': \'xarray/tests/test_dataarray.py\', \'content\': \'1. import pickle\\n2. import sys\\n3. import warnings\\n4. from copy import deepcopy\\n5. from textwrap import dedent\\n6. \\n7. import numpy as np\\n8. import pandas as pd\\n9. import pytest\\n10. \\n11. import xarray as xr\\n12. from xarray import DataArray, Dataset, IndexVariable, Variable, align, broadcast\\n13. from xarray.coding.times import CFDatetimeCoder\\n14. from xarray.convert import from_cdms2\\n15. from xarray.core import dtypes\\n16. from xarray.core.common import full_like\\n17. from xarray.core.indexes import propagate_indexes\\n18. from xarray.core.utils import is_scalar\\n19. from xarray.tests import (\\n20.     LooseVersion,\\n21.     ReturnItem,\\n22.     assert_allclose,\\n23.     assert_array_equal,\\n24.     assert_equal,\\n25.     assert_identical,\\n26.     has_dask,\\n27.     raises_regex,\\n28.     requires_bottleneck,\\n29.     requires_dask,\\n30.     requires_iris,\\n31.     requires_numbagg,\\n32.     requires_scipy,\\n33.     requires_sparse,\\n34.     source_ndarray,\\n35. )\\n36. \\n37. from .test_dask import raise_if_dask_computes\\n38. \\n39. \\n40. class TestDataArray:\\n41.     @pytest.fixture(autouse=True)\\n42.     def setup(self):\\n43.         self.attrs = {"attr1": "value1", "attr2": 2929}\\n44.         self.x = np.random.random((10, 20))\\n45.         self.v = Variable(["x", "y"], self.x)\\n46.         self.va = Variable(["x", "y"], self.x, self.attrs)\\n47.         self.ds = Dataset({"foo": self.v})\\n48.         self.dv = self.ds["foo"]\\n49. \\n50.         self.mindex = pd.MultiIndex.from_product(\\n51.             [["a", "b"], [1, 2]], names=("level_1", "level_2")\\n52.         )\\n53.         self.mda = DataArray([0, 1, 2, 3], coords={"x": self.mindex}, dims="x")\\n54. \\n55.     def test_repr(self):\\n56.         v = Variable(["time", "x"], [[1, 2, 3], [4, 5, 6]], {"foo": "bar"})\\n57.         coords = {"x": np.arange(3, dtype=np.int64), "other": np.int64(0)}\\n58.         data_array = DataArray(v, coords, name="my_variable")\\n59.         expected = dedent(\\n60.             """\\\\\\n61.             <xarray.DataArray \\\'my_variable\\\' (time: 2, x: 3)>\\n62.             array([[1, 2, 3],\\n63.                    [4, 5, 6]])\\n64.             Coordinates:\\n65.               * x        (x) int64 0 1 2\\n66.                 other    int64 0\\n67.             Dimensions without coordinates: time\\n68.             Attributes:\\n69.                 foo:      bar"""\\n70.         )\\n71.         assert expected == repr(data_array)\\n72. \\n73.     def test_repr_multiindex(self):\\n74.         expected = dedent(\\n75.             """\\\\\\n76.             <xarray.DataArray (x: 4)>\\n77.             array([0, 1, 2, 3])\\n78.             Coordinates:\\n79.               * x        (x) MultiIndex\\n80.               - level_1  (x) object \\\'a\\\' \\\'a\\\' \\\'b\\\' \\\'b\\\'\\n81.               - level_2  (x) int64 1 2 1 2"""\\n82.         )\\n83.         assert expected == repr(self.mda)\\n84. \\n85.     @pytest.mark.skipif(\\n86.         LooseVersion(np.__version__) < "1.16",\\n87.         reason="old versions of numpy have different printing behavior",\\n88.     )\\n89.     def test_repr_multiindex_long(self):\\n90.         mindex_long = pd.MultiIndex.from_product(\\n91.             [["a", "b", "c", "d"], [1, 2, 3, 4, 5, 6, 7, 8]],\\n92.             names=("level_1", "level_2"),\\n93.         )\\n94.         mda_long = DataArray(list(range(32)), coords={"x": mindex_long}, dims="x")\\n95.         expected = dedent(\\n96.             """\\\\\\n97.             <xarray.DataArray (x: 32)>\\n98.             array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\\n99.                    17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\\n100.             Coordinates:\\n101.               * x        (x) MultiIndex\\n102.               - level_1  (x) object \\\'a\\\' \\\'a\\\' \\\'a\\\' \\\'a\\\' \\\'a\\\' \\\'a\\\' \\\'a\\\' ... \\\'d\\\' \\\'d\\\' \\\'d\\\' \\\'d\\\' \\\'d\\\' \\\'d\\\'\\n103.               - level_2  (x) int64 1 2 3 4 5 6 7 8 1 2 3 4 5 6 ... 4 5 6 7 8 1 2 3 4 5 6 7 8"""\\n104.         )\\n105.         assert expected == repr(mda_long)\\n106. \\n107.     def test_properties(self):\\n108.         assert_equal(self.dv.variable, self.v)\\n109.         assert_array_equal(self.dv.values, self.v.values)\\n110.         for attr in ["dims", "dtype", "shape", "size", "nbytes", "ndim", "attrs"]:\\n111.             assert getattr(self.dv, attr) == getattr(self.v, attr)\\n112.         assert len(self.dv) == len(self.v)\\n113.         assert_equal(self.dv.variable, self.v)\\n114.         assert set(self.dv.coords) == set(self.ds.coords)\\n115.         for k, v in self.dv.coords.items():\\n116.             assert_array_equal(v, self.ds.coords[k])\\n117.         with pytest.raises(AttributeError):\\n118.             self.dv.dataset\\n119.         assert isinstance(self.ds["x"].to_index(), pd.Index)\\n120.         with raises_regex(ValueError, "must be 1-dimensional"):\\n121.             self.ds["foo"].to_index()\\n122.         with pytest.raises(AttributeError):\\n123.             self.dv.variable = self.v\\n124. \\n125.     def test_data_property(self):\\n126.         array = DataArray(np.zeros((3, 4)))\\n127.         actual = array.copy()\\n128.         actual.values = np.ones((3, 4))\\n129.         assert_array_equal(np.ones((3, 4)), actual.values)\\n130.         actual.data = 2 * np.ones((3, 4))\\n131.         assert_array_equal(2 * np.ones((3, 4)), actual.data)\\n132.         assert_array_equal(actual.data, actual.values)\\n133. \\n134.     def test_indexes(self):\\n135.         array = DataArray(np.zeros((2, 3)), [("x", [0, 1]), ("y", ["a", "b", "c"])])\\n136.         expected = {"x": pd.Index([0, 1]), "y": pd.Index(["a", "b", "c"])}\\n137.         assert array.indexes.keys() == expected.keys()\\n138.         for k in expected:\\n139.             assert array.indexes[k].equals(expected[k])\\n140. \\n141.     def test_get_index(self):\\n142.         array = DataArray(np.zeros((2, 3)), coords={"x": ["a", "b"]}, dims=["x", "y"])\\n143.         assert array.get_index("x").equals(pd.Index(["a", "b"]))\\n144.         assert array.get_index("y").equals(pd.Index([0, 1, 2]))\\n145.         with pytest.raises(KeyError):\\n146.             array.get_index("z")\\n147. \\n148.     def test_get_index_size_zero(self):\\n149.         array = DataArray(np.zeros((0,)), dims=["x"])\\n150.         actual = array.get_index("x")\\n151.         expected = pd.Index([], dtype=np.int64)\\n152.         assert actual.equals(expected)\\n153.         assert actual.dtype == expected.dtype\\n154. \\n155.     def test_struct_array_dims(self):\\n156.         """\\n157.         This test checks subraction of two DataArrays for the case\\n158.         when dimension is a structured array.\\n159.         """\\n160.         # GH837, GH861\\n161.         # checking array subtraction when dims are the same\\n162.         p_data = np.array(\\n163.             [("Abe", 180), ("Stacy", 150), ("Dick", 200)],\\n164.             dtype=[("name", "|S256"), ("height", object)],\\n165.         )\\n166.         weights_0 = DataArray(\\n167.             [80, 56, 120], dims=["participant"], coords={"participant": p_data}\\n168.         )\\n169.         weights_1 = DataArray(\\n170.             [81, 52, 115], dims=["participant"], coords={"participant": p_data}\\n171.         )\\n172.         actual = weights_1 - weights_0\\n173. \\n174.         expected = DataArray(\\n175.             [1, -4, -5], dims=["participant"], coords={"participant": p_data}\\n176.         )\\n177. \\n178.         assert_identical(actual, expected)\\n179. \\n180.         # checking array subraction when dims are not the same\\n181.         p_data_alt = np.array(\\n182.             [("Abe", 180), ("Stacy", 151), ("Dick", 200)],\\n183.             dtype=[("name", "|S256"), ("height", object)],\\n184.         )\\n185.         weights_1 = DataArray(\\n186.             [81, 52, 115], dims=["participant"], coords={"participant": p_data_alt}\\n187.         )\\n188.         actual = weights_1 - weights_0\\n189. \\n190.         expected = DataArray(\\n191.             [1, -5], dims=["participant"], coords={"participant": p_data[[0, 2]]}\\n192.         )\\n193. \\n194.         assert_identical(actual, expected)\\n195. \\n196.         # checking array subraction when dims are not the same and one\\n197.         # is np.nan\\n198.         p_data_nan = np.array(\\n199.             [("Abe", 180), ("Stacy", np.nan), ("Dick", 200)],\\n200.             dtype=[("name", "|S256"), ("height", object)],\\n201.         )\\n202.         weights_1 = DataArray(\\n203.             [81, 52, 115], dims=["participant"], coords={"participant": p_data_nan}\\n204.         )\\n205.         actual = weights_1 - weights_0\\n206. \\n207.         expected = DataArray(\\n208.             [1, -5], dims=["participant"], coords={"participant": p_data[[0, 2]]}\\n209.         )\\n210. \\n211.         assert_identical(actual, expected)\\n212. \\n213.     def test_name(self):\\n214.         arr = self.dv\\n215.         assert arr.name == "foo"\\n216. \\n217.         copied = arr.copy()\\n218.         arr.name = "bar"\\n219.         assert arr.name == "bar"\\n220.         assert_equal(copied, arr)\\n221. \\n222.         actual = DataArray(IndexVariable("x", [3]))\\n223.         actual.name = "y"\\n224.         expected = DataArray([3], [("x", [3])], name="y")\\n225.         assert_identical(actual, expected)\\n226. \\n227.     def test_dims(self):\\n228.         arr = self.dv\\n229.         assert arr.dims == ("x", "y")\\n230. \\n231.         with raises_regex(AttributeError, "you cannot assign"):\\n232.             arr.dims = ("w", "z")\\n233. \\n234.     def test_sizes(self):\\n235.         array = DataArray(np.zeros((3, 4)), dims=["x", "y"])\\n236.         assert array.sizes == {"x": 3, "y": 4}\\n237.         assert tuple(array.sizes) == array.dims\\n238.         with pytest.raises(TypeError):\\n239.             array.sizes["foo"] = 5\\n240. \\n241.     def test_encoding(self):\\n242.         expected = {"foo": "bar"}\\n243.         self.dv.encoding["foo"] = "bar"\\n244.         assert expected == self.dv.encoding\\n245. \\n246.         expected = {"baz": 0}\\n247.         self.dv.encoding = expected\\n248. \\n249.         assert expected is not self.dv.encoding\\n250. \\n251.     def test_constructor(self):\\n252.         data = np.random.random((2, 3))\\n253. \\n254.         actual = DataArray(data)\\n255.         expected = Dataset({None: (["dim_0", "dim_1"], data)})[None]\\n256.         assert_identical(expected, actual)\\n257. \\n258.         actual = DataArray(data, [["a", "b"], [-1, -2, -3]])\\n259.         expected = Dataset(\\n260.             {\\n261.                 None: (["dim_0", "dim_1"], data),\\n262.                 "dim_0": ("dim_0", ["a", "b"]),\\n263.                 "dim_1": ("dim_1", [-1, -2, -3]),\\n264.             }\\n265.         )[None]\\n266.         assert_identical(expected, actual)\\n267. \\n268.         actual = DataArray(\\n269.             data, [pd.Index(["a", "b"], name="x"), pd.Index([-1, -2, -3], name="y")]\\n270.         )\\n271.         expected = Dataset(\\n272.             {None: (["x", "y"], data), "x": ("x", ["a", "b"]), "y": ("y", [-1, -2, -3])}\\n273.         )[None]\\n274.         assert_identical(expected, actual)\\n275. \\n276.         coords = [["a", "b"], [-1, -2, -3]]\\n277.         actual = DataArray(data, coords, ["x", "y"])\\n278.         assert_identical(expected, actual)\\n279. \\n280.         coords = [pd.Index(["a", "b"], name="A"), pd.Index([-1, -2, -3], name="B")]\\n281.         actual = DataArray(data, coords, ["x", "y"])\\n282.         assert_identical(expected, actual)\\n283. \\n284.         coords = {"x": ["a", "b"], "y": [-1, -2, -3]}\\n285.         actual = DataArray(data, coords, ["x", "y"])\\n286.         assert_identical(expected, actual)\\n287. \\n288.         coords = [("x", ["a", "b"]), ("y", [-1, -2, -3])]\\n289.         actual = DataArray(data, coords)\\n290.         assert_identical(expected, actual)\\n291. \\n292.         expected = Dataset({None: (["x", "y"], data), "x": ("x", ["a", "b"])})[None]\\n293.         actual = DataArray(data, {"x": ["a", "b"]}, ["x", "y"])\\n294.         assert_identical(expected, actual)\\n295. \\n296.         actual = DataArray(data, dims=["x", "y"])\\n297.         expected = Dataset({None: (["x", "y"], data)})[None]\\n298.         assert_identical(expected, actual)\\n299. \\n300.         actual = DataArray(data, dims=["x", "y"], name="foo")\\n301.         expected = Dataset({"foo": (["x", "y"], data)})["foo"]\\n302.         assert_identical(expected, actual)\\n303. \\n304.         actual = DataArray(data, name="foo")\\n305.         expected = Dataset({"foo": (["dim_0", "dim_1"], data)})["foo"]\\n306.         assert_identical(expected, actual)\\n307. \\n308.         actual = DataArray(data, dims=["x", "y"], attrs={"bar": 2})\\n309.         expected = Dataset({None: (["x", "y"], data, {"bar": 2})})[None]\\n310.         assert_identical(expected, actual)\\n311. \\n312.         actual = DataArray(data, dims=["x", "y"])\\n313.         expected = Dataset({None: (["x", "y"], data, {}, {"bar": 2})})[None]\\n314.         assert_identical(expected, actual)\\n315. \\n316.     def test_constructor_invalid(self):\\n317.         data = np.random.randn(3, 2)\\n318. \\n319.         with raises_regex(ValueError, "coords is not dict-like"):\\n320.             DataArray(data, [[0, 1, 2]], ["x", "y"])\\n321. \\n322.         with raises_regex(ValueError, "not a subset of the .* dim"):\\n323.             DataArray(data, {"x": [0, 1, 2]}, ["a", "b"])\\n324.         with raises_regex(ValueError, "not a subset of the .* dim"):\\n325.             DataArray(data, {"x": [0, 1, 2]})\\n326. \\n327.         with raises_regex(TypeError, "is not a string"):\\n328.             DataArray(data, dims=["x", None])\\n329. \\n330.         with raises_regex(ValueError, "conflicting sizes for dim"):\\n331.             DataArray([1, 2, 3], coords=[("x", [0, 1])])\\n332.         with raises_regex(ValueError, "conflicting sizes for dim"):\\n333.             DataArray([1, 2], coords={"x": [0, 1], "y": ("x", [1])}, dims="x")\\n334. \\n335.         with raises_regex(ValueError, "conflicting MultiIndex"):\\n336.             DataArray(np.random.rand(4, 4), [("x", self.mindex), ("y", self.mindex)])\\n337.         with raises_regex(ValueError, "conflicting MultiIndex"):\\n338.             DataArray(np.random.rand(4, 4), [("x", self.mindex), ("level_1", range(4))])\\n339. \\n340.         with raises_regex(ValueError, "matching the dimension size"):\\n341.             DataArray(data, coords={"x": 0}, dims=["x", "y"])\\n342. \\n343.     def test_constructor_from_self_described(self):\\n344.         data = [[-0.1, 21], [0, 2]]\\n345.         expected = DataArray(\\n346.             data,\\n347.             coords={"x": ["a", "b"], "y": [-1, -2]},\\n348.             dims=["x", "y"],\\n349.             name="foobar",\\n350.             attrs={"bar": 2},\\n351.         )\\n352.         actual = DataArray(expected)\\n353.         assert_identical(expected, actual)\\n354. \\n355.         actual = DataArray(expected.values, actual.coords)\\n356.         assert_equal(expected, actual)\\n357. \\n358.         frame = pd.DataFrame(\\n359.             data,\\n360.             index=pd.Index(["a", "b"], name="x"),\\n361.             columns=pd.Index([-1, -2], name="y"),\\n362.         )\\n363.         actual = DataArray(frame)\\n364.         assert_equal(expected, actual)\\n365. \\n366.         series = pd.Series(data[0], index=pd.Index([-1, -2], name="y"))\\n367.         actual = DataArray(series)\\n368.         assert_equal(expected[0].reset_coords("x", drop=True), actual)\\n369. \\n370.         if LooseVersion(pd.__version__) < "0.25.0":\\n371.             with warnings.catch_warnings():\\n372.                 warnings.filterwarnings("ignore", r"\\\\W*Panel is deprecated")\\n373.                 panel = pd.Panel({0: frame})\\n374.             actual = DataArray(panel)\\n375.             expected = DataArray([data], expected.coords, ["dim_0", "x", "y"])\\n376.             expected["dim_0"] = [0]\\n377.             assert_identical(expected, actual)\\n378. \\n379.         expected = DataArray(\\n380.             data,\\n381.             coords={"x": ["a", "b"], "y": [-1, -2], "a": 0, "z": ("x", [-0.5, 0.5])},\\n382.             dims=["x", "y"],\\n383.         )\\n384.         actual = DataArray(expected)\\n385.         assert_identical(expected, actual)\\n386. \\n387.         actual = DataArray(expected.values, expected.coords)\\n388.         assert_identical(expected, actual)\\n389. \\n390.         expected = Dataset({"foo": ("foo", ["a", "b"])})["foo"]\\n391.         actual = DataArray(pd.Index(["a", "b"], name="foo"))\\n392.         assert_identical(expected, actual)\\n393. \\n394.         actual = DataArray(IndexVariable("foo", ["a", "b"]))\\n395.         assert_identical(expected, actual)\\n396. \\n397.     def test_constructor_from_0d(self):\\n398.         expected = Dataset({None: ([], 0)})[None]\\n399.         actual = DataArray(0)\\n400.         assert_identical(expected, actual)\\n401. \\n402.     @requires_dask\\n403.     def test_constructor_dask_coords(self):\\n404.         # regression test for GH1684\\n405.         import dask.array as da\\n406. \\n407.         coord = da.arange(8, chunks=(4,))\\n408.         data = da.random.random((8, 8), chunks=(4, 4)) + 1\\n409.         actual = DataArray(data, coords={"x": coord, "y": coord}, dims=["x", "y"])\\n410. \\n411.         ecoord = np.arange(8)\\n412.         expected = DataArray(data, coords={"x": ecoord, "y": ecoord}, dims=["x", "y"])\\n413.         assert_equal(actual, expected)\\n414. \\n415.     def test_equals_and_identical(self):\\n416.         orig = DataArray(np.arange(5.0), {"a": 42}, dims="x")\\n417. \\n418.         expected = orig\\n419.         actual = orig.copy()\\n420.         assert expected.equals(actual)\\n421.         assert expected.identical(actual)\\n422. \\n423.         actual = expected.rename("baz")\\n424.         assert expected.equals(actual)\\n425.         assert not expected.identical(actual)\\n426. \\n427.         actual = expected.rename({"x": "xxx"})\\n428.         assert not expected.equals(actual)\\n429.         assert not expected.identical(actual)\\n430. \\n431.         actual = expected.copy()\\n432.         actual.attrs["foo"] = "bar"\\n433.         assert expected.equals(actual)\\n434.         assert not expected.identical(actual)\\n435. \\n436.         actual = expected.copy()\\n437.         actual["x"] = ("x", -np.arange(5))\\n438.         assert not expected.equals(actual)\\n439.         assert not expected.identical(actual)\\n440. \\n441.         actual = expected.reset_coords(drop=True)\\n442.         assert not expected.equals(actual)\\n443.         assert not expected.identical(actual)\\n444. \\n445.         actual = orig.copy()\\n446.         actual[0] = np.nan\\n447.         expected = actual.copy()\\n448.         assert expected.equals(actual)\\n449.         assert expected.identical(actual)\\n450. \\n451.         actual[:] = np.nan\\n452.         assert not expected.equals(actual)\\n453.         assert not expected.identical(actual)\\n454. \\n455.         actual = expected.copy()\\n456.         actual["a"] = 100000\\n457.         assert not expected.equals(actual)\\n458.         assert not expected.identical(actual)\\n459. \\n460.     def test_equals_failures(self):\\n461.         orig = DataArray(np.arange(5.0), {"a": 42}, dims="x")\\n462.         assert not orig.equals(np.arange(5))\\n463.         assert not orig.identical(123)\\n464.         assert not orig.broadcast_equals({1: 2})\\n465. \\n466.     def test_broadcast_equals(self):\\n467.         a = DataArray([0, 0], {"y": 0}, dims="x")\\n468.         b = DataArray([0, 0], {"y": ("x", [0, 0])}, dims="x")\\n469.         assert a.broadcast_equals(b)\\n470.         assert b.broadcast_equals(a)\\n471.         assert not a.equals(b)\\n472.         assert not a.identical(b)\\n473. \\n474.         c = DataArray([0], coords={"x": 0}, dims="y")\\n475.         assert not a.broadcast_equals(c)\\n476.         assert not c.broadcast_equals(a)\\n477. \\n478.     def test_getitem(self):\\n479.         # strings pull out dataarrays\\n480.         assert_identical(self.dv, self.ds["foo"])\\n481.         x = self.dv["x"]\\n482.         y = self.dv["y"]\\n483.         assert_identical(self.ds["x"], x)\\n484.         assert_identical(self.ds["y"], y)\\n485. \\n486.         arr = ReturnItem()\\n487.         for i in [\\n488.             arr[:],\\n489.             arr[...],\\n490.             arr[x.values],\\n491.             arr[x.variable],\\n492.             arr[x],\\n493.             arr[x, y],\\n494.             arr[x.values > -1],\\n495.             arr[x.variable > -1],\\n496.             arr[x > -1],\\n497.             arr[x > -1, y > -1],\\n498.         ]:\\n499.             assert_equal(self.dv, self.dv[i])\\n500.         for i in [\\n501.             arr[0],\\n502.             arr[:, 0],\\n503.             arr[:3, :2],\\n504.             arr[x.values[:3]],\\n505.             arr[x.variable[:3]],\\n506.             arr[x[:3]],\\n507.             arr[x[:3], y[:4]],\\n508.             arr[x.values > 3],\\n509.             arr[x.variable > 3],\\n510.             arr[x > 3],\\n511.             arr[x > 3, y > 3],\\n512.         ]:\\n513.             assert_array_equal(self.v[i], self.dv[i])\\n514. \\n515.     def test_getitem_dict(self):\\n516.         actual = self.dv[{"x": slice(3), "y": 0}]\\n517.         expected = self.dv.isel(x=slice(3), y=0)\\n518.         assert_identical(expected, actual)\\n519. \\n520.     def test_getitem_coords(self):\\n521.         orig = DataArray(\\n522.             [[10], [20]],\\n523.             {\\n524.                 "x": [1, 2],\\n525.                 "y": [3],\\n526.                 "z": 4,\\n527.                 "x2": ("x", ["a", "b"]),\\n528.                 "y2": ("y", ["c"]),\\n529.                 "xy": (["y", "x"], [["d", "e"]]),\\n530.             },\\n531.             dims=["x", "y"],\\n532.         )\\n533. \\n534.         assert_identical(orig, orig[:])\\n535.         assert_identical(orig, orig[:, :])\\n536.         assert_identical(orig, orig[...])\\n537.         assert_identical(orig, orig[:2, :1])\\n538.         assert_identical(orig, orig[[0, 1], [0]])\\n539. \\n540.         actual = orig[0, 0]\\n541.         expected = DataArray(\\n542.             10, {"x": 1, "y": 3, "z": 4, "x2": "a", "y2": "c", "xy": "d"}\\n543.         )\\n544.         assert_identical(expected, actual)\\n545. \\n546.         actual = orig[0, :]\\n547.         expected = DataArray(\\n548.             [10],\\n549.             {\\n550.                 "x": 1,\\n551.                 "y": [3],\\n552.                 "z": 4,\\n553.                 "x2": "a",\\n554.                 "y2": ("y", ["c"]),\\n555.                 "xy": ("y", ["d"]),\\n556.             },\\n557.             dims="y",\\n558.         )\\n559.         assert_identical(expected, actual)\\n560. \\n561.         actual = orig[:, 0]\\n562.         expected = DataArray(\\n563.             [10, 20],\\n564.             {\\n565.                 "x": [1, 2],\\n566.                 "y": 3,\\n567.                 "z": 4,\\n568.                 "x2": ("x", ["a", "b"]),\\n569.                 "y2": "c",\\n570.                 "xy": ("x", ["d", "e"]),\\n571.             },\\n572.             dims="x",\\n573.         )\\n574.         assert_identical(expected, actual)\\n575. \\n576.     def test_getitem_dataarray(self):\\n577.         # It should not conflict\\n578.         da = DataArray(np.arange(12).reshape((3, 4)), dims=["x", "y"])\\n579.         ind = DataArray([[0, 1], [0, 1]], dims=["x", "z"])\\n580.         actual = da[ind]\\n581.         assert_array_equal(actual, da.values[[[0, 1], [0, 1]], :])\\n582. \\n583.         da = DataArray(\\n584.             np.arange(12).reshape((3, 4)),\\n585.             dims=["x", "y"],\\n586.             coords={"x": [0, 1, 2], "y": ["a", "b", "c", "d"]},\\n587.         )\\n588.         ind = xr.DataArray([[0, 1], [0, 1]], dims=["X", "Y"])\\n589.         actual = da[ind]\\n590.         expected = da.values[[[0, 1], [0, 1]], :]\\n591.         assert_array_equal(actual, expected)\\n592.         assert actual.dims == ("X", "Y", "y")\\n593. \\n594.         # boolean indexing\\n595.         ind = xr.DataArray([True, True, False], dims=["x"])\\n596.         assert_equal(da[ind], da[[0, 1], :])\\n597.         assert_equal(da[ind], da[[0, 1]])\\n598.         assert_equal(da[ind], da[ind.values])\\n599. \\n600.     def test_getitem_empty_index(self):\\n601.         da = DataArray(np.arange(12).reshape((3, 4)), dims=["x", "y"])\\n602.         assert_identical(da[{"x": []}], DataArray(np.zeros((0, 4)), dims=["x", "y"]))\\n603.         assert_identical(\\n604.             da.loc[{"y": []}], DataArray(np.zeros((3, 0)), dims=["x", "y"])\\n605.         )\\n606.         assert_identical(da[[]], DataArray(np.zeros((0, 4)), dims=["x", "y"]))\\n607. \\n608.     def test_setitem(self):\\n609.         # basic indexing should work as numpy\\\'s indexing\\n610.         tuples = [\\n611.             (0, 0),\\n612.             (0, slice(None, None)),\\n613.             (slice(None, None), slice(None, None)),\\n614.             (slice(None, None), 0),\\n615.             ([1, 0], slice(None, None)),\\n616.             (slice(None, None), [1, 0]),\\n617.         ]\\n618.         for t in tuples:\\n619.             expected = np.arange(6).reshape(3, 2)\\n620.             orig = DataArray(\\n621.                 np.arange(6).reshape(3, 2),\\n622.                 {\\n623.                     "x": [1, 2, 3],\\n624.                     "y": ["a", "b"],\\n625.                     "z": 4,\\n626.                     "x2": ("x", ["a", "b", "c"]),\\n627.                     "y2": ("y", ["d", "e"]),\\n628.                 },\\n629.                 dims=["x", "y"],\\n630.             )\\n631.             orig[t] = 1\\n632.             expected[t] = 1\\n633.             assert_array_equal(orig.values, expected)\\n634. \\n635.     def test_setitem_fancy(self):\\n636.         # vectorized indexing\\n637.         da = DataArray(np.ones((3, 2)), dims=["x", "y"])\\n638.         ind = Variable(["a"], [0, 1])\\n639.         da[dict(x=ind, y=ind)] = 0\\n640.         expected = DataArray([[0, 1], [1, 0], [1, 1]], dims=["x", "y"])\\n641.         assert_identical(expected, da)\\n642.         # assign another 0d-variable\\n643.         da[dict(x=ind, y=ind)] = Variable((), 0)\\n644.         expected = DataArray([[0, 1], [1, 0], [1, 1]], dims=["x", "y"])\\n645.         assert_identical(expected, da)\\n646.         # assign another 1d-variable\\n647.         da[dict(x=ind, y=ind)] = Variable(["a"], [2, 3])\\n648.         expected = DataArray([[2, 1], [1, 3], [1, 1]], dims=["x", "y"])\\n649.         assert_identical(expected, da)\\n650. \\n651.         # 2d-vectorized indexing\\n652.         da = DataArray(np.ones((3, 2)), dims=["x", "y"])\\n653.         ind_x = DataArray([[0, 1]], dims=["a", "b"])\\n654.         ind_y = DataArray([[1, 0]], dims=["a", "b"])\\n655.         da[dict(x=ind_x, y=ind_y)] = 0\\n656.         expected = DataArray([[1, 0], [0, 1], [1, 1]], dims=["x", "y"])\\n657.         assert_identical(expected, da)\\n658. \\n659.         da = DataArray(np.ones((3, 2)), dims=["x", "y"])\\n660.         ind = Variable(["a"], [0, 1])\\n661.         da[ind] = 0\\n662.         expected = DataArray([[0, 0], [0, 0], [1, 1]], dims=["x", "y"])\\n663.         assert_identical(expected, da)\\n664. \\n665.     def test_setitem_dataarray(self):\\n666.         def get_data():\\n667.             return DataArray(\\n668.                 np.ones((4, 3, 2)),\\n669.                 dims=["x", "y", "z"],\\n670.                 coords={\\n671.                     "x": np.arange(4),\\n672.                     "y": ["a", "b", "c"],\\n673.                     "non-dim": ("x", [1, 3, 4, 2]),\\n674.                 },\\n675.             )\\n676. \\n677.         da = get_data()\\n678.         # indexer with inconsistent coordinates.\\n679.         ind = DataArray(np.arange(1, 4), dims=["x"], coords={"x": np.random.randn(3)})\\n680.         with raises_regex(IndexError, "dimension coordinate \\\'x\\\'"):\\n681.             da[dict(x=ind)] = 0\\n682. \\n683.         # indexer with consistent coordinates.\\n684.         ind = DataArray(np.arange(1, 4), dims=["x"], coords={"x": np.arange(1, 4)})\\n685.         da[dict(x=ind)] = 0  # should not raise\\n686.         assert np.allclose(da[dict(x=ind)].values, 0)\\n687.         assert_identical(da["x"], get_data()["x"])\\n688.         assert_identical(da["non-dim"], get_data()["non-dim"])\\n689. \\n690.         da = get_data()\\n691.         # conflict in the assigning values\\n692.         value = xr.DataArray(\\n693.             np.zeros((3, 3, 2)),\\n694.             dims=["x", "y", "z"],\\n695.             coords={"x": [0, 1, 2], "non-dim": ("x", [0, 2, 4])},\\n696.         )\\n697.         with raises_regex(IndexError, "dimension coordinate \\\'x\\\'"):\\n698.             da[dict(x=ind)] = value\\n699. \\n700.         # consistent coordinate in the assigning values\\n701.         value = xr.DataArray(\\n702.             np.zeros((3, 3, 2)),\\n703.             dims=["x", "y", "z"],\\n704.             coords={"x": [1, 2, 3], "non-dim": ("x", [0, 2, 4])},\\n705.         )\\n706.         da[dict(x=ind)] = value\\n707.         assert np.allclose(da[dict(x=ind)].values, 0)\\n708.         assert_identical(da["x"], get_data()["x"])\\n709.         assert_identical(da["non-dim"], get_data()["non-dim"])\\n710. \\n711.         # Conflict in the non-dimension coordinate\\n712.         value = xr.DataArray(\\n713.             np.zeros((3, 3, 2)),\\n714.             dims=["x", "y", "z"],\\n715.             coords={"x": [1, 2, 3], "non-dim": ("x", [0, 2, 4])},\\n716.         )\\n717.         da[dict(x=ind)] = value  # should not raise\\n718. \\n719.         # conflict in the assigning values\\n720.         value = xr.DataArray(\\n721.             np.zeros((3, 3, 2)),\\n722.             dims=["x", "y", "z"],\\n723.             coords={"x": [0, 1, 2], "non-dim": ("x", [0, 2, 4])},\\n724.         )\\n725.         with raises_regex(IndexError, "dimension coordinate \\\'x\\\'"):\\n726.             da[dict(x=ind)] = value\\n727. \\n728.         # consistent coordinate in the assigning values\\n729.         value = xr.DataArray(\\n730.             np.zeros((3, 3, 2)),\\n731.             dims=["x", "y", "z"],\\n732.             coords={"x": [1, 2, 3], "non-dim": ("x", [0, 2, 4])},\\n733.         )\\n734.         da[dict(x=ind)] = value  # should not raise\\n735. \\n736.     def test_contains(self):\\n737.         data_array = DataArray([1, 2])\\n738.         assert 1 in data_array\\n739.         assert 3 not in data_array\\n740. \\n741.     def test_attr_sources_multiindex(self):\\n742.         # make sure attr-style access for multi-index levels\\n743.         # returns DataArray objects\\n744.         assert isinstance(self.mda.level_1, DataArray)\\n745. \\n746.     def test_pickle(self):\\n747.         data = DataArray(np.random.random((3, 3)), dims=("id", "time"))\\n748.         roundtripped = pickle.loads(pickle.dumps(data))\\n749.         assert_identical(data, roundtripped)\\n750. \\n751.     @requires_dask\\n752.     def test_chunk(self):\\n753.         unblocked = DataArray(np.ones((3, 4)))\\n754.         assert unblocked.chunks is None\\n755. \\n756.         blocked = unblocked.chunk()\\n757.         assert blocked.chunks == ((3,), (4,))\\n758.         first_dask_name = blocked.data.name\\n759. \\n760.         blocked = unblocked.chunk(chunks=((2, 1), (2, 2)))\\n761.         assert blocked.chunks == ((2, 1), (2, 2))\\n762.         assert blocked.data.name != first_dask_name\\n763. \\n764.         blocked = unblocked.chunk(chunks=(3, 3))\\n765.         assert blocked.chunks == ((3,), (3, 1))\\n766.         assert blocked.data.name != first_dask_name\\n767. \\n768.         # name doesn\\\'t change when rechunking by same amount\\n769.         # this fails if ReprObject doesn\\\'t have __dask_tokenize__ defined\\n770.         assert unblocked.chunk(2).data.name == unblocked.chunk(2).data.name\\n771. \\n772.         assert blocked.load().chunks is None\\n773. \\n774.         # Check that kwargs are passed\\n775.         import dask.array as da\\n776. \\n777.         blocked = unblocked.chunk(name_prefix="testname_")\\n778.         assert isinstance(blocked.data, da.Array)\\n779.         assert "testname_" in blocked.data.name\\n780. \\n781.     def test_isel(self):\\n782.         assert_identical(self.dv[0], self.dv.isel(x=0))\\n783.         assert_identical(self.dv, self.dv.isel(x=slice(None)))\\n784.         assert_identical(self.dv[:3], self.dv.isel(x=slice(3)))\\n785.         assert_identical(self.dv[:3, :5], self.dv.isel(x=slice(3), y=slice(5)))\\n786.         with raises_regex(\\n787.             ValueError,\\n788.             r"dimensions {\\\'not_a_dim\\\'} do not exist. Expected "\\n789.             r"one or more of \\\\(\\\'x\\\', \\\'y\\\'\\\\)",\\n790.         ):\\n791.             self.dv.isel(not_a_dim=0)\\n792.         with pytest.warns(\\n793.             UserWarning,\\n794.             match=r"dimensions {\\\'not_a_dim\\\'} do not exist. "\\n795.             r"Expected one or more of \\\\(\\\'x\\\', \\\'y\\\'\\\\)",\\n796.         ):\\n797.             self.dv.isel(not_a_dim=0, missing_dims="warn")\\n798.         assert_identical(self.dv, self.dv.isel(not_a_dim=0, missing_dims="ignore"))\\n799. \\n800.     def test_isel_types(self):\\n801.         # regression test for #1405\\n802.         da = DataArray([1, 2, 3], dims="x")\\n803.         # uint64\\n804.         assert_identical(\\n805.             da.isel(x=np.array([0], dtype="uint64")), da.isel(x=np.array([0]))\\n806.         )\\n807.         # uint32\\n808.         assert_identical(\\n809.             da.isel(x=np.array([0], dtype="uint32")), da.isel(x=np.array([0]))\\n810.         )\\n811.         # int64\\n812.         assert_identical(\\n813.             da.isel(x=np.array([0], dtype="int64")), da.isel(x=np.array([0]))\\n814.         )\\n815. \\n816.     @pytest.mark.filterwarnings("ignore::DeprecationWarning")\\n817.     def test_isel_fancy(self):\\n818.         shape = (10, 7, 6)\\n819.         np_array = np.random.random(shape)\\n820.         da = DataArray(\\n821.             np_array, dims=["time", "y", "x"], coords={"time": np.arange(0, 100, 10)}\\n822.         )\\n823.         y = [1, 3]\\n824.         x = [3, 0]\\n825. \\n826.         expected = da.values[:, y, x]\\n827. \\n828.         actual = da.isel(y=(("test_coord",), y), x=(("test_coord",), x))\\n829.         assert actual.coords["test_coord"].shape == (len(y),)\\n830.         assert list(actual.coords) == ["time"]\\n831.         assert actual.dims == ("time", "test_coord")\\n832. \\n833.         np.testing.assert_equal(actual, expected)\\n834. \\n835.         # a few corner cases\\n836.         da.isel(\\n837.             time=(("points",), [1, 2]), x=(("points",), [2, 2]), y=(("points",), [3, 4])\\n838.         )\\n839.         np.testing.assert_allclose(\\n840.             da.isel(\\n841.                 time=(("p",), [1]), x=(("p",), [2]), y=(("p",), [4])\\n842.             ).values.squeeze(),\\n843.             np_array[1, 4, 2].squeeze(),\\n844.         )\\n845.         da.isel(time=(("points",), [1, 2]))\\n846.         y = [-1, 0]\\n847.         x = [-2, 2]\\n848.         expected = da.values[:, y, x]\\n849.         actual = da.isel(x=(("points",), x), y=(("points",), y)).values\\n850.         np.testing.assert_equal(actual, expected)\\n851. \\n852.         # test that the order of the indexers doesn\\\'t matter\\n853.         assert_identical(\\n854.             da.isel(y=(("points",), y), x=(("points",), x)),\\n855.             da.isel(x=(("points",), x), y=(("points",), y)),\\n856.         )\\n857. \\n858.         # make sure we\\\'re raising errors in the right places\\n859.         with raises_regex(IndexError, "Dimensions of indexers mismatch"):\\n860.             da.isel(y=(("points",), [1, 2]), x=(("points",), [1, 2, 3]))\\n861. \\n862.         # tests using index or DataArray as indexers\\n863.         stations = Dataset()\\n864.         stations["station"] = (("station",), ["A", "B", "C"])\\n865.         stations["dim1s"] = (("station",), [1, 2, 3])\\n866.         stations["dim2s"] = (("station",), [4, 5, 1])\\n867. \\n868.         actual = da.isel(x=stations["dim1s"], y=stations["dim2s"])\\n869.         assert "station" in actual.coords\\n870.         assert "station" in actual.dims\\n871.         assert_identical(actual["station"], stations["station"])\\n872. \\n873.         with raises_regex(ValueError, "conflicting values for "):\\n874.             da.isel(\\n875.                 x=DataArray([0, 1, 2], dims="station", coords={"station": [0, 1, 2]}),\\n876.                 y=DataArray([0, 1, 2], dims="station", coords={"station": [0, 1, 3]}),\\n877.             )\\n878. \\n879.         # multi-dimensional selection\\n880.         stations = Dataset()\\n881.         stations["a"] = (("a",), ["A", "B", "C"])\\n882.         stations["b"] = (("b",), [0, 1])\\n883.         stations["dim1s"] = (("a", "b"), [[1, 2], [2, 3], [3, 4]])\\n884.         stations["dim2s"] = (("a",), [4, 5, 1])\\n885. \\n886.         actual = da.isel(x=stations["dim1s"], y=stations["dim2s"])\\n887.         assert "a" in actual.coords\\n888.         assert "a" in actual.dims\\n889.         assert "b" in actual.coords\\n890.         assert "b" in actual.dims\\n891.         assert_identical(actual["a"], stations["a"])\\n892.         assert_identical(actual["b"], stations["b"])\\n893.         expected = da.variable[\\n894.             :, stations["dim2s"].variable, stations["dim1s"].variable\\n895.         ]\\n896.         assert_array_equal(actual, expected)\\n897. \\n898.     def test_sel(self):\\n899.         self.ds["x"] = ("x", np.array(list("abcdefghij")))\\n900.         da = self.ds["foo"]\\n901.         assert_identical(da, da.sel(x=slice(None)))\\n902.         assert_identical(da[1], da.sel(x="b"))\\n903.         assert_identical(da[:3], da.sel(x=slice("c")))\\n904.         assert_identical(da[:3], da.sel(x=["a", "b", "c"]))\\n905.         assert_identical(da[:, :4], da.sel(y=(self.ds["y"] < 4)))\\n906.         # verify that indexing with a dataarray works\\n907.         b = DataArray("b")\\n908.         assert_identical(da[1], da.sel(x=b))\\n909.         assert_identical(da[[1]], da.sel(x=slice(b, b)))\\n910. \\n911.     def test_sel_dataarray(self):\\n912.         # indexing with DataArray\\n913.         self.ds["x"] = ("x", np.array(list("abcdefghij")))\\n914.         da = self.ds["foo"]\\n915. \\n916.         ind = DataArray(["a", "b", "c"], dims=["x"])\\n917.         actual = da.sel(x=ind)\\n918.         assert_identical(actual, da.isel(x=[0, 1, 2]))\\n919. \\n920.         # along new dimension\\n921.         ind = DataArray(["a", "b", "c"], dims=["new_dim"])\\n922.         actual = da.sel(x=ind)\\n923.         assert_array_equal(actual, da.isel(x=[0, 1, 2]))\\n924.         assert "new_dim" in actual.dims\\n925. \\n926.         # with coordinate\\n927.         ind = DataArray(\\n928.             ["a", "b", "c"], dims=["new_dim"], coords={"new_dim": [0, 1, 2]}\\n929.         )\\n930.         actual = da.sel(x=ind)\\n931.         assert_array_equal(actual, da.isel(x=[0, 1, 2]))\\n932.         assert "new_dim" in actual.dims\\n933.         assert "new_dim" in actual.coords\\n934.         assert_equal(actual["new_dim"].drop_vars("x"), ind["new_dim"])\\n935. \\n936.     def test_sel_invalid_slice(self):\\n937.         array = DataArray(np.arange(10), [("x", np.arange(10))])\\n938.         with raises_regex(ValueError, "cannot use non-scalar arrays"):\\n939.             array.sel(x=slice(array.x))\\n940. \\n941.     def test_sel_dataarray_datetime(self):\\n942.         # regression test for GH1240\\n943.         times = pd.date_range("2000-01-01", freq="D", periods=365)\\n944.         array = DataArray(np.arange(365), [("time", times)])\\n945.         result = array.sel(time=slice(array.time[0], array.time[-1]))\\n946.         assert_equal(result, array)\\n947. \\n948.         array = DataArray(np.arange(365), [("delta", times - times[0])])\\n949.         result = array.sel(delta=slice(array.delta[0], array.delta[-1]))\\n950.         assert_equal(result, array)\\n951. \\n952.     def test_sel_float(self):\\n953.         data_values = np.arange(4)\\n954. \\n955.         # case coords are float32 and label is list of floats\\n956.         float_values = [0.0, 0.111, 0.222, 0.333]\\n957.         coord_values = np.asarray(float_values, dtype="float32")\\n958.         array = DataArray(data_values, [("float32_coord", coord_values)])\\n959.         expected = DataArray(data_values[1:3], [("float32_coord", coord_values[1:3])])\\n960.         actual = array.sel(float32_coord=float_values[1:3])\\n961.         # case coords are float16 and label is list of floats\\n962.         coord_values_16 = np.asarray(float_values, dtype="float16")\\n963.         expected_16 = DataArray(\\n964.             data_values[1:3], [("float16_coord", coord_values_16[1:3])]\\n965.         )\\n966.         array_16 = DataArray(data_values, [("float16_coord", coord_values_16)])\\n967.         actual_16 = array_16.sel(float16_coord=float_values[1:3])\\n968. \\n969.         # case coord, label are scalars\\n970.         expected_scalar = DataArray(\\n971.             data_values[2], coords={"float32_coord": coord_values[2]}\\n972.         )\\n973.         actual_scalar = array.sel(float32_coord=float_values[2])\\n974. \\n975.         assert_equal(expected, actual)\\n976.         assert_equal(expected_scalar, actual_scalar)\\n977.         assert_equal(expected_16, actual_16)\\n978. \\n979.     def test_sel_no_index(self):\\n980.         array = DataArray(np.arange(10), dims="x")\\n981.         assert_identical(array[0], array.sel(x=0))\\n982.         assert_identical(array[:5], array.sel(x=slice(5)))\\n983.         assert_identical(array[[0, -1]], array.sel(x=[0, -1]))\\n984.         assert_identical(array[array < 5], array.sel(x=(array < 5)))\\n985. \\n986.     def test_sel_method(self):\\n987.         data = DataArray(np.random.randn(3, 4), [("x", [0, 1, 2]), ("y", list("abcd"))])\\n988. \\n989.         expected = data.sel(y=["a", "b"])\\n990.         actual = data.sel(y=["ab", "ba"], method="pad")\\n991.         assert_identical(expected, actual)\\n992. \\n993.         expected = data.sel(x=[1, 2])\\n994.         actual = data.sel(x=[0.9, 1.9], method="backfill", tolerance=1)\\n995.         assert_identical(expected, actual)\\n996. \\n997.     def test_sel_drop(self):\\n998.         data = DataArray([1, 2, 3], [("x", [0, 1, 2])])\\n999.         expected = DataArray(1)\\n1000.         selected = data.sel(x=0, drop=True)\\n1001.         assert_identical(expected, selected)\\n1002. \\n1003.         expected = DataArray(1, {"x": 0})\\n1004.         selected = data.sel(x=0, drop=False)\\n1005.         assert_identical(expected, selected)\\n1006. \\n1007.         data = DataArray([1, 2, 3], dims=["x"])\\n1008.         expected = DataArray(1)\\n1009.         selected = data.sel(x=0, drop=True)\\n1010.         assert_identical(expected, selected)\\n1011. \\n1012.     def test_isel_drop(self):\\n1013.         data = DataArray([1, 2, 3], [("x", [0, 1, 2])])\\n1014.         expected = DataArray(1)\\n1015.         selected = data.isel(x=0, drop=True)\\n1016.         assert_identical(expected, selected)\\n1017. \\n1018.         expected = DataArray(1, {"x": 0})\\n1019.         selected = data.isel(x=0, drop=False)\\n1020.         assert_identical(expected, selected)\\n1021. \\n1022.     def test_head(self):\\n1023.         assert_equal(self.dv.isel(x=slice(5)), self.dv.head(x=5))\\n1024.         assert_equal(self.dv.isel(x=slice(0)), self.dv.head(x=0))\\n1025.         assert_equal(\\n1026.             self.dv.isel({dim: slice(6) for dim in self.dv.dims}), self.dv.head(6)\\n1027.         )\\n1028.         assert_equal(\\n1029.             self.dv.isel({dim: slice(5) for dim in self.dv.dims}), self.dv.head()\\n1030.         )\\n1031.         with raises_regex(TypeError, "either dict-like or a single int"):\\n1032.             self.dv.head([3])\\n1033.         with raises_regex(TypeError, "expected integer type"):\\n1034.             self.dv.head(x=3.1)\\n1035.         with raises_regex(ValueError, "expected positive int"):\\n1036.             self.dv.head(-3)\\n1037. \\n1038.     def test_tail(self):\\n1039.         assert_equal(self.dv.isel(x=slice(-5, None)), self.dv.tail(x=5))\\n1040.         assert_equal(self.dv.isel(x=slice(0)), self.dv.tail(x=0))\\n1041.         assert_equal(\\n1042.             self.dv.isel({dim: slice(-6, None) for dim in self.dv.dims}),\\n1043.             self.dv.tail(6),\\n1044.         )\\n1045.         assert_equal(\\n1046.             self.dv.isel({dim: slice(-5, None) for dim in self.dv.dims}), self.dv.tail()\\n1047.         )\\n1048.         with raises_regex(TypeError, "either dict-like or a single int"):\\n1049.             self.dv.tail([3])\\n1050.         with raises_regex(TypeError, "expected integer type"):\\n1051.             self.dv.tail(x=3.1)\\n1052.         with raises_regex(ValueError, "expected positive int"):\\n1053.             self.dv.tail(-3)\\n1054. \\n1055.     def test_thin(self):\\n1056.         assert_equal(self.dv.isel(x=slice(None, None, 5)), self.dv.thin(x=5))\\n1057.         assert_equal(\\n1058.             self.dv.isel({dim: slice(None, None, 6) for dim in self.dv.dims}),\\n1059.             self.dv.thin(6),\\n1060.         )\\n1061.         with raises_regex(TypeError, "either dict-like or a single int"):\\n1062.             self.dv.thin([3])\\n1063.         with raises_regex(TypeError, "expected integer type"):\\n1064.             self.dv.thin(x=3.1)\\n1065.         with raises_regex(ValueError, "expected positive int"):\\n1066.             self.dv.thin(-3)\\n1067.         with raises_regex(ValueError, "cannot be zero"):\\n1068.             self.dv.thin(time=0)\\n1069. \\n1070.     def test_loc(self):\\n1071.         self.ds["x"] = ("x", np.array(list("abcdefghij")))\\n1072.         da = self.ds["foo"]\\n1073.         assert_identical(da[:3], da.loc[:"c"])\\n1074.         assert_identical(da[1], da.loc["b"])\\n1075.         assert_identical(da[1], da.loc[{"x": "b"}])\\n1076.         assert_identical(da[1], da.loc["b", ...])\\n1077.         assert_identical(da[:3], da.loc[["a", "b", "c"]])\\n1078.         assert_identical(da[:3, :4], da.loc[["a", "b", "c"], np.arange(4)])\\n1079.         assert_identical(da[:, :4], da.loc[:, self.ds["y"] < 4])\\n1080. \\n1081.     def test_loc_assign(self):\\n1082.         self.ds["x"] = ("x", np.array(list("abcdefghij")))\\n1083.         da = self.ds["foo"]\\n1084.         # assignment\\n1085.         da.loc["a":"j"] = 0\\n1086.         assert np.all(da.values == 0)\\n1087.         da.loc[{"x": slice("a", "j")}] = 2\\n1088.         assert np.all(da.values == 2)\\n1089. \\n1090.         da.loc[{"x": slice("a", "j")}] = 2\\n1091.         assert np.all(da.values == 2)\\n1092. \\n1093.         # Multi dimensional case\\n1094.         da = DataArray(np.arange(12).reshape(3, 4), dims=["x", "y"])\\n1095.         da.loc[0, 0] = 0\\n1096.         assert da.values[0, 0] == 0\\n1097.         assert da.values[0, 1] != 0\\n1098. \\n1099.         da = DataArray(np.arange(12).reshape(3, 4), dims=["x", "y"])\\n1100.         da.loc[0] = 0\\n1101.         assert np.all(da.values[0] == np.zeros(4))\\n1102.         assert da.values[1, 0] != 0\\n1103. \\n1104.     def test_loc_assign_dataarray(self):\\n1105.         def get_data():\\n1106.             return DataArray(\\n1107.                 np.ones((4, 3, 2)),\\n1108.                 dims=["x", "y", "z"],\\n1109.                 coords={\\n1110.                     "x": np.arange(4),\\n1111.                     "y": ["a", "b", "c"],\\n1112.                     "non-dim": ("x", [1, 3, 4, 2]),\\n1113.                 },\\n1114.             )\\n1115. \\n1116.         da = get_data()\\n1117.         # indexer with inconsistent coordinates.\\n1118.         ind = DataArray(np.arange(1, 4), dims=["y"], coords={"y": np.random.randn(3)})\\n1119.         with raises_regex(IndexError, "dimension coordinate \\\'y\\\'"):\\n1120.             da.loc[dict(x=ind)] = 0\\n1121. \\n1122.         # indexer with consistent coordinates.\\n1123.         ind = DataArray(np.arange(1, 4), dims=["x"], coords={"x": np.arange(1, 4)})\\n1124.         da.loc[dict(x=ind)] = 0  # should not raise\\n1125.         assert np.allclose(da[dict(x=ind)].values, 0)\\n1126.         assert_identical(da["x"], get_data()["x"])\\n1127.         assert_identical(da["non-dim"], get_data()["non-dim"])\\n1128. \\n1129.         da = get_data()\\n1130.         # conflict in the assigning values\\n1131.         value = xr.DataArray(\\n1132.             np.zeros((3, 3, 2)),\\n1133.             dims=["x", "y", "z"],\\n1134.             coords={"x": [0, 1, 2], "non-dim": ("x", [0, 2, 4])},\\n1135.         )\\n1136.         with raises_regex(IndexError, "dimension coordinate \\\'x\\\'"):\\n1137.             da.loc[dict(x=ind)] = value\\n1138. \\n1139.         # consistent coordinate in the assigning values\\n1140.         value = xr.DataArray(\\n1141.             np.zeros((3, 3, 2)),\\n1142.             dims=["x", "y", "z"],\\n1143.             coords={"x": [1, 2, 3], "non-dim": ("x", [0, 2, 4])},\\n1144.         )\\n1145.         da.loc[dict(x=ind)] = value\\n1146.         assert np.allclose(da[dict(x=ind)].values, 0)\\n1147.         assert_identical(da["x"], get_data()["x"])\\n1148.         assert_identical(da["non-dim"], get_data()["non-dim"])\\n1149. \\n1150.     def test_loc_single_boolean(self):\\n1151.         data = DataArray([0, 1], coords=[[True, False]])\\n1152.         assert data.loc[True] == 0\\n1153.         assert data.loc[False] == 1\\n1154. \\n1155.     def test_selection_multiindex(self):\\n1156.         mindex = pd.MultiIndex.from_product(\\n1157.             [["a", "b"], [1, 2], [-1, -2]], names=("one", "two", "three")\\n1158.         )\\n1159.         mdata = DataArray(range(8), [("x", mindex)])\\n1160. \\n1161.         def test_sel(lab_indexer, pos_indexer, replaced_idx=False, renamed_dim=None):\\n1162.             da = mdata.sel(x=lab_indexer)\\n1163.             expected_da = mdata.isel(x=pos_indexer)\\n1164.             if not replaced_idx:\\n1165.                 assert_identical(da, expected_da)\\n1166.             else:\\n1167.                 if renamed_dim:\\n1168.                     assert da.dims[0] == renamed_dim\\n1169.                     da = da.rename({renamed_dim: "x"})\\n1170.                 assert_identical(da.variable, expected_da.variable)\\n1171.                 assert not da["x"].equals(expected_da["x"])\\n1172. \\n1173.         test_sel(("a", 1, -1), 0)\\n1174.         test_sel(("b", 2, -2), -1)\\n1175.         test_sel(("a", 1), [0, 1], replaced_idx=True, renamed_dim="three")\\n1176.         test_sel(("a",), range(4), replaced_idx=True)\\n1177.         test_sel("a", range(4), replaced_idx=True)\\n1178.         test_sel([("a", 1, -1), ("b", 2, -2)], [0, 7])\\n1179.         test_sel(slice("a", "b"), range(8))\\n1180.         test_sel(slice(("a", 1), ("b", 1)), range(6))\\n1181.         test_sel({"one": "a", "two": 1, "three": -1}, 0)\\n1182.         test_sel({"one": "a", "two": 1}, [0, 1], replaced_idx=True, renamed_dim="three")\\n1183.         test_sel({"one": "a"}, range(4), replaced_idx=True)\\n1184. \\n1185.         assert_identical(mdata.loc["a"], mdata.sel(x="a"))\\n1186.         assert_identical(mdata.loc[("a", 1), ...], mdata.sel(x=("a", 1)))\\n1187.         assert_identical(mdata.loc[{"one": "a"}, ...], mdata.sel(x={"one": "a"}))\\n1188.         with pytest.raises(IndexError):\\n1189.             mdata.loc[("a", 1)]\\n1190. \\n1191.         assert_identical(mdata.sel(x={"one": "a", "two": 1}), mdata.sel(one="a", two=1))\\n1192. \\n1193.     def test_selection_multiindex_remove_unused(self):\\n1194.         # GH2619. For MultiIndex, we need to call remove_unused.\\n1195.         ds = xr.DataArray(\\n1196.             np.arange(40).reshape(8, 5),\\n1197.             dims=["x", "y"],\\n1198.             coords={"x": np.arange(8), "y": np.arange(5)},\\n1199.         )\\n1200.         ds = ds.stack(xy=["x", "y"])\\n1201.         ds_isel = ds.isel(xy=ds["x"] < 4)\\n1202.         with pytest.raises(KeyError):\\n1203.             ds_isel.sel(x=5)\\n1204. \\n1205.         actual = ds_isel.unstack()\\n1206.         expected = ds.reset_index("xy").isel(xy=ds["x"] < 4)\\n1207.         expected = expected.set_index(xy=["x", "y"]).unstack()\\n1208.         assert_identical(expected, actual)\\n1209. \\n1210.     def test_selection_multiindex_from_level(self):\\n1211.         # GH: 3512\\n1212.         da = DataArray([0, 1], dims=["x"], coords={"x": [0, 1], "y": "a"})\\n1213.         db = DataArray([2, 3], dims=["x"], coords={"x": [0, 1], "y": "b"})\\n1214.         data = xr.concat([da, db], dim="x").set_index(xy=["x", "y"])\\n1215.         assert data.dims == ("xy",)\\n1216.         actual = data.sel(y="a")\\n1217.         expected = data.isel(xy=[0, 1]).unstack("xy").squeeze("y").drop_vars("y")\\n1218.         assert_equal(actual, expected)\\n1219. \\n1220.     def test_stack_groupby_unsorted_coord(self):\\n1221.         data = [[0, 1], [2, 3]]\\n1222.         data_flat = [0, 1, 2, 3]\\n1223.         dims = ["x", "y"]\\n1224.         y_vals = [2, 3]\\n1225. \\n1226.         arr = xr.DataArray(data, dims=dims, coords={"y": y_vals})\\n1227.         actual1 = arr.stack(z=dims).groupby("z").first()\\n1228.         midx1 = pd.MultiIndex.from_product([[0, 1], [2, 3]], names=dims)\\n1229.         expected1 = xr.DataArray(data_flat, dims=["z"], coords={"z": midx1})\\n1230.         xr.testing.assert_equal(actual1, expected1)\\n1231. \\n1232.         # GH: 3287.  Note that y coord values are not in sorted order.\\n1233.         arr = xr.DataArray(data, dims=dims, coords={"y": y_vals[::-1]})\\n1234.         actual2 = arr.stack(z=dims).groupby("z").first()\\n1235.         midx2 = pd.MultiIndex.from_product([[0, 1], [3, 2]], names=dims)\\n1236.         expected2 = xr.DataArray(data_flat, dims=["z"], coords={"z": midx2})\\n1237.         xr.testing.assert_equal(actual2, expected2)\\n1238. \\n1239.     def test_virtual_default_coords(self):\\n1240.         array = DataArray(np.zeros((5,)), dims="x")\\n1241.         expected = DataArray(range(5), dims="x", name="x")\\n1242.         assert_identical(expected, array["x"])\\n1243.         assert_identical(expected, array.coords["x"])\\n1244. \\n1245.     def test_virtual_time_components(self):\\n1246.         dates = pd.date_range("2000-01-01", periods=10)\\n1247.         da = DataArray(np.arange(1, 11), [("time", dates)])\\n1248. \\n1249.         assert_array_equal(da["time.dayofyear"], da.values)\\n1250.         assert_array_equal(da.coords["time.dayofyear"], da.values)\\n1251. \\n1252.     def test_coords(self):\\n1253.         # use int64 to ensure repr() consistency on windows\\n1254.         coords = [\\n1255.             IndexVariable("x", np.array([-1, -2], "int64")),\\n1256.             IndexVariable("y", np.array([0, 1, 2], "int64")),\\n1257.         ]\\n1258.         da = DataArray(np.random.randn(2, 3), coords, name="foo")\\n1259. \\n1260.         assert 2 == len(da.coords)\\n1261. \\n1262.         assert ["x", "y"] == list(da.coords)\\n1263. \\n1264.         assert coords[0].identical(da.coords["x"])\\n1265.         assert coords[1].identical(da.coords["y"])\\n1266. \\n1267.         assert "x" in da.coords\\n1268.         assert 0 not in da.coords\\n1269.         assert "foo" not in da.coords\\n1270. \\n1271.         with pytest.raises(KeyError):\\n1272.             da.coords[0]\\n1273.         with pytest.raises(KeyError):\\n1274.             da.coords["foo"]\\n1275. \\n1276.         expected = dedent(\\n1277.             """\\\\\\n1278.         Coordinates:\\n1279.           * x        (x) int64 -1 -2\\n1280.           * y        (y) int64 0 1 2"""\\n1281.         )\\n1282.         actual = repr(da.coords)\\n1283.         assert expected == actual\\n1284. \\n1285.         del da.coords["x"]\\n1286.         da._indexes = propagate_indexes(da._indexes, exclude="x")\\n1287.         expected = DataArray(da.values, {"y": [0, 1, 2]}, dims=["x", "y"], name="foo")\\n1288.         assert_identical(da, expected)\\n1289. \\n1290.         with raises_regex(ValueError, "conflicting MultiIndex"):\\n1291.             self.mda["level_1"] = np.arange(4)\\n1292.             self.mda.coords["level_1"] = np.arange(4)\\n1293. \\n1294.     def test_coords_to_index(self):\\n1295.         da = DataArray(np.zeros((2, 3)), [("x", [1, 2]), ("y", list("abc"))])\\n1296. \\n1297.         with raises_regex(ValueError, "no valid index"):\\n1298.             da[0, 0].coords.to_index()\\n1299. \\n1300.         expected = pd.Index(["a", "b", "c"], name="y")\\n1301.         actual = da[0].coords.to_index()\\n1302.         assert expected.equals(actual)\\n1303. \\n1304.         expected = pd.MultiIndex.from_product(\\n1305.             [[1, 2], ["a", "b", "c"]], names=["x", "y"]\\n1306.         )\\n1307.         actual = da.coords.to_index()\\n1308.         assert expected.equals(actual)\\n1309. \\n1310.         expected = pd.MultiIndex.from_product(\\n1311.             [["a", "b", "c"], [1, 2]], names=["y", "x"]\\n1312.         )\\n1313.         actual = da.coords.to_index(["y", "x"])\\n1314.         assert expected.equals(actual)\\n1315. \\n1316.         with raises_regex(ValueError, "ordered_dims must match"):\\n1317.             da.coords.to_index(["x"])\\n1318. \\n1319.     def test_coord_coords(self):\\n1320.         orig = DataArray(\\n1321.             [10, 20], {"x": [1, 2], "x2": ("x", ["a", "b"]), "z": 4}, dims="x"\\n1322.         )\\n1323. \\n1324.         actual = orig.coords["x"]\\n1325.         expected = DataArray(\\n1326.             [1, 2], {"z": 4, "x2": ("x", ["a", "b"]), "x": [1, 2]}, dims="x", name="x"\\n1327.         )\\n1328.         assert_identical(expected, actual)\\n1329. \\n1330.         del actual.coords["x2"]\\n1331.         assert_identical(expected.reset_coords("x2", drop=True), actual)\\n1332. \\n1333.         actual.coords["x3"] = ("x", ["a", "b"])\\n1334.         expected = DataArray(\\n1335.             [1, 2], {"z": 4, "x3": ("x", ["a", "b"]), "x": [1, 2]}, dims="x", name="x"\\n1336.         )\\n1337.         assert_identical(expected, actual)\\n1338. \\n1339.     def test_reset_coords(self):\\n1340.         data = DataArray(\\n1341.             np.zeros((3, 4)),\\n1342.             {"bar": ("x", ["a", "b", "c"]), "baz": ("y", range(4)), "y": range(4)},\\n1343.             dims=["x", "y"],\\n1344.             name="foo",\\n1345.         )\\n1346. \\n1347.         actual = data.reset_coords()\\n1348.         expected = Dataset(\\n1349.             {\\n1350.                 "foo": (["x", "y"], np.zeros((3, 4))),\\n1351.                 "bar": ("x", ["a", "b", "c"]),\\n1352.                 "baz": ("y", range(4)),\\n1353.                 "y": range(4),\\n1354.             }\\n1355.         )\\n1356.         assert_identical(actual, expected)\\n1357. \\n1358.         actual = data.reset_coords(["bar", "baz"])\\n1359.         assert_identical(actual, expected)\\n1360. \\n1361.         actual = data.reset_coords("bar")\\n1362.         expected = Dataset(\\n1363.             {"foo": (["x", "y"], np.zeros((3, 4))), "bar": ("x", ["a", "b", "c"])},\\n1364.             {"baz": ("y", range(4)), "y": range(4)},\\n1365.         )\\n1366.         assert_identical(actual, expected)\\n1367. \\n1368.         actual = data.reset_coords(["bar"])\\n1369.         assert_identical(actual, expected)\\n1370. \\n1371.         actual = data.reset_coords(drop=True)\\n1372.         expected = DataArray(\\n1373.             np.zeros((3, 4)), coords={"y": range(4)}, dims=["x", "y"], name="foo"\\n1374.         )\\n1375.         assert_identical(actual, expected)\\n1376. \\n1377.         actual = data.copy()\\n1378.         actual = actual.reset_coords(drop=True)\\n1379.         assert_identical(actual, expected)\\n1380. \\n1381.         actual = data.reset_coords("bar", drop=True)\\n1382.         expected = DataArray(\\n1383.             np.zeros((3, 4)),\\n1384.             {"baz": ("y", range(4)), "y": range(4)},\\n1385.             dims=["x", "y"],\\n1386.             name="foo",\\n1387.         )\\n1388.         assert_identical(actual, expected)\\n1389. \\n1390.         with pytest.raises(TypeError):\\n1391.             data = data.reset_coords(inplace=True)\\n1392.         with raises_regex(ValueError, "cannot be found"):\\n1393.             data.reset_coords("foo", drop=True)\\n1394.         with raises_regex(ValueError, "cannot be found"):\\n1395.             data.reset_coords("not_found")\\n1396.         with raises_regex(ValueError, "cannot remove index"):\\n1397.             data.reset_coords("y")\\n1398. \\n1399.     def test_assign_coords(self):\\n1400.         array = DataArray(10)\\n1401.         actual = array.assign_coords(c=42)\\n1402.         expected = DataArray(10, {"c": 42})\\n1403.         assert_identical(actual, expected)\\n1404. \\n1405.         array = DataArray([1, 2, 3, 4], {"c": ("x", [0, 0, 1, 1])}, dims="x")\\n1406.         actual = array.groupby("c").assign_coords(d=lambda a: a.mean())\\n1407.         expected = array.copy()\\n1408.         expected.coords["d"] = ("x", [1.5, 1.5, 3.5, 3.5])\\n1409.         assert_identical(actual, expected)\\n1410. \\n1411.         with raises_regex(ValueError, "conflicting MultiIndex"):\\n1412.             self.mda.assign_coords(level_1=range(4))\\n1413. \\n1414.         # GH: 2112\\n1415.         da = xr.DataArray([0, 1, 2], dims="x")\\n1416.         with pytest.raises(ValueError):\\n1417.             da["x"] = [0, 1, 2, 3]  # size conflict\\n1418.         with pytest.raises(ValueError):\\n1419.             da.coords["x"] = [0, 1, 2, 3]  # size conflict\\n1420. \\n1421.     def test_coords_alignment(self):\\n1422.         lhs = DataArray([1, 2, 3], [("x", [0, 1, 2])])\\n1423.         rhs = DataArray([2, 3, 4], [("x", [1, 2, 3])])\\n1424.         lhs.coords["rhs"] = rhs\\n1425. \\n1426.         expected = DataArray(\\n1427.             [1, 2, 3], coords={"rhs": ("x", [np.nan, 2, 3]), "x": [0, 1, 2]}, dims="x"\\n1428.         )\\n1429.         assert_identical(lhs, expected)\\n1430. \\n1431.     def test_set_coords_update_index(self):\\n1432.         actual = DataArray([1, 2, 3], [("x", [1, 2, 3])])\\n1433.         actual.coords["x"] = ["a", "b", "c"]\\n1434.         assert actual.indexes["x"].equals(pd.Index(["a", "b", "c"]))\\n1435. \\n1436.     def test_coords_replacement_alignment(self):\\n1437.         # regression test for GH725\\n1438.         arr = DataArray([0, 1, 2], dims=["abc"])\\n1439.         new_coord = DataArray([1, 2, 3], dims=["abc"], coords=[[1, 2, 3]])\\n1440.         arr["abc"] = new_coord\\n1441.         expected = DataArray([0, 1, 2], coords=[("abc", [1, 2, 3])])\\n1442.         assert_identical(arr, expected)\\n1443. \\n1444.     def test_coords_non_string(self):\\n1445.         arr = DataArray(0, coords={1: 2})\\n1446.         actual = arr.coords[1]\\n1447.         expected = DataArray(2, coords={1: 2}, name=1)\\n1448.         assert_identical(actual, expected)\\n1449. \\n1450.     def test_coords_delitem_delete_indexes(self):\\n1451.         # regression test for GH3746\\n1452.         arr = DataArray(np.ones((2,)), dims="x", coords={"x": [0, 1]})\\n1453.         del arr.coords["x"]\\n1454.         assert "x" not in arr.indexes\\n1455. \\n1456.     def test_broadcast_like(self):\\n1457.         arr1 = DataArray(\\n1458.             np.ones((2, 3)),\\n1459.             dims=["x", "y"],\\n1460.             coords={"x": ["a", "b"], "y": ["a", "b", "c"]},\\n1461.         )\\n1462.         arr2 = DataArray(\\n1463.             np.ones((3, 2)),\\n1464.             dims=["x", "y"],\\n1465.             coords={"x": ["a", "b", "c"], "y": ["a", "b"]},\\n1466.         )\\n1467.         orig1, orig2 = broadcast(arr1, arr2)\\n1468.         new1 = arr1.broadcast_like(arr2)\\n1469.         new2 = arr2.broadcast_like(arr1)\\n1470. \\n1471.         assert orig1.identical(new1)\\n1472.         assert orig2.identical(new2)\\n1473. \\n1474.         orig3 = DataArray(np.random.randn(5), [("x", range(5))])\\n1475.         orig4 = DataArray(np.random.randn(6), [("y", range(6))])\\n1476.         new3, new4 = broadcast(orig3, orig4)\\n1477. \\n1478.         assert_identical(orig3.broadcast_like(orig4), new3.transpose("y", "x"))\\n1479.         assert_identical(orig4.broadcast_like(orig3), new4)\\n1480. \\n1481.     def test_reindex_like(self):\\n1482.         foo = DataArray(np.random.randn(5, 6), [("x", range(5)), ("y", range(6))])\\n1483.         bar = foo[:2, :2]\\n1484.         assert_identical(foo.reindex_like(bar), bar)\\n1485. \\n1486.         expected = foo.copy()\\n1487.         expected[:] = np.nan\\n1488.         expected[:2, :2] = bar\\n1489.         assert_identical(bar.reindex_like(foo), expected)\\n1490. \\n1491.     def test_reindex_like_no_index(self):\\n1492.         foo = DataArray(np.random.randn(5, 6), dims=["x", "y"])\\n1493.         assert_identical(foo, foo.reindex_like(foo))\\n1494. \\n1495.         bar = foo[:4]\\n1496.         with raises_regex(ValueError, "different size for unlabeled"):\\n1497.             foo.reindex_like(bar)\\n1498. \\n1499.     def test_reindex_regressions(self):\\n1500.         da = DataArray(np.random.randn(5), coords=[("time", range(5))])\\n1501.         time2 = DataArray(np.arange(5), dims="time2")\\n1502.         with pytest.raises(ValueError):\\n1503.             da.reindex(time=time2)\\n1504. \\n1505.         # regression test for #736, reindex can not change complex nums dtype\\n1506.         x = np.array([1, 2, 3], dtype=complex)\\n1507.         x = DataArray(x, coords=[[0.1, 0.2, 0.3]])\\n1508.         y = DataArray([2, 5, 6, 7, 8], coords=[[-1.1, 0.21, 0.31, 0.41, 0.51]])\\n1509.         re_dtype = x.reindex_like(y, method="pad").dtype\\n1510.         assert x.dtype == re_dtype\\n1511. \\n1512.     def test_reindex_method(self):\\n1513.         x = DataArray([10, 20], dims="y", coords={"y": [0, 1]})\\n1514.         y = [-0.1, 0.5, 1.1]\\n1515.         actual = x.reindex(y=y, method="backfill", tolerance=0.2)\\n1516.         expected = DataArray([10, np.nan, np.nan], coords=[("y", y)])\\n1517.         assert_identical(expected, actual)\\n1518. \\n1519.         alt = Dataset({"y": y})\\n1520.         actual = x.reindex_like(alt, method="backfill")\\n1521.         expected = DataArray([10, 20, np.nan], coords=[("y", y)])\\n1522.         assert_identical(expected, actual)\\n1523. \\n1524.     @pytest.mark.parametrize("fill_value", [dtypes.NA, 2, 2.0])\\n1525.     def test_reindex_fill_value(self, fill_value):\\n1526.         x = DataArray([10, 20], dims="y", coords={"y": [0, 1]})\\n1527.         y = [0, 1, 2]\\n1528.         if fill_value == dtypes.NA:\\n1529.             # if we supply the default, we expect the missing value for a\\n1530.             # float array\\n1531.             fill_value = np.nan\\n1532.         actual = x.reindex(y=y, fill_value=fill_value)\\n1533.         expected = DataArray([10, 20, fill_value], coords=[("y", y)])\\n1534.         assert_identical(expected, actual)\\n1535. \\n1536.     def test_rename(self):\\n1537.         renamed = self.dv.rename("bar")\\n1538.         assert_identical(renamed.to_dataset(), self.ds.rename({"foo": "bar"}))\\n1539.         assert renamed.name == "bar"\\n1540. \\n1541.         renamed = self.dv.x.rename({"x": "z"}).rename("z")\\n1542.         assert_identical(renamed, self.ds.rename({"x": "z"}).z)\\n1543.         assert renamed.name == "z"\\n1544.         assert renamed.dims == ("z",)\\n1545. \\n1546.         renamed_kwargs = self.dv.x.rename(x="z").rename("z")\\n1547.         assert_identical(renamed, renamed_kwargs)\\n1548. \\n1549.     def test_init_value(self):\\n1550.         expected = DataArray(\\n1551.             np.full((3, 4), 3), dims=["x", "y"], coords=[range(3), range(4)]\\n1552.         )\\n1553.         actual = DataArray(3, dims=["x", "y"], coords=[range(3), range(4)])\\n1554.         assert_identical(expected, actual)\\n1555. \\n1556.         expected = DataArray(\\n1557.             np.full((1, 10, 2), 0),\\n1558.             dims=["w", "x", "y"],\\n1559.             coords={"x": np.arange(10), "y": ["north", "south"]},\\n1560.         )\\n1561.         actual = DataArray(0, dims=expected.dims, coords=expected.coords)\\n1562.         assert_identical(expected, actual)\\n1563. \\n1564.         expected = DataArray(\\n1565.             np.full((10, 2), np.nan), coords=[("x", np.arange(10)), ("y", ["a", "b"])]\\n1566.         )\\n1567.         actual = DataArray(coords=[("x", np.arange(10)), ("y", ["a", "b"])])\\n1568.         assert_identical(expected, actual)\\n1569. \\n1570.         with raises_regex(ValueError, "different number of dim"):\\n1571.             DataArray(np.array(1), coords={"x": np.arange(10)}, dims=["x"])\\n1572.         with raises_regex(ValueError, "does not match the 0 dim"):\\n1573.             DataArray(np.array(1), coords=[("x", np.arange(10))])\\n1574. \\n1575.     def test_swap_dims(self):\\n1576.         array = DataArray(np.random.randn(3), {"y": ("x", list("abc"))}, "x")\\n1577.         expected = DataArray(array.values, {"y": list("abc")}, dims="y")\\n1578.         actual = array.swap_dims({"x": "y"})\\n1579.         assert_identical(expected, actual)\\n1580.         for dim_name in set().union(expected.indexes.keys(), actual.indexes.keys()):\\n1581.             pd.testing.assert_index_equal(\\n1582.                 expected.indexes[dim_name], actual.indexes[dim_name]\\n1583.             )\\n1584. \\n1585.         array = DataArray(np.random.randn(3), {"x": list("abc")}, "x")\\n1586.         expected = DataArray(array.values, {"x": ("y", list("abc"))}, dims="y")\\n1587.         actual = array.swap_dims({"x": "y"})\\n1588.         assert_identical(expected, actual)\\n1589.         for dim_name in set().union(expected.indexes.keys(), actual.indexes.keys()):\\n1590.             pd.testing.assert_index_equal(\\n1591.                 expected.indexes[dim_name], actual.indexes[dim_name]\\n1592.             )\\n1593. \\n1594.         # multiindex case\\n1595.         idx = pd.MultiIndex.from_arrays([list("aab"), list("yzz")], names=["y1", "y2"])\\n1596.         array = DataArray(np.random.randn(3), {"y": ("x", idx)}, "x")\\n1597.         expected = DataArray(array.values, {"y": idx}, "y")\\n1598.         actual = array.swap_dims({"x": "y"})\\n1599.         assert_identical(expected, actual)\\n1600.         for dim_name in set().union(expected.indexes.keys(), actual.indexes.keys()):\\n1601.             pd.testing.assert_index_equal(\\n1602.                 expected.indexes[dim_name], actual.indexes[dim_name]\\n1603.             )\\n1604. \\n1605.     def test_expand_dims_error(self):\\n1606.         array = DataArray(\\n1607.             np.random.randn(3, 4),\\n1608.             dims=["x", "dim_0"],\\n1609.             coords={"x": np.linspace(0.0, 1.0, 3)},\\n1610.             attrs={"key": "entry"},\\n1611.         )\\n1612. \\n1613.         with raises_regex(TypeError, "dim should be hashable or"):\\n1614.             array.expand_dims(0)\\n1615.         with raises_regex(ValueError, "lengths of dim and axis"):\\n1616.             # dims and axis argument should be the same length\\n1617.             array.expand_dims(dim=["a", "b"], axis=[1, 2, 3])\\n1618.         with raises_regex(ValueError, "Dimension x already"):\\n1619.             # Should not pass the already existing dimension.\\n1620.             array.expand_dims(dim=["x"])\\n1621.         # raise if duplicate\\n1622.         with raises_regex(ValueError, "duplicate values"):\\n1623.             array.expand_dims(dim=["y", "y"])\\n1624.         with raises_regex(ValueError, "duplicate values"):\\n1625.             array.expand_dims(dim=["y", "z"], axis=[1, 1])\\n1626.         with raises_regex(ValueError, "duplicate values"):\\n1627.             array.expand_dims(dim=["y", "z"], axis=[2, -2])\\n1628. \\n1629.         # out of bounds error, axis must be in [-4, 3]\\n1630.         with pytest.raises(IndexError):\\n1631.             array.expand_dims(dim=["y", "z"], axis=[2, 4])\\n1632.         with pytest.raises(IndexError):\\n1633.             array.expand_dims(dim=["y", "z"], axis=[2, -5])\\n1634.         # Does not raise an IndexError\\n1635.         array.expand_dims(dim=["y", "z"], axis=[2, -4])\\n1636.         array.expand_dims(dim=["y", "z"], axis=[2, 3])\\n1637. \\n1638.         array = DataArray(\\n1639.             np.random.randn(3, 4),\\n1640.             dims=["x", "dim_0"],\\n1641.             coords={"x": np.linspace(0.0, 1.0, 3)},\\n1642.             attrs={"key": "entry"},\\n1643.         )\\n1644.         with pytest.raises(TypeError):\\n1645.             array.expand_dims({"new_dim": 3.2})\\n1646. \\n1647.         # Attempt to use both dim and kwargs\\n1648.         with pytest.raises(ValueError):\\n1649.             array.expand_dims({"d": 4}, e=4)\\n1650. \\n1651.     def test_expand_dims(self):\\n1652.         array = DataArray(\\n1653.             np.random.randn(3, 4),\\n1654.             dims=["x", "dim_0"],\\n1655.             coords={"x": np.linspace(0.0, 1.0, 3)},\\n1656.             attrs={"key": "entry"},\\n1657.         )\\n1658.         # pass only dim label\\n1659.         actual = array.expand_dims(dim="y")\\n1660.         expected = DataArray(\\n1661.             np.expand_dims(array.values, 0),\\n1662.             dims=["y", "x", "dim_0"],\\n1663.             coords={"x": np.linspace(0.0, 1.0, 3)},\\n1664.             attrs={"key": "entry"},\\n1665.         )\\n1666.         assert_identical(expected, actual)\\n1667.         roundtripped = actual.squeeze("y", drop=True)\\n1668.         assert_identical(array, roundtripped)\\n1669. \\n1670.         # pass multiple dims\\n1671.         actual = array.expand_dims(dim=["y", "z"])\\n1672.         expected = DataArray(\\n1673.             np.expand_dims(np.expand_dims(array.values, 0), 0),\\n1674.             dims=["y", "z", "x", "dim_0"],\\n1675.             coords={"x": np.linspace(0.0, 1.0, 3)},\\n1676.             attrs={"key": "entry"},\\n1677.         )\\n1678.         assert_identical(expected, actual)\\n1679.         roundtripped = actual.squeeze(["y", "z"], drop=True)\\n1680.         assert_identical(array, roundtripped)\\n1681. \\n1682.         # pass multiple dims and axis. Axis is out of order\\n1683.         actual = array.expand_dims(dim=["z", "y"], axis=[2, 1])\\n1684.         expected = DataArray(\\n1685.             np.expand_dims(np.expand_dims(array.values, 1), 2),\\n1686.             dims=["x", "y", "z", "dim_0"],\\n1687.             coords={"x": np.linspace(0.0, 1.0, 3)},\\n1688.             attrs={"key": "entry"},\\n1689.         )\\n1690.         assert_identical(expected, actual)\\n1691.         # make sure the attrs are tracked\\n1692.         assert actual.attrs["key"] == "entry"\\n1693.         roundtripped = actual.squeeze(["z", "y"], drop=True)\\n1694.         assert_identical(array, roundtripped)\\n1695. \\n1696.         # Negative axis and they are out of order\\n1697.         actual = array.expand_dims(dim=["y", "z"], axis=[-1, -2])\\n1698.         expected = DataArray(\\n1699.             np.expand_dims(np.expand_dims(array.values, -1), -1),\\n1700.             dims=["x", "dim_0", "z", "y"],\\n1701.             coords={"x": np.linspace(0.0, 1.0, 3)},\\n1702.             attrs={"key": "entry"},\\n1703.         )\\n1704.         assert_identical(expected, actual)\\n1705.         assert actual.attrs["key"] == "entry"\\n1706.         roundtripped = actual.squeeze(["y", "z"], drop=True)\\n1707.         assert_identical(array, roundtripped)\\n1708. \\n1709.     def test_expand_dims_with_scalar_coordinate(self):\\n1710.         array = DataArray(\\n1711.             np.random.randn(3, 4),\\n1712.             dims=["x", "dim_0"],\\n1713.             coords={"x": np.linspace(0.0, 1.0, 3), "z": 1.0},\\n1714.             attrs={"key": "entry"},\\n1715.         )\\n1716.         actual = array.expand_dims(dim="z")\\n1717.         expected = DataArray(\\n1718.             np.expand_dims(array.values, 0),\\n1719.             dims=["z", "x", "dim_0"],\\n1720.             coords={"x": np.linspace(0.0, 1.0, 3), "z": np.ones(1)},\\n1721.             attrs={"key": "entry"},\\n1722.         )\\n1723.         assert_identical(expected, actual)\\n1724.         roundtripped = actual.squeeze(["z"], drop=False)\\n1725.         assert_identical(array, roundtripped)\\n1726. \\n1727.     def test_expand_dims_with_greater_dim_size(self):\\n1728.         array = DataArray(\\n1729.             np.random.randn(3, 4),\\n1730.             dims=["x", "dim_0"],\\n1731.             coords={"x": np.linspace(0.0, 1.0, 3), "z": 1.0},\\n1732.             attrs={"key": "entry"},\\n1733.         )\\n1734.         actual = array.expand_dims({"y": 2, "z": 1, "dim_1": ["a", "b", "c"]})\\n1735. \\n1736.         expected_coords = {\\n1737.             "y": [0, 1],\\n1738.             "z": [1.0],\\n1739.             "dim_1": ["a", "b", "c"],\\n1740.             "x": np.linspace(0, 1, 3),\\n1741.             "dim_0": range(4),\\n1742.         }\\n1743.         expected = DataArray(\\n1744.             array.values * np.ones([2, 1, 3, 3, 4]),\\n1745.             coords=expected_coords,\\n1746.             dims=list(expected_coords.keys()),\\n1747.             attrs={"key": "entry"},\\n1748.         ).drop_vars(["y", "dim_0"])\\n1749.         assert_identical(expected, actual)\\n1750. \\n1751.         # Test with kwargs instead of passing dict to dim arg.\\n1752. \\n1753.         other_way = array.expand_dims(dim_1=["a", "b", "c"])\\n1754. \\n1755.         other_way_expected = DataArray(\\n1756.             array.values * np.ones([3, 3, 4]),\\n1757.             coords={\\n1758.                 "dim_1": ["a", "b", "c"],\\n1759.                 "x": np.linspace(0, 1, 3),\\n1760.                 "dim_0": range(4),\\n1761.                 "z": 1.0,\\n1762.             },\\n1763.             dims=["dim_1", "x", "dim_0"],\\n1764.             attrs={"key": "entry"},\\n1765.         ).drop_vars("dim_0")\\n1766.         assert_identical(other_way_expected, other_way)\\n1767. \\n1768.     def test_set_index(self):\\n1769.         indexes = [self.mindex.get_level_values(n) for n in self.mindex.names]\\n1770.         coords = {idx.name: ("x", idx) for idx in indexes}\\n1771.         array = DataArray(self.mda.values, coords=coords, dims="x")\\n1772.         expected = self.mda.copy()\\n1773.         level_3 = ("x", [1, 2, 3, 4])\\n1774.         array["level_3"] = level_3\\n1775.         expected["level_3"] = level_3\\n1776. \\n1777.         obj = array.set_index(x=self.mindex.names)\\n1778.         assert_identical(obj, expected)\\n1779. \\n1780.         obj = obj.set_index(x="level_3", append=True)\\n1781.         expected = array.set_index(x=["level_1", "level_2", "level_3"])\\n1782.         assert_identical(obj, expected)\\n1783. \\n1784.         array = array.set_index(x=["level_1", "level_2", "level_3"])\\n1785.         assert_identical(array, expected)\\n1786. \\n1787.         array2d = DataArray(\\n1788.             np.random.rand(2, 2),\\n1789.             coords={"x": ("x", [0, 1]), "level": ("y", [1, 2])},\\n1790.             dims=("x", "y"),\\n1791.         )\\n1792.         with raises_regex(ValueError, "dimension mismatch"):\\n1793.             array2d.set_index(x="level")\\n1794. \\n1795.         # Issue 3176: Ensure clear error message on key error.\\n1796.         with pytest.raises(ValueError) as excinfo:\\n1797.             obj.set_index(x="level_4")\\n1798.         assert str(excinfo.value) == "level_4 is not the name of an existing variable."\\n1799. \\n1800.     def test_reset_index(self):\\n1801.         indexes = [self.mindex.get_level_values(n) for n in self.mindex.names]\\n1802.         coords = {idx.name: ("x", idx) for idx in indexes}\\n1803.         expected = DataArray(self.mda.values, coords=coords, dims="x")\\n1804. \\n1805.         obj = self.mda.reset_index("x")\\n1806.         assert_identical(obj, expected)\\n1807.         obj = self.mda.reset_index(self.mindex.names)\\n1808.         assert_identical(obj, expected)\\n1809.         obj = self.mda.reset_index(["x", "level_1"])\\n1810.         assert_identical(obj, expected)\\n1811. \\n1812.         coords = {\\n1813.             "x": ("x", self.mindex.droplevel("level_1")),\\n1814.             "level_1": ("x", self.mindex.get_level_values("level_1")),\\n1815.         }\\n1816.         expected = DataArray(self.mda.values, coords=coords, dims="x")\\n1817.         obj = self.mda.reset_index(["level_1"])\\n1818.         assert_identical(obj, expected)\\n1819. \\n1820.         expected = DataArray(self.mda.values, dims="x")\\n1821.         obj = self.mda.reset_index("x", drop=True)\\n1822.         assert_identical(obj, expected)\\n1823. \\n1824.         array = self.mda.copy()\\n1825.         array = array.reset_index(["x"], drop=True)\\n1826.         assert_identical(array, expected)\\n1827. \\n1828.         # single index\\n1829.         array = DataArray([1, 2], coords={"x": ["a", "b"]}, dims="x")\\n1830.         expected = DataArray([1, 2], coords={"x_": ("x", ["a", "b"])}, dims="x")\\n1831.         assert_identical(array.reset_index("x"), expected)\\n1832. \\n1833.     def test_reset_index_keep_attrs(self):\\n1834.         coord_1 = DataArray([1, 2], dims=["coord_1"], attrs={"attrs": True})\\n1835.         da = DataArray([1, 0], [coord_1])\\n1836.         expected = DataArray([1, 0], {"coord_1_": coord_1}, dims=["coord_1"])\\n1837.         obj = da.reset_index("coord_1")\\n1838.         assert_identical(expected, obj)\\n1839. \\n1840.     def test_reorder_levels(self):\\n1841.         midx = self.mindex.reorder_levels(["level_2", "level_1"])\\n1842.         expected = DataArray(self.mda.values, coords={"x": midx}, dims="x")\\n1843. \\n1844.         obj = self.mda.reorder_levels(x=["level_2", "level_1"])\\n1845.         assert_identical(obj, expected)\\n1846. \\n1847.         with pytest.raises(TypeError):\\n1848.             array = self.mda.copy()\\n1849.             array.reorder_levels(x=["level_2", "level_1"], inplace=True)\\n1850. \\n1851.         array = DataArray([1, 2], dims="x")\\n1852.         with pytest.raises(KeyError):\\n1853.             array.reorder_levels(x=["level_1", "level_2"])\\n1854. \\n1855.         array["x"] = [0, 1]\\n1856.         with raises_regex(ValueError, "has no MultiIndex"):\\n1857.             array.reorder_levels(x=["level_1", "level_2"])\\n1858. \\n1859.     def test_dataset_getitem(self):\\n1860.         dv = self.ds["foo"]\\n1861.         assert_identical(dv, self.dv)\\n1862. \\n1863.     def test_array_interface(self):\\n1864.         assert_array_equal(np.asarray(self.dv), self.x)\\n1865.         # test patched in methods\\n1866.         assert_array_equal(self.dv.astype(float), self.v.astype(float))\\n1867.         assert_array_equal(self.dv.argsort(), self.v.argsort())\\n1868.         assert_array_equal(self.dv.clip(2, 3), self.v.clip(2, 3))\\n1869.         # test ufuncs\\n1870.         expected = deepcopy(self.ds)\\n1871.         expected["foo"][:] = np.sin(self.x)\\n1872.         assert_equal(expected["foo"], np.sin(self.dv))\\n1873.         assert_array_equal(self.dv, np.maximum(self.v, self.dv))\\n1874.         bar = Variable(["x", "y"], np.zeros((10, 20)))\\n1875.         assert_equal(self.dv, np.maximum(self.dv, bar))\\n1876. \\n1877.     def test_astype_attrs(self):\\n1878.         for v in [self.va.copy(), self.mda.copy(), self.ds.copy()]:\\n1879.             v.attrs["foo"] = "bar"\\n1880.             assert v.attrs == v.astype(float).attrs\\n1881.             assert not v.astype(float, keep_attrs=False).attrs\\n1882. \\n1883.     def test_astype_dtype(self):\\n1884.         original = DataArray([-1, 1, 2, 3, 1000])\\n1885.         converted = original.astype(float)\\n1886.         assert_array_equal(original, converted)\\n1887.         assert np.issubdtype(original.dtype, np.integer)\\n1888.         assert np.issubdtype(converted.dtype, np.floating)\\n1889. \\n1890.     def test_is_null(self):\\n1891.         x = np.random.RandomState(42).randn(5, 6)\\n1892.         x[x < 0] = np.nan\\n1893.         original = DataArray(x, [-np.arange(5), np.arange(6)], ["x", "y"])\\n1894.         expected = DataArray(pd.isnull(x), [-np.arange(5), np.arange(6)], ["x", "y"])\\n1895.         assert_identical(expected, original.isnull())\\n1896.         assert_identical(~expected, original.notnull())\\n1897. \\n1898.     def test_math(self):\\n1899.         x = self.x\\n1900.         v = self.v\\n1901.         a = self.dv\\n1902.         # variable math was already tested extensively, so let\\\'s just make sure\\n1903.         # that all types are properly converted here\\n1904.         assert_equal(a, +a)\\n1905.         assert_equal(a, a + 0)\\n1906.         assert_equal(a, 0 + a)\\n1907.         assert_equal(a, a + 0 * v)\\n1908.         assert_equal(a, 0 * v + a)\\n1909.         assert_equal(a, a + 0 * x)\\n1910.         assert_equal(a, 0 * x + a)\\n1911.         assert_equal(a, a + 0 * a)\\n1912.         assert_equal(a, 0 * a + a)\\n1913. \\n1914.     def test_math_automatic_alignment(self):\\n1915.         a = DataArray(range(5), [("x", range(5))])\\n1916.         b = DataArray(range(5), [("x", range(1, 6))])\\n1917.         expected = DataArray(np.ones(4), [("x", [1, 2, 3, 4])])\\n1918.         assert_identical(a - b, expected)\\n1919. \\n1920.     def test_non_overlapping_dataarrays_return_empty_result(self):\\n1921. \\n1922.         a = DataArray(range(5), [("x", range(5))])\\n1923.         result = a.isel(x=slice(2)) + a.isel(x=slice(2, None))\\n1924.         assert len(result["x"]) == 0\\n1925. \\n1926.     def test_empty_dataarrays_return_empty_result(self):\\n1927. \\n1928.         a = DataArray(data=[])\\n1929.         result = a * a\\n1930.         assert len(result["dim_0"]) == 0\\n1931. \\n1932.     def test_inplace_math_basics(self):\\n1933.         x = self.x\\n1934.         a = self.dv\\n1935.         v = a.variable\\n1936.         b = a\\n1937.         b += 1\\n1938.         assert b is a\\n1939.         assert b.variable is v\\n1940.         assert_array_equal(b.values, x)\\n1941.         assert source_ndarray(b.values) is x\\n1942. \\n1943.     def test_inplace_math_automatic_alignment(self):\\n1944.         a = DataArray(range(5), [("x", range(5))])\\n1945.         b = DataArray(range(1, 6), [("x", range(1, 6))])\\n1946.         with pytest.raises(xr.MergeError, match="Automatic alignment is not supported"):\\n1947.             a += b\\n1948.         with pytest.raises(xr.MergeError, match="Automatic alignment is not supported"):\\n1949.             b += a\\n1950. \\n1951.     def test_math_name(self):\\n1952.         # Verify that name is preserved only when it can be done unambiguously.\\n1953.         # The rule (copied from pandas.Series) is keep the current name only if\\n1954.         # the other object has the same name or no name attribute and this\\n1955.         # object isn\\\'t a coordinate; otherwise reset to None.\\n1956.         a = self.dv\\n1957.         assert (+a).name == "foo"\\n1958.         assert (a + 0).name == "foo"\\n1959.         assert (a + a.rename(None)).name is None\\n1960.         assert (a + a.rename("bar")).name is None\\n1961.         assert (a + a).name == "foo"\\n1962.         assert (+a["x"]).name == "x"\\n1963.         assert (a["x"] + 0).name == "x"\\n1964.         assert (a + a["x"]).name is None\\n1965. \\n1966.     def test_math_with_coords(self):\\n1967.         coords = {\\n1968.             "x": [-1, -2],\\n1969.             "y": ["ab", "cd", "ef"],\\n1970.             "lat": (["x", "y"], [[1, 2, 3], [-1, -2, -3]]),\\n1971.             "c": -999,\\n1972.         }\\n1973.         orig = DataArray(np.random.randn(2, 3), coords, dims=["x", "y"])\\n1974. \\n1975.         actual = orig + 1\\n1976.         expected = DataArray(orig.values + 1, orig.coords)\\n1977.         assert_identical(expected, actual)\\n1978. \\n1979.         actual = 1 + orig\\n1980.         assert_identical(expected, actual)\\n1981. \\n1982.         actual = orig + orig[0, 0]\\n1983.         exp_coords = {k: v for k, v in coords.items() if k != "lat"}\\n1984.         expected = DataArray(\\n1985.             orig.values + orig.values[0, 0], exp_coords, dims=["x", "y"]\\n1986.         )\\n1987.         assert_identical(expected, actual)\\n1988. \\n1989.         actual = orig[0, 0] + orig\\n1990.         assert_identical(expected, actual)\\n1991. \\n1992.         actual = orig[0, 0] + orig[-1, -1]\\n1993.         expected = DataArray(orig.values[0, 0] + orig.values[-1, -1], {"c": -999})\\n1994.         assert_identical(expected, actual)\\n1995. \\n1996.         actual = orig[:, 0] + orig[0, :]\\n1997.         exp_values = orig[:, 0].values[:, None] + orig[0, :].values[None, :]\\n1998.         expected = DataArray(exp_values, exp_coords, dims=["x", "y"])\\n1999.         assert_identical(expected, actual)\\n2000. \\n2001.         actual = orig[0, :] + orig[:, 0]\\n2002.         assert_identical(expected.transpose(transpose_coords=True), actual)\\n2003. \\n2004.         actual = orig - orig.transpose(transpose_coords=True)\\n2005.         expected = DataArray(np.zeros((2, 3)), orig.coords)\\n2006.         assert_identical(expected, actual)\\n2007. \\n2008.         actual = orig.transpose(transpose_coords=True) - orig\\n2009.         assert_identical(expected.transpose(transpose_coords=True), actual)\\n2010. \\n2011.         alt = DataArray([1, 1], {"x": [-1, -2], "c": "foo", "d": 555}, "x")\\n2012.         actual = orig + alt\\n2013.         expected = orig + 1\\n2014.         expected.coords["d"] = 555\\n2015.         del expected.coords["c"]\\n2016.         assert_identical(expected, actual)\\n2017. \\n2018.         actual = alt + orig\\n2019.         assert_identical(expected, actual)\\n2020. \\n2021.     def test_index_math(self):\\n2022.         orig = DataArray(range(3), dims="x", name="x")\\n2023.         actual = orig + 1\\n2024.         expected = DataArray(1 + np.arange(3), dims="x", name="x")\\n2025.         assert_identical(expected, actual)\\n2026. \\n2027.         # regression tests for #254\\n2028.         actual = orig[0] < orig\\n2029.         expected = DataArray([False, True, True], dims="x", name="x")\\n2030.         assert_identical(expected, actual)\\n2031. \\n2032.         actual = orig > orig[0]\\n2033.         assert_identical(expected, actual)\\n2034. \\n2035.     def test_dataset_math(self):\\n2036.         # more comprehensive tests with multiple dataset variables\\n2037.         obs = Dataset(\\n2038.             {"tmin": ("x", np.arange(5)), "tmax": ("x", 10 + np.arange(5))},\\n2039.             {"x": ("x", 0.5 * np.arange(5)), "loc": ("x", range(-2, 3))},\\n2040.         )\\n2041. \\n2042.         actual = 2 * obs["tmax"]\\n2043.         expected = DataArray(2 * (10 + np.arange(5)), obs.coords, name="tmax")\\n2044.         assert_identical(actual, expected)\\n2045. \\n2046.         actual = obs["tmax"] - obs["tmin"]\\n2047.         expected = DataArray(10 * np.ones(5), obs.coords)\\n2048.         assert_identical(actual, expected)\\n2049. \\n2050.         sim = Dataset(\\n2051.             {\\n2052.                 "tmin": ("x", 1 + np.arange(5)),\\n2053.                 "tmax": ("x", 11 + np.arange(5)),\\n2054.                 # does *not* include \\\'loc\\\' as a coordinate\\n2055.                 "x": ("x", 0.5 * np.arange(5)),\\n2056.             }\\n2057.         )\\n2058. \\n2059.         actual = sim["tmin"] - obs["tmin"]\\n2060.         expected = DataArray(np.ones(5), obs.coords, name="tmin")\\n2061.         assert_identical(actual, expected)\\n2062. \\n2063.         actual = -obs["tmin"] + sim["tmin"]\\n2064.         assert_identical(actual, expected)\\n2065. \\n2066.         actual = sim["tmin"].copy()\\n2067.         actual -= obs["tmin"]\\n2068.         assert_identical(actual, expected)\\n2069. \\n2070.         actual = sim.copy()\\n2071.         actual["tmin"] = sim["tmin"] - obs["tmin"]\\n2072.         expected = Dataset(\\n2073.             {"tmin": ("x", np.ones(5)), "tmax": ("x", sim["tmax"].values)}, obs.coords\\n2074.         )\\n2075.         assert_identical(actual, expected)\\n2076. \\n2077.         actual = sim.copy()\\n2078.         actual["tmin"] -= obs["tmin"]\\n2079.         assert_identical(actual, expected)\\n2080. \\n2081.     def test_stack_unstack(self):\\n2082.         orig = DataArray([[0, 1], [2, 3]], dims=["x", "y"], attrs={"foo": 2})\\n2083.         assert_identical(orig, orig.unstack())\\n2084. \\n2085.         # test GH3000\\n2086.         a = orig[:0, :1].stack(dim=("x", "y")).dim.to_index()\\n2087.         if pd.__version__ < "0.24.0":\\n2088.             b = pd.MultiIndex(\\n2089.                 levels=[pd.Int64Index([]), pd.Int64Index([0])],\\n2090.                 labels=[[], []],\\n2091.                 names=["x", "y"],\\n2092.             )\\n2093.         else:\\n2094.             b = pd.MultiIndex(\\n2095.                 levels=[pd.Int64Index([]), pd.Int64Index([0])],\\n2096.                 codes=[[], []],\\n2097.                 names=["x", "y"],\\n2098.             )\\n2099.         pd.testing.assert_index_equal(a, b)\\n2100. \\n2101.         actual = orig.stack(z=["x", "y"]).unstack("z").drop_vars(["x", "y"])\\n2102.         assert_identical(orig, actual)\\n2103. \\n2104.         actual = orig.stack(z=[...]).unstack("z").drop_vars(["x", "y"])\\n2105.         assert_identical(orig, actual)\\n2106. \\n2107.         dims = ["a", "b", "c", "d", "e"]\\n2108.         orig = xr.DataArray(np.random.rand(1, 2, 3, 2, 1), dims=dims)\\n2109.         stacked = orig.stack(ab=["a", "b"], cd=["c", "d"])\\n2110. \\n2111.         unstacked = stacked.unstack(["ab", "cd"])\\n2112.         roundtripped = unstacked.drop_vars(["a", "b", "c", "d"]).transpose(*dims)\\n2113.         assert_identical(orig, roundtripped)\\n2114. \\n2115.         unstacked = stacked.unstack()\\n2116.         roundtripped = unstacked.drop_vars(["a", "b", "c", "d"]).transpose(*dims)\\n2117.         assert_identical(orig, roundtripped)\\n2118. \\n2119.     def test_stack_unstack_decreasing_coordinate(self):\\n2120.         # regression test for GH980\\n2121.         orig = DataArray(\\n2122.             np.random.rand(3, 4),\\n2123.             dims=("y", "x"),\\n2124.             coords={"x": np.arange(4), "y": np.arange(3, 0, -1)},\\n2125.         )\\n2126.         stacked = orig.stack(allpoints=["y", "x"])\\n2127.         actual = stacked.unstack("allpoints")\\n2128.         assert_identical(orig, actual)\\n2129. \\n2130.     def test_unstack_pandas_consistency(self):\\n2131.         df = pd.DataFrame({"foo": range(3), "x": ["a", "b", "b"], "y": [0, 0, 1]})\\n2132.         s = df.set_index(["x", "y"])["foo"]\\n2133.         expected = DataArray(s.unstack(), name="foo")\\n2134.         actual = DataArray(s, dims="z").unstack("z")\\n2135.         assert_identical(expected, actual)\\n2136. \\n2137.     def test_stack_nonunique_consistency(self):\\n2138.         orig = DataArray(\\n2139.             [[0, 1], [2, 3]], dims=["x", "y"], coords={"x": [0, 1], "y": [0, 0]}\\n2140.         )\\n2141.         actual = orig.stack(z=["x", "y"])\\n2142.         expected = DataArray(orig.to_pandas().stack(), dims="z")\\n2143.         assert_identical(expected, actual)\\n2144. \\n2145.     def test_to_unstacked_dataset_raises_value_error(self):\\n2146.         data = DataArray([0, 1], dims="x", coords={"x": [0, 1]})\\n2147.         with pytest.raises(ValueError, match="\\\'x\\\' is not a stacked coordinate"):\\n2148.             data.to_unstacked_dataset("x", 0)\\n2149. \\n2150.     def test_transpose(self):\\n2151.         da = DataArray(\\n2152.             np.random.randn(3, 4, 5),\\n2153.             dims=("x", "y", "z"),\\n2154.             coords={\\n2155.                 "x": range(3),\\n2156.                 "y": range(4),\\n2157.                 "z": range(5),\\n2158.                 "xy": (("x", "y"), np.random.randn(3, 4)),\\n2159.             },\\n2160.         )\\n2161. \\n2162.         actual = da.transpose(transpose_coords=False)\\n2163.         expected = DataArray(da.values.T, dims=("z", "y", "x"), coords=da.coords)\\n2164.         assert_equal(expected, actual)\\n2165. \\n2166.         actual = da.transpose("z", "y", "x", transpose_coords=True)\\n2167.         expected = DataArray(\\n2168.             da.values.T,\\n2169.             dims=("z", "y", "x"),\\n2170.             coords={\\n2171.                 "x": da.x.values,\\n2172.                 "y": da.y.values,\\n2173.                 "z": da.z.values,\\n2174.                 "xy": (("y", "x"), da.xy.values.T),\\n2175.             },\\n2176.         )\\n2177.         assert_equal(expected, actual)\\n2178. \\n2179.         # same as previous but with ellipsis\\n2180.         actual = da.transpose("z", ..., "x", transpose_coords=True)\\n2181.         assert_equal(expected, actual)\\n2182. \\n2183.         with pytest.raises(ValueError):\\n2184.             da.transpose("x", "y")\\n2185. \\n2186.     def test_squeeze(self):\\n2187.         assert_equal(self.dv.variable.squeeze(), self.dv.squeeze().variable)\\n2188. \\n2189.     def test_squeeze_drop(self):\\n2190.         array = DataArray([1], [("x", [0])])\\n2191.         expected = DataArray(1)\\n2192.         actual = array.squeeze(drop=True)\\n2193.         assert_identical(expected, actual)\\n2194. \\n2195.         expected = DataArray(1, {"x": 0})\\n2196.         actual = array.squeeze(drop=False)\\n2197.         assert_identical(expected, actual)\\n2198. \\n2199.         array = DataArray([[[0.0, 1.0]]], dims=["dim_0", "dim_1", "dim_2"])\\n2200.         expected = DataArray([[0.0, 1.0]], dims=["dim_1", "dim_2"])\\n2201.         actual = array.squeeze(axis=0)\\n2202.         assert_identical(expected, actual)\\n2203. \\n2204.         array = DataArray([[[[0.0, 1.0]]]], dims=["dim_0", "dim_1", "dim_2", "dim_3"])\\n2205.         expected = DataArray([[0.0, 1.0]], dims=["dim_1", "dim_3"])\\n2206.         actual = array.squeeze(axis=(0, 2))\\n2207.         assert_identical(expected, actual)\\n2208. \\n2209.         array = DataArray([[[0.0, 1.0]]], dims=["dim_0", "dim_1", "dim_2"])\\n2210.         with pytest.raises(ValueError):\\n2211.             array.squeeze(axis=0, dim="dim_1")\\n2212. \\n2213.     def test_drop_coordinates(self):\\n2214.         expected = DataArray(np.random.randn(2, 3), dims=["x", "y"])\\n2215.         arr = expected.copy()\\n2216.         arr.coords["z"] = 2\\n2217.         actual = arr.drop_vars("z")\\n2218.         assert_identical(expected, actual)\\n2219. \\n2220.         with pytest.raises(ValueError):\\n2221.             arr.drop_vars("not found")\\n2222. \\n2223.         actual = expected.drop_vars("not found", errors="ignore")\\n2224.         assert_identical(actual, expected)\\n2225. \\n2226.         with raises_regex(ValueError, "cannot be found"):\\n2227.             arr.drop_vars("w")\\n2228. \\n2229.         actual = expected.drop_vars("w", errors="ignore")\\n2230.         assert_identical(actual, expected)\\n2231. \\n2232.         renamed = arr.rename("foo")\\n2233.         with raises_regex(ValueError, "cannot be found"):\\n2234.             renamed.drop_vars("foo")\\n2235. \\n2236.         actual = renamed.drop_vars("foo", errors="ignore")\\n2237.         assert_identical(actual, renamed)\\n2238. \\n2239.     def test_drop_index_labels(self):\\n2240.         arr = DataArray(np.random.randn(2, 3), coords={"y": [0, 1, 2]}, dims=["x", "y"])\\n2241.         actual = arr.drop_sel(y=[0, 1])\\n2242.         expected = arr[:, 2:]\\n2243.         assert_identical(actual, expected)\\n2244. \\n2245.         with raises_regex((KeyError, ValueError), "not .* in axis"):\\n2246.             actual = arr.drop_sel(y=[0, 1, 3])\\n2247. \\n2248.         actual = arr.drop_sel(y=[0, 1, 3], errors="ignore")\\n2249.         assert_identical(actual, expected)\\n2250. \\n2251.         with pytest.warns(DeprecationWarning):\\n2252.             arr.drop([0, 1, 3], dim="y", errors="ignore")\\n2253. \\n2254.     def test_dropna(self):\\n2255.         x = np.random.randn(4, 4)\\n2256.         x[::2, 0] = np.nan\\n2257.         arr = DataArray(x, dims=["a", "b"])\\n2258. \\n2259.         actual = arr.dropna("a")\\n2260.         expected = arr[1::2]\\n2261.         assert_identical(actual, expected)\\n2262. \\n2263.         actual = arr.dropna("b", how="all")\\n2264.         assert_identical(actual, arr)\\n2265. \\n2266.         actual = arr.dropna("a", thresh=1)\\n2267.         assert_identical(actual, arr)\\n2268. \\n2269.         actual = arr.dropna("b", thresh=3)\\n2270.         expected = arr[:, 1:]\\n2271.         assert_identical(actual, expected)\\n2272. \\n2273.     def test_where(self):\\n2274.         arr = DataArray(np.arange(4), dims="x")\\n2275.         expected = arr.sel(x=slice(2))\\n2276.         actual = arr.where(arr.x < 2, drop=True)\\n2277.         assert_identical(actual, expected)\\n2278. \\n2279.     def test_where_lambda(self):\\n2280.         arr = DataArray(np.arange(4), dims="y")\\n2281.         expected = arr.sel(y=slice(2))\\n2282.         actual = arr.where(lambda x: x.y < 2, drop=True)\\n2283.         assert_identical(actual, expected)\\n2284. \\n2285.     def test_where_string(self):\\n2286.         array = DataArray(["a", "b"])\\n2287.         expected = DataArray(np.array(["a", np.nan], dtype=object))\\n2288.         actual = array.where([True, False])\\n2289.         assert_identical(actual, expected)\\n2290. \\n2291.     def test_cumops(self):\\n2292.         coords = {\\n2293.             "x": [-1, -2],\\n2294.             "y": ["ab", "cd", "ef"],\\n2295.             "lat": (["x", "y"], [[1, 2, 3], [-1, -2, -3]]),\\n2296.             "c": -999,\\n2297.         }\\n2298.         orig = DataArray([[-1, 0, 1], [-3, 0, 3]], coords, dims=["x", "y"])\\n2299. \\n2300.         actual = orig.cumsum()\\n2301.         expected = DataArray([[-1, -1, 0], [-4, -4, 0]], coords, dims=["x", "y"])\\n2302.         assert_identical(expected, actual)\\n2303. \\n2304.         actual = orig.cumsum("x")\\n2305.         expected = DataArray([[-1, 0, 1], [-4, 0, 4]], coords, dims=["x", "y"])\\n2306.         assert_identical(expected, actual)\\n2307. \\n2308.         actual = orig.cumsum("y")\\n2309.         expected = DataArray([[-1, -1, 0], [-3, -3, 0]], coords, dims=["x", "y"])\\n2310.         assert_identical(expected, actual)\\n2311. \\n2312.         actual = orig.cumprod("x")\\n2313.         expected = DataArray([[-1, 0, 1], [3, 0, 3]], coords, dims=["x", "y"])\\n2314.         assert_identical(expected, actual)\\n2315. \\n2316.         actual = orig.cumprod("y")\\n2317.         expected = DataArray([[-1, 0, 0], [-3, 0, 0]], coords, dims=["x", "y"])\\n2318.         assert_identical(expected, actual)\\n2319. \\n2320.     def test_reduce(self):\\n2321.         coords = {\\n2322.             "x": [-1, -2],\\n2323.             "y": ["ab", "cd", "ef"],\\n2324.             "lat": (["x", "y"], [[1, 2, 3], [-1, -2, -3]]),\\n2325.             "c": -999,\\n2326.         }\\n2327.         orig = DataArray([[-1, 0, 1], [-3, 0, 3]], coords, dims=["x", "y"])\\n2328. \\n2329.         actual = orig.mean()\\n2330.         expected = DataArray(0, {"c": -999})\\n2331.         assert_identical(expected, actual)\\n2332. \\n2333.         actual = orig.mean(["x", "y"])\\n2334.         assert_identical(expected, actual)\\n2335. \\n2336.         actual = orig.mean("x")\\n2337.         expected = DataArray([-2, 0, 2], {"y": coords["y"], "c": -999}, "y")\\n2338.         assert_identical(expected, actual)\\n2339. \\n2340.         actual = orig.mean(["x"])\\n2341.         assert_identical(expected, actual)\\n2342. \\n2343.         actual = orig.mean("y")\\n2344.         expected = DataArray([0, 0], {"x": coords["x"], "c": -999}, "x")\\n2345.         assert_identical(expected, actual)\\n2346. \\n2347.         assert_equal(self.dv.reduce(np.mean, "x").variable, self.v.reduce(np.mean, "x"))\\n2348. \\n2349.         orig = DataArray([[1, 0, np.nan], [3, 0, 3]], coords, dims=["x", "y"])\\n2350.         actual = orig.count()\\n2351.         expected = DataArray(5, {"c": -999})\\n2352.         assert_identical(expected, actual)\\n2353. \\n2354.         # uint support\\n2355.         orig = DataArray(np.arange(6).reshape(3, 2).astype("uint"), dims=["x", "y"])\\n2356.         assert orig.dtype.kind == "u"\\n2357.         actual = orig.mean(dim="x", skipna=True)\\n2358.         expected = DataArray(orig.values.astype(int), dims=["x", "y"]).mean("x")\\n2359.         assert_equal(actual, expected)\\n2360. \\n2361.     def test_reduce_keepdims(self):\\n2362.         coords = {\\n2363.             "x": [-1, -2],\\n2364.             "y": ["ab", "cd", "ef"],\\n2365.             "lat": (["x", "y"], [[1, 2, 3], [-1, -2, -3]]),\\n2366.             "c": -999,\\n2367.         }\\n2368.         orig = DataArray([[-1, 0, 1], [-3, 0, 3]], coords, dims=["x", "y"])\\n2369. \\n2370.         # Mean on all axes loses non-constant coordinates\\n2371.         actual = orig.mean(keepdims=True)\\n2372.         expected = DataArray(\\n2373.             orig.data.mean(keepdims=True),\\n2374.             dims=orig.dims,\\n2375.             coords={k: v for k, v in coords.items() if k in ["c"]},\\n2376.         )\\n2377.         assert_equal(actual, expected)\\n2378. \\n2379.         assert actual.sizes["x"] == 1\\n2380.         assert actual.sizes["y"] == 1\\n2381. \\n2382.         # Mean on specific axes loses coordinates not involving that axis\\n2383.         actual = orig.mean("y", keepdims=True)\\n2384.         expected = DataArray(\\n2385.             orig.data.mean(axis=1, keepdims=True),\\n2386.             dims=orig.dims,\\n2387.             coords={k: v for k, v in coords.items() if k not in ["y", "lat"]},\\n2388.         )\\n2389.         assert_equal(actual, expected)\\n2390. \\n2391.     @requires_bottleneck\\n2392.     def test_reduce_keepdims_bottleneck(self):\\n2393.         import bottleneck\\n2394. \\n2395.         coords = {\\n2396.             "x": [-1, -2],\\n2397.             "y": ["ab", "cd", "ef"],\\n2398.             "lat": (["x", "y"], [[1, 2, 3], [-1, -2, -3]]),\\n2399.             "c": -999,\\n2400.         }\\n2401.         orig = DataArray([[-1, 0, 1], [-3, 0, 3]], coords, dims=["x", "y"])\\n2402. \\n2403.         # Bottleneck does not have its own keepdims implementation\\n2404.         actual = orig.reduce(bottleneck.nanmean, keepdims=True)\\n2405.         expected = orig.mean(keepdims=True)\\n2406.         assert_equal(actual, expected)\\n2407. \\n2408.     def test_reduce_dtype(self):\\n2409.         coords = {\\n2410.             "x": [-1, -2],\\n2411.             "y": ["ab", "cd", "ef"],\\n2412.             "lat": (["x", "y"], [[1, 2, 3], [-1, -2, -3]]),\\n2413.             "c": -999,\\n2414.         }\\n2415.         orig = DataArray([[-1, 0, 1], [-3, 0, 3]], coords, dims=["x", "y"])\\n2416. \\n2417.         for dtype in [np.float16, np.float32, np.float64]:\\n2418.             assert orig.astype(float).mean(dtype=dtype).dtype == dtype\\n2419. \\n2420.     def test_reduce_out(self):\\n2421.         coords = {\\n2422.             "x": [-1, -2],\\n2423.             "y": ["ab", "cd", "ef"],\\n2424.             "lat": (["x", "y"], [[1, 2, 3], [-1, -2, -3]]),\\n2425.             "c": -999,\\n2426.         }\\n2427.         orig = DataArray([[-1, 0, 1], [-3, 0, 3]], coords, dims=["x", "y"])\\n2428. \\n2429.         with pytest.raises(TypeError):\\n2430.             orig.mean(out=np.ones(orig.shape))\\n2431. \\n2432.     @pytest.mark.parametrize("skipna", [True, False])\\n2433.     @pytest.mark.parametrize("q", [0.25, [0.50], [0.25, 0.75]])\\n2434.     @pytest.mark.parametrize(\\n2435.         "axis, dim", zip([None, 0, [0], [0, 1]], [None, "x", ["x"], ["x", "y"]])\\n2436.     )\\n2437.     def test_quantile(self, q, axis, dim, skipna):\\n2438.         actual = DataArray(self.va).quantile(q, dim=dim, keep_attrs=True, skipna=skipna)\\n2439.         _percentile_func = np.nanpercentile if skipna else np.percentile\\n2440.         expected = _percentile_func(self.dv.values, np.array(q) * 100, axis=axis)\\n2441.         np.testing.assert_allclose(actual.values, expected)\\n2442.         if is_scalar(q):\\n2443.             assert "quantile" not in actual.dims\\n2444.         else:\\n2445.             assert "quantile" in actual.dims\\n2446. \\n2447.         assert actual.attrs == self.attrs\\n2448. \\n2449.     def test_reduce_keep_attrs(self):\\n2450.         # Test dropped attrs\\n2451.         vm = self.va.mean()\\n2452.         assert len(vm.attrs) == 0\\n2453.         assert vm.attrs == {}\\n2454. \\n2455.         # Test kept attrs\\n2456.         vm = self.va.mean(keep_attrs=True)\\n2457.         assert len(vm.attrs) == len(self.attrs)\\n2458.         assert vm.attrs == self.attrs\\n2459. \\n2460.     def test_assign_attrs(self):\\n2461.         expected = DataArray([], attrs=dict(a=1, b=2))\\n2462.         expected.attrs["a"] = 1\\n2463.         expected.attrs["b"] = 2\\n2464.         new = DataArray([])\\n2465.         actual = DataArray([]).assign_attrs(a=1, b=2)\\n2466.         assert_identical(actual, expected)\\n2467.         assert new.attrs == {}\\n2468. \\n2469.         expected.attrs["c"] = 3\\n2470.         new_actual = actual.assign_attrs({"c": 3})\\n2471.         assert_identical(new_actual, expected)\\n2472.         assert actual.attrs == {"a": 1, "b": 2}\\n2473. \\n2474.     def test_fillna(self):\\n2475.         a = DataArray([np.nan, 1, np.nan, 3], coords={"x": range(4)}, dims="x")\\n2476.         actual = a.fillna(-1)\\n2477.         expected = DataArray([-1, 1, -1, 3], coords={"x": range(4)}, dims="x")\\n2478.         assert_identical(expected, actual)\\n2479. \\n2480.         b = DataArray(range(4), coords={"x": range(4)}, dims="x")\\n2481.         actual = a.fillna(b)\\n2482.         expected = b.copy()\\n2483.         assert_identical(expected, actual)\\n2484. \\n2485.         actual = a.fillna(range(4))\\n2486.         assert_identical(expected, actual)\\n2487. \\n2488.         actual = a.fillna(b[:3])\\n2489.         assert_identical(expected, actual)\\n2490. \\n2491.         actual = a.fillna(b[:0])\\n2492.         assert_identical(a, actual)\\n2493. \\n2494.         with raises_regex(TypeError, "fillna on a DataArray"):\\n2495.             a.fillna({0: 0})\\n2496. \\n2497.         with raises_regex(ValueError, "broadcast"):\\n2498.             a.fillna([1, 2])\\n2499. \\n2500.         fill_value = DataArray([0, 1], dims="y")\\n2501.         actual = a.fillna(fill_value)\\n2502.         expected = DataArray(\\n2503.             [[0, 1], [1, 1], [0, 1], [3, 3]], coords={"x": range(4)}, dims=("x", "y")\\n2504.         )\\n2505.         assert_identical(expected, actual)\\n2506. \\n2507.         expected = b.copy()\\n2508.         for target in [a, expected]:\\n2509.             target.coords["b"] = ("x", [0, 0, 1, 1])\\n2510.         actual = a.groupby("b").fillna(DataArray([0, 2], dims="b"))\\n2511.         assert_identical(expected, actual)\\n2512. \\n2513.     def test_groupby_iter(self):\\n2514.         for ((act_x, act_dv), (exp_x, exp_ds)) in zip(\\n2515.             self.dv.groupby("y"), self.ds.groupby("y")\\n2516.         ):\\n2517.             assert exp_x == act_x\\n2518.             assert_identical(exp_ds["foo"], act_dv)\\n2519.         for ((_, exp_dv), act_dv) in zip(self.dv.groupby("x"), self.dv):\\n2520.             assert_identical(exp_dv, act_dv)\\n2521. \\n2522.     def make_groupby_example_array(self):\\n2523.         da = self.dv.copy()\\n2524.         da.coords["abc"] = ("y", np.array(["a"] * 9 + ["c"] + ["b"] * 10))\\n2525.         da.coords["y"] = 20 + 100 * da["y"]\\n2526.         return da\\n2527. \\n2528.     def test_groupby_properties(self):\\n2529.         grouped = self.make_groupby_example_array().groupby("abc")\\n2530.         expected_groups = {"a": range(0, 9), "c": [9], "b": range(10, 20)}\\n2531.         assert expected_groups.keys() == grouped.groups.keys()\\n2532.         for key in expected_groups:\\n2533.             assert_array_equal(expected_groups[key], grouped.groups[key])\\n2534.         assert 3 == len(grouped)\\n2535. \\n2536.     def test_groupby_map_identity(self):\\n2537.         expected = self.make_groupby_example_array()\\n2538.         idx = expected.coords["y"]\\n2539. \\n2540.         def identity(x):\\n2541.             return x\\n2542. \\n2543.         for g in ["x", "y", "abc", idx]:\\n2544.             for shortcut in [False, True]:\\n2545.                 for squeeze in [False, True]:\\n2546.                     grouped = expected.groupby(g, squeeze=squeeze)\\n2547.                     actual = grouped.map(identity, shortcut=shortcut)\\n2548.                     assert_identical(expected, actual)\\n2549. \\n2550.     def test_groupby_sum(self):\\n2551.         array = self.make_groupby_example_array()\\n2552.         grouped = array.groupby("abc")\\n2553. \\n2554.         expected_sum_all = Dataset(\\n2555.             {\\n2556.                 "foo": Variable(\\n2557.                     ["abc"],\\n2558.                     np.array(\\n2559.                         [\\n2560.                             self.x[:, :9].sum(),\\n2561.                             self.x[:, 10:].sum(),\\n2562.                             self.x[:, 9:10].sum(),\\n2563.                         ]\\n2564.                     ).T,\\n2565.                 ),\\n2566.                 "abc": Variable(["abc"], np.array(["a", "b", "c"])),\\n2567.             }\\n2568.         )["foo"]\\n2569.         assert_allclose(expected_sum_all, grouped.reduce(np.sum, dim=...))\\n2570.         assert_allclose(expected_sum_all, grouped.sum(...))\\n2571. \\n2572.         expected = DataArray(\\n2573.             [\\n2574.                 array["y"].values[idx].sum()\\n2575.                 for idx in [slice(9), slice(10, None), slice(9, 10)]\\n2576.             ],\\n2577.             [["a", "b", "c"]],\\n2578.             ["abc"],\\n2579.         )\\n2580.         actual = array["y"].groupby("abc").map(np.sum)\\n2581.         assert_allclose(expected, actual)\\n2582.         actual = array["y"].groupby("abc").sum(...)\\n2583.         assert_allclose(expected, actual)\\n2584. \\n2585.         expected_sum_axis1 = Dataset(\\n2586.             {\\n2587.                 "foo": (\\n2588.                     ["x", "abc"],\\n2589.                     np.array(\\n2590.                         [\\n2591.                             self.x[:, :9].sum(1),\\n2592.                             self.x[:, 10:].sum(1),\\n2593.                             self.x[:, 9:10].sum(1),\\n2594.                         ]\\n2595.                     ).T,\\n2596.                 ),\\n2597.                 "abc": Variable(["abc"], np.array(["a", "b", "c"])),\\n2598.             }\\n2599.         )["foo"]\\n2600.         assert_allclose(expected_sum_axis1, grouped.reduce(np.sum, "y"))\\n2601.         assert_allclose(expected_sum_axis1, grouped.sum("y"))\\n2602. \\n2603.     def test_groupby_sum_default(self):\\n2604.         array = self.make_groupby_example_array()\\n2605.         grouped = array.groupby("abc")\\n2606. \\n2607.         expected_sum_all = Dataset(\\n2608.             {\\n2609.                 "foo": Variable(\\n2610.                     ["x", "abc"],\\n2611.                     np.array(\\n2612.                         [\\n2613.                             self.x[:, :9].sum(axis=-1),\\n2614.                             self.x[:, 10:].sum(axis=-1),\\n2615.                             self.x[:, 9:10].sum(axis=-1),\\n2616.                         ]\\n2617.                     ).T,\\n2618.                 ),\\n2619.                 "abc": Variable(["abc"], np.array(["a", "b", "c"])),\\n2620.             }\\n2621.         )["foo"]\\n2622. \\n2623.         assert_allclose(expected_sum_all, grouped.sum(dim="y"))\\n2624. \\n2625.     def test_groupby_count(self):\\n2626.         array = DataArray(\\n2627.             [0, 0, np.nan, np.nan, 0, 0],\\n2628.             coords={"cat": ("x", ["a", "b", "b", "c", "c", "c"])},\\n2629.             dims="x",\\n2630.         )\\n2631.         actual = array.groupby("cat").count()\\n2632.         expected = DataArray([1, 1, 2], coords=[("cat", ["a", "b", "c"])])\\n2633.         assert_identical(actual, expected)\\n2634. \\n2635.     @pytest.mark.skip("needs to be fixed for shortcut=False, keep_attrs=False")\\n2636.     def test_groupby_reduce_attrs(self):\\n2637.         array = self.make_groupby_example_array()\\n2638.         array.attrs["foo"] = "bar"\\n2639. \\n2640.         for shortcut in [True, False]:\\n2641.             for keep_attrs in [True, False]:\\n2642.                 print(f"shortcut={shortcut}, keep_attrs={keep_attrs}")\\n2643.                 actual = array.groupby("abc").reduce(\\n2644.                     np.mean, keep_attrs=keep_attrs, shortcut=shortcut\\n2645.                 )\\n2646.                 expected = array.groupby("abc").mean()\\n2647.                 if keep_attrs:\\n2648.                     expected.attrs["foo"] = "bar"\\n2649.                 assert_identical(expected, actual)\\n2650. \\n2651.     def test_groupby_map_center(self):\\n2652.         def center(x):\\n2653.             return x - np.mean(x)\\n2654. \\n2655.         array = self.make_groupby_example_array()\\n2656.         grouped = array.groupby("abc")\\n2657. \\n2658.         expected_ds = array.to_dataset()\\n2659.         exp_data = np.hstack(\\n2660.             [center(self.x[:, :9]), center(self.x[:, 9:10]), center(self.x[:, 10:])]\\n2661.         )\\n2662.         expected_ds["foo"] = (["x", "y"], exp_data)\\n2663.         expected_centered = expected_ds["foo"]\\n2664.         assert_allclose(expected_centered, grouped.map(center))\\n2665. \\n2666.     def test_groupby_map_ndarray(self):\\n2667.         # regression test for #326\\n2668.         array = self.make_groupby_example_array()\\n2669.         grouped = array.groupby("abc")\\n2670.         actual = grouped.map(np.asarray)\\n2671.         assert_equal(array, actual)\\n2672. \\n2673.     def test_groupby_map_changes_metadata(self):\\n2674.         def change_metadata(x):\\n2675.             x.coords["x"] = x.coords["x"] * 2\\n2676.             x.attrs["fruit"] = "lemon"\\n2677.             return x\\n2678. \\n2679.         array = self.make_groupby_example_array()\\n2680.         grouped = array.groupby("abc")\\n2681.         actual = grouped.map(change_metadata)\\n2682.         expected = array.copy()\\n2683.         expected = change_metadata(expected)\\n2684.         assert_equal(expected, actual)\\n2685. \\n2686.     def test_groupby_math(self):\\n2687.         array = self.make_groupby_example_array()\\n2688.         for squeeze in [True, False]:\\n2689.             grouped = array.groupby("x", squeeze=squeeze)\\n2690. \\n2691.             expected = array + array.coords["x"]\\n2692.             actual = grouped + array.coords["x"]\\n2693.             assert_identical(expected, actual)\\n2694. \\n2695.             actual = array.coords["x"] + grouped\\n2696.             assert_identical(expected, actual)\\n2697. \\n2698.             ds = array.coords["x"].to_dataset(name="X")\\n2699.             expected = array + ds\\n2700.             actual = grouped + ds\\n2701.             assert_identical(expected, actual)\\n2702. \\n2703.             actual = ds + grouped\\n2704.             assert_identical(expected, actual)\\n2705. \\n2706.         grouped = array.groupby("abc")\\n2707.         expected_agg = (grouped.mean(...) - np.arange(3)).rename(None)\\n2708.         actual = grouped - DataArray(range(3), [("abc", ["a", "b", "c"])])\\n2709.         actual_agg = actual.groupby("abc").mean(...)\\n2710.         assert_allclose(expected_agg, actual_agg)\\n2711. \\n2712.         with raises_regex(TypeError, "only support binary ops"):\\n2713.             grouped + 1\\n2714.         with raises_regex(TypeError, "only support binary ops"):\\n2715.             grouped + grouped\\n2716.         with raises_regex(TypeError, "in-place operations"):\\n2717.             array += grouped\\n2718. \\n2719.     def test_groupby_math_not_aligned(self):\\n2720.         array = DataArray(\\n2721.             range(4), {"b": ("x", [0, 0, 1, 1]), "x": [0, 1, 2, 3]}, dims="x"\\n2722.         )\\n2723.         other = DataArray([10], coords={"b": [0]}, dims="b")\\n2724.         actual = array.groupby("b") + other\\n2725.         expected = DataArray([10, 11, np.nan, np.nan], array.coords)\\n2726.         assert_identical(expected, actual)\\n2727. \\n2728.         other = DataArray([10], coords={"c": 123, "b": [0]}, dims="b")\\n2729.         actual = array.groupby("b") + other\\n2730.         expected.coords["c"] = (["x"], [123] * 2 + [np.nan] * 2)\\n2731.         assert_identical(expected, actual)\\n2732. \\n2733.         other = Dataset({"a": ("b", [10])}, {"b": [0]})\\n2734.         actual = array.groupby("b") + other\\n2735.         expected = Dataset({"a": ("x", [10, 11, np.nan, np.nan])}, array.coords)\\n2736.         assert_identical(expected, actual)\\n2737. \\n2738.     def test_groupby_restore_dim_order(self):\\n2739.         array = DataArray(\\n2740.             np.random.randn(5, 3),\\n2741.             coords={"a": ("x", range(5)), "b": ("y", range(3))},\\n2742.             dims=["x", "y"],\\n2743.         )\\n2744.         for by, expected_dims in [\\n2745.             ("x", ("x", "y")),\\n2746.             ("y", ("x", "y")),\\n2747.             ("a", ("a", "y")),\\n2748.             ("b", ("x", "b")),\\n2749.         ]:\\n2750.             result = array.groupby(by).map(lambda x: x.squeeze())\\n2751.             assert result.dims == expected_dims\\n2752. \\n2753.     def test_groupby_restore_coord_dims(self):\\n2754.         array = DataArray(\\n2755.             np.random.randn(5, 3),\\n2756.             coords={\\n2757.                 "a": ("x", range(5)),\\n2758.                 "b": ("y", range(3)),\\n2759.                 "c": (("x", "y"), np.random.randn(5, 3)),\\n2760.             },\\n2761.             dims=["x", "y"],\\n2762.         )\\n2763. \\n2764.         for by, expected_dims in [\\n2765.             ("x", ("x", "y")),\\n2766.             ("y", ("x", "y")),\\n2767.             ("a", ("a", "y")),\\n2768.             ("b", ("x", "b")),\\n2769.         ]:\\n2770.             result = array.groupby(by, restore_coord_dims=True).map(\\n2771.                 lambda x: x.squeeze()\\n2772.             )["c"]\\n2773.             assert result.dims == expected_dims\\n2774. \\n2775.     def test_groupby_first_and_last(self):\\n2776.         array = DataArray([1, 2, 3, 4, 5], dims="x")\\n2777.         by = DataArray(["a"] * 2 + ["b"] * 3, dims="x", name="ab")\\n2778. \\n2779.         expected = DataArray([1, 3], [("ab", ["a", "b"])])\\n2780.         actual = array.groupby(by).first()\\n2781.         assert_identical(expected, actual)\\n2782. \\n2783.         expected = DataArray([2, 5], [("ab", ["a", "b"])])\\n2784.         actual = array.groupby(by).last()\\n2785.         assert_identical(expected, actual)\\n2786. \\n2787.         array = DataArray(np.random.randn(5, 3), dims=["x", "y"])\\n2788.         expected = DataArray(array[[0, 2]], {"ab": ["a", "b"]}, ["ab", "y"])\\n2789.         actual = array.groupby(by).first()\\n2790.         assert_identical(expected, actual)\\n2791. \\n2792.         actual = array.groupby("x").first()\\n2793.         expected = array  # should be a no-op\\n2794.         assert_identical(expected, actual)\\n2795. \\n2796.     def make_groupby_multidim_example_array(self):\\n2797.         return DataArray(\\n2798.             [[[0, 1], [2, 3]], [[5, 10], [15, 20]]],\\n2799.             coords={\\n2800.                 "lon": (["ny", "nx"], [[30, 40], [40, 50]]),\\n2801.                 "lat": (["ny", "nx"], [[10, 10], [20, 20]]),\\n2802.             },\\n2803.             dims=["time", "ny", "nx"],\\n2804.         )\\n2805. \\n2806.     def test_groupby_multidim(self):\\n2807.         array = self.make_groupby_multidim_example_array()\\n2808.         for dim, expected_sum in [\\n2809.             ("lon", DataArray([5, 28, 23], coords=[("lon", [30.0, 40.0, 50.0])])),\\n2810.             ("lat", DataArray([16, 40], coords=[("lat", [10.0, 20.0])])),\\n2811.         ]:\\n2812.             actual_sum = array.groupby(dim).sum(...)\\n2813.             assert_identical(expected_sum, actual_sum)\\n2814. \\n2815.     def test_groupby_multidim_map(self):\\n2816.         array = self.make_groupby_multidim_example_array()\\n2817.         actual = array.groupby("lon").map(lambda x: x - x.mean())\\n2818.         expected = DataArray(\\n2819.             [[[-2.5, -6.0], [-5.0, -8.5]], [[2.5, 3.0], [8.0, 8.5]]],\\n2820.             coords=array.coords,\\n2821.             dims=array.dims,\\n2822.         )\\n2823.         assert_identical(expected, actual)\\n2824. \\n2825.     def test_groupby_bins(self):\\n2826.         array = DataArray(np.arange(4), dims="dim_0")\\n2827.         # the first value should not be part of any group ("right" binning)\\n2828.         array[0] = 99\\n2829.         # bins follow conventions for pandas.cut\\n2830.         # http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\\n2831.         bins = [0, 1.5, 5]\\n2832.         bin_coords = pd.cut(array["dim_0"], bins).categories\\n2833.         expected = DataArray(\\n2834.             [1, 5], dims="dim_0_bins", coords={"dim_0_bins": bin_coords}\\n2835.         )\\n2836.         # the problem with this is that it overwrites the dimensions of array!\\n2837.         # actual = array.groupby(\\\'dim_0\\\', bins=bins).sum()\\n2838.         actual = array.groupby_bins("dim_0", bins).map(lambda x: x.sum())\\n2839.         assert_identical(expected, actual)\\n2840.         # make sure original array dims are unchanged\\n2841.         assert len(array.dim_0) == 4\\n2842. \\n2843.     def test_groupby_bins_empty(self):\\n2844.         array = DataArray(np.arange(4), [("x", range(4))])\\n2845.         # one of these bins will be empty\\n2846.         bins = [0, 4, 5]\\n2847.         bin_coords = pd.cut(array["x"], bins).categories\\n2848.         actual = array.groupby_bins("x", bins).sum()\\n2849.         expected = DataArray([6, np.nan], dims="x_bins", coords={"x_bins": bin_coords})\\n2850.         assert_identical(expected, actual)\\n2851.         # make sure original array is unchanged\\n2852.         # (was a problem in earlier versions)\\n2853.         assert len(array.x) == 4\\n2854. \\n2855.     def test_groupby_bins_multidim(self):\\n2856.         array = self.make_groupby_multidim_example_array()\\n2857.         bins = [0, 15, 20]\\n2858.         bin_coords = pd.cut(array["lat"].values.flat, bins).categories\\n2859.         expected = DataArray([16, 40], dims="lat_bins", coords={"lat_bins": bin_coords})\\n2860.         actual = array.groupby_bins("lat", bins).map(lambda x: x.sum())\\n2861.         assert_identical(expected, actual)\\n2862.         # modify the array coordinates to be non-monotonic after unstacking\\n2863.         array["lat"].data = np.array([[10.0, 20.0], [20.0, 10.0]])\\n2864.         expected = DataArray([28, 28], dims="lat_bins", coords={"lat_bins": bin_coords})\\n2865.         actual = array.groupby_bins("lat", bins).map(lambda x: x.sum())\\n2866.         assert_identical(expected, actual)\\n2867. \\n2868.     def test_groupby_bins_sort(self):\\n2869.         data = xr.DataArray(\\n2870.             np.arange(100), dims="x", coords={"x": np.linspace(-100, 100, num=100)}\\n2871.         )\\n2872.         binned_mean = data.groupby_bins("x", bins=11).mean()\\n2873.         assert binned_mean.to_index().is_monotonic\\n2874. \\n2875.     def test_resample(self):\\n2876.         times = pd.date_range("2000-01-01", freq="6H", periods=10)\\n2877.         array = DataArray(np.arange(10), [("time", times)])\\n2878. \\n2879.         actual = array.resample(time="24H").mean()\\n2880.         expected = DataArray(array.to_series().resample("24H").mean())\\n2881.         assert_identical(expected, actual)\\n2882. \\n2883.         actual = array.resample(time="24H").reduce(np.mean)\\n2884.         assert_identical(expected, actual)\\n2885. \\n2886.         actual = array.resample(time="24H", loffset="-12H").mean()\\n2887.         expected = DataArray(array.to_series().resample("24H", loffset="-12H").mean())\\n2888.         assert_identical(expected, actual)\\n2889. \\n2890.         with raises_regex(ValueError, "index must be monotonic"):\\n2891.             array[[2, 0, 1]].resample(time="1D")\\n2892. \\n2893.     def test_da_resample_func_args(self):\\n2894.         def func(arg1, arg2, arg3=0.0):\\n2895.             return arg1.mean("time") + arg2 + arg3\\n2896. \\n2897.         times = pd.date_range("2000", periods=3, freq="D")\\n2898.         da = xr.DataArray([1.0, 1.0, 1.0], coords=[times], dims=["time"])\\n2899.         expected = xr.DataArray([3.0, 3.0, 3.0], coords=[times], dims=["time"])\\n2900.         actual = da.resample(time="D").map(func, args=(1.0,), arg3=1.0)\\n2901.         assert_identical(actual, expected)\\n2902. \\n2903.     def test_resample_first(self):\\n2904.         times = pd.date_range("2000-01-01", freq="6H", periods=10)\\n2905.         array = DataArray(np.arange(10), [("time", times)])\\n2906. \\n2907.         actual = array.resample(time="1D").first()\\n2908.         expected = DataArray([0, 4, 8], [("time", times[::4])])\\n2909.         assert_identical(expected, actual)\\n2910. \\n2911.         # verify that labels don\\\'t use the first value\\n2912.         actual = array.resample(time="24H").first()\\n2913.         expected = DataArray(array.to_series().resample("24H").first())\\n2914.         assert_identical(expected, actual)\\n2915. \\n2916.         # missing values\\n2917.         array = array.astype(float)\\n2918.         array[:2] = np.nan\\n2919.         actual = array.resample(time="1D").first()\\n2920.         expected = DataArray([2, 4, 8], [("time", times[::4])])\\n2921.         assert_identical(expected, actual)\\n2922. \\n2923.         actual = array.resample(time="1D").first(skipna=False)\\n2924.         expected = DataArray([np.nan, 4, 8], [("time", times[::4])])\\n2925.         assert_identical(expected, actual)\\n2926. \\n2927.         # regression test for http://stackoverflow.com/questions/33158558/\\n2928.         array = Dataset({"time": times})["time"]\\n2929.         actual = array.resample(time="1D").last()\\n2930.         expected_times = pd.to_datetime(\\n2931.             ["2000-01-01T18", "2000-01-02T18", "2000-01-03T06"]\\n2932.         )\\n2933.         expected = DataArray(expected_times, [("time", times[::4])], name="time")\\n2934.         assert_identical(expected, actual)\\n2935. \\n2936.     def test_resample_bad_resample_dim(self):\\n2937.         times = pd.date_range("2000-01-01", freq="6H", periods=10)\\n2938.         array = DataArray(np.arange(10), [("__resample_dim__", times)])\\n2939.         with raises_regex(ValueError, "Proxy resampling dimension"):\\n2940.             array.resample(**{"__resample_dim__": "1D"}).first()\\n2941. \\n2942.     @requires_scipy\\n2943.     def test_resample_drop_nondim_coords(self):\\n2944.         xs = np.arange(6)\\n2945.         ys = np.arange(3)\\n2946.         times = pd.date_range("2000-01-01", freq="6H", periods=5)\\n2947.         data = np.tile(np.arange(5), (6, 3, 1))\\n2948.         xx, yy = np.meshgrid(xs * 5, ys * 2.5)\\n2949.         tt = np.arange(len(times), dtype=int)\\n2950.         array = DataArray(data, {"time": times, "x": xs, "y": ys}, ("x", "y", "time"))\\n2951.         xcoord = DataArray(xx.T, {"x": xs, "y": ys}, ("x", "y"))\\n2952.         ycoord = DataArray(yy.T, {"x": xs, "y": ys}, ("x", "y"))\\n2953.         tcoord = DataArray(tt, {"time": times}, ("time",))\\n2954.         ds = Dataset({"data": array, "xc": xcoord, "yc": ycoord, "tc": tcoord})\\n2955.         ds = ds.set_coords(["xc", "yc", "tc"])\\n2956. \\n2957.         # Select the data now, with the auxiliary coordinates in place\\n2958.         array = ds["data"]\\n2959. \\n2960.         # Re-sample\\n2961.         actual = array.resample(time="12H", restore_coord_dims=True).mean("time")\\n2962.         assert "tc" not in actual.coords\\n2963. \\n2964.         # Up-sample - filling\\n2965.         actual = array.resample(time="1H", restore_coord_dims=True).ffill()\\n2966.         assert "tc" not in actual.coords\\n2967. \\n2968.         # Up-sample - interpolation\\n2969.         actual = array.resample(time="1H", restore_coord_dims=True).interpolate(\\n2970.             "linear"\\n2971.         )\\n2972.         assert "tc" not in actual.coords\\n2973. \\n2974.     def test_resample_keep_attrs(self):\\n2975.         times = pd.date_range("2000-01-01", freq="6H", periods=10)\\n2976.         array = DataArray(np.ones(10), [("time", times)])\\n2977.         array.attrs["meta"] = "data"\\n2978. \\n2979.         result = array.resample(time="1D").mean(keep_attrs=True)\\n2980.         expected = DataArray([1, 1, 1], [("time", times[::4])], attrs=array.attrs)\\n2981.         assert_identical(result, expected)\\n2982. \\n2983.     def test_resample_skipna(self):\\n2984.         times = pd.date_range("2000-01-01", freq="6H", periods=10)\\n2985.         array = DataArray(np.ones(10), [("time", times)])\\n2986.         array[1] = np.nan\\n2987. \\n2988.         result = array.resample(time="1D").mean(skipna=False)\\n2989.         expected = DataArray([np.nan, 1, 1], [("time", times[::4])])\\n2990.         assert_identical(result, expected)\\n2991. \\n2992.     def test_upsample(self):\\n2993.         times = pd.date_range("2000-01-01", freq="6H", periods=5)\\n2994.         array = DataArray(np.arange(5), [("time", times)])\\n2995. \\n2996.         # Forward-fill\\n2997.         actual = array.resample(time="3H").ffill()\\n2998.         expected = DataArray(array.to_series().resample("3H").ffill())\\n2999.         assert_identical(expected, actual)\\n3000. \\n3001.         # Backward-fill\\n3002.         actual = array.resample(time="3H").bfill()\\n3003.         expected = DataArray(array.to_series().resample("3H").bfill())\\n3004.         assert_identical(expected, actual)\\n3005. \\n3006.         # As frequency\\n3007.         actual = array.resample(time="3H").asfreq()\\n3008.         expected = DataArray(array.to_series().resample("3H").asfreq())\\n3009.         assert_identical(expected, actual)\\n3010. \\n3011.         # Pad\\n3012.         actual = array.resample(time="3H").pad()\\n3013.         expected = DataArray(array.to_series().resample("3H").pad())\\n3014.         assert_identical(expected, actual)\\n3015. \\n3016.         # Nearest\\n3017.         rs = array.resample(time="3H")\\n3018.         actual = rs.nearest()\\n3019.         new_times = rs._full_index\\n3020.         expected = DataArray(array.reindex(time=new_times, method="nearest"))\\n3021.         assert_identical(expected, actual)\\n3022. \\n3023.     def test_upsample_nd(self):\\n3024.         # Same as before, but now we try on multi-dimensional DataArrays.\\n3025.         xs = np.arange(6)\\n3026.         ys = np.arange(3)\\n3027.         times = pd.date_range("2000-01-01", freq="6H", periods=5)\\n3028.         data = np.tile(np.arange(5), (6, 3, 1))\\n3029.         array = DataArray(data, {"time": times, "x": xs, "y": ys}, ("x", "y", "time"))\\n3030. \\n3031.         # Forward-fill\\n3032.         actual = array.resample(time="3H").ffill()\\n3033.         expected_data = np.repeat(data, 2, axis=-1)\\n3034.         expected_times = times.to_series().resample("3H").asfreq().index\\n3035.         expected_data = expected_data[..., : len(expected_times)]\\n3036.         expected = DataArray(\\n3037.             expected_data,\\n3038.             {"time": expected_times, "x": xs, "y": ys},\\n3039.             ("x", "y", "time"),\\n3040.         )\\n3041.         assert_identical(expected, actual)\\n3042. \\n3043.         # Backward-fill\\n3044.         actual = array.resample(time="3H").ffill()\\n3045.         expected_data = np.repeat(np.flipud(data.T).T, 2, axis=-1)\\n3046.         expected_data = np.flipud(expected_data.T).T\\n3047.         expected_times = times.to_series().resample("3H").asfreq().index\\n3048.         expected_data = expected_data[..., : len(expected_times)]\\n3049.         expected = DataArray(\\n3050.             expected_data,\\n3051.             {"time": expected_times, "x": xs, "y": ys},\\n3052.             ("x", "y", "time"),\\n3053.         )\\n3054.         assert_identical(expected, actual)\\n3055. \\n3056.         # As frequency\\n3057.         actual = array.resample(time="3H").asfreq()\\n3058.         expected_data = np.repeat(data, 2, axis=-1).astype(float)[..., :-1]\\n3059.         expected_data[..., 1::2] = np.nan\\n3060.         expected_times = times.to_series().resample("3H").asfreq().index\\n3061.         expected = DataArray(\\n3062.             expected_data,\\n3063.             {"time": expected_times, "x": xs, "y": ys},\\n3064.             ("x", "y", "time"),\\n3065.         )\\n3066.         assert_identical(expected, actual)\\n3067. \\n3068.         # Pad\\n3069.         actual = array.resample(time="3H").pad()\\n3070.         expected_data = np.repeat(data, 2, axis=-1)\\n3071.         expected_data[..., 1::2] = expected_data[..., ::2]\\n3072.         expected_data = expected_data[..., :-1]\\n3073.         expected_times = times.to_series().resample("3H").asfreq().index\\n3074.         expected = DataArray(\\n3075.             expected_data,\\n3076.             {"time": expected_times, "x": xs, "y": ys},\\n3077.             ("x", "y", "time"),\\n3078.         )\\n3079.         assert_identical(expected, actual)\\n3080. \\n3081.     def test_upsample_tolerance(self):\\n3082.         # Test tolerance keyword for upsample methods bfill, pad, nearest\\n3083.         times = pd.date_range("2000-01-01", freq="1D", periods=2)\\n3084.         times_upsampled = pd.date_range("2000-01-01", freq="6H", periods=5)\\n3085.         array = DataArray(np.arange(2), [("time", times)])\\n3086. \\n3087.         # Forward fill\\n3088.         actual = array.resample(time="6H").ffill(tolerance="12H")\\n3089.         expected = DataArray([0.0, 0.0, 0.0, np.nan, 1.0], [("time", times_upsampled)])\\n3090.         assert_identical(expected, actual)\\n3091. \\n3092.         # Backward fill\\n3093.         actual = array.resample(time="6H").bfill(tolerance="12H")\\n3094.         expected = DataArray([0.0, np.nan, 1.0, 1.0, 1.0], [("time", times_upsampled)])\\n3095.         assert_identical(expected, actual)\\n3096. \\n3097.         # Nearest\\n3098.         actual = array.resample(time="6H").nearest(tolerance="6H")\\n3099.         expected = DataArray([0, 0, np.nan, 1, 1], [("time", times_upsampled)])\\n3100.         assert_identical(expected, actual)\\n3101. \\n3102.     @requires_scipy\\n3103.     def test_upsample_interpolate(self):\\n3104.         from scipy.interpolate import interp1d\\n3105. \\n3106.         xs = np.arange(6)\\n3107.         ys = np.arange(3)\\n3108.         times = pd.date_range("2000-01-01", freq="6H", periods=5)\\n3109. \\n3110.         z = np.arange(5) ** 2\\n3111.         data = np.tile(z, (6, 3, 1))\\n3112.         array = DataArray(data, {"time": times, "x": xs, "y": ys}, ("x", "y", "time"))\\n3113. \\n3114.         expected_times = times.to_series().resample("1H").asfreq().index\\n3115.         # Split the times into equal sub-intervals to simulate the 6 hour\\n3116.         # to 1 hour up-sampling\\n3117.         new_times_idx = np.linspace(0, len(times) - 1, len(times) * 5)\\n3118.         for kind in ["linear", "nearest", "zero", "slinear", "quadratic", "cubic"]:\\n3119.             actual = array.resample(time="1H").interpolate(kind)\\n3120.             f = interp1d(\\n3121.                 np.arange(len(times)),\\n3122.                 data,\\n3123.                 kind=kind,\\n3124.                 axis=-1,\\n3125.                 bounds_error=True,\\n3126.                 assume_sorted=True,\\n3127.             )\\n3128.             expected_data = f(new_times_idx)\\n3129.             expected = DataArray(\\n3130.                 expected_data,\\n3131.                 {"time": expected_times, "x": xs, "y": ys},\\n3132.                 ("x", "y", "time"),\\n3133.             )\\n3134.             # Use AllClose because there are some small differences in how\\n3135.             # we upsample timeseries versus the integer indexing as I\\\'ve\\n3136.             # done here due to floating point arithmetic\\n3137.             assert_allclose(expected, actual, rtol=1e-16)\\n3138. \\n3139.     @requires_scipy\\n3140.     def test_upsample_interpolate_bug_2197(self):\\n3141.         dates = pd.date_range("2007-02-01", "2007-03-01", freq="D")\\n3142.         da = xr.DataArray(np.arange(len(dates)), [("time", dates)])\\n3143.         result = da.resample(time="M").interpolate("linear")\\n3144.         expected_times = np.array(\\n3145.             [np.datetime64("2007-02-28"), np.datetime64("2007-03-31")]\\n3146.         )\\n3147.         expected = xr.DataArray([27.0, np.nan], [("time", expected_times)])\\n3148.         assert_equal(result, expected)\\n3149. \\n3150.     @requires_scipy\\n3151.     def test_upsample_interpolate_regression_1605(self):\\n3152.         dates = pd.date_range("2016-01-01", "2016-03-31", freq="1D")\\n3153.         expected = xr.DataArray(\\n3154.             np.random.random((len(dates), 2, 3)),\\n3155.             dims=("time", "x", "y"),\\n3156.             coords={"time": dates},\\n3157.         )\\n3158.         actual = expected.resample(time="1D").interpolate("linear")\\n3159.         assert_allclose(actual, expected, rtol=1e-16)\\n3160. \\n3161.     @requires_dask\\n3162.     @requires_scipy\\n3163.     @pytest.mark.parametrize("chunked_time", [True, False])\\n3164.     def test_upsample_interpolate_dask(self, chunked_time):\\n3165.         from scipy.interpolate import interp1d\\n3166. \\n3167.         xs = np.arange(6)\\n3168.         ys = np.arange(3)\\n3169.         times = pd.date_range("2000-01-01", freq="6H", periods=5)\\n3170. \\n3171.         z = np.arange(5) ** 2\\n3172.         data = np.tile(z, (6, 3, 1))\\n3173.         array = DataArray(data, {"time": times, "x": xs, "y": ys}, ("x", "y", "time"))\\n3174.         chunks = {"x": 2, "y": 1}\\n3175.         if chunked_time:\\n3176.             chunks["time"] = 3\\n3177. \\n3178.         expected_times = times.to_series().resample("1H").asfreq().index\\n3179.         # Split the times into equal sub-intervals to simulate the 6 hour\\n3180.         # to 1 hour up-sampling\\n3181.         new_times_idx = np.linspace(0, len(times) - 1, len(times) * 5)\\n3182.         for kind in ["linear", "nearest", "zero", "slinear", "quadratic", "cubic"]:\\n3183.             actual = array.chunk(chunks).resample(time="1H").interpolate(kind)\\n3184.             actual = actual.compute()\\n3185.             f = interp1d(\\n3186.                 np.arange(len(times)),\\n3187.                 data,\\n3188.                 kind=kind,\\n3189.                 axis=-1,\\n3190.                 bounds_error=True,\\n3191.                 assume_sorted=True,\\n3192.             )\\n3193.             expected_data = f(new_times_idx)\\n3194.             expected = DataArray(\\n3195.                 expected_data,\\n3196.                 {"time": expected_times, "x": xs, "y": ys},\\n3197.                 ("x", "y", "time"),\\n3198.             )\\n3199.             # Use AllClose because there are some small differences in how\\n3200.             # we upsample timeseries versus the integer indexing as I\\\'ve\\n3201.             # done here due to floating point arithmetic\\n3202.             assert_allclose(expected, actual, rtol=1e-16)\\n3203. \\n3204.     def test_align(self):\\n3205.         array = DataArray(\\n3206.             np.random.random((6, 8)), coords={"x": list("abcdef")}, dims=["x", "y"]\\n3207.         )\\n3208.         array1, array2 = align(array, array[:5], join="inner")\\n3209.         assert_identical(array1, array[:5])\\n3210.         assert_identical(array2, array[:5])\\n3211. \\n3212.     def test_align_dtype(self):\\n3213.         # regression test for #264\\n3214.         x1 = np.arange(30)\\n3215.         x2 = np.arange(5, 35)\\n3216.         a = DataArray(np.random.random((30,)).astype(np.float32), [("x", x1)])\\n3217.         b = DataArray(np.random.random((30,)).astype(np.float32), [("x", x2)])\\n3218.         c, d = align(a, b, join="outer")\\n3219.         assert c.dtype == np.float32\\n3220. \\n3221.     def test_align_copy(self):\\n3222.         x = DataArray([1, 2, 3], coords=[("a", [1, 2, 3])])\\n3223.         y = DataArray([1, 2], coords=[("a", [3, 1])])\\n3224. \\n3225.         expected_x2 = x\\n3226.         expected_y2 = DataArray([2, np.nan, 1], coords=[("a", [1, 2, 3])])\\n3227. \\n3228.         x2, y2 = align(x, y, join="outer", copy=False)\\n3229.         assert_identical(expected_x2, x2)\\n3230.         assert_identical(expected_y2, y2)\\n3231.         assert source_ndarray(x2.data) is source_ndarray(x.data)\\n3232. \\n3233.         x2, y2 = align(x, y, join="outer", copy=True)\\n3234.         assert_identical(expected_x2, x2)\\n3235.         assert_identical(expected_y2, y2)\\n3236.         assert source_ndarray(x2.data) is not source_ndarray(x.data)\\n3237. \\n3238.         # Trivial align - 1 element\\n3239.         x = DataArray([1, 2, 3], coords=[("a", [1, 2, 3])])\\n3240.         (x2,) = align(x, copy=False)\\n3241.         assert_identical(x, x2)\\n3242.         assert source_ndarray(x2.data) is source_ndarray(x.data)\\n3243. \\n3244.         (x2,) = align(x, copy=True)\\n3245.         assert_identical(x, x2)\\n3246.         assert source_ndarray(x2.data) is not source_ndarray(x.data)\\n3247. \\n3248.     def test_align_override(self):\\n3249.         left = DataArray([1, 2, 3], dims="x", coords={"x": [0, 1, 2]})\\n3250.         right = DataArray(\\n3251.             np.arange(9).reshape((3, 3)),\\n3252.             dims=["x", "y"],\\n3253.             coords={"x": [0.1, 1.1, 2.1], "y": [1, 2, 3]},\\n3254.         )\\n3255. \\n3256.         expected_right = DataArray(\\n3257.             np.arange(9).reshape(3, 3),\\n3258.             dims=["x", "y"],\\n3259.             coords={"x": [0, 1, 2], "y": [1, 2, 3]},\\n3260.         )\\n3261. \\n3262.         new_left, new_right = align(left, right, join="override")\\n3263.         assert_identical(left, new_left)\\n3264.         assert_identical(new_right, expected_right)\\n3265. \\n3266.         new_left, new_right = align(left, right, exclude="x", join="override")\\n3267.         assert_identical(left, new_left)\\n3268.         assert_identical(right, new_right)\\n3269. \\n3270.         new_left, new_right = xr.align(\\n3271.             left.isel(x=0, drop=True), right, exclude="x", join="override"\\n3272.         )\\n3273.         assert_identical(left.isel(x=0, drop=True), new_left)\\n3274.         assert_identical(right, new_right)\\n3275. \\n3276.         with raises_regex(ValueError, "Indexes along dimension \\\'x\\\' don\\\'t have"):\\n3277.             align(left.isel(x=0).expand_dims("x"), right, join="override")\\n3278. \\n3279.     @pytest.mark.parametrize(\\n3280.         "darrays",\\n3281.         [\\n3282.             [\\n3283.                 DataArray(0),\\n3284.                 DataArray([1], [("x", [1])]),\\n3285.                 DataArray([2, 3], [("x", [2, 3])]),\\n3286.             ],\\n3287.             [\\n3288.                 DataArray([2, 3], [("x", [2, 3])]),\\n3289.                 DataArray([1], [("x", [1])]),\\n3290.                 DataArray(0),\\n3291.             ],\\n3292.         ],\\n3293.     )\\n3294.     def test_align_override_error(self, darrays):\\n3295.         with raises_regex(ValueError, "Indexes along dimension \\\'x\\\' don\\\'t have"):\\n3296.             xr.align(*darrays, join="override")\\n3297. \\n3298.     def test_align_exclude(self):\\n3299.         x = DataArray([[1, 2], [3, 4]], coords=[("a", [-1, -2]), ("b", [3, 4])])\\n3300.         y = DataArray([[1, 2], [3, 4]], coords=[("a", [-1, 20]), ("b", [5, 6])])\\n3301.         z = DataArray([1], dims=["a"], coords={"a": [20], "b": 7})\\n3302. \\n3303.         x2, y2, z2 = align(x, y, z, join="outer", exclude=["b"])\\n3304.         expected_x2 = DataArray(\\n3305.             [[3, 4], [1, 2], [np.nan, np.nan]],\\n3306.             coords=[("a", [-2, -1, 20]), ("b", [3, 4])],\\n3307.         )\\n3308.         expected_y2 = DataArray(\\n3309.             [[np.nan, np.nan], [1, 2], [3, 4]],\\n3310.             coords=[("a", [-2, -1, 20]), ("b", [5, 6])],\\n3311.         )\\n3312.         expected_z2 = DataArray(\\n3313.             [np.nan, np.nan, 1], dims=["a"], coords={"a": [-2, -1, 20], "b": 7}\\n3314.         )\\n3315.         assert_identical(expected_x2, x2)\\n3316.         assert_identical(expected_y2, y2)\\n3317.         assert_identical(expected_z2, z2)\\n3318. \\n3319.     def test_align_indexes(self):\\n3320.         x = DataArray([1, 2, 3], coords=[("a", [-1, 10, -2])])\\n3321.         y = DataArray([1, 2], coords=[("a", [-2, -1])])\\n3322. \\n3323.         x2, y2 = align(x, y, join="outer", indexes={"a": [10, -1, -2]})\\n3324.         expected_x2 = DataArray([2, 1, 3], coords=[("a", [10, -1, -2])])\\n3325.         expected_y2 = DataArray([np.nan, 2, 1], coords=[("a", [10, -1, -2])])\\n3326.         assert_identical(expected_x2, x2)\\n3327.         assert_identical(expected_y2, y2)\\n3328. \\n3329.         (x2,) = align(x, join="outer", indexes={"a": [-2, 7, 10, -1]})\\n3330.         expected_x2 = DataArray([3, np.nan, 2, 1], coords=[("a", [-2, 7, 10, -1])])\\n3331.         assert_identical(expected_x2, x2)\\n3332. \\n3333.     def test_align_without_indexes_exclude(self):\\n3334.         arrays = [DataArray([1, 2, 3], dims=["x"]), DataArray([1, 2], dims=["x"])]\\n3335.         result0, result1 = align(*arrays, exclude=["x"])\\n3336.         assert_identical(result0, arrays[0])\\n3337.         assert_identical(result1, arrays[1])\\n3338. \\n3339.     def test_align_mixed_indexes(self):\\n3340.         array_no_coord = DataArray([1, 2], dims=["x"])\\n3341.         array_with_coord = DataArray([1, 2], coords=[("x", ["a", "b"])])\\n3342.         result0, result1 = align(array_no_coord, array_with_coord)\\n3343.         assert_identical(result0, array_with_coord)\\n3344.         assert_identical(result1, array_with_coord)\\n3345. \\n3346.         result0, result1 = align(array_no_coord, array_with_coord, exclude=["x"])\\n3347.         assert_identical(result0, array_no_coord)\\n3348.         assert_identical(result1, array_with_coord)\\n3349. \\n3350.     def test_align_without_indexes_errors(self):\\n3351.         with raises_regex(ValueError, "cannot be aligned"):\\n3352.             align(DataArray([1, 2, 3], dims=["x"]), DataArray([1, 2], dims=["x"]))\\n3353. \\n3354.         with raises_regex(ValueError, "cannot be aligned"):\\n3355.             align(\\n3356.                 DataArray([1, 2, 3], dims=["x"]),\\n3357.                 DataArray([1, 2], coords=[("x", [0, 1])]),\\n3358.             )\\n3359. \\n3360.     def test_broadcast_arrays(self):\\n3361.         x = DataArray([1, 2], coords=[("a", [-1, -2])], name="x")\\n3362.         y = DataArray([1, 2], coords=[("b", [3, 4])], name="y")\\n3363.         x2, y2 = broadcast(x, y)\\n3364.         expected_coords = [("a", [-1, -2]), ("b", [3, 4])]\\n3365.         expected_x2 = DataArray([[1, 1], [2, 2]], expected_coords, name="x")\\n3366.         expected_y2 = DataArray([[1, 2], [1, 2]], expected_coords, name="y")\\n3367.         assert_identical(expected_x2, x2)\\n3368.         assert_identical(expected_y2, y2)\\n3369. \\n3370.         x = DataArray(np.random.randn(2, 3), dims=["a", "b"])\\n3371.         y = DataArray(np.random.randn(3, 2), dims=["b", "a"])\\n3372.         x2, y2 = broadcast(x, y)\\n3373.         expected_x2 = x\\n3374.         expected_y2 = y.T\\n3375.         assert_identical(expected_x2, x2)\\n3376.         assert_identical(expected_y2, y2)\\n3377. \\n3378.     def test_broadcast_arrays_misaligned(self):\\n3379.         # broadcast on misaligned coords must auto-align\\n3380.         x = DataArray([[1, 2], [3, 4]], coords=[("a", [-1, -2]), ("b", [3, 4])])\\n3381.         y = DataArray([1, 2], coords=[("a", [-1, 20])])\\n3382.         expected_x2 = DataArray(\\n3383.             [[3, 4], [1, 2], [np.nan, np.nan]],\\n3384.             coords=[("a", [-2, -1, 20]), ("b", [3, 4])],\\n3385.         )\\n3386.         expected_y2 = DataArray(\\n3387.             [[np.nan, np.nan], [1, 1], [2, 2]],\\n3388.             coords=[("a", [-2, -1, 20]), ("b", [3, 4])],\\n3389.         )\\n3390.         x2, y2 = broadcast(x, y)\\n3391.         assert_identical(expected_x2, x2)\\n3392.         assert_identical(expected_y2, y2)\\n3393. \\n3394.     def test_broadcast_arrays_nocopy(self):\\n3395.         # Test that input data is not copied over in case\\n3396.         # no alteration is needed\\n3397.         x = DataArray([1, 2], coords=[("a", [-1, -2])], name="x")\\n3398.         y = DataArray(3, name="y")\\n3399.         expected_x2 = DataArray([1, 2], coords=[("a", [-1, -2])], name="x")\\n3400.         expected_y2 = DataArray([3, 3], coords=[("a", [-1, -2])], name="y")\\n3401. \\n3402.         x2, y2 = broadcast(x, y)\\n3403.         assert_identical(expected_x2, x2)\\n3404.         assert_identical(expected_y2, y2)\\n3405.         assert source_ndarray(x2.data) is source_ndarray(x.data)\\n3406. \\n3407.         # single-element broadcast (trivial case)\\n3408.         (x2,) = broadcast(x)\\n3409.         assert_identical(x, x2)\\n3410.         assert source_ndarray(x2.data) is source_ndarray(x.data)\\n3411. \\n3412.     def test_broadcast_arrays_exclude(self):\\n3413.         x = DataArray([[1, 2], [3, 4]], coords=[("a", [-1, -2]), ("b", [3, 4])])\\n3414.         y = DataArray([1, 2], coords=[("a", [-1, 20])])\\n3415.         z = DataArray(5, coords={"b": 5})\\n3416. \\n3417.         x2, y2, z2 = broadcast(x, y, z, exclude=["b"])\\n3418.         expected_x2 = DataArray(\\n3419.             [[3, 4], [1, 2], [np.nan, np.nan]],\\n3420.             coords=[("a", [-2, -1, 20]), ("b", [3, 4])],\\n3421.         )\\n3422.         expected_y2 = DataArray([np.nan, 1, 2], coords=[("a", [-2, -1, 20])])\\n3423.         expected_z2 = DataArray(\\n3424.             [5, 5, 5], dims=["a"], coords={"a": [-2, -1, 20], "b": 5}\\n3425.         )\\n3426.         assert_identical(expected_x2, x2)\\n3427.         assert_identical(expected_y2, y2)\\n3428.         assert_identical(expected_z2, z2)\\n3429. \\n3430.     def test_broadcast_coordinates(self):\\n3431.         # regression test for GH649\\n3432.         ds = Dataset({"a": (["x", "y"], np.ones((5, 6)))})\\n3433.         x_bc, y_bc, a_bc = broadcast(ds.x, ds.y, ds.a)\\n3434.         assert_identical(ds.a, a_bc)\\n3435. \\n3436.         X, Y = np.meshgrid(np.arange(5), np.arange(6), indexing="ij")\\n3437.         exp_x = DataArray(X, dims=["x", "y"], name="x")\\n3438.         exp_y = DataArray(Y, dims=["x", "y"], name="y")\\n3439.         assert_identical(exp_x, x_bc)\\n3440.         assert_identical(exp_y, y_bc)\\n3441. \\n3442.     def test_to_pandas(self):\\n3443.         # 0d\\n3444.         actual = DataArray(42).to_pandas()\\n3445.         expected = np.array(42)\\n3446.         assert_array_equal(actual, expected)\\n3447. \\n3448.         # 1d\\n3449.         values = np.random.randn(3)\\n3450.         index = pd.Index(["a", "b", "c"], name="x")\\n3451.         da = DataArray(values, coords=[index])\\n3452.         actual = da.to_pandas()\\n3453.         assert_array_equal(actual.values, values)\\n3454.         assert_array_equal(actual.index, index)\\n3455.         assert_array_equal(actual.index.name, "x")\\n3456. \\n3457.         # 2d\\n3458.         values = np.random.randn(3, 2)\\n3459.         da = DataArray(\\n3460.             values, coords=[("x", ["a", "b", "c"]), ("y", [0, 1])], name="foo"\\n3461.         )\\n3462.         actual = da.to_pandas()\\n3463.         assert_array_equal(actual.values, values)\\n3464.         assert_array_equal(actual.index, ["a", "b", "c"])\\n3465.         assert_array_equal(actual.columns, [0, 1])\\n3466. \\n3467.         # roundtrips\\n3468.         for shape in [(3,), (3, 4)]:\\n3469.             dims = list("abc")[: len(shape)]\\n3470.             da = DataArray(np.random.randn(*shape), dims=dims)\\n3471.             roundtripped = DataArray(da.to_pandas()).drop_vars(dims)\\n3472.             assert_identical(da, roundtripped)\\n3473. \\n3474.         with raises_regex(ValueError, "cannot convert"):\\n3475.             DataArray(np.random.randn(1, 2, 3, 4, 5)).to_pandas()\\n3476. \\n3477.     def test_to_dataframe(self):\\n3478.         # regression test for #260\\n3479.         arr_np = np.random.randn(3, 4)\\n3480. \\n3481.         arr = DataArray(arr_np, [("B", [1, 2, 3]), ("A", list("cdef"))], name="foo")\\n3482.         expected = arr.to_series()\\n3483.         actual = arr.to_dataframe()["foo"]\\n3484.         assert_array_equal(expected.values, actual.values)\\n3485.         assert_array_equal(expected.name, actual.name)\\n3486.         assert_array_equal(expected.index.values, actual.index.values)\\n3487. \\n3488.         actual = arr.to_dataframe(dim_order=["A", "B"])["foo"]\\n3489.         assert_array_equal(arr_np.transpose().reshape(-1), actual.values)\\n3490. \\n3491.         # regression test for coords with different dimensions\\n3492.         arr.coords["C"] = ("B", [-1, -2, -3])\\n3493.         expected = arr.to_series().to_frame()\\n3494.         expected["C"] = [-1] * 4 + [-2] * 4 + [-3] * 4\\n3495.         expected = expected[["C", "foo"]]\\n3496.         actual = arr.to_dataframe()\\n3497.         assert_array_equal(expected.values, actual.values)\\n3498.         assert_array_equal(expected.columns.values, actual.columns.values)\\n3499.         assert_array_equal(expected.index.values, actual.index.values)\\n3500. \\n3501.         with pytest.raises(ValueError, match="does not match the set of dimensions"):\\n3502.             arr.to_dataframe(dim_order=["B", "A", "C"])\\n3503. \\n3504.         arr.name = None  # unnamed\\n3505.         with raises_regex(ValueError, "unnamed"):\\n3506.             arr.to_dataframe()\\n3507. \\n3508.     def test_to_pandas_name_matches_coordinate(self):\\n3509.         # coordinate with same name as array\\n3510.         arr = DataArray([1, 2, 3], dims="x", name="x")\\n3511.         series = arr.to_series()\\n3512.         assert_array_equal([1, 2, 3], series.values)\\n3513.         assert_array_equal([0, 1, 2], series.index.values)\\n3514.         assert "x" == series.name\\n3515.         assert "x" == series.index.name\\n3516. \\n3517.         frame = arr.to_dataframe()\\n3518.         expected = series.to_frame()\\n3519.         assert expected.equals(frame)\\n3520. \\n3521.     def test_to_and_from_series(self):\\n3522.         expected = self.dv.to_dataframe()["foo"]\\n3523.         actual = self.dv.to_series()\\n3524.         assert_array_equal(expected.values, actual.values)\\n3525.         assert_array_equal(expected.index.values, actual.index.values)\\n3526.         assert "foo" == actual.name\\n3527.         # test roundtrip\\n3528.         assert_identical(self.dv, DataArray.from_series(actual).drop_vars(["x", "y"]))\\n3529.         # test name is None\\n3530.         actual.name = None\\n3531.         expected_da = self.dv.rename(None)\\n3532.         assert_identical(\\n3533.             expected_da, DataArray.from_series(actual).drop_vars(["x", "y"])\\n3534.         )\\n3535. \\n3536.     def test_from_series_multiindex(self):\\n3537.         # GH:3951\\n3538.         df = pd.DataFrame({"B": [1, 2, 3], "A": [4, 5, 6]})\\n3539.         df = df.rename_axis("num").rename_axis("alpha", axis=1)\\n3540.         actual = df.stack("alpha").to_xarray()\\n3541.         assert (actual.sel(alpha="B") == [1, 2, 3]).all()\\n3542.         assert (actual.sel(alpha="A") == [4, 5, 6]).all()\\n3543. \\n3544.     @requires_sparse\\n3545.     def test_from_series_sparse(self):\\n3546.         import sparse\\n3547. \\n3548.         series = pd.Series([1, 2], index=[("a", 1), ("b", 2)])\\n3549. \\n3550.         actual_sparse = DataArray.from_series(series, sparse=True)\\n3551.         actual_dense = DataArray.from_series(series, sparse=False)\\n3552. \\n3553.         assert isinstance(actual_sparse.data, sparse.COO)\\n3554.         actual_sparse.data = actual_sparse.data.todense()\\n3555.         assert_identical(actual_sparse, actual_dense)\\n3556. \\n3557.     @requires_sparse\\n3558.     def test_from_multiindex_series_sparse(self):\\n3559.         # regression test for GH4019\\n3560.         import sparse\\n3561. \\n3562.         idx = pd.MultiIndex.from_product([np.arange(3), np.arange(5)], names=["a", "b"])\\n3563.         series = pd.Series(np.random.RandomState(0).random(len(idx)), index=idx).sample(\\n3564.             n=5, random_state=3\\n3565.         )\\n3566. \\n3567.         dense = DataArray.from_series(series, sparse=False)\\n3568.         expected_coords = sparse.COO.from_numpy(dense.data, np.nan).coords\\n3569. \\n3570.         actual_sparse = xr.DataArray.from_series(series, sparse=True)\\n3571.         actual_coords = actual_sparse.data.coords\\n3572. \\n3573.         np.testing.assert_equal(actual_coords, expected_coords)\\n3574. \\n3575.     def test_to_and_from_empty_series(self):\\n3576.         # GH697\\n3577.         expected = pd.Series([], dtype=np.float64)\\n3578.         da = DataArray.from_series(expected)\\n3579.         assert len(da) == 0\\n3580.         actual = da.to_series()\\n3581.         assert len(actual) == 0\\n3582.         assert expected.equals(actual)\\n3583. \\n3584.     def test_series_categorical_index(self):\\n3585.         # regression test for GH700\\n3586.         if not hasattr(pd, "CategoricalIndex"):\\n3587.             pytest.skip("requires pandas with CategoricalIndex")\\n3588. \\n3589.         s = pd.Series(np.arange(5), index=pd.CategoricalIndex(list("aabbc")))\\n3590.         arr = DataArray(s)\\n3591.         assert "\\\'a\\\'" in repr(arr)  # should not error\\n3592. \\n3593.     def test_to_and_from_dict(self):\\n3594.         array = DataArray(\\n3595.             np.random.randn(2, 3), {"x": ["a", "b"]}, ["x", "y"], name="foo"\\n3596.         )\\n3597.         expected = {\\n3598.             "name": "foo",\\n3599.             "dims": ("x", "y"),\\n3600.             "data": array.values.tolist(),\\n3601.             "attrs": {},\\n3602.             "coords": {"x": {"dims": ("x",), "data": ["a", "b"], "attrs": {}}},\\n3603.         }\\n3604.         actual = array.to_dict()\\n3605. \\n3606.         # check that they are identical\\n3607.         assert expected == actual\\n3608. \\n3609.         # check roundtrip\\n3610.         assert_identical(array, DataArray.from_dict(actual))\\n3611. \\n3612.         # a more bare bones representation still roundtrips\\n3613.         d = {\\n3614.             "name": "foo",\\n3615.             "dims": ("x", "y"),\\n3616.             "data": array.values.tolist(),\\n3617.             "coords": {"x": {"dims": "x", "data": ["a", "b"]}},\\n3618.         }\\n3619.         assert_identical(array, DataArray.from_dict(d))\\n3620. \\n3621.         # and the most bare bones representation still roundtrips\\n3622.         d = {"name": "foo", "dims": ("x", "y"), "data": array.values}\\n3623.         assert_identical(array.drop_vars("x"), DataArray.from_dict(d))\\n3624. \\n3625.         # missing a dims in the coords\\n3626.         d = {\\n3627.             "dims": ("x", "y"),\\n3628.             "data": array.values,\\n3629.             "coords": {"x": {"data": ["a", "b"]}},\\n3630.         }\\n3631.         with raises_regex(\\n3632.             ValueError, "cannot convert dict when coords are missing the key \\\'dims\\\'"\\n3633.         ):\\n3634.             DataArray.from_dict(d)\\n3635. \\n3636.         # this one is missing some necessary information\\n3637.         d = {"dims": ("t")}\\n3638.         with raises_regex(ValueError, "cannot convert dict without the key \\\'data\\\'"):\\n3639.             DataArray.from_dict(d)\\n3640. \\n3641.         # check the data=False option\\n3642.         expected_no_data = expected.copy()\\n3643.         del expected_no_data["data"]\\n3644.         del expected_no_data["coords"]["x"]["data"]\\n3645.         endiantype = "<U1" if sys.byteorder == "little" else ">U1"\\n3646.         expected_no_data["coords"]["x"].update({"dtype": endiantype, "shape": (2,)})\\n3647.         expected_no_data.update({"dtype": "float64", "shape": (2, 3)})\\n3648.         actual_no_data = array.to_dict(data=False)\\n3649.         assert expected_no_data == actual_no_data\\n3650. \\n3651.     def test_to_and_from_dict_with_time_dim(self):\\n3652.         x = np.random.randn(10, 3)\\n3653.         t = pd.date_range("20130101", periods=10)\\n3654.         lat = [77.7, 83.2, 76]\\n3655.         da = DataArray(x, {"t": t, "lat": lat}, dims=["t", "lat"])\\n3656.         roundtripped = DataArray.from_dict(da.to_dict())\\n3657.         assert_identical(da, roundtripped)\\n3658. \\n3659.     def test_to_and_from_dict_with_nan_nat(self):\\n3660.         y = np.random.randn(10, 3)\\n3661.         y[2] = np.nan\\n3662.         t = pd.Series(pd.date_range("20130101", periods=10))\\n3663.         t[2] = np.nan\\n3664.         lat = [77.7, 83.2, 76]\\n3665.         da = DataArray(y, {"t": t, "lat": lat}, dims=["t", "lat"])\\n3666.         roundtripped = DataArray.from_dict(da.to_dict())\\n3667.         assert_identical(da, roundtripped)\\n3668. \\n3669.     def test_to_dict_with_numpy_attrs(self):\\n3670.         # this doesn\\\'t need to roundtrip\\n3671.         x = np.random.randn(10, 3)\\n3672.         t = list("abcdefghij")\\n3673.         lat = [77.7, 83.2, 76]\\n3674.         attrs = {\\n3675.             "created": np.float64(1998),\\n3676.             "coords": np.array([37, -110.1, 100]),\\n3677.             "maintainer": "bar",\\n3678.         }\\n3679.         da = DataArray(x, {"t": t, "lat": lat}, dims=["t", "lat"], attrs=attrs)\\n3680.         expected_attrs = {\\n3681.             "created": attrs["created"].item(),\\n3682.             "coords": attrs["coords"].tolist(),\\n3683.             "maintainer": "bar",\\n3684.         }\\n3685.         actual = da.to_dict()\\n3686. \\n3687.         # check that they are identical\\n3688.         assert expected_attrs == actual["attrs"]\\n3689. \\n3690.     def test_to_masked_array(self):\\n3691.         rs = np.random.RandomState(44)\\n3692.         x = rs.random_sample(size=(10, 20))\\n3693.         x_masked = np.ma.masked_where(x < 0.5, x)\\n3694.         da = DataArray(x_masked)\\n3695. \\n3696.         # Test round trip\\n3697.         x_masked_2 = da.to_masked_array()\\n3698.         da_2 = DataArray(x_masked_2)\\n3699.         assert_array_equal(x_masked, x_masked_2)\\n3700.         assert_equal(da, da_2)\\n3701. \\n3702.         da_masked_array = da.to_masked_array(copy=True)\\n3703.         assert isinstance(da_masked_array, np.ma.MaskedArray)\\n3704.         # Test masks\\n3705.         assert_array_equal(da_masked_array.mask, x_masked.mask)\\n3706.         # Test that mask is unpacked correctly\\n3707.         assert_array_equal(da.values, x_masked.filled(np.nan))\\n3708.         # Test that the underlying data (including nans) hasn\\\'t changed\\n3709.         assert_array_equal(da_masked_array, x_masked.filled(np.nan))\\n3710. \\n3711.         # Test that copy=False gives access to values\\n3712.         masked_array = da.to_masked_array(copy=False)\\n3713.         masked_array[0, 0] = 10.0\\n3714.         assert masked_array[0, 0] == 10.0\\n3715.         assert da[0, 0].values == 10.0\\n3716.         assert masked_array.base is da.values\\n3717.         assert isinstance(masked_array, np.ma.MaskedArray)\\n3718. \\n3719.         # Test with some odd arrays\\n3720.         for v in [4, np.nan, True, "4", "four"]:\\n3721.             da = DataArray(v)\\n3722.             ma = da.to_masked_array()\\n3723.             assert isinstance(ma, np.ma.MaskedArray)\\n3724. \\n3725.         # Fix GH issue 684 - masked arrays mask should be an array not a scalar\\n3726.         N = 4\\n3727.         v = range(N)\\n3728.         da = DataArray(v)\\n3729.         ma = da.to_masked_array()\\n3730.         assert len(ma.mask) == N\\n3731. \\n3732.     def test_to_and_from_cdms2_classic(self):\\n3733.         """Classic with 1D axes"""\\n3734.         pytest.importorskip("cdms2")\\n3735. \\n3736.         original = DataArray(\\n3737.             np.arange(6).reshape(2, 3),\\n3738.             [\\n3739.                 ("distance", [-2, 2], {"units": "meters"}),\\n3740.                 ("time", pd.date_range("2000-01-01", periods=3)),\\n3741.             ],\\n3742.             name="foo",\\n3743.             attrs={"baz": 123},\\n3744.         )\\n3745.         expected_coords = [\\n3746.             IndexVariable("distance", [-2, 2]),\\n3747.             IndexVariable("time", [0, 1, 2]),\\n3748.         ]\\n3749.         actual = original.to_cdms2()\\n3750.         assert_array_equal(actual.asma(), original)\\n3751.         assert actual.id == original.name\\n3752.         assert tuple(actual.getAxisIds()) == original.dims\\n3753.         for axis, coord in zip(actual.getAxisList(), expected_coords):\\n3754.             assert axis.id == coord.name\\n3755.             assert_array_equal(axis, coord.values)\\n3756.         assert actual.baz == original.attrs["baz"]\\n3757. \\n3758.         component_times = actual.getAxis(1).asComponentTime()\\n3759.         assert len(component_times) == 3\\n3760.         assert str(component_times[0]) == "2000-1-1 0:0:0.0"\\n3761. \\n3762.         roundtripped = DataArray.from_cdms2(actual)\\n3763.         assert_identical(original, roundtripped)\\n3764. \\n3765.         back = from_cdms2(actual)\\n3766.         assert original.dims == back.dims\\n3767.         assert original.coords.keys() == back.coords.keys()\\n3768.         for coord_name in original.coords.keys():\\n3769.             assert_array_equal(original.coords[coord_name], back.coords[coord_name])\\n3770. \\n3771.     def test_to_and_from_cdms2_sgrid(self):\\n3772.         """Curvilinear (structured) grid\\n3773. \\n3774.         The rectangular grid case is covered by the classic case\\n3775.         """\\n3776.         pytest.importorskip("cdms2")\\n3777. \\n3778.         lonlat = np.mgrid[:3, :4]\\n3779.         lon = DataArray(lonlat[1], dims=["y", "x"], name="lon")\\n3780.         lat = DataArray(lonlat[0], dims=["y", "x"], name="lat")\\n3781.         x = DataArray(np.arange(lon.shape[1]), dims=["x"], name="x")\\n3782.         y = DataArray(np.arange(lon.shape[0]), dims=["y"], name="y")\\n3783.         original = DataArray(\\n3784.             lonlat.sum(axis=0),\\n3785.             dims=["y", "x"],\\n3786.             coords=dict(x=x, y=y, lon=lon, lat=lat),\\n3787.             name="sst",\\n3788.         )\\n3789.         actual = original.to_cdms2()\\n3790.         assert tuple(actual.getAxisIds()) == original.dims\\n3791.         assert_array_equal(original.coords["lon"], actual.getLongitude().asma())\\n3792.         assert_array_equal(original.coords["lat"], actual.getLatitude().asma())\\n3793. \\n3794.         back = from_cdms2(actual)\\n3795.         assert original.dims == back.dims\\n3796.         assert set(original.coords.keys()) == set(back.coords.keys())\\n3797.         assert_array_equal(original.coords["lat"], back.coords["lat"])\\n3798.         assert_array_equal(original.coords["lon"], back.coords["lon"])\\n3799. \\n3800.     def test_to_and_from_cdms2_ugrid(self):\\n3801.         """Unstructured grid"""\\n3802.         pytest.importorskip("cdms2")\\n3803. \\n3804.         lon = DataArray(np.random.uniform(size=5), dims=["cell"], name="lon")\\n3805.         lat = DataArray(np.random.uniform(size=5), dims=["cell"], name="lat")\\n3806.         cell = DataArray(np.arange(5), dims=["cell"], name="cell")\\n3807.         original = DataArray(\\n3808.             np.arange(5), dims=["cell"], coords={"lon": lon, "lat": lat, "cell": cell}\\n3809.         )\\n3810.         actual = original.to_cdms2()\\n3811.         assert tuple(actual.getAxisIds()) == original.dims\\n3812.         assert_array_equal(original.coords["lon"], actual.getLongitude().getValue())\\n3813.         assert_array_equal(original.coords["lat"], actual.getLatitude().getValue())\\n3814. \\n3815.         back = from_cdms2(actual)\\n3816.         assert set(original.dims) == set(back.dims)\\n3817.         assert set(original.coords.keys()) == set(back.coords.keys())\\n3818.         assert_array_equal(original.coords["lat"], back.coords["lat"])\\n3819.         assert_array_equal(original.coords["lon"], back.coords["lon"])\\n3820. \\n3821.     def test_to_dataset_whole(self):\\n3822.         unnamed = DataArray([1, 2], dims="x")\\n3823.         with raises_regex(ValueError, "unable to convert unnamed"):\\n3824.             unnamed.to_dataset()\\n3825. \\n3826.         actual = unnamed.to_dataset(name="foo")\\n3827.         expected = Dataset({"foo": ("x", [1, 2])})\\n3828.         assert_identical(expected, actual)\\n3829. \\n3830.         named = DataArray([1, 2], dims="x", name="foo", attrs={"y": "testattr"})\\n3831.         actual = named.to_dataset()\\n3832.         expected = Dataset({"foo": ("x", [1, 2], {"y": "testattr"})})\\n3833.         assert_identical(expected, actual)\\n3834. \\n3835.         # Test promoting attrs\\n3836.         actual = named.to_dataset(promote_attrs=True)\\n3837.         expected = Dataset(\\n3838.             {"foo": ("x", [1, 2], {"y": "testattr"})}, attrs={"y": "testattr"}\\n3839.         )\\n3840.         assert_identical(expected, actual)\\n3841. \\n3842.         with pytest.raises(TypeError):\\n3843.             actual = named.to_dataset("bar")\\n3844. \\n3845.     def test_to_dataset_split(self):\\n3846.         array = DataArray([1, 2, 3], coords=[("x", list("abc"))], attrs={"a": 1})\\n3847.         expected = Dataset({"a": 1, "b": 2, "c": 3}, attrs={"a": 1})\\n3848.         actual = array.to_dataset("x")\\n3849.         assert_identical(expected, actual)\\n3850. \\n3851.         with pytest.raises(TypeError):\\n3852.             array.to_dataset("x", name="foo")\\n3853. \\n3854.         roundtripped = actual.to_array(dim="x")\\n3855.         assert_identical(array, roundtripped)\\n3856. \\n3857.         array = DataArray([1, 2, 3], dims="x")\\n3858.         expected = Dataset({0: 1, 1: 2, 2: 3})\\n3859.         actual = array.to_dataset("x")\\n3860.         assert_identical(expected, actual)\\n3861. \\n3862.     def test_to_dataset_retains_keys(self):\\n3863. \\n3864.         # use dates as convenient non-str objects. Not a specific date test\\n3865.         import datetime\\n3866. \\n3867.         dates = [datetime.date(2000, 1, d) for d in range(1, 4)]\\n3868. \\n3869.         array = DataArray([1, 2, 3], coords=[("x", dates)], attrs={"a": 1})\\n3870. \\n3871.         # convert to dateset and back again\\n3872.         result = array.to_dataset("x").to_array(dim="x")\\n3873. \\n3874.         assert_equal(array, result)\\n3875. \\n3876.     def test__title_for_slice(self):\\n3877.         array = DataArray(\\n3878.             np.ones((4, 3, 2)),\\n3879.             dims=["a", "b", "c"],\\n3880.             coords={"a": range(4), "b": range(3), "c": range(2)},\\n3881.         )\\n3882.         assert "" == array._title_for_slice()\\n3883.         assert "c = 0" == array.isel(c=0)._title_for_slice()\\n3884.         title = array.isel(b=1, c=0)._title_for_slice()\\n3885.         assert "b = 1, c = 0" == title or "c = 0, b = 1" == title\\n3886. \\n3887.         a2 = DataArray(np.ones((4, 1)), dims=["a", "b"])\\n3888.         assert "" == a2._title_for_slice()\\n3889. \\n3890.     def test__title_for_slice_truncate(self):\\n3891.         array = DataArray(np.ones(4))\\n3892.         array.coords["a"] = "a" * 100\\n3893.         array.coords["b"] = "b" * 100\\n3894. \\n3895.         nchar = 80\\n3896.         title = array._title_for_slice(truncate=nchar)\\n3897. \\n3898.         assert nchar == len(title)\\n3899.         assert title.endswith("...")\\n3900. \\n3901.     def test_dataarray_diff_n1(self):\\n3902.         da = DataArray(np.random.randn(3, 4), dims=["x", "y"])\\n3903.         actual = da.diff("y")\\n3904.         expected = DataArray(np.diff(da.values, axis=1), dims=["x", "y"])\\n3905.         assert_equal(expected, actual)\\n3906. \\n3907.     def test_coordinate_diff(self):\\n3908.         # regression test for GH634\\n3909.         arr = DataArray(range(0, 20, 2), dims=["lon"], coords=[range(10)])\\n3910.         lon = arr.coords["lon"]\\n3911.         expected = DataArray([1] * 9, dims=["lon"], coords=[range(1, 10)], name="lon")\\n3912.         actual = lon.diff("lon")\\n3913.         assert_equal(expected, actual)\\n3914. \\n3915.     @pytest.mark.parametrize("offset", [-5, 0, 1, 2])\\n3916.     @pytest.mark.parametrize("fill_value, dtype", [(2, int), (dtypes.NA, float)])\\n3917.     def test_shift(self, offset, fill_value, dtype):\\n3918.         arr = DataArray([1, 2, 3], dims="x")\\n3919.         actual = arr.shift(x=1, fill_value=fill_value)\\n3920.         if fill_value == dtypes.NA:\\n3921.             # if we supply the default, we expect the missing value for a\\n3922.             # float array\\n3923.             fill_value = np.nan\\n3924.         expected = DataArray([fill_value, 1, 2], dims="x")\\n3925.         assert_identical(expected, actual)\\n3926.         assert actual.dtype == dtype\\n3927. \\n3928.         arr = DataArray([1, 2, 3], [("x", ["a", "b", "c"])])\\n3929.         expected = DataArray(arr.to_pandas().shift(offset))\\n3930.         actual = arr.shift(x=offset)\\n3931.         assert_identical(expected, actual)\\n3932. \\n3933.     def test_roll_coords(self):\\n3934.         arr = DataArray([1, 2, 3], coords={"x": range(3)}, dims="x")\\n3935.         actual = arr.roll(x=1, roll_coords=True)\\n3936.         expected = DataArray([3, 1, 2], coords=[("x", [2, 0, 1])])\\n3937.         assert_identical(expected, actual)\\n3938. \\n3939.     def test_roll_no_coords(self):\\n3940.         arr = DataArray([1, 2, 3], coords={"x": range(3)}, dims="x")\\n3941.         actual = arr.roll(x=1, roll_coords=False)\\n3942.         expected = DataArray([3, 1, 2], coords=[("x", [0, 1, 2])])\\n3943.         assert_identical(expected, actual)\\n3944. \\n3945.     def test_roll_coords_none(self):\\n3946.         arr = DataArray([1, 2, 3], coords={"x": range(3)}, dims="x")\\n3947. \\n3948.         with pytest.warns(FutureWarning):\\n3949.             actual = arr.roll(x=1, roll_coords=None)\\n3950. \\n3951.         expected = DataArray([3, 1, 2], coords=[("x", [2, 0, 1])])\\n3952.         assert_identical(expected, actual)\\n3953. \\n3954.     def test_copy_with_data(self):\\n3955.         orig = DataArray(\\n3956.             np.random.random(size=(2, 2)),\\n3957.             dims=("x", "y"),\\n3958.             attrs={"attr1": "value1"},\\n3959.             coords={"x": [4, 3]},\\n3960.             name="helloworld",\\n3961.         )\\n3962.         new_data = np.arange(4).reshape(2, 2)\\n3963.         actual = orig.copy(data=new_data)\\n3964.         expected = orig.copy()\\n3965.         expected.data = new_data\\n3966.         assert_identical(expected, actual)\\n3967. \\n3968.     @pytest.mark.xfail(raises=AssertionError)\\n3969.     @pytest.mark.parametrize(\\n3970.         "deep, expected_orig",\\n3971.         [\\n3972.             [\\n3973.                 True,\\n3974.                 xr.DataArray(\\n3975.                     xr.IndexVariable("a", np.array([1, 2])),\\n3976.                     coords={"a": [1, 2]},\\n3977.                     dims=["a"],\\n3978.                 ),\\n3979.             ],\\n3980.             [\\n3981.                 False,\\n3982.                 xr.DataArray(\\n3983.                     xr.IndexVariable("a", np.array([999, 2])),\\n3984.                     coords={"a": [999, 2]},\\n3985.                     dims=["a"],\\n3986.                 ),\\n3987.             ],\\n3988.         ],\\n3989.     )\\n3990.     def test_copy_coords(self, deep, expected_orig):\\n3991.         """The test fails for the shallow copy, and apparently only on Windows\\n3992.         for some reason. In windows coords seem to be immutable unless it\\\'s one\\n3993.         dataarray deep copied from another."""\\n3994.         da = xr.DataArray(\\n3995.             np.ones([2, 2, 2]),\\n3996.             coords={"a": [1, 2], "b": ["x", "y"], "c": [0, 1]},\\n3997.             dims=["a", "b", "c"],\\n3998.         )\\n3999.         da_cp = da.copy(deep)\\n4000.         da_cp["a"].data[0] = 999\\n4001. \\n4002.         expected_cp = xr.DataArray(\\n4003.             xr.IndexVariable("a", np.array([999, 2])),\\n4004.             coords={"a": [999, 2]},\\n4005.             dims=["a"],\\n4006.         )\\n4007.         assert_identical(da_cp["a"], expected_cp)\\n4008. \\n4009.         assert_identical(da["a"], expected_orig)\\n4010. \\n4011.     def test_real_and_imag(self):\\n4012.         array = DataArray(1 + 2j)\\n4013.         assert_identical(array.real, DataArray(1))\\n4014.         assert_identical(array.imag, DataArray(2))\\n4015. \\n4016.     def test_setattr_raises(self):\\n4017.         array = DataArray(0, coords={"scalar": 1}, attrs={"foo": "bar"})\\n4018.         with raises_regex(AttributeError, "cannot set attr"):\\n4019.             array.scalar = 2\\n4020.         with raises_regex(AttributeError, "cannot set attr"):\\n4021.             array.foo = 2\\n4022.         with raises_regex(AttributeError, "cannot set attr"):\\n4023.             array.other = 2\\n4024. \\n4025.     def test_full_like(self):\\n4026.         # For more thorough tests, see test_variable.py\\n4027.         da = DataArray(\\n4028.             np.random.random(size=(2, 2)),\\n4029.             dims=("x", "y"),\\n4030.             attrs={"attr1": "value1"},\\n4031.             coords={"x": [4, 3]},\\n4032.             name="helloworld",\\n4033.         )\\n4034. \\n4035.         actual = full_like(da, 2)\\n4036.         expect = da.copy(deep=True)\\n4037.         expect.values = [[2.0, 2.0], [2.0, 2.0]]\\n4038.         assert_identical(expect, actual)\\n4039. \\n4040.         # override dtype\\n4041.         actual = full_like(da, fill_value=True, dtype=bool)\\n4042.         expect.values = [[True, True], [True, True]]\\n4043.         assert expect.dtype == bool\\n4044.         assert_identical(expect, actual)\\n4045. \\n4046.     def test_dot(self):\\n4047.         x = np.linspace(-3, 3, 6)\\n4048.         y = np.linspace(-3, 3, 5)\\n4049.         z = range(4)\\n4050.         da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\\n4051.         da = DataArray(da_vals, coords=[x, y, z], dims=["x", "y", "z"])\\n4052. \\n4053.         dm_vals = range(4)\\n4054.         dm = DataArray(dm_vals, coords=[z], dims=["z"])\\n4055. \\n4056.         # nd dot 1d\\n4057.         actual = da.dot(dm)\\n4058.         expected_vals = np.tensordot(da_vals, dm_vals, [2, 0])\\n4059.         expected = DataArray(expected_vals, coords=[x, y], dims=["x", "y"])\\n4060.         assert_equal(expected, actual)\\n4061. \\n4062.         # all shared dims\\n4063.         actual = da.dot(da)\\n4064.         expected_vals = np.tensordot(da_vals, da_vals, axes=([0, 1, 2], [0, 1, 2]))\\n4065.         expected = DataArray(expected_vals)\\n4066.         assert_equal(expected, actual)\\n4067. \\n4068.         # multiple shared dims\\n4069.         dm_vals = np.arange(20 * 5 * 4).reshape((20, 5, 4))\\n4070.         j = np.linspace(-3, 3, 20)\\n4071.         dm = DataArray(dm_vals, coords=[j, y, z], dims=["j", "y", "z"])\\n4072.         actual = da.dot(dm)\\n4073.         expected_vals = np.tensordot(da_vals, dm_vals, axes=([1, 2], [1, 2]))\\n4074.         expected = DataArray(expected_vals, coords=[x, j], dims=["x", "j"])\\n4075.         assert_equal(expected, actual)\\n4076. \\n4077.         # Ellipsis: all dims are shared\\n4078.         actual = da.dot(da, dims=...)\\n4079.         expected = da.dot(da)\\n4080.         assert_equal(expected, actual)\\n4081. \\n4082.         # Ellipsis: not all dims are shared\\n4083.         actual = da.dot(dm, dims=...)\\n4084.         expected = da.dot(dm, dims=("j", "x", "y", "z"))\\n4085.         assert_equal(expected, actual)\\n4086. \\n4087.         with pytest.raises(NotImplementedError):\\n4088.             da.dot(dm.to_dataset(name="dm"))\\n4089.         with pytest.raises(TypeError):\\n4090.             da.dot(dm.values)\\n4091. \\n4092.     def test_dot_align_coords(self):\\n4093.         # GH 3694\\n4094. \\n4095.         x = np.linspace(-3, 3, 6)\\n4096.         y = np.linspace(-3, 3, 5)\\n4097.         z_a = range(4)\\n4098.         da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\\n4099.         da = DataArray(da_vals, coords=[x, y, z_a], dims=["x", "y", "z"])\\n4100. \\n4101.         z_m = range(2, 6)\\n4102.         dm_vals = range(4)\\n4103.         dm = DataArray(dm_vals, coords=[z_m], dims=["z"])\\n4104. \\n4105.         with xr.set_options(arithmetic_join="exact"):\\n4106.             with raises_regex(ValueError, "indexes along dimension"):\\n4107.                 da.dot(dm)\\n4108. \\n4109.         da_aligned, dm_aligned = xr.align(da, dm, join="inner")\\n4110. \\n4111.         # nd dot 1d\\n4112.         actual = da.dot(dm)\\n4113.         expected_vals = np.tensordot(da_aligned.values, dm_aligned.values, [2, 0])\\n4114.         expected = DataArray(expected_vals, coords=[x, da_aligned.y], dims=["x", "y"])\\n4115.         assert_equal(expected, actual)\\n4116. \\n4117.         # multiple shared dims\\n4118.         dm_vals = np.arange(20 * 5 * 4).reshape((20, 5, 4))\\n4119.         j = np.linspace(-3, 3, 20)\\n4120.         dm = DataArray(dm_vals, coords=[j, y, z_m], dims=["j", "y", "z"])\\n4121.         da_aligned, dm_aligned = xr.align(da, dm, join="inner")\\n4122.         actual = da.dot(dm)\\n4123.         expected_vals = np.tensordot(\\n4124.             da_aligned.values, dm_aligned.values, axes=([1, 2], [1, 2])\\n4125.         )\\n4126.         expected = DataArray(expected_vals, coords=[x, j], dims=["x", "j"])\\n4127.         assert_equal(expected, actual)\\n4128. \\n4129.     def test_matmul(self):\\n4130. \\n4131.         # copied from above (could make a fixture)\\n4132.         x = np.linspace(-3, 3, 6)\\n4133.         y = np.linspace(-3, 3, 5)\\n4134.         z = range(4)\\n4135.         da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\\n4136.         da = DataArray(da_vals, coords=[x, y, z], dims=["x", "y", "z"])\\n4137. \\n4138.         result = da @ da\\n4139.         expected = da.dot(da)\\n4140.         assert_identical(result, expected)\\n4141. \\n4142.     def test_matmul_align_coords(self):\\n4143.         # GH 3694\\n4144. \\n4145.         x_a = np.arange(6)\\n4146.         x_b = np.arange(2, 8)\\n4147.         da_vals = np.arange(6)\\n4148.         da_a = DataArray(da_vals, coords=[x_a], dims=["x"])\\n4149.         da_b = DataArray(da_vals, coords=[x_b], dims=["x"])\\n4150. \\n4151.         # only test arithmetic_join="inner" (=default)\\n4152.         result = da_a @ da_b\\n4153.         expected = da_a.dot(da_b)\\n4154.         assert_identical(result, expected)\\n4155. \\n4156.         with xr.set_options(arithmetic_join="exact"):\\n4157.             with raises_regex(ValueError, "indexes along dimension"):\\n4158.                 da_a @ da_b\\n4159. \\n4160.     def test_binary_op_propagate_indexes(self):\\n4161.         # regression test for GH2227\\n4162.         self.dv["x"] = np.arange(self.dv.sizes["x"])\\n4163.         expected = self.dv.indexes["x"]\\n4164. \\n4165.         actual = (self.dv * 10).indexes["x"]\\n4166.         assert expected is actual\\n4167. \\n4168.         actual = (self.dv > 10).indexes["x"]\\n4169.         assert expected is actual\\n4170. \\n4171.     def test_binary_op_join_setting(self):\\n4172.         dim = "x"\\n4173.         align_type = "outer"\\n4174.         coords_l, coords_r = [0, 1, 2], [1, 2, 3]\\n4175.         missing_3 = xr.DataArray(coords_l, [(dim, coords_l)])\\n4176.         missing_0 = xr.DataArray(coords_r, [(dim, coords_r)])\\n4177.         with xr.set_options(arithmetic_join=align_type):\\n4178.             actual = missing_0 + missing_3\\n4179.         missing_0_aligned, missing_3_aligned = xr.align(\\n4180.             missing_0, missing_3, join=align_type\\n4181.         )\\n4182.         expected = xr.DataArray([np.nan, 2, 4, np.nan], [(dim, [0, 1, 2, 3])])\\n4183.         assert_equal(actual, expected)\\n4184. \\n4185.     def test_combine_first(self):\\n4186.         ar0 = DataArray([[0, 0], [0, 0]], [("x", ["a", "b"]), ("y", [-1, 0])])\\n4187.         ar1 = DataArray([[1, 1], [1, 1]], [("x", ["b", "c"]), ("y", [0, 1])])\\n4188.         ar2 = DataArray([2], [("x", ["d"])])\\n4189. \\n4190.         actual = ar0.combine_first(ar1)\\n4191.         expected = DataArray(\\n4192.             [[0, 0, np.nan], [0, 0, 1], [np.nan, 1, 1]],\\n4193.             [("x", ["a", "b", "c"]), ("y", [-1, 0, 1])],\\n4194.         )\\n4195.         assert_equal(actual, expected)\\n4196. \\n4197.         actual = ar1.combine_first(ar0)\\n4198.         expected = DataArray(\\n4199.             [[0, 0, np.nan], [0, 1, 1], [np.nan, 1, 1]],\\n4200.             [("x", ["a", "b", "c"]), ("y", [-1, 0, 1])],\\n4201.         )\\n4202.         assert_equal(actual, expected)\\n4203. \\n4204.         actual = ar0.combine_first(ar2)\\n4205.         expected = DataArray(\\n4206.             [[0, 0], [0, 0], [2, 2]], [("x", ["a", "b", "d"]), ("y", [-1, 0])]\\n4207.         )\\n4208.         assert_equal(actual, expected)\\n4209. \\n4210.     def test_sortby(self):\\n4211.         da = DataArray(\\n4212.             [[1, 2], [3, 4], [5, 6]], [("x", ["c", "b", "a"]), ("y", [1, 0])]\\n4213.         )\\n4214. \\n4215.         sorted1d = DataArray(\\n4216.             [[5, 6], [3, 4], [1, 2]], [("x", ["a", "b", "c"]), ("y", [1, 0])]\\n4217.         )\\n4218. \\n4219.         sorted2d = DataArray(\\n4220.             [[6, 5], [4, 3], [2, 1]], [("x", ["a", "b", "c"]), ("y", [0, 1])]\\n4221.         )\\n4222. \\n4223.         expected = sorted1d\\n4224.         dax = DataArray([100, 99, 98], [("x", ["c", "b", "a"])])\\n4225.         actual = da.sortby(dax)\\n4226.         assert_equal(actual, expected)\\n4227. \\n4228.         # test descending order sort\\n4229.         actual = da.sortby(dax, ascending=False)\\n4230.         assert_equal(actual, da)\\n4231. \\n4232.         # test alignment (fills in nan for \\\'c\\\')\\n4233.         dax_short = DataArray([98, 97], [("x", ["b", "a"])])\\n4234.         actual = da.sortby(dax_short)\\n4235.         assert_equal(actual, expected)\\n4236. \\n4237.         # test multi-dim sort by 1D dataarray values\\n4238.         expected = sorted2d\\n4239.         dax = DataArray([100, 99, 98], [("x", ["c", "b", "a"])])\\n4240.         day = DataArray([90, 80], [("y", [1, 0])])\\n4241.         actual = da.sortby([day, dax])\\n4242.         assert_equal(actual, expected)\\n4243. \\n4244.         expected = sorted1d\\n4245.         actual = da.sortby("x")\\n4246.         assert_equal(actual, expected)\\n4247. \\n4248.         expected = sorted2d\\n4249.         actual = da.sortby(["x", "y"])\\n4250.         assert_equal(actual, expected)\\n4251. \\n4252.     @requires_bottleneck\\n4253.     def test_rank(self):\\n4254.         # floats\\n4255.         ar = DataArray([[3, 4, np.nan, 1]])\\n4256.         expect_0 = DataArray([[1, 1, np.nan, 1]])\\n4257.         expect_1 = DataArray([[2, 3, np.nan, 1]])\\n4258.         assert_equal(ar.rank("dim_0"), expect_0)\\n4259.         assert_equal(ar.rank("dim_1"), expect_1)\\n4260.         # int\\n4261.         x = DataArray([3, 2, 1])\\n4262.         assert_equal(x.rank("dim_0"), x)\\n4263.         # str\\n4264.         y = DataArray(["c", "b", "a"])\\n4265.         assert_equal(y.rank("dim_0"), x)\\n4266. \\n4267.         x = DataArray([3.0, 1.0, np.nan, 2.0, 4.0], dims=("z",))\\n4268.         y = DataArray([0.75, 0.25, np.nan, 0.5, 1.0], dims=("z",))\\n4269.         assert_equal(y.rank("z", pct=True), y)\\n4270. \\n4271.     @pytest.mark.parametrize("use_dask", [True, False])\\n4272.     @pytest.mark.parametrize("use_datetime", [True, False])\\n4273.     def test_polyfit(self, use_dask, use_datetime):\\n4274.         if use_dask and not has_dask:\\n4275.             pytest.skip("requires dask")\\n4276.         xcoord = xr.DataArray(\\n4277.             pd.date_range("1970-01-01", freq="D", periods=10), dims=("x",), name="x"\\n4278.         )\\n4279.         x = xr.core.missing.get_clean_interp_index(xcoord, "x")\\n4280.         if not use_datetime:\\n4281.             xcoord = x\\n4282. \\n4283.         da_raw = DataArray(\\n4284.             np.stack(\\n4285.                 (10 + 1e-15 * x + 2e-28 * x ** 2, 30 + 2e-14 * x + 1e-29 * x ** 2)\\n4286.             ),\\n4287.             dims=("d", "x"),\\n4288.             coords={"x": xcoord, "d": [0, 1]},\\n4289.         )\\n4290. \\n4291.         if use_dask:\\n4292.             da = da_raw.chunk({"d": 1})\\n4293.         else:\\n4294.             da = da_raw\\n4295. \\n4296.         out = da.polyfit("x", 2)\\n4297.         expected = DataArray(\\n4298.             [[2e-28, 1e-15, 10], [1e-29, 2e-14, 30]],\\n4299.             dims=("d", "degree"),\\n4300.             coords={"degree": [2, 1, 0], "d": [0, 1]},\\n4301.         ).T\\n4302.         assert_allclose(out.polyfit_coefficients, expected, rtol=1e-3)\\n4303. \\n4304.         # With NaN\\n4305.         da_raw[0, 1] = np.nan\\n4306.         if use_dask:\\n4307.             da = da_raw.chunk({"d": 1})\\n4308.         else:\\n4309.             da = da_raw\\n4310.         out = da.polyfit("x", 2, skipna=True, cov=True)\\n4311.         assert_allclose(out.polyfit_coefficients, expected, rtol=1e-3)\\n4312.         assert "polyfit_covariance" in out\\n4313. \\n4314.         # Skipna + Full output\\n4315.         out = da.polyfit("x", 2, skipna=True, full=True)\\n4316.         assert_allclose(out.polyfit_coefficients, expected, rtol=1e-3)\\n4317.         assert out.x_matrix_rank == 3\\n4318.         np.testing.assert_almost_equal(out.polyfit_residuals, [0, 0])\\n4319. \\n4320.     def test_pad_constant(self):\\n4321.         ar = DataArray(np.arange(3 * 4 * 5).reshape(3, 4, 5))\\n4322.         actual = ar.pad(dim_0=(1, 3))\\n4323.         expected = DataArray(\\n4324.             np.pad(\\n4325.                 np.arange(3 * 4 * 5).reshape(3, 4, 5).astype(np.float32),\\n4326.                 mode="constant",\\n4327.                 pad_width=((1, 3), (0, 0), (0, 0)),\\n4328.                 constant_values=np.nan,\\n4329.             )\\n4330.         )\\n4331.         assert actual.shape == (7, 4, 5)\\n4332.         assert_identical(actual, expected)\\n4333. \\n4334.     def test_pad_coords(self):\\n4335.         ar = DataArray(\\n4336.             np.arange(3 * 4 * 5).reshape(3, 4, 5),\\n4337.             [("x", np.arange(3)), ("y", np.arange(4)), ("z", np.arange(5))],\\n4338.         )\\n4339.         actual = ar.pad(x=(1, 3), constant_values=1)\\n4340.         expected = DataArray(\\n4341.             np.pad(\\n4342.                 np.arange(3 * 4 * 5).reshape(3, 4, 5),\\n4343.                 mode="constant",\\n4344.                 pad_width=((1, 3), (0, 0), (0, 0)),\\n4345.                 constant_values=1,\\n4346.             ),\\n4347.             [\\n4348.                 (\\n4349.                     "x",\\n4350.                     np.pad(\\n4351.                         np.arange(3).astype(np.float32),\\n4352.                         mode="constant",\\n4353.                         pad_width=(1, 3),\\n4354.                         constant_values=np.nan,\\n4355.                     ),\\n4356.                 ),\\n4357.                 ("y", np.arange(4)),\\n4358.                 ("z", np.arange(5)),\\n4359.             ],\\n4360.         )\\n4361.         assert_identical(actual, expected)\\n4362. \\n4363.     @pytest.mark.parametrize("mode", ("minimum", "maximum", "mean", "median"))\\n4364.     @pytest.mark.parametrize(\\n4365.         "stat_length", (None, 3, (1, 3), {"dim_0": (2, 1), "dim_2": (4, 2)})\\n4366.     )\\n4367.     def test_pad_stat_length(self, mode, stat_length):\\n4368.         ar = DataArray(np.arange(3 * 4 * 5).reshape(3, 4, 5))\\n4369.         actual = ar.pad(dim_0=(1, 3), dim_2=(2, 2), mode=mode, stat_length=stat_length)\\n4370.         if isinstance(stat_length, dict):\\n4371.             stat_length = (stat_length["dim_0"], (4, 4), stat_length["dim_2"])\\n4372.         expected = DataArray(\\n4373.             np.pad(\\n4374.                 np.arange(3 * 4 * 5).reshape(3, 4, 5),\\n4375.                 pad_width=((1, 3), (0, 0), (2, 2)),\\n4376.                 mode=mode,\\n4377.                 stat_length=stat_length,\\n4378.             )\\n4379.         )\\n4380.         assert actual.shape == (7, 4, 9)\\n4381.         assert_identical(actual, expected)\\n4382. \\n4383.     @pytest.mark.parametrize(\\n4384.         "end_values", (None, 3, (3, 5), {"dim_0": (2, 1), "dim_2": (4, 2)})\\n4385.     )\\n4386.     def test_pad_linear_ramp(self, end_values):\\n4387.         ar = DataArray(np.arange(3 * 4 * 5).reshape(3, 4, 5))\\n4388.         actual = ar.pad(\\n4389.             dim_0=(1, 3), dim_2=(2, 2), mode="linear_ramp", end_values=end_values\\n4390.         )\\n4391.         if end_values is None:\\n4392.             end_values = 0\\n4393.         elif isinstance(end_values, dict):\\n4394.             end_values = (end_values["dim_0"], (4, 4), end_values["dim_2"])\\n4395.         expected = DataArray(\\n4396.             np.pad(\\n4397.                 np.arange(3 * 4 * 5).reshape(3, 4, 5),\\n4398.                 pad_width=((1, 3), (0, 0), (2, 2)),\\n4399.                 mode="linear_ramp",\\n4400.                 end_values=end_values,\\n4401.             )\\n4402.         )\\n4403.         assert actual.shape == (7, 4, 9)\\n4404.         assert_identical(actual, expected)\\n4405. \\n4406.     @pytest.mark.parametrize("mode", ("reflect", "symmetric"))\\n4407.     @pytest.mark.parametrize("reflect_type", (None, "even", "odd"))\\n4408.     def test_pad_reflect(self, mode, reflect_type):\\n4409. \\n4410.         ar = DataArray(np.arange(3 * 4 * 5).reshape(3, 4, 5))\\n4411.         actual = ar.pad(\\n4412.             dim_0=(1, 3), dim_2=(2, 2), mode=mode, reflect_type=reflect_type\\n4413.         )\\n4414.         np_kwargs = {\\n4415.             "array": np.arange(3 * 4 * 5).reshape(3, 4, 5),\\n4416.             "pad_width": ((1, 3), (0, 0), (2, 2)),\\n4417.             "mode": mode,\\n4418.         }\\n4419.         # numpy does not support reflect_type=None\\n4420.         if reflect_type is not None:\\n4421.             np_kwargs["reflect_type"] = reflect_type\\n4422.         expected = DataArray(np.pad(**np_kwargs))\\n4423. \\n4424.         assert actual.shape == (7, 4, 9)\\n4425.         assert_identical(actual, expected)\\n4426. \\n4427. \\n4428. class TestReduce:\\n4429.     @pytest.fixture(autouse=True)\\n4430.     def setup(self):\\n4431.         self.attrs = {"attr1": "value1", "attr2": 2929}\\n4432. \\n4433. \\n4434. @pytest.mark.parametrize(\\n4435.     "x, minindex, maxindex, nanindex",\\n4436.     [\\n4437.         (np.array([0, 1, 2, 0, -2, -4, 2]), 5, 2, None),\\n4438.         (np.array([0.0, 1.0, 2.0, 0.0, -2.0, -4.0, 2.0]), 5, 2, None),\\n4439.         (np.array([1.0, np.NaN, 2.0, np.NaN, -2.0, -4.0, 2.0]), 5, 2, 1),\\n4440.         (\\n4441.             np.array([1.0, np.NaN, 2.0, np.NaN, -2.0, -4.0, 2.0]).astype("object"),\\n4442.             5,\\n4443.             2,\\n4444.             1,\\n4445.         ),\\n4446.         (np.array([np.NaN, np.NaN]), np.NaN, np.NaN, 0),\\n4447.         (\\n4448.             np.array(\\n4449.                 ["2015-12-31", "2020-01-02", "2020-01-01", "2016-01-01"],\\n4450.                 dtype="datetime64[ns]",\\n4451.             ),\\n4452.             0,\\n4453.             1,\\n4454.             None,\\n4455.         ),\\n4456.     ],\\n4457. )\\n4458. class TestReduce1D(TestReduce):\\n4459.     def test_min(self, x, minindex, maxindex, nanindex):\\n4460.         ar = xr.DataArray(\\n4461.             x, dims=["x"], coords={"x": np.arange(x.size) * 4}, attrs=self.attrs\\n4462.         )\\n4463. \\n4464.         if np.isnan(minindex):\\n4465.             minindex = 0\\n4466. \\n4467.         expected0 = ar.isel(x=minindex, drop=True)\\n4468.         result0 = ar.min(keep_attrs=True)\\n4469.         assert_identical(result0, expected0)\\n4470. \\n4471.         result1 = ar.min()\\n4472.         expected1 = expected0.copy()\\n4473.         expected1.attrs = {}\\n4474.         assert_identical(result1, expected1)\\n4475. \\n4476.         result2 = ar.min(skipna=False)\\n4477.         if nanindex is not None and ar.dtype.kind != "O":\\n4478.             expected2 = ar.isel(x=nanindex, drop=True)\\n4479.             expected2.attrs = {}\\n4480.         else:\\n4481.             expected2 = expected1\\n4482. \\n4483.         assert_identical(result2, expected2)\\n4484. \\n4485.     def test_max(self, x, minindex, maxindex, nanindex):\\n4486.         ar = xr.DataArray(\\n4487.             x, dims=["x"], coords={"x": np.arange(x.size) * 4}, attrs=self.attrs\\n4488.         )\\n4489. \\n4490.         if np.isnan(minindex):\\n4491.             maxindex = 0\\n4492. \\n4493.         expected0 = ar.isel(x=maxindex, drop=True)\\n4494.         result0 = ar.max(keep_attrs=True)\\n4495.         assert_identical(result0, expected0)\\n4496. \\n4497.         result1 = ar.max()\\n4498.         expected1 = expected0.copy()\\n4499.         expected1.attrs = {}\\n4500.         assert_identical(result1, expected1)\\n4501. \\n4502.         result2 = ar.max(skipna=False)\\n4503.         if nanindex is not None and ar.dtype.kind != "O":\\n4504.             expected2 = ar.isel(x=nanindex, drop=True)\\n4505.             expected2.attrs = {}\\n4506.         else:\\n4507.             expected2 = expected1\\n4508. \\n4509.         assert_identical(result2, expected2)\\n4510. \\n4511.     @pytest.mark.filterwarnings(\\n4512.         "ignore:Behaviour of argmin/argmax with neither dim nor :DeprecationWarning"\\n4513.     )\\n4514.     def test_argmin(self, x, minindex, maxindex, nanindex):\\n4515.         ar = xr.DataArray(\\n4516.             x, dims=["x"], coords={"x": np.arange(x.size) * 4}, attrs=self.attrs\\n4517.         )\\n4518.         indarr = xr.DataArray(np.arange(x.size, dtype=np.intp), dims=["x"])\\n4519. \\n4520.         if np.isnan(minindex):\\n4521.             with pytest.raises(ValueError):\\n4522.                 ar.argmin()\\n4523.             return\\n4524. \\n4525.         expected0 = indarr[minindex]\\n4526.         result0 = ar.argmin()\\n4527.         assert_identical(result0, expected0)\\n4528. \\n4529.         result1 = ar.argmin(keep_attrs=True)\\n4530.         expected1 = expected0.copy()\\n4531.         expected1.attrs = self.attrs\\n4532.         assert_identical(result1, expected1)\\n4533. \\n4534.         result2 = ar.argmin(skipna=False)\\n4535.         if nanindex is not None and ar.dtype.kind != "O":\\n4536.             expected2 = indarr.isel(x=nanindex, drop=True)\\n4537.             expected2.attrs = {}\\n4538.         else:\\n4539.             expected2 = expected0\\n4540. \\n4541.         assert_identical(result2, expected2)\\n4542. \\n4543.     @pytest.mark.filterwarnings(\\n4544.         "ignore:Behaviour of argmin/argmax with neither dim nor :DeprecationWarning"\\n4545.     )\\n4546.     def test_argmax(self, x, minindex, maxindex, nanindex):\\n4547.         ar = xr.DataArray(\\n4548.             x, dims=["x"], coords={"x": np.arange(x.size) * 4}, attrs=self.attrs\\n4549.         )\\n4550.         indarr = xr.DataArray(np.arange(x.size, dtype=np.intp), dims=["x"])\\n4551. \\n4552.         if np.isnan(maxindex):\\n4553.             with pytest.raises(ValueError):\\n4554.                 ar.argmax()\\n4555.             return\\n4556. \\n4557.         expected0 = indarr[maxindex]\\n4558.         result0 = ar.argmax()\\n4559.         assert_identical(result0, expected0)\\n4560. \\n4561.         result1 = ar.argmax(keep_attrs=True)\\n4562.         expected1 = expected0.copy()\\n4563.         expected1.attrs = self.attrs\\n4564.         assert_identical(result1, expected1)\\n4565. \\n4566.         result2 = ar.argmax(skipna=False)\\n4567.         if nanindex is not None and ar.dtype.kind != "O":\\n4568.             expected2 = indarr.isel(x=nanindex, drop=True)\\n4569.             expected2.attrs = {}\\n4570.         else:\\n4571.             expected2 = expected0\\n4572. \\n4573.         assert_identical(result2, expected2)\\n4574. \\n4575.     @pytest.mark.parametrize("use_dask", [True, False])\\n4576.     def test_idxmin(self, x, minindex, maxindex, nanindex, use_dask):\\n4577.         if use_dask and not has_dask:\\n4578.             pytest.skip("requires dask")\\n4579.         if use_dask and x.dtype.kind == "M":\\n4580.             pytest.xfail("dask operation \\\'argmin\\\' breaks when dtype is datetime64 (M)")\\n4581.         ar0_raw = xr.DataArray(\\n4582.             x, dims=["x"], coords={"x": np.arange(x.size) * 4}, attrs=self.attrs\\n4583.         )\\n4584. \\n4585.         if use_dask:\\n4586.             ar0 = ar0_raw.chunk({})\\n4587.         else:\\n4588.             ar0 = ar0_raw\\n4589. \\n4590.         # dim doesn\\\'t exist\\n4591.         with pytest.raises(KeyError):\\n4592.             ar0.idxmin(dim="spam")\\n4593. \\n4594.         # Scalar Dataarray\\n4595.         with pytest.raises(ValueError):\\n4596.             xr.DataArray(5).idxmin()\\n4597. \\n4598.         coordarr0 = xr.DataArray(ar0.coords["x"], dims=["x"])\\n4599.         coordarr1 = coordarr0.copy()\\n4600. \\n4601.         hasna = np.isnan(minindex)\\n4602.         if np.isnan(minindex):\\n4603.             minindex = 0\\n4604. \\n4605.         if hasna:\\n4606.             coordarr1[...] = 1\\n4607.             fill_value_0 = np.NaN\\n4608.         else:\\n4609.             fill_value_0 = 1\\n4610. \\n4611.         expected0 = (\\n4612.             (coordarr1 * fill_value_0).isel(x=minindex, drop=True).astype("float")\\n4613.         )\\n4614.         expected0.name = "x"\\n4615. \\n4616.         # Default fill value (NaN)\\n4617.         result0 = ar0.idxmin()\\n4618.         assert_identical(result0, expected0)\\n4619. \\n4620.         # Manually specify NaN fill_value\\n4621.         result1 = ar0.idxmin(fill_value=np.NaN)\\n4622.         assert_identical(result1, expected0)\\n4623. \\n4624.         # keep_attrs\\n4625.         result2 = ar0.idxmin(keep_attrs=True)\\n4626.         expected2 = expected0.copy()\\n4627.         expected2.attrs = self.attrs\\n4628.         assert_identical(result2, expected2)\\n4629. \\n4630.         # skipna=False\\n4631.         if nanindex is not None and ar0.dtype.kind != "O":\\n4632.             expected3 = coordarr0.isel(x=nanindex, drop=True).astype("float")\\n4633.             expected3.name = "x"\\n4634.             expected3.attrs = {}\\n4635.         else:\\n4636.             expected3 = expected0.copy()\\n4637. \\n4638.         result3 = ar0.idxmin(skipna=False)\\n4639.         assert_identical(result3, expected3)\\n4640. \\n4641.         # fill_value should be ignored with skipna=False\\n4642.         result4 = ar0.idxmin(skipna=False, fill_value=-100j)\\n4643.         assert_identical(result4, expected3)\\n4644. \\n4645.         # Float fill_value\\n4646.         if hasna:\\n4647.             fill_value_5 = -1.1\\n4648.         else:\\n4649.             fill_value_5 = 1\\n4650. \\n4651.         expected5 = (coordarr1 * fill_value_5).isel(x=minindex, drop=True)\\n4652.         expected5.name = "x"\\n4653. \\n4654.         result5 = ar0.idxmin(fill_value=-1.1)\\n4655.         assert_identical(result5, expected5)\\n4656. \\n4657.         # Integer fill_value\\n4658.         if hasna:\\n4659.             fill_value_6 = -1\\n4660.         else:\\n4661.             fill_value_6 = 1\\n4662. \\n4663.         expected6 = (coordarr1 * fill_value_6).isel(x=minindex, drop=True)\\n4664.         expected6.name = "x"\\n4665. \\n4666.         result6 = ar0.idxmin(fill_value=-1)\\n4667.         assert_identical(result6, expected6)\\n4668. \\n4669.         # Complex fill_value\\n4670.         if hasna:\\n4671.             fill_value_7 = -1j\\n4672.         else:\\n4673.             fill_value_7 = 1\\n4674. \\n4675.         expected7 = (coordarr1 * fill_value_7).isel(x=minindex, drop=True)\\n4676.         expected7.name = "x"\\n4677. \\n4678.         result7 = ar0.idxmin(fill_value=-1j)\\n4679.         assert_identical(result7, expected7)\\n4680. \\n4681.     @pytest.mark.parametrize("use_dask", [True, False])\\n4682.     def test_idxmax(self, x, minindex, maxindex, nanindex, use_dask):\\n4683.         if use_dask and not has_dask:\\n4684.             pytest.skip("requires dask")\\n4685.         if use_dask and x.dtype.kind == "M":\\n4686.             pytest.xfail("dask operation \\\'argmax\\\' breaks when dtype is datetime64 (M)")\\n4687.         ar0_raw = xr.DataArray(\\n4688.             x, dims=["x"], coords={"x": np.arange(x.size) * 4}, attrs=self.attrs\\n4689.         )\\n4690. \\n4691.         if use_dask:\\n4692.             ar0 = ar0_raw.chunk({})\\n4693.         else:\\n4694.             ar0 = ar0_raw\\n4695. \\n4696.         # dim doesn\\\'t exist\\n4697.         with pytest.raises(KeyError):\\n4698.             ar0.idxmax(dim="spam")\\n4699. \\n4700.         # Scalar Dataarray\\n4701.         with pytest.raises(ValueError):\\n4702.             xr.DataArray(5).idxmax()\\n4703. \\n4704.         coordarr0 = xr.DataArray(ar0.coords["x"], dims=["x"])\\n4705.         coordarr1 = coordarr0.copy()\\n4706. \\n4707.         hasna = np.isnan(maxindex)\\n4708.         if np.isnan(maxindex):\\n4709.             maxindex = 0\\n4710. \\n4711.         if hasna:\\n4712.             coordarr1[...] = 1\\n4713.             fill_value_0 = np.NaN\\n4714.         else:\\n4715.             fill_value_0 = 1\\n4716. \\n4717.         expected0 = (\\n4718.             (coordarr1 * fill_value_0).isel(x=maxindex, drop=True).astype("float")\\n4719.         )\\n4720.         expected0.name = "x"\\n4721. \\n4722.         # Default fill value (NaN)\\n4723.         result0 = ar0.idxmax()\\n4724.         assert_identical(result0, expected0)\\n4725. \\n4726.         # Manually specify NaN fill_value\\n4727.         result1 = ar0.idxmax(fill_value=np.NaN)\\n4728.         assert_identical(result1, expected0)\\n4729. \\n4730.         # keep_attrs\\n4731.         result2 = ar0.idxmax(keep_attrs=True)\\n4732.         expected2 = expected0.copy()\\n4733.         expected2.attrs = self.attrs\\n4734.         assert_identical(result2, expected2)\\n4735. \\n4736.         # skipna=False\\n4737.         if nanindex is not None and ar0.dtype.kind != "O":\\n4738.             expected3 = coordarr0.isel(x=nanindex, drop=True).astype("float")\\n4739.             expected3.name = "x"\\n4740.             expected3.attrs = {}\\n4741.         else:\\n4742.             expected3 = expected0.copy()\\n4743. \\n4744.         result3 = ar0.idxmax(skipna=False)\\n4745.         assert_identical(result3, expected3)\\n4746. \\n4747.         # fill_value should be ignored with skipna=False\\n4748.         result4 = ar0.idxmax(skipna=False, fill_value=-100j)\\n4749.         assert_identical(result4, expected3)\\n4750. \\n4751.         # Float fill_value\\n4752.         if hasna:\\n4753.             fill_value_5 = -1.1\\n4754.         else:\\n4755.             fill_value_5 = 1\\n4756. \\n4757.         expected5 = (coordarr1 * fill_value_5).isel(x=maxindex, drop=True)\\n4758.         expected5.name = "x"\\n4759. \\n4760.         result5 = ar0.idxmax(fill_value=-1.1)\\n4761.         assert_identical(result5, expected5)\\n4762. \\n4763.         # Integer fill_value\\n4764.         if hasna:\\n4765.             fill_value_6 = -1\\n4766.         else:\\n4767.             fill_value_6 = 1\\n4768. \\n4769.         expected6 = (coordarr1 * fill_value_6).isel(x=maxindex, drop=True)\\n4770.         expected6.name = "x"\\n4771. \\n4772.         result6 = ar0.idxmax(fill_value=-1)\\n4773.         assert_identical(result6, expected6)\\n4774. \\n4775.         # Complex fill_value\\n4776.         if hasna:\\n4777.             fill_value_7 = -1j\\n4778.         else:\\n4779.             fill_value_7 = 1\\n4780. \\n4781.         expected7 = (coordarr1 * fill_value_7).isel(x=maxindex, drop=True)\\n4782.         expected7.name = "x"\\n4783. \\n4784.         result7 = ar0.idxmax(fill_value=-1j)\\n4785.         assert_identical(result7, expected7)\\n4786. \\n4787.     @pytest.mark.filterwarnings(\\n4788.         "ignore:Behaviour of argmin/argmax with neither dim nor :DeprecationWarning"\\n4789.     )\\n4790.     def test_argmin_dim(self, x, minindex, maxindex, nanindex):\\n4791.         ar = xr.DataArray(\\n4792.             x, dims=["x"], coords={"x": np.arange(x.size) * 4}, attrs=self.attrs\\n4793.         )\\n4794.         indarr = xr.DataArray(np.arange(x.size, dtype=np.intp), dims=["x"])\\n4795. \\n4796.         if np.isnan(minindex):\\n4797.             with pytest.raises(ValueError):\\n4798.                 ar.argmin()\\n4799.             return\\n4800. \\n4801.         expected0 = {"x": indarr[minindex]}\\n4802.         result0 = ar.argmin(...)\\n4803.         for key in expected0:\\n4804.             assert_identical(result0[key], expected0[key])\\n4805. \\n4806.         result1 = ar.argmin(..., keep_attrs=True)\\n4807.         expected1 = deepcopy(expected0)\\n4808.         for da in expected1.values():\\n4809.             da.attrs = self.attrs\\n4810.         for key in expected1:\\n4811.             assert_identical(result1[key], expected1[key])\\n4812. \\n4813.         result2 = ar.argmin(..., skipna=False)\\n4814.         if nanindex is not None and ar.dtype.kind != "O":\\n4815.             expected2 = {"x": indarr.isel(x=nanindex, drop=True)}\\n4816.             expected2["x"].attrs = {}\\n4817.         else:\\n4818.             expected2 = expected0\\n4819. \\n4820.         for key in expected2:\\n4821.             assert_identical(result2[key], expected2[key])\\n4822. \\n4823.     @pytest.mark.filterwarnings(\\n4824.         "ignore:Behaviour of argmin/argmax with neither dim nor :DeprecationWarning"\\n4825.     )\\n4826.     def test_argmax_dim(self, x, minindex, maxindex, nanindex):\\n4827.         ar = xr.DataArray(\\n4828.             x, dims=["x"], coords={"x": np.arange(x.size) * 4}, attrs=self.attrs\\n4829.         )\\n4830.         indarr = xr.DataArray(np.arange(x.size, dtype=np.intp), dims=["x"])\\n4831. \\n4832.         if np.isnan(maxindex):\\n4833.             with pytest.raises(ValueError):\\n4834.                 ar.argmax()\\n4835.             return\\n4836. \\n4837.         expected0 = {"x": indarr[maxindex]}\\n4838.         result0 = ar.argmax(...)\\n4839.         for key in expected0:\\n4840.             assert_identical(result0[key], expected0[key])\\n4841. \\n4842.         result1 = ar.argmax(..., keep_attrs=True)\\n4843.         expected1 = deepcopy(expected0)\\n4844.         for da in expected1.values():\\n4845.             da.attrs = self.attrs\\n4846.         for key in expected1:\\n4847.             assert_identical(result1[key], expected1[key])\\n4848. \\n4849.         result2 = ar.argmax(..., skipna=False)\\n4850.         if nanindex is not None and ar.dtype.kind != "O":\\n4851.             expected2 = {"x": indarr.isel(x=nanindex, drop=True)}\\n4852.             expected2["x"].attrs = {}\\n4853.         else:\\n4854.             expected2 = expected0\\n4855. \\n4856.         for key in expected2:\\n4857.             assert_identical(result2[key], expected2[key])\\n4858. \\n4859. \\n4860. @pytest.mark.parametrize(\\n4861.     "x, minindex, maxindex, nanindex",\\n4862.     [\\n4863.         (\\n4864.             np.array(\\n4865.                 [\\n4866.                     [0, 1, 2, 0, -2, -4, 2],\\n4867.                     [1, 1, 1, 1, 1, 1, 1],\\n4868.                     [0, 0, -10, 5, 20, 0, 0],\\n4869.                 ]\\n4870.             ),\\n4871.             [5, 0, 2],\\n4872.             [2, 0, 4],\\n4873.             [None, None, None],\\n4874.         ),\\n4875.         (\\n4876.             np.array(\\n4877.                 [\\n4878.                     [2.0, 1.0, 2.0, 0.0, -2.0, -4.0, 2.0],\\n4879.                     [-4.0, np.NaN, 2.0, np.NaN, -2.0, -4.0, 2.0],\\n4880.                     [np.NaN] * 7,\\n4881.                 ]\\n4882.             ),\\n4883.             [5, 0, np.NaN],\\n4884.             [0, 2, np.NaN],\\n4885.             [None, 1, 0],\\n4886.         ),\\n4887.         (\\n4888.             np.array(\\n4889.                 [\\n4890.                     [2.0, 1.0, 2.0, 0.0, -2.0, -4.0, 2.0],\\n4891.                     [-4.0, np.NaN, 2.0, np.NaN, -2.0, -4.0, 2.0],\\n4892.                     [np.NaN] * 7,\\n4893.                 ]\\n4894.             ).astype("object"),\\n4895.             [5, 0, np.NaN],\\n4896.             [0, 2, np.NaN],\\n4897.             [None, 1, 0],\\n4898.         ),\\n4899.         (\\n4900.             np.array(\\n4901.                 [\\n4902.                     ["2015-12-31", "2020-01-02", "2020-01-01", "2016-01-01"],\\n4903.                     ["2020-01-02", "2020-01-02", "2020-01-02", "2020-01-02"],\\n4904.                     ["1900-01-01", "1-02-03", "1900-01-02", "1-02-03"],\\n4905.                 ],\\n4906.                 dtype="datetime64[ns]",\\n4907.             ),\\n4908.             [0, 0, 1],\\n4909.             [1, 0, 2],\\n4910.             [None, None, None],\\n4911.         ),\\n4912.     ],\\n4913. )\\n4914. class TestReduce2D(TestReduce):\\n4915.     def test_min(self, x, minindex, maxindex, nanindex):\\n4916.         ar = xr.DataArray(\\n4917.             x,\\n4918.             dims=["y", "x"],\\n4919.             coords={"x": np.arange(x.shape[1]) * 4, "y": 1 - np.arange(x.shape[0])},\\n4920.             attrs=self.attrs,\\n4921.         )\\n4922. \\n4923.         minindex = [x if not np.isnan(x) else 0 for x in minindex]\\n4924.         expected0 = [\\n4925.             ar.isel(y=yi).isel(x=indi, drop=True) for yi, indi in enumerate(minindex)\\n4926.         ]\\n4927.         expected0 = xr.concat(expected0, dim="y")\\n4928. \\n4929.         result0 = ar.min(dim="x", keep_attrs=True)\\n4930.         assert_identical(result0, expected0)\\n4931. \\n4932.         result1 = ar.min(dim="x")\\n4933.         expected1 = expected0\\n4934.         expected1.attrs = {}\\n4935.         assert_identical(result1, expected1)\\n4936. \\n4937.         result2 = ar.min(axis=1)\\n4938.         assert_identical(result2, expected1)\\n4939. \\n4940.         minindex = [\\n4941.             x if y is None or ar.dtype.kind == "O" else y\\n4942.             for x, y in zip(minindex, nanindex)\\n4943.         ]\\n4944.         expected2 = [\\n4945.             ar.isel(y=yi).isel(x=indi, drop=True) for yi, indi in enumerate(minindex)\\n4946.         ]\\n4947.         expected2 = xr.concat(expected2, dim="y")\\n4948.         expected2.attrs = {}\\n4949. \\n4950.         result3 = ar.min(dim="x", skipna=False)\\n4951. \\n4952.         assert_identical(result3, expected2)\\n4953. \\n4954.     def test_max(self, x, minindex, maxindex, nanindex):\\n4955.         ar = xr.DataArray(\\n4956.             x,\\n4957.             dims=["y", "x"],\\n4958.             coords={"x": np.arange(x.shape[1]) * 4, "y": 1 - np.arange(x.shape[0])},\\n4959.             attrs=self.attrs,\\n4960.         )\\n4961. \\n4962.         maxindex = [x if not np.isnan(x) else 0 for x in maxindex]\\n4963.         expected0 = [\\n4964.             ar.isel(y=yi).isel(x=indi, drop=True) for yi, indi in enumerate(maxindex)\\n4965.         ]\\n4966.         expected0 = xr.concat(expected0, dim="y")\\n4967. \\n4968.         result0 = ar.max(dim="x", keep_attrs=True)\\n4969.         assert_identical(result0, expected0)\\n4970. \\n4971.         result1 = ar.max(dim="x")\\n4972.         expected1 = expected0.copy()\\n4973.         expected1.attrs = {}\\n4974.         assert_identical(result1, expected1)\\n4975. \\n4976.         result2 = ar.max(axis=1)\\n4977.         assert_identical(result2, expected1)\\n4978. \\n4979.         maxindex = [\\n4980.             x if y is None or ar.dtype.kind == "O" else y\\n4981.             for x, y in zip(maxindex, nanindex)\\n4982.         ]\\n4983.         expected2 = [\\n4984.             ar.isel(y=yi).isel(x=indi, drop=True) for yi, indi in enumerate(maxindex)\\n4985.         ]\\n4986.         expected2 = xr.concat(expected2, dim="y")\\n4987.         expected2.attrs = {}\\n4988. \\n4989.         result3 = ar.max(dim="x", skipna=False)\\n4990. \\n4991.         assert_identical(result3, expected2)\\n4992. \\n4993.     def test_argmin(self, x, minindex, maxindex, nanindex):\\n4994.         ar = xr.DataArray(\\n4995.             x,\\n4996.             dims=["y", "x"],\\n4997.             coords={"x": np.arange(x.shape[1]) * 4, "y": 1 - np.arange(x.shape[0])},\\n4998.             attrs=self.attrs,\\n4999.         )\\n5000.         indarr = np.tile(np.arange(x.shape[1], dtype=np.intp), [x.shape[0], 1])\\n5001.         indarr = xr.DataArray(indarr, dims=ar.dims, coords=ar.coords)\\n5002. \\n5003.         if np.isnan(minindex).any():\\n5004.             with pytest.raises(ValueError):\\n5005.                 ar.argmin(dim="x")\\n5006.             return\\n5007. \\n5008.         expected0 = [\\n5009.             indarr.isel(y=yi).isel(x=indi, drop=True)\\n5010.             for yi, indi in enumerate(minindex)\\n5011.         ]\\n5012.         expected0 = xr.concat(expected0, dim="y")\\n5013. \\n5014.         result0 = ar.argmin(dim="x")\\n5015.         assert_identical(result0, expected0)\\n5016. \\n5017.         result1 = ar.argmin(axis=1)\\n5018.         assert_identical(result1, expected0)\\n5019. \\n5020.         result2 = ar.argmin(dim="x", keep_attrs=True)\\n5021.         expected1 = expected0.copy()\\n5022.         expected1.attrs = self.attrs\\n5023.         assert_identical(result2, expected1)\\n5024. \\n5025.         minindex = [\\n5026.             x if y is None or ar.dtype.kind == "O" else y\\n5027.             for x, y in zip(minindex, nanindex)\\n5028.         ]\\n5029.         expected2 = [\\n5030.             indarr.isel(y=yi).isel(x=indi, drop=True)\\n5031.             for yi, indi in enumerate(minindex)\\n5032.         ]\\n5033.         expected2 = xr.concat(expected2, dim="y")\\n5034.         expected2.attrs = {}\\n5035. \\n5036.         result3 = ar.argmin(dim="x", skipna=False)\\n5037. \\n5038.         assert_identical(result3, expected2)\\n5039. \\n5040.     def test_argmax(self, x, minindex, maxindex, nanindex):\\n5041.         ar = xr.DataArray(\\n5042.             x,\\n5043.             dims=["y", "x"],\\n5044.             coords={"x": np.arange(x.shape[1]) * 4, "y": 1 - np.arange(x.shape[0])},\\n5045.             attrs=self.attrs,\\n5046.         )\\n5047.         indarr = np.tile(np.arange(x.shape[1], dtype=np.intp), [x.shape[0], 1])\\n5048.         indarr = xr.DataArray(indarr, dims=ar.dims, coords=ar.coords)\\n5049. \\n5050.         if np.isnan(maxindex).any():\\n5051.             with pytest.raises(ValueError):\\n5052.                 ar.argmax(dim="x")\\n5053.             return\\n5054. \\n5055.         expected0 = [\\n5056.             indarr.isel(y=yi).isel(x=indi, drop=True)\\n5057.             for yi, indi in enumerate(maxindex)\\n5058.         ]\\n5059.         expected0 = xr.concat(expected0, dim="y")\\n5060. \\n5061.         result0 = ar.argmax(dim="x")\\n5062.         assert_identical(result0, expected0)\\n5063. \\n5064.         result1 = ar.argmax(axis=1)\\n5065.         assert_identical(result1, expected0)\\n5066. \\n5067.         result2 = ar.argmax(dim="x", keep_attrs=True)\\n5068.         expected1 = expected0.copy()\\n5069.         expected1.attrs = self.attrs\\n5070.         assert_identical(result2, expected1)\\n5071. \\n5072.         maxindex = [\\n5073.             x if y is None or ar.dtype.kind == "O" else y\\n5074.             for x, y in zip(maxindex, nanindex)\\n5075.         ]\\n5076.         expected2 = [\\n5077.             indarr.isel(y=yi).isel(x=indi, drop=True)\\n5078.             for yi, indi in enumerate(maxindex)\\n5079.         ]\\n5080.         expected2 = xr.concat(expected2, dim="y")\\n5081.         expected2.attrs = {}\\n5082. \\n5083.         result3 = ar.argmax(dim="x", skipna=False)\\n5084. \\n5085.         assert_identical(result3, expected2)\\n5086. \\n5087.     @pytest.mark.parametrize("use_dask", [True, False])\\n5088.     def test_idxmin(self, x, minindex, maxindex, nanindex, use_dask):\\n5089.         if use_dask and not has_dask:\\n5090.             pytest.skip("requires dask")\\n5091.         if use_dask and x.dtype.kind == "M":\\n5092.             pytest.xfail("dask operation \\\'argmin\\\' breaks when dtype is datetime64 (M)")\\n5093. \\n5094.         if x.dtype.kind == "O":\\n5095.             # TODO: nanops._nan_argminmax_object computes once to check for all-NaN slices.\\n5096.             max_computes = 1\\n5097.         else:\\n5098.             max_computes = 0\\n5099. \\n5100.         ar0_raw = xr.DataArray(\\n5101.             x,\\n5102.             dims=["y", "x"],\\n5103.             coords={"x": np.arange(x.shape[1]) * 4, "y": 1 - np.arange(x.shape[0])},\\n5104.             attrs=self.attrs,\\n5105.         )\\n5106. \\n5107.         if use_dask:\\n5108.             ar0 = ar0_raw.chunk({})\\n5109.         else:\\n5110.             ar0 = ar0_raw\\n5111. \\n5112.         assert_identical(ar0, ar0)\\n5113. \\n5114.         # No dimension specified\\n5115.         with pytest.raises(ValueError):\\n5116.             ar0.idxmin()\\n5117. \\n5118.         # dim doesn\\\'t exist\\n5119.         with pytest.raises(KeyError):\\n5120.             ar0.idxmin(dim="Y")\\n5121. \\n5122.         assert_identical(ar0, ar0)\\n5123. \\n5124.         coordarr0 = xr.DataArray(\\n5125.             np.tile(ar0.coords["x"], [x.shape[0], 1]), dims=ar0.dims, coords=ar0.coords\\n5126.         )\\n5127. \\n5128.         hasna = [np.isnan(x) for x in minindex]\\n5129.         coordarr1 = coordarr0.copy()\\n5130.         coordarr1[hasna, :] = 1\\n5131.         minindex0 = [x if not np.isnan(x) else 0 for x in minindex]\\n5132. \\n5133.         nan_mult_0 = np.array([np.NaN if x else 1 for x in hasna])[:, None]\\n5134.         expected0 = [\\n5135.             (coordarr1 * nan_mult_0).isel(y=yi).isel(x=indi, drop=True)\\n5136.             for yi, indi in enumerate(minindex0)\\n5137.         ]\\n5138.         expected0 = xr.concat(expected0, dim="y")\\n5139.         expected0.name = "x"\\n5140. \\n5141.         # Default fill value (NaN)\\n5142.         with raise_if_dask_computes(max_computes=max_computes):\\n5143.             result0 = ar0.idxmin(dim="x")\\n5144.         assert_identical(result0, expected0)\\n5145. \\n5146.         # Manually specify NaN fill_value\\n5147.         with raise_if_dask_computes(max_computes=max_computes):\\n5148.             result1 = ar0.idxmin(dim="x", fill_value=np.NaN)\\n5149.         assert_identical(result1, expected0)\\n5150. \\n5151.         # keep_attrs\\n5152.         with raise_if_dask_computes(max_computes=max_computes):\\n5153.             result2 = ar0.idxmin(dim="x", keep_attrs=True)\\n5154.         expected2 = expected0.copy()\\n5155.         expected2.attrs = self.attrs\\n5156.         assert_identical(result2, expected2)\\n5157. \\n5158.         # skipna=False\\n5159.         minindex3 = [\\n5160.             x if y is None or ar0.dtype.kind == "O" else y\\n5161.             for x, y in zip(minindex0, nanindex)\\n5162.         ]\\n5163.         expected3 = [\\n5164.             coordarr0.isel(y=yi).isel(x=indi, drop=True)\\n5165.             for yi, indi in enumerate(minindex3)\\n5166.         ]\\n5167.         expected3 = xr.concat(expected3, dim="y")\\n5168.         expected3.name = "x"\\n5169.         expected3.attrs = {}\\n5170. \\n5171.         with raise_if_dask_computes(max_computes=max_computes):\\n5172.             result3 = ar0.idxmin(dim="x", skipna=False)\\n5173.         assert_identical(result3, expected3)\\n5174. \\n5175.         # fill_value should be ignored with skipna=False\\n5176.         with raise_if_dask_computes(max_computes=max_computes):\\n5177.             result4 = ar0.idxmin(dim="x", skipna=False, fill_value=-100j)\\n5178.         assert_identical(result4, expected3)\\n5179. \\n5180.         # Float fill_value\\n5181.         nan_mult_5 = np.array([-1.1 if x else 1 for x in hasna])[:, None]\\n5182.         expected5 = [\\n5183.             (coordarr1 * nan_mult_5).isel(y=yi).isel(x=indi, drop=True)\\n5184.             for yi, indi in enumerate(minindex0)\\n5185.         ]\\n5186.         expected5 = xr.concat(expected5, dim="y")\\n5187.         expected5.name = "x"\\n5188. \\n5189.         with raise_if_dask_computes(max_computes=max_computes):\\n5190.             result5 = ar0.idxmin(dim="x", fill_value=-1.1)\\n5191.         assert_identical(result5, expected5)\\n5192. \\n5193.         # Integer fill_value\\n5194.         nan_mult_6 = np.array([-1 if x else 1 for x in hasna])[:, None]\\n5195.         expected6 = [\\n5196.             (coordarr1 * nan_mult_6).isel(y=yi).isel(x=indi, drop=True)\\n5197.             for yi, indi in enumerate(minindex0)\\n5198.         ]\\n5199.         expected6 = xr.concat(expected6, dim="y")\\n5200.         expected6.name = "x"\\n5201. \\n5202.         with raise_if_dask_computes(max_computes=max_computes):\\n5203.             result6 = ar0.idxmin(dim="x", fill_value=-1)\\n5204.         assert_identical(result6, expected6)\\n5205. \\n5206.         # Complex fill_value\\n5207.         nan_mult_7 = np.array([-5j if x else 1 for x in hasna])[:, None]\\n5208.         expected7 = [\\n5209.             (coordarr1 * nan_mult_7).isel(y=yi).isel(x=indi, drop=True)\\n5210.             for yi, indi in enumerate(minindex0)\\n5211.         ]\\n5212.         expected7 = xr.concat(expected7, dim="y")\\n5213.         expected7.name = "x"\\n5214. \\n5215.         with raise_if_dask_computes(max_computes=max_computes):\\n5216.             result7 = ar0.idxmin(dim="x", fill_value=-5j)\\n5217.         assert_identical(result7, expected7)\\n5218. \\n5219.     @pytest.mark.parametrize("use_dask", [True, False])\\n5220.     def test_idxmax(self, x, minindex, maxindex, nanindex, use_dask):\\n5221.         if use_dask and not has_dask:\\n5222.             pytest.skip("requires dask")\\n5223.         if use_dask and x.dtype.kind == "M":\\n5224.             pytest.xfail("dask operation \\\'argmax\\\' breaks when dtype is datetime64 (M)")\\n5225. \\n5226.         if x.dtype.kind == "O":\\n5227.             # TODO: nanops._nan_argminmax_object computes once to check for all-NaN slices.\\n5228.             max_computes = 1\\n5229.         else:\\n5230.             max_computes = 0\\n5231. \\n5232.         ar0_raw = xr.DataArray(\\n5233.             x,\\n5234.             dims=["y", "x"],\\n5235.             coords={"x": np.arange(x.shape[1]) * 4, "y": 1 - np.arange(x.shape[0])},\\n5236.             attrs=self.attrs,\\n5237.         )\\n5238. \\n5239.         if use_dask:\\n5240.             ar0 = ar0_raw.chunk({})\\n5241.         else:\\n5242.             ar0 = ar0_raw\\n5243. \\n5244.         # No dimension specified\\n5245.         with pytest.raises(ValueError):\\n5246.             ar0.idxmax()\\n5247. \\n5248.         # dim doesn\\\'t exist\\n5249.         with pytest.raises(KeyError):\\n5250.             ar0.idxmax(dim="Y")\\n5251. \\n5252.         ar1 = ar0.copy()\\n5253.         del ar1.coords["y"]\\n5254.         with pytest.raises(KeyError):\\n5255.             ar1.idxmax(dim="y")\\n5256. \\n5257.         coordarr0 = xr.DataArray(\\n5258.             np.tile(ar0.coords["x"], [x.shape[0], 1]), dims=ar0.dims, coords=ar0.coords\\n5259.         )\\n5260. \\n5261.         hasna = [np.isnan(x) for x in maxindex]\\n5262.         coordarr1 = coordarr0.copy()\\n5263.         coordarr1[hasna, :] = 1\\n5264.         maxindex0 = [x if not np.isnan(x) else 0 for x in maxindex]\\n5265. \\n5266.         nan_mult_0 = np.array([np.NaN if x else 1 for x in hasna])[:, None]\\n5267.         expected0 = [\\n5268.             (coordarr1 * nan_mult_0).isel(y=yi).isel(x=indi, drop=True)\\n5269.             for yi, indi in enumerate(maxindex0)\\n5270.         ]\\n5271.         expected0 = xr.concat(expected0, dim="y")\\n5272.         expected0.name = "x"\\n5273. \\n5274.         # Default fill value (NaN)\\n5275.         with raise_if_dask_computes(max_computes=max_computes):\\n5276.             result0 = ar0.idxmax(dim="x")\\n5277.         assert_identical(result0, expected0)\\n5278. \\n5279.         # Manually specify NaN fill_value\\n5280.         with raise_if_dask_computes(max_computes=max_computes):\\n5281.             result1 = ar0.idxmax(dim="x", fill_value=np.NaN)\\n5282.         assert_identical(result1, expected0)\\n5283. \\n5284.         # keep_attrs\\n5285.         with raise_if_dask_computes(max_computes=max_computes):\\n5286.             result2 = ar0.idxmax(dim="x", keep_attrs=True)\\n5287.         expected2 = expected0.copy()\\n5288.         expected2.attrs = self.attrs\\n5289.         assert_identical(result2, expected2)\\n5290. \\n5291.         # skipna=False\\n5292.         maxindex3 = [\\n5293.             x if y is None or ar0.dtype.kind == "O" else y\\n5294.             for x, y in zip(maxindex0, nanindex)\\n5295.         ]\\n5296.         expected3 = [\\n5297.             coordarr0.isel(y=yi).isel(x=indi, drop=True)\\n5298.             for yi, indi in enumerate(maxindex3)\\n5299.         ]\\n5300.         expected3 = xr.concat(expected3, dim="y")\\n5301.         expected3.name = "x"\\n5302.         expected3.attrs = {}\\n5303. \\n5304.         with raise_if_dask_computes(max_computes=max_computes):\\n5305.             result3 = ar0.idxmax(dim="x", skipna=False)\\n5306.         assert_identical(result3, expected3)\\n5307. \\n5308.         # fill_value should be ignored with skipna=False\\n5309.         with raise_if_dask_computes(max_computes=max_computes):\\n5310.             result4 = ar0.idxmax(dim="x", skipna=False, fill_value=-100j)\\n5311.         assert_identical(result4, expected3)\\n5312. \\n5313.         # Float fill_value\\n5314.         nan_mult_5 = np.array([-1.1 if x else 1 for x in hasna])[:, None]\\n5315.         expected5 = [\\n5316.             (coordarr1 * nan_mult_5).isel(y=yi).isel(x=indi, drop=True)\\n5317.             for yi, indi in enumerate(maxindex0)\\n5318.         ]\\n5319.         expected5 = xr.concat(expected5, dim="y")\\n5320.         expected5.name = "x"\\n5321. \\n5322.         with raise_if_dask_computes(max_computes=max_computes):\\n5323.             result5 = ar0.idxmax(dim="x", fill_value=-1.1)\\n5324.         assert_identical(result5, expected5)\\n5325. \\n5326.         # Integer fill_value\\n5327.         nan_mult_6 = np.array([-1 if x else 1 for x in hasna])[:, None]\\n5328.         expected6 = [\\n5329.             (coordarr1 * nan_mult_6).isel(y=yi).isel(x=indi, drop=True)\\n5330.             for yi, indi in enumerate(maxindex0)\\n5331.         ]\\n5332.         expected6 = xr.concat(expected6, dim="y")\\n5333.         expected6.name = "x"\\n5334. \\n5335.         with raise_if_dask_computes(max_computes=max_computes):\\n5336.             result6 = ar0.idxmax(dim="x", fill_value=-1)\\n5337.         assert_identical(result6, expected6)\\n5338. \\n5339.         # Complex fill_value\\n5340.         nan_mult_7 = np.array([-5j if x else 1 for x in hasna])[:, None]\\n5341.         expected7 = [\\n5342.             (coordarr1 * nan_mult_7).isel(y=yi).isel(x=indi, drop=True)\\n5343.             for yi, indi in enumerate(maxindex0)\\n5344.         ]\\n5345.         expected7 = xr.concat(expected7, dim="y")\\n5346.         expected7.name = "x"\\n5347. \\n5348.         with raise_if_dask_computes(max_computes=max_computes):\\n5349.             result7 = ar0.idxmax(dim="x", fill_value=-5j)\\n5350.         assert_identical(result7, expected7)\\n5351. \\n5352.     @pytest.mark.filterwarnings(\\n5353.         "ignore:Behaviour of argmin/argmax with neither dim nor :DeprecationWarning"\\n5354.     )\\n5355.     def test_argmin_dim(self, x, minindex, maxindex, nanindex):\\n5356.         ar = xr.DataArray(\\n5357.             x,\\n5358.             dims=["y", "x"],\\n5359.             coords={"x": np.arange(x.shape[1]) * 4, "y": 1 - np.arange(x.shape[0])},\\n5360.             attrs=self.attrs,\\n5361.         )\\n5362.         indarr = np.tile(np.arange(x.shape[1], dtype=np.intp), [x.shape[0], 1])\\n5363.         indarr = xr.DataArray(indarr, dims=ar.dims, coords=ar.coords)\\n5364. \\n5365.         if np.isnan(minindex).any():\\n5366.             with pytest.raises(ValueError):\\n5367.                 ar.argmin(dim="x")\\n5368.             return\\n5369. \\n5370.         expected0 = [\\n5371.             indarr.isel(y=yi).isel(x=indi, drop=True)\\n5372.             for yi, indi in enumerate(minindex)\\n5373.         ]\\n5374.         expected0 = {"x": xr.concat(expected0, dim="y")}\\n5375. \\n5376.         result0 = ar.argmin(dim=["x"])\\n5377.         for key in expected0:\\n5378.             assert_identical(result0[key], expected0[key])\\n5379. \\n5380.         result1 = ar.argmin(dim=["x"], keep_attrs=True)\\n5381.         expected1 = deepcopy(expected0)\\n5382.         expected1["x"].attrs = self.attrs\\n5383.         for key in expected1:\\n5384.             assert_identical(result1[key], expected1[key])\\n5385. \\n5386.         minindex = [\\n5387.             x if y is None or ar.dtype.kind == "O" else y\\n5388.             for x, y in zip(minindex, nanindex)\\n5389.         ]\\n5390.         expected2 = [\\n5391.             indarr.isel(y=yi).isel(x=indi, drop=True)\\n5392.             for yi, indi in enumerate(minindex)\\n5393.         ]\\n5394.         expected2 = {"x": xr.concat(expected2, dim="y")}\\n5395.         expected2["x"].attrs = {}\\n5396. \\n5397.         result2 = ar.argmin(dim=["x"], skipna=False)\\n5398. \\n5399.         for key in expected2:\\n5400.             assert_identical(result2[key], expected2[key])\\n5401. \\n5402.         result3 = ar.argmin(...)\\n5403.         min_xind = ar.isel(expected0).argmin()\\n5404.         expected3 = {\\n5405.             "y": DataArray(min_xind),\\n5406.             "x": DataArray(minindex[min_xind.item()]),\\n5407.         }\\n5408. \\n5409.         for key in expected3:\\n5410.             assert_identical(result3[key], expected3[key])\\n5411. \\n5412.     @pytest.mark.filterwarnings(\\n5413.         "ignore:Behaviour of argmin/argmax with neither dim nor :DeprecationWarning"\\n5414.     )\\n5415.     def test_argmax_dim(self, x, minindex, maxindex, nanindex):\\n5416.         ar = xr.DataArray(\\n5417.             x,\\n5418.             dims=["y", "x"],\\n5419.             coords={"x": np.arange(x.shape[1]) * 4, "y": 1 - np.arange(x.shape[0])},\\n5420.             attrs=self.attrs,\\n5421.         )\\n5422.         indarr = np.tile(np.arange(x.shape[1], dtype=np.intp), [x.shape[0], 1])\\n5423.         indarr = xr.DataArray(indarr, dims=ar.dims, coords=ar.coords)\\n5424. \\n5425.         if np.isnan(maxindex).any():\\n5426.             with pytest.raises(ValueError):\\n5427.                 ar.argmax(dim="x")\\n5428.             return\\n5429. \\n5430.         expected0 = [\\n5431.             indarr.isel(y=yi).isel(x=indi, drop=True)\\n5432.             for yi, indi in enumerate(maxindex)\\n5433.         ]\\n5434.         expected0 = {"x": xr.concat(expected0, dim="y")}\\n5435. \\n5436.         result0 = ar.argmax(dim=["x"])\\n5437.         for key in expected0:\\n5438.             assert_identical(result0[key], expected0[key])\\n5439. \\n5440.         result1 = ar.argmax(dim=["x"], keep_attrs=True)\\n5441.         expected1 = deepcopy(expected0)\\n5442.         expected1["x"].attrs = self.attrs\\n5443.         for key in expected1:\\n5444.             assert_identical(result1[key], expected1[key])\\n5445. \\n5446.         maxindex = [\\n5447.             x if y is None or ar.dtype.kind == "O" else y\\n5448.             for x, y in zip(maxindex, nanindex)\\n5449.         ]\\n5450.         expected2 = [\\n5451.             indarr.isel(y=yi).isel(x=indi, drop=True)\\n5452.             for yi, indi in enumerate(maxindex)\\n5453.         ]\\n5454.         expected2 = {"x": xr.concat(expected2, dim="y")}\\n5455.         expected2["x"].attrs = {}\\n5456. \\n5457.         result2 = ar.argmax(dim=["x"], skipna=False)\\n5458. \\n5459.         for key in expected2:\\n5460.             assert_identical(result2[key], expected2[key])\\n5461. \\n5462.         result3 = ar.argmax(...)\\n5463.         max_xind = ar.isel(expected0).argmax()\\n5464.         expected3 = {\\n5465.             "y": DataArray(max_xind),\\n5466.             "x": DataArray(maxindex[max_xind.item()]),\\n5467.         }\\n5468. \\n5469.         for key in expected3:\\n5470.             assert_identical(result3[key], expected3[key])\\n5471. \\n5472. \\n5473. @pytest.mark.parametrize(\\n5474.     "x, minindices_x, minindices_y, minindices_z, minindices_xy, "\\n5475.     "minindices_xz, minindices_yz, minindices_xyz, maxindices_x, "\\n5476.     "maxindices_y, maxindices_z, maxindices_xy, maxindices_xz, maxindices_yz, "\\n5477.     "maxindices_xyz, nanindices_x, nanindices_y, nanindices_z, nanindices_xy, "\\n5478.     "nanindices_xz, nanindices_yz, nanindices_xyz",\\n5479.     [\\n5480.         (\\n5481.             np.array(\\n5482.                 [\\n5483.                     [[0, 1, 2, 0], [-2, -4, 2, 0]],\\n5484.                     [[1, 1, 1, 1], [1, 1, 1, 1]],\\n5485.                     [[0, 0, -10, 5], [20, 0, 0, 0]],\\n5486.                 ]\\n5487.             ),\\n5488.             {"x": np.array([[0, 2, 2, 0], [0, 0, 2, 0]])},\\n5489.             {"y": np.array([[1, 1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1]])},\\n5490.             {"z": np.array([[0, 1], [0, 0], [2, 1]])},\\n5491.             {"x": np.array([0, 0, 2, 0]), "y": np.array([1, 1, 0, 0])},\\n5492.             {"x": np.array([2, 0]), "z": np.array([2, 1])},\\n5493.             {"y": np.array([1, 0, 0]), "z": np.array([1, 0, 2])},\\n5494.             {"x": np.array(2), "y": np.array(0), "z": np.array(2)},\\n5495.             {"x": np.array([[1, 0, 0, 2], [2, 1, 0, 1]])},\\n5496.             {"y": np.array([[0, 0, 0, 0], [0, 0, 0, 0], [1, 0, 1, 0]])},\\n5497.             {"z": np.array([[2, 2], [0, 0], [3, 0]])},\\n5498.             {"x": np.array([2, 0, 0, 2]), "y": np.array([1, 0, 0, 0])},\\n5499.             {"x": np.array([2, 2]), "z": np.array([3, 0])},\\n5500.             {"y": np.array([0, 0, 1]), "z": np.array([2, 0, 0])},\\n5501.             {"x": np.array(2), "y": np.array(1), "z": np.array(0)},\\n5502.             {"x": np.array([[None, None, None, None], [None, None, None, None]])},\\n5503.             {\\n5504.                 "y": np.array(\\n5505.                     [\\n5506.                         [None, None, None, None],\\n5507.                         [None, None, None, None],\\n5508.                         [None, None, None, None],\\n5509.                     ]\\n5510.                 )\\n5511.             },\\n5512.             {"z": np.array([[None, None], [None, None], [None, None]])},\\n5513.             {\\n5514.                 "x": np.array([None, None, None, None]),\\n5515.                 "y": np.array([None, None, None, None]),\\n5516.             },\\n5517.             {"x": np.array([None, None]), "z": np.array([None, None])},\\n5518.             {"y": np.array([None, None, None]), "z": np.array([None, None, None])},\\n5519.             {"x": np.array(None), "y": np.array(None), "z": np.array(None)},\\n5520.         ),\\n5521.         (\\n5522.             np.array(\\n5523.                 [\\n5524.                     [[2.0, 1.0, 2.0, 0.0], [-2.0, -4.0, 2.0, 0.0]],\\n5525.                     [[-4.0, np.NaN, 2.0, np.NaN], [-2.0, -4.0, 2.0, 0.0]],\\n5526.                     [[np.NaN] * 4, [np.NaN] * 4],\\n5527.                 ]\\n5528.             ),\\n5529.             {"x": np.array([[1, 0, 0, 0], [0, 0, 0, 0]])},\\n5530.             {\\n5531.                 "y": np.array(\\n5532.                     [[1, 1, 0, 0], [0, 1, 0, 1], [np.NaN, np.NaN, np.NaN, np.NaN]]\\n5533.                 )\\n5534.             },\\n5535.             {"z": np.array([[3, 1], [0, 1], [np.NaN, np.NaN]])},\\n5536.             {"x": np.array([1, 0, 0, 0]), "y": np.array([0, 1, 0, 0])},\\n5537.             {"x": np.array([1, 0]), "z": np.array([0, 1])},\\n5538.             {"y": np.array([1, 0, np.NaN]), "z": np.array([1, 0, np.NaN])},\\n5539.             {"x": np.array(0), "y": np.array(1), "z": np.array(1)},\\n5540.             {"x": np.array([[0, 0, 0, 0], [0, 0, 0, 0]])},\\n5541.             {\\n5542.                 "y": np.array(\\n5543.                     [[0, 0, 0, 0], [1, 1, 0, 1], [np.NaN, np.NaN, np.NaN, np.NaN]]\\n5544.                 )\\n5545.             },\\n5546.             {"z": np.array([[0, 2], [2, 2], [np.NaN, np.NaN]])},\\n5547.             {"x": np.array([0, 0, 0, 0]), "y": np.array([0, 0, 0, 0])},\\n5548.             {"x": np.array([0, 0]), "z": np.array([2, 2])},\\n5549.             {"y": np.array([0, 0, np.NaN]), "z": np.array([0, 2, np.NaN])},\\n5550.             {"x": np.array(0), "y": np.array(0), "z": np.array(0)},\\n5551.             {"x": np.array([[2, 1, 2, 1], [2, 2, 2, 2]])},\\n5552.             {\\n5553.                 "y": np.array(\\n5554.                     [[None, None, None, None], [None, 0, None, 0], [0, 0, 0, 0]]\\n5555.                 )\\n5556.             },\\n5557.             {"z": np.array([[None, None], [1, None], [0, 0]])},\\n5558.             {"x": np.array([2, 1, 2, 1]), "y": np.array([0, 0, 0, 0])},\\n5559.             {"x": np.array([1, 2]), "z": np.array([1, 0])},\\n5560.             {"y": np.array([None, 0, 0]), "z": np.array([None, 1, 0])},\\n5561.             {"x": np.array(1), "y": np.array(0), "z": np.array(1)},\\n5562.         ),\\n5563.         (\\n5564.             np.array(\\n5565.                 [\\n5566.                     [[2.0, 1.0, 2.0, 0.0], [-2.0, -4.0, 2.0, 0.0]],\\n5567.                     [[-4.0, np.NaN, 2.0, np.NaN], [-2.0, -4.0, 2.0, 0.0]],\\n5568.                     [[np.NaN] * 4, [np.NaN] * 4],\\n5569.                 ]\\n5570.             ).astype("object"),\\n5571.             {"x": np.array([[1, 0, 0, 0], [0, 0, 0, 0]])},\\n5572.             {\\n5573.                 "y": np.array(\\n5574.                     [[1, 1, 0, 0], [0, 1, 0, 1], [np.NaN, np.NaN, np.NaN, np.NaN]]\\n5575.                 )\\n5576.             },\\n5577.             {"z": np.array([[3, 1], [0, 1], [np.NaN, np.NaN]])},\\n5578.             {"x": np.array([1, 0, 0, 0]), "y": np.array([0, 1, 0, 0])},\\n5579.             {"x": np.array([1, 0]), "z": np.array([0, 1])},\\n5580.             {"y": np.array([1, 0, np.NaN]), "z": np.array([1, 0, np.NaN])},\\n5581.             {"x": np.array(0), "y": np.array(1), "z": np.array(1)},\\n5582.             {"x": np.array([[0, 0, 0, 0], [0, 0, 0, 0]])},\\n5583.             {\\n5584.                 "y": np.array(\\n5585.                     [[0, 0, 0, 0], [1, 1, 0, 1], [np.NaN, np.NaN, np.NaN, np.NaN]]\\n5586.                 )\\n5587.             },\\n5588.             {"z": np.array([[0, 2], [2, 2], [np.NaN, np.NaN]])},\\n5589.             {"x": np.array([0, 0, 0, 0]), "y": np.array([0, 0, 0, 0])},\\n5590.             {"x": np.array([0, 0]), "z": np.array([2, 2])},\\n5591.             {"y": np.array([0, 0, np.NaN]), "z": np.array([0, 2, np.NaN])},\\n5592.             {"x": np.array(0), "y": np.array(0), "z": np.array(0)},\\n5593.             {"x": np.array([[2, 1, 2, 1], [2, 2, 2, 2]])},\\n5594.             {\\n5595.                 "y": np.array(\\n5596.                     [[None, None, None, None], [None, 0, None, 0], [0, 0, 0, 0]]\\n5597.                 )\\n5598.             },\\n5599.             {"z": np.array([[None, None], [1, None], [0, 0]])},\\n5600.             {"x": np.array([2, 1, 2, 1]), "y": np.array([0, 0, 0, 0])},\\n5601.             {"x": np.array([1, 2]), "z": np.array([1, 0])},\\n5602.             {"y": np.array([None, 0, 0]), "z": np.array([None, 1, 0])},\\n5603.             {"x": np.array(1), "y": np.array(0), "z": np.array(1)},\\n5604.         ),\\n5605.         (\\n5606.             np.array(\\n5607.                 [\\n5608.                     [["2015-12-31", "2020-01-02"], ["2020-01-01", "2016-01-01"]],\\n5609.                     [["2020-01-02", "2020-01-02"], ["2020-01-02", "2020-01-02"]],\\n5610.                     [["1900-01-01", "1-02-03"], ["1900-01-02", "1-02-03"]],\\n5611.                 ],\\n5612.                 dtype="datetime64[ns]",\\n5613.             ),\\n5614.             {"x": np.array([[2, 2], [2, 2]])},\\n5615.             {"y": np.array([[0, 1], [0, 0], [0, 0]])},\\n5616.             {"z": np.array([[0, 1], [0, 0], [1, 1]])},\\n5617.             {"x": np.array([2, 2]), "y": np.array([0, 0])},\\n5618.             {"x": np.array([2, 2]), "z": np.array([1, 1])},\\n5619.             {"y": np.array([0, 0, 0]), "z": np.array([0, 0, 1])},\\n5620.             {"x": np.array(2), "y": np.array(0), "z": np.array(1)},\\n5621.             {"x": np.array([[1, 0], [1, 1]])},\\n5622.             {"y": np.array([[1, 0], [0, 0], [1, 0]])},\\n5623.             {"z": np.array([[1, 0], [0, 0], [0, 0]])},\\n5624.             {"x": np.array([1, 0]), "y": np.array([0, 0])},\\n5625.             {"x": np.array([0, 1]), "z": np.array([1, 0])},\\n5626.             {"y": np.array([0, 0, 1]), "z": np.array([1, 0, 0])},\\n5627.             {"x": np.array(0), "y": np.array(0), "z": np.array(1)},\\n5628.             {"x": np.array([[None, None], [None, None]])},\\n5629.             {"y": np.array([[None, None], [None, None], [None, None]])},\\n5630.             {"z": np.array([[None, None], [None, None], [None, None]])},\\n5631.             {"x": np.array([None, None]), "y": np.array([None, None])},\\n5632.             {"x": np.array([None, None]), "z": np.array([None, None])},\\n5633.             {"y": np.array([None, None, None]), "z": np.array([None, None, None])},\\n5634.             {"x": np.array(None), "y": np.array(None), "z": np.array(None)},\\n5635.         ),\\n5636.     ],\\n5637. )\\n5638. class TestReduce3D(TestReduce):\\n5639.     def test_argmin_dim(\\n5640.         self,\\n5641.         x,\\n5642.         minindices_x,\\n5643.         minindices_y,\\n5644.         minindices_z,\\n5645.         minindices_xy,\\n5646.         minindices_xz,\\n5647.         minindices_yz,\\n5648.         minindices_xyz,\\n5649.         maxindices_x,\\n5650.         maxindices_y,\\n5651.         maxindices_z,\\n5652.         maxindices_xy,\\n5653.         maxindices_xz,\\n5654.         maxindices_yz,\\n5655.         maxindices_xyz,\\n5656.         nanindices_x,\\n5657.         nanindices_y,\\n5658.         nanindices_z,\\n5659.         nanindices_xy,\\n5660.         nanindices_xz,\\n5661.         nanindices_yz,\\n5662.         nanindices_xyz,\\n5663.     ):\\n5664. \\n5665.         ar = xr.DataArray(\\n5666.             x,\\n5667.             dims=["x", "y", "z"],\\n5668.             coords={\\n5669.                 "x": np.arange(x.shape[0]) * 4,\\n5670.                 "y": 1 - np.arange(x.shape[1]),\\n5671.                 "z": 2 + 3 * np.arange(x.shape[2]),\\n5672.             },\\n5673.             attrs=self.attrs,\\n5674.         )\\n5675.         xindarr = np.tile(\\n5676.             np.arange(x.shape[0], dtype=np.intp)[:, np.newaxis, np.newaxis],\\n5677.             [1, x.shape[1], x.shape[2]],\\n5678.         )\\n5679.         xindarr = xr.DataArray(xindarr, dims=ar.dims, coords=ar.coords)\\n5680.         yindarr = np.tile(\\n5681.             np.arange(x.shape[1], dtype=np.intp)[np.newaxis, :, np.newaxis],\\n5682.             [x.shape[0], 1, x.shape[2]],\\n5683.         )\\n5684.         yindarr = xr.DataArray(yindarr, dims=ar.dims, coords=ar.coords)\\n5685.         zindarr = np.tile(\\n5686.             np.arange(x.shape[2], dtype=np.intp)[np.newaxis, np.newaxis, :],\\n5687.             [x.shape[0], x.shape[1], 1],\\n5688.         )\\n5689.         zindarr = xr.DataArray(zindarr, dims=ar.dims, coords=ar.coords)\\n5690. \\n5691.         for inds in [\\n5692.             minindices_x,\\n5693.             minindices_y,\\n5694.             minindices_z,\\n5695.             minindices_xy,\\n5696.             minindices_xz,\\n5697.             minindices_yz,\\n5698.             minindices_xyz,\\n5699.         ]:\\n5700.             if np.array([np.isnan(i) for i in inds.values()]).any():\\n5701.                 with pytest.raises(ValueError):\\n5702.                     ar.argmin(dim=[d for d in inds])\\n5703.                 return\\n5704. \\n5705.         result0 = ar.argmin(dim=["x"])\\n5706.         expected0 = {\\n5707.             key: xr.DataArray(value, dims=("y", "z"))\\n5708.             for key, value in minindices_x.items()\\n5709.         }\\n5710.         for key in expected0:\\n5711.             assert_identical(result0[key].drop_vars(["y", "z"]), expected0[key])\\n5712. \\n5713.         result1 = ar.argmin(dim=["y"])\\n5714.         expected1 = {\\n5715.             key: xr.DataArray(value, dims=("x", "z"))\\n5716.             for key, value in minindices_y.items()\\n5717.         }\\n5718.         for key in expected1:\\n5719.             assert_identical(result1[key].drop_vars(["x", "z"]), expected1[key])\\n5720. \\n5721.         result2 = ar.argmin(dim=["z"])\\n5722.         expected2 = {\\n5723.             key: xr.DataArray(value, dims=("x", "y"))\\n5724.             for key, value in minindices_z.items()\\n5725.         }\\n5726.         for key in expected2:\\n5727.             assert_identical(result2[key].drop_vars(["x", "y"]), expected2[key])\\n5728. \\n5729.         result3 = ar.argmin(dim=("x", "y"))\\n5730.         expected3 = {\\n5731.             key: xr.DataArray(value, dims=("z")) for key, value in minindices_xy.items()\\n5732.         }\\n5733.         for key in expected3:\\n5734.             assert_identical(result3[key].drop_vars("z"), expected3[key])\\n5735. \\n5736.         result4 = ar.argmin(dim=("x", "z"))\\n5737.         expected4 = {\\n5738.             key: xr.DataArray(value, dims=("y")) for key, value in minindices_xz.items()\\n5739.         }\\n5740.         for key in expected4:\\n5741.             assert_identical(result4[key].drop_vars("y"), expected4[key])\\n5742. \\n5743.         result5 = ar.argmin(dim=("y", "z"))\\n5744.         expected5 = {\\n5745.             key: xr.DataArray(value, dims=("x")) for key, value in minindices_yz.items()\\n5746.         }\\n5747.         for key in expected5:\\n5748.             assert_identical(result5[key].drop_vars("x"), expected5[key])\\n5749. \\n5750.         result6 = ar.argmin(...)\\n5751.         expected6 = {key: xr.DataArray(value) for key, value in minindices_xyz.items()}\\n5752.         for key in expected6:\\n5753.             assert_identical(result6[key], expected6[key])\\n5754. \\n5755.         minindices_x = {\\n5756.             key: xr.where(\\n5757.                 nanindices_x[key] == None,  # noqa: E711\\n5758.                 minindices_x[key],\\n5759.                 nanindices_x[key],\\n5760.             )\\n5761.             for key in minindices_x\\n5762.         }\\n5763.         expected7 = {\\n5764.             key: xr.DataArray(value, dims=("y", "z"))\\n5765.             for key, value in minindices_x.items()\\n5766.         }\\n5767. \\n5768.         result7 = ar.argmin(dim=["x"], skipna=False)\\n5769.         for key in expected7:\\n5770.             assert_identical(result7[key].drop_vars(["y", "z"]), expected7[key])\\n5771. \\n5772.         minindices_y = {\\n5773.             key: xr.where(\\n5774.                 nanindices_y[key] == None,  # noqa: E711\\n5775.                 minindices_y[key],\\n5776.                 nanindices_y[key],\\n5777.             )\\n5778.             for key in minindices_y\\n5779.         }\\n5780.         expected8 = {\\n5781.             key: xr.DataArray(value, dims=("x", "z"))\\n5782.             for key, value in minindices_y.items()\\n5783.         }\\n5784. \\n5785.         result8 = ar.argmin(dim=["y"], skipna=False)\\n5786.         for key in expected8:\\n5787.             assert_identical(result8[key].drop_vars(["x", "z"]), expected8[key])\\n5788. \\n5789.         minindices_z = {\\n5790.             key: xr.where(\\n5791.                 nanindices_z[key] == None,  # noqa: E711\\n5792.                 minindices_z[key],\\n5793.                 nanindices_z[key],\\n5794.             )\\n5795.             for key in minindices_z\\n5796.         }\\n5797.         expected9 = {\\n5798.             key: xr.DataArray(value, dims=("x", "y"))\\n5799.             for key, value in minindices_z.items()\\n5800.         }\\n5801. \\n5802.         result9 = ar.argmin(dim=["z"], skipna=False)\\n5803.         for key in expected9:\\n5804.             assert_identical(result9[key].drop_vars(["x", "y"]), expected9[key])\\n5805. \\n5806.         minindices_xy = {\\n5807.             key: xr.where(\\n5808.                 nanindices_xy[key] == None,  # noqa: E711\\n5809.                 minindices_xy[key],\\n5810.                 nanindices_xy[key],\\n5811.             )\\n5812.             for key in minindices_xy\\n5813.         }\\n5814.         expected10 = {\\n5815.             key: xr.DataArray(value, dims="z") for key, value in minindices_xy.items()\\n5816.         }\\n5817. \\n5818.         result10 = ar.argmin(dim=("x", "y"), skipna=False)\\n5819.         for key in expected10:\\n5820.             assert_identical(result10[key].drop_vars("z"), expected10[key])\\n5821. \\n5822.         minindices_xz = {\\n5823.             key: xr.where(\\n5824.                 nanindices_xz[key] == None,  # noqa: E711\\n5825.                 minindices_xz[key],\\n5826.                 nanindices_xz[key],\\n5827.             )\\n5828.             for key in minindices_xz\\n5829.         }\\n5830.         expected11 = {\\n5831.             key: xr.DataArray(value, dims="y") for key, value in minindices_xz.items()\\n5832.         }\\n5833. \\n5834.         result11 = ar.argmin(dim=("x", "z"), skipna=False)\\n5835.         for key in expected11:\\n5836.             assert_identical(result11[key].drop_vars("y"), expected11[key])\\n5837. \\n5838.         minindices_yz = {\\n5839.             key: xr.where(\\n5840.                 nanindices_yz[key] == None,  # noqa: E711\\n5841.                 minindices_yz[key],\\n5842.                 nanindices_yz[key],\\n5843.             )\\n5844.             for key in minindices_yz\\n5845.         }\\n5846.         expected12 = {\\n5847.             key: xr.DataArray(value, dims="x") for key, value in minindices_yz.items()\\n5848.         }\\n5849. \\n5850.         result12 = ar.argmin(dim=("y", "z"), skipna=False)\\n5851.         for key in expected12:\\n5852.             assert_identical(result12[key].drop_vars("x"), expected12[key])\\n5853. \\n5854.         minindices_xyz = {\\n5855.             key: xr.where(\\n5856.                 nanindices_xyz[key] == None,  # noqa: E711\\n5857.                 minindices_xyz[key],\\n5858.                 nanindices_xyz[key],\\n5859.             )\\n5860.             for key in minindices_xyz\\n5861.         }\\n5862.         expected13 = {key: xr.DataArray(value) for key, value in minindices_xyz.items()}\\n5863. \\n5864.         result13 = ar.argmin(..., skipna=False)\\n5865.         for key in expected13:\\n5866.             assert_identical(result13[key], expected13[key])\\n5867. \\n5868.     def test_argmax_dim(\\n5869.         self,\\n5870.         x,\\n5871.         minindices_x,\\n5872.         minindices_y,\\n5873.         minindices_z,\\n5874.         minindices_xy,\\n5875.         minindices_xz,\\n5876.         minindices_yz,\\n5877.         minindices_xyz,\\n5878.         maxindices_x,\\n5879.         maxindices_y,\\n5880.         maxindices_z,\\n5881.         maxindices_xy,\\n5882.         maxindices_xz,\\n5883.         maxindices_yz,\\n5884.         maxindices_xyz,\\n5885.         nanindices_x,\\n5886.         nanindices_y,\\n5887.         nanindices_z,\\n5888.         nanindices_xy,\\n5889.         nanindices_xz,\\n5890.         nanindices_yz,\\n5891.         nanindices_xyz,\\n5892.     ):\\n5893. \\n5894.         ar = xr.DataArray(\\n5895.             x,\\n5896.             dims=["x", "y", "z"],\\n5897.             coords={\\n5898.                 "x": np.arange(x.shape[0]) * 4,\\n5899.                 "y": 1 - np.arange(x.shape[1]),\\n5900.                 "z": 2 + 3 * np.arange(x.shape[2]),\\n5901.             },\\n5902.             attrs=self.attrs,\\n5903.         )\\n5904.         xindarr = np.tile(\\n5905.             np.arange(x.shape[0], dtype=np.intp)[:, np.newaxis, np.newaxis],\\n5906.             [1, x.shape[1], x.shape[2]],\\n5907.         )\\n5908.         xindarr = xr.DataArray(xindarr, dims=ar.dims, coords=ar.coords)\\n5909.         yindarr = np.tile(\\n5910.             np.arange(x.shape[1], dtype=np.intp)[np.newaxis, :, np.newaxis],\\n5911.             [x.shape[0], 1, x.shape[2]],\\n5912.         )\\n5913.         yindarr = xr.DataArray(yindarr, dims=ar.dims, coords=ar.coords)\\n5914.         zindarr = np.tile(\\n5915.             np.arange(x.shape[2], dtype=np.intp)[np.newaxis, np.newaxis, :],\\n5916.             [x.shape[0], x.shape[1], 1],\\n5917.         )\\n5918.         zindarr = xr.DataArray(zindarr, dims=ar.dims, coords=ar.coords)\\n5919. \\n5920.         for inds in [\\n5921.             maxindices_x,\\n5922.             maxindices_y,\\n5923.             maxindices_z,\\n5924.             maxindices_xy,\\n5925.             maxindices_xz,\\n5926.             maxindices_yz,\\n5927.             maxindices_xyz,\\n5928.         ]:\\n5929.             if np.array([np.isnan(i) for i in inds.values()]).any():\\n5930.                 with pytest.raises(ValueError):\\n5931.                     ar.argmax(dim=[d for d in inds])\\n5932.                 return\\n5933. \\n5934.         result0 = ar.argmax(dim=["x"])\\n5935.         expected0 = {\\n5936.             key: xr.DataArray(value, dims=("y", "z"))\\n5937.             for key, value in maxindices_x.items()\\n5938.         }\\n5939.         for key in expected0:\\n5940.             assert_identical(result0[key].drop_vars(["y", "z"]), expected0[key])\\n5941. \\n5942.         result1 = ar.argmax(dim=["y"])\\n5943.         expected1 = {\\n5944.             key: xr.DataArray(value, dims=("x", "z"))\\n5945.             for key, value in maxindices_y.items()\\n5946.         }\\n5947.         for key in expected1:\\n5948.             assert_identical(result1[key].drop_vars(["x", "z"]), expected1[key])\\n5949. \\n5950.         result2 = ar.argmax(dim=["z"])\\n5951.         expected2 = {\\n5952.             key: xr.DataArray(value, dims=("x", "y"))\\n5953.             for key, value in maxindices_z.items()\\n5954.         }\\n5955.         for key in expected2:\\n5956.             assert_identical(result2[key].drop_vars(["x", "y"]), expected2[key])\\n5957. \\n5958.         result3 = ar.argmax(dim=("x", "y"))\\n5959.         expected3 = {\\n5960.             key: xr.DataArray(value, dims=("z")) for key, value in maxindices_xy.items()\\n5961.         }\\n5962.         for key in expected3:\\n5963.             assert_identical(result3[key].drop_vars("z"), expected3[key])\\n5964. \\n5965.         result4 = ar.argmax(dim=("x", "z"))\\n5966.         expected4 = {\\n5967.             key: xr.DataArray(value, dims=("y")) for key, value in maxindices_xz.items()\\n5968.         }\\n5969.         for key in expected4:\\n5970.             assert_identical(result4[key].drop_vars("y"), expected4[key])\\n5971. \\n5972.         result5 = ar.argmax(dim=("y", "z"))\\n5973.         expected5 = {\\n5974.             key: xr.DataArray(value, dims=("x")) for key, value in maxindices_yz.items()\\n5975.         }\\n5976.         for key in expected5:\\n5977.             assert_identical(result5[key].drop_vars("x"), expected5[key])\\n5978. \\n5979.         result6 = ar.argmax(...)\\n5980.         expected6 = {key: xr.DataArray(value) for key, value in maxindices_xyz.items()}\\n5981.         for key in expected6:\\n5982.             assert_identical(result6[key], expected6[key])\\n5983. \\n5984.         maxindices_x = {\\n5985.             key: xr.where(\\n5986.                 nanindices_x[key] == None,  # noqa: E711\\n5987.                 maxindices_x[key],\\n5988.                 nanindices_x[key],\\n5989.             )\\n5990.             for key in maxindices_x\\n5991.         }\\n5992.         expected7 = {\\n5993.             key: xr.DataArray(value, dims=("y", "z"))\\n5994.             for key, value in maxindices_x.items()\\n5995.         }\\n5996. \\n5997.         result7 = ar.argmax(dim=["x"], skipna=False)\\n5998.         for key in expected7:\\n5999.             assert_identical(result7[key].drop_vars(["y", "z"]), expected7[key])\\n6000. \\n6001.         maxindices_y = {\\n6002.             key: xr.where(\\n6003.                 nanindices_y[key] == None,  # noqa: E711\\n6004.                 maxindices_y[key],\\n6005.                 nanindices_y[key],\\n6006.             )\\n6007.             for key in maxindices_y\\n6008.         }\\n6009.         expected8 = {\\n6010.             key: xr.DataArray(value, dims=("x", "z"))\\n6011.             for key, value in maxindices_y.items()\\n6012.         }\\n6013. \\n6014.         result8 = ar.argmax(dim=["y"], skipna=False)\\n6015.         for key in expected8:\\n6016.             assert_identical(result8[key].drop_vars(["x", "z"]), expected8[key])\\n6017. \\n6018.         maxindices_z = {\\n6019.             key: xr.where(\\n6020.                 nanindices_z[key] == None,  # noqa: E711\\n6021.                 maxindices_z[key],\\n6022.                 nanindices_z[key],\\n6023.             )\\n6024.             for key in maxindices_z\\n6025.         }\\n6026.         expected9 = {\\n6027.             key: xr.DataArray(value, dims=("x", "y"))\\n6028.             for key, value in maxindices_z.items()\\n6029.         }\\n6030. \\n6031.         result9 = ar.argmax(dim=["z"], skipna=False)\\n6032.         for key in expected9:\\n6033.             assert_identical(result9[key].drop_vars(["x", "y"]), expected9[key])\\n6034. \\n6035.         maxindices_xy = {\\n6036.             key: xr.where(\\n6037.                 nanindices_xy[key] == None,  # noqa: E711\\n6038.                 maxindices_xy[key],\\n6039.                 nanindices_xy[key],\\n6040.             )\\n6041.             for key in maxindices_xy\\n6042.         }\\n6043.         expected10 = {\\n6044.             key: xr.DataArray(value, dims="z") for key, value in maxindices_xy.items()\\n6045.         }\\n6046. \\n6047.         result10 = ar.argmax(dim=("x", "y"), skipna=False)\\n6048.         for key in expected10:\\n6049.             assert_identical(result10[key].drop_vars("z"), expected10[key])\\n6050. \\n6051.         maxindices_xz = {\\n6052.             key: xr.where(\\n6053.                 nanindices_xz[key] == None,  # noqa: E711\\n6054.                 maxindices_xz[key],\\n6055.                 nanindices_xz[key],\\n6056.             )\\n6057.             for key in maxindices_xz\\n6058.         }\\n6059.         expected11 = {\\n6060.             key: xr.DataArray(value, dims="y") for key, value in maxindices_xz.items()\\n6061.         }\\n6062. \\n6063.         result11 = ar.argmax(dim=("x", "z"), skipna=False)\\n6064.         for key in expected11:\\n6065.             assert_identical(result11[key].drop_vars("y"), expected11[key])\\n6066. \\n6067.         maxindices_yz = {\\n6068.             key: xr.where(\\n6069.                 nanindices_yz[key] == None,  # noqa: E711\\n6070.                 maxindices_yz[key],\\n6071.                 nanindices_yz[key],\\n6072.             )\\n6073.             for key in maxindices_yz\\n6074.         }\\n6075.         expected12 = {\\n6076.             key: xr.DataArray(value, dims="x") for key, value in maxindices_yz.items()\\n6077.         }\\n6078. \\n6079.         result12 = ar.argmax(dim=("y", "z"), skipna=False)\\n6080.         for key in expected12:\\n6081.             assert_identical(result12[key].drop_vars("x"), expected12[key])\\n6082. \\n6083.         maxindices_xyz = {\\n6084.             key: xr.where(\\n6085.                 nanindices_xyz[key] == None,  # noqa: E711\\n6086.                 maxindices_xyz[key],\\n6087.                 nanindices_xyz[key],\\n6088.             )\\n6089.             for key in maxindices_xyz\\n6090.         }\\n6091.         expected13 = {key: xr.DataArray(value) for key, value in maxindices_xyz.items()}\\n6092. \\n6093.         result13 = ar.argmax(..., skipna=False)\\n6094.         for key in expected13:\\n6095.             assert_identical(result13[key], expected13[key])\\n6096. \\n6097. \\n6098. class TestReduceND(TestReduce):\\n6099.     @pytest.mark.parametrize("op", ["idxmin", "idxmax"])\\n6100.     @pytest.mark.parametrize("ndim", [3, 5])\\n6101.     def test_idxminmax_dask(self, op, ndim):\\n6102.         if not has_dask:\\n6103.             pytest.skip("requires dask")\\n6104. \\n6105.         ar0_raw = xr.DataArray(\\n6106.             np.random.random_sample(size=[10] * ndim),\\n6107.             dims=[i for i in "abcdefghij"[: ndim - 1]] + ["x"],\\n6108.             coords={"x": np.arange(10)},\\n6109.             attrs=self.attrs,\\n6110.         )\\n6111. \\n6112.         ar0_dsk = ar0_raw.chunk({})\\n6113.         # Assert idx is the same with dask and without\\n6114.         assert_equal(getattr(ar0_dsk, op)(dim="x"), getattr(ar0_raw, op)(dim="x"))\\n6115. \\n6116. \\n6117. @pytest.fixture(params=[1])\\n6118. def da(request):\\n6119.     if request.param == 1:\\n6120.         times = pd.date_range("2000-01-01", freq="1D", periods=21)\\n6121.         values = np.random.random((3, 21, 4))\\n6122.         da = DataArray(values, dims=("a", "time", "x"))\\n6123.         da["time"] = times\\n6124.         return da\\n6125. \\n6126.     if request.param == 2:\\n6127.         return DataArray([0, np.nan, 1, 2, np.nan, 3, 4, 5, np.nan, 6, 7], dims="time")\\n6128. \\n6129.     if request.param == "repeating_ints":\\n6130.         return DataArray(\\n6131.             np.tile(np.arange(12), 5).reshape(5, 4, 3),\\n6132.             coords={"x": list("abc"), "y": list("defg")},\\n6133.             dims=list("zyx"),\\n6134.         )\\n6135. \\n6136. \\n6137. @pytest.fixture\\n6138. def da_dask(seed=123):\\n6139.     pytest.importorskip("dask.array")\\n6140.     rs = np.random.RandomState(seed)\\n6141.     times = pd.date_range("2000-01-01", freq="1D", periods=21)\\n6142.     values = rs.normal(size=(1, 21, 1))\\n6143.     da = DataArray(values, dims=("a", "time", "x")).chunk({"time": 7})\\n6144.     da["time"] = times\\n6145.     return da\\n6146. \\n6147. \\n6148. @pytest.mark.parametrize("da", ("repeating_ints",), indirect=True)\\n6149. def test_isin(da):\\n6150. \\n6151.     expected = DataArray(\\n6152.         np.asarray([[0, 0, 0], [1, 0, 0]]),\\n6153.         dims=list("yx"),\\n6154.         coords={"x": list("abc"), "y": list("de")},\\n6155.     ).astype("bool")\\n6156. \\n6157.     result = da.isin([3]).sel(y=list("de"), z=0)\\n6158.     assert_equal(result, expected)\\n6159. \\n6160.     expected = DataArray(\\n6161.         np.asarray([[0, 0, 1], [1, 0, 0]]),\\n6162.         dims=list("yx"),\\n6163.         coords={"x": list("abc"), "y": list("de")},\\n6164.     ).astype("bool")\\n6165.     result = da.isin([2, 3]).sel(y=list("de"), z=0)\\n6166.     assert_equal(result, expected)\\n6167. \\n6168. \\n6169. @pytest.mark.parametrize("da", (1, 2), indirect=True)\\n6170. def test_rolling_iter(da):\\n6171. \\n6172.     rolling_obj = da.rolling(time=7)\\n6173.     with warnings.catch_warnings():\\n6174.         warnings.filterwarnings("ignore", "Mean of empty slice")\\n6175.         rolling_obj_mean = rolling_obj.mean()\\n6176. \\n6177.     assert len(rolling_obj.window_labels) == len(da["time"])\\n6178.     assert_identical(rolling_obj.window_labels, da["time"])\\n6179. \\n6180.     for i, (label, window_da) in enumerate(rolling_obj):\\n6181.         assert label == da["time"].isel(time=i)\\n6182. \\n6183.         with warnings.catch_warnings():\\n6184.             warnings.filterwarnings("ignore", "Mean of empty slice")\\n6185.             actual = rolling_obj_mean.isel(time=i)\\n6186.             expected = window_da.mean("time")\\n6187. \\n6188.         # TODO add assert_allclose_with_nan, which compares nan position\\n6189.         # as well as the closeness of the values.\\n6190.         assert_array_equal(actual.isnull(), expected.isnull())\\n6191.         if (~actual.isnull()).sum() > 0:\\n6192.             np.allclose(\\n6193.                 actual.values[actual.values.nonzero()],\\n6194.                 expected.values[expected.values.nonzero()],\\n6195.             )\\n6196. \\n6197. \\n6198. @pytest.mark.parametrize("da", (1,), indirect=True)\\n6199. def test_rolling_repr(da):\\n6200.     rolling_obj = da.rolling(time=7)\\n6201.     assert repr(rolling_obj) == "DataArrayRolling [time->7]"\\n6202.     rolling_obj = da.rolling(time=7, center=True)\\n6203.     assert repr(rolling_obj) == "DataArrayRolling [time->7(center)]"\\n6204.     rolling_obj = da.rolling(time=7, x=3, center=True)\\n6205.     assert repr(rolling_obj) == "DataArrayRolling [time->7(center),x->3(center)]"\\n6206. \\n6207. \\n6208. def test_rolling_doc(da):\\n6209.     rolling_obj = da.rolling(time=7)\\n6210. \\n6211.     # argument substitution worked\\n6212.     assert "`mean`" in rolling_obj.mean.__doc__\\n6213. \\n6214. \\n6215. def test_rolling_properties(da):\\n6216.     rolling_obj = da.rolling(time=4)\\n6217. \\n6218.     assert rolling_obj.obj.get_axis_num("time") == 1\\n6219. \\n6220.     # catching invalid args\\n6221.     with pytest.raises(ValueError, match="window must be > 0"):\\n6222.         da.rolling(time=-2)\\n6223.     with pytest.raises(ValueError, match="min_periods must be greater than zero"):\\n6224.         da.rolling(time=2, min_periods=0)\\n6225. \\n6226. \\n6227. @pytest.mark.parametrize("name", ("sum", "mean", "std", "min", "max", "median"))\\n6228. @pytest.mark.parametrize("center", (True, False, None))\\n6229. @pytest.mark.parametrize("min_periods", (1, None))\\n6230. def test_rolling_wrapped_bottleneck(da, name, center, min_periods):\\n6231.     bn = pytest.importorskip("bottleneck", minversion="1.1")\\n6232. \\n6233.     # Test all bottleneck functions\\n6234.     rolling_obj = da.rolling(time=7, min_periods=min_periods)\\n6235. \\n6236.     func_name = f"move_{name}"\\n6237.     actual = getattr(rolling_obj, name)()\\n6238.     expected = getattr(bn, func_name)(\\n6239.         da.values, window=7, axis=1, min_count=min_periods\\n6240.     )\\n6241.     assert_array_equal(actual.values, expected)\\n6242. \\n6243.     with pytest.warns(DeprecationWarning, match="Reductions will be applied"):\\n6244.         getattr(rolling_obj, name)(dim="time")\\n6245. \\n6246.     # Test center\\n6247.     rolling_obj = da.rolling(time=7, center=center)\\n6248.     actual = getattr(rolling_obj, name)()["time"]\\n6249.     assert_equal(actual, da["time"])\\n6250. \\n6251. \\n6252. @requires_dask\\n6253. @pytest.mark.parametrize("name", ("mean", "count"))\\n6254. @pytest.mark.parametrize("center", (True, False, None))\\n6255. @pytest.mark.parametrize("min_periods", (1, None))\\n6256. @pytest.mark.parametrize("window", (7, 8))\\n6257. def test_rolling_wrapped_dask(da_dask, name, center, min_periods, window):\\n6258.     # dask version\\n6259.     rolling_obj = da_dask.rolling(time=window, min_periods=min_periods, center=center)\\n6260.     actual = getattr(rolling_obj, name)().load()\\n6261.     if name != "count":\\n6262.         with pytest.warns(DeprecationWarning, match="Reductions will be applied"):\\n6263.             getattr(rolling_obj, name)(dim="time")\\n6264.     # numpy version\\n6265.     rolling_obj = da_dask.load().rolling(\\n6266.         time=window, min_periods=min_periods, center=center\\n6267.     )\\n6268.     expected = getattr(rolling_obj, name)()\\n6269. \\n6270.     # using all-close because rolling over ghost cells introduces some\\n6271.     # precision errors\\n6272.     assert_allclose(actual, expected)\\n6273. \\n6274.     # with zero chunked array GH:2113\\n6275.     rolling_obj = da_dask.chunk().rolling(\\n6276.         time=window, min_periods=min_periods, center=center\\n6277.     )\\n6278.     actual = getattr(rolling_obj, name)().load()\\n6279.     assert_allclose(actual, expected)\\n6280. \\n6281. \\n6282. @pytest.mark.parametrize("center", (True, None))\\n6283. def test_rolling_wrapped_dask_nochunk(center):\\n6284.     # GH:2113\\n6285.     pytest.importorskip("dask.array")\\n6286. \\n6287.     da_day_clim = xr.DataArray(\\n6288.         np.arange(1, 367), coords=[np.arange(1, 367)], dims="dayofyear"\\n6289.     )\\n6290.     expected = da_day_clim.rolling(dayofyear=31, center=center).mean()\\n6291.     actual = da_day_clim.chunk().rolling(dayofyear=31, center=center).mean()\\n6292.     assert_allclose(actual, expected)\\n6293. \\n6294. \\n6295. @pytest.mark.parametrize("center", (True, False))\\n6296. @pytest.mark.parametrize("min_periods", (None, 1, 2, 3))\\n6297. @pytest.mark.parametrize("window", (1, 2, 3, 4))\\n6298. def test_rolling_pandas_compat(center, window, min_periods):\\n6299.     s = pd.Series(np.arange(10))\\n6300.     da = DataArray.from_series(s)\\n6301. \\n6302.     if min_periods is not None and window < min_periods:\\n6303.         min_periods = window\\n6304. \\n6305.     s_rolling = s.rolling(window, center=center, min_periods=min_periods).mean()\\n6306.     da_rolling = da.rolling(index=window, center=center, min_periods=min_periods).mean()\\n6307.     da_rolling_np = da.rolling(\\n6308.         index=window, center=center, min_periods=min_periods\\n6309.     ).reduce(np.nanmean)\\n6310. \\n6311.     np.testing.assert_allclose(s_rolling.values, da_rolling.values)\\n6312.     np.testing.assert_allclose(s_rolling.index, da_rolling["index"])\\n6313.     np.testing.assert_allclose(s_rolling.values, da_rolling_np.values)\\n6314.     np.testing.assert_allclose(s_rolling.index, da_rolling_np["index"])\\n6315. \\n6316. \\n6317. @pytest.mark.parametrize("center", (True, False))\\n6318. @pytest.mark.parametrize("window", (1, 2, 3, 4))\\n6319. def test_rolling_construct(center, window):\\n6320.     s = pd.Series(np.arange(10))\\n6321.     da = DataArray.from_series(s)\\n6322. \\n6323.     s_rolling = s.rolling(window, center=center, min_periods=1).mean()\\n6324.     da_rolling = da.rolling(index=window, center=center, min_periods=1)\\n6325. \\n6326.     da_rolling_mean = da_rolling.construct("window").mean("window")\\n6327.     np.testing.assert_allclose(s_rolling.values, da_rolling_mean.values)\\n6328.     np.testing.assert_allclose(s_rolling.index, da_rolling_mean["index"])\\n6329. \\n6330.     # with stride\\n6331.     da_rolling_mean = da_rolling.construct("window", stride=2).mean("window")\\n6332.     np.testing.assert_allclose(s_rolling.values[::2], da_rolling_mean.values)\\n6333.     np.testing.assert_allclose(s_rolling.index[::2], da_rolling_mean["index"])\\n6334. \\n6335.     # with fill_value\\n6336.     da_rolling_mean = da_rolling.construct("window", stride=2, fill_value=0.0).mean(\\n6337.         "window"\\n6338.     )\\n6339.     assert da_rolling_mean.isnull().sum() == 0\\n6340.     assert (da_rolling_mean == 0.0).sum() >= 0\\n6341. \\n6342. \\n6343. @pytest.mark.parametrize("da", (1, 2), indirect=True)\\n6344. @pytest.mark.parametrize("center", (True, False))\\n6345. @pytest.mark.parametrize("min_periods", (None, 1, 2, 3))\\n6346. @pytest.mark.parametrize("window", (1, 2, 3, 4))\\n6347. @pytest.mark.parametrize("name", ("sum", "mean", "std", "max"))\\n6348. def test_rolling_reduce(da, center, min_periods, window, name):\\n6349. \\n6350.     if min_periods is not None and window < min_periods:\\n6351.         min_periods = window\\n6352. \\n6353.     if da.isnull().sum() > 1 and window == 1:\\n6354.         # this causes all nan slices\\n6355.         window = 2\\n6356. \\n6357.     rolling_obj = da.rolling(time=window, center=center, min_periods=min_periods)\\n6358. \\n6359.     # add nan prefix to numpy methods to get similar # behavior as bottleneck\\n6360.     actual = rolling_obj.reduce(getattr(np, "nan%s" % name))\\n6361.     expected = getattr(rolling_obj, name)()\\n6362.     assert_allclose(actual, expected)\\n6363.     assert actual.dims == expected.dims\\n6364. \\n6365. \\n6366. @pytest.mark.parametrize("center", (True, False))\\n6367. @pytest.mark.parametrize("min_periods", (None, 1, 2, 3))\\n6368. @pytest.mark.parametrize("window", (1, 2, 3, 4))\\n6369. @pytest.mark.parametrize("name", ("sum", "max"))\\n6370. def test_rolling_reduce_nonnumeric(center, min_periods, window, name):\\n6371.     da = DataArray(\\n6372.         [0, np.nan, 1, 2, np.nan, 3, 4, 5, np.nan, 6, 7], dims="time"\\n6373.     ).isnull()\\n6374. \\n6375.     if min_periods is not None and window < min_periods:\\n6376.         min_periods = window\\n6377. \\n6378.     rolling_obj = da.rolling(time=window, center=center, min_periods=min_periods)\\n6379. \\n6380.     # add nan prefix to numpy methods to get similar behavior as bottleneck\\n6381.     actual = rolling_obj.reduce(getattr(np, "nan%s" % name))\\n6382.     expected = getattr(rolling_obj, name)()\\n6383.     assert_allclose(actual, expected)\\n6384.     assert actual.dims == expected.dims\\n6385. \\n6386. \\n6387. def test_rolling_count_correct():\\n6388. \\n6389.     da = DataArray([0, np.nan, 1, 2, np.nan, 3, 4, 5, np.nan, 6, 7], dims="time")\\n6390. \\n6391.     kwargs = [\\n6392.         {"time": 11, "min_periods": 1},\\n6393.         {"time": 11, "min_periods": None},\\n6394.         {"time": 7, "min_periods": 2},\\n6395.     ]\\n6396.     expecteds = [\\n6397.         DataArray([1, 1, 2, 3, 3, 4, 5, 6, 6, 7, 8], dims="time"),\\n6398.         DataArray(\\n6399.             [\\n6400.                 np.nan,\\n6401.                 np.nan,\\n6402.                 np.nan,\\n6403.                 np.nan,\\n6404.                 np.nan,\\n6405.                 np.nan,\\n6406.                 np.nan,\\n6407.                 np.nan,\\n6408.                 np.nan,\\n6409.                 np.nan,\\n6410.                 np.nan,\\n6411.             ],\\n6412.             dims="time",\\n6413.         ),\\n6414.         DataArray([np.nan, np.nan, 2, 3, 3, 4, 5, 5, 5, 5, 5], dims="time"),\\n6415.     ]\\n6416. \\n6417.     for kwarg, expected in zip(kwargs, expecteds):\\n6418.         result = da.rolling(**kwarg).count()\\n6419.         assert_equal(result, expected)\\n6420. \\n6421.         result = da.to_dataset(name="var1").rolling(**kwarg).count()["var1"]\\n6422.         assert_equal(result, expected)\\n6423. \\n6424. \\n6425. @pytest.mark.parametrize("da", (1,), indirect=True)\\n6426. @pytest.mark.parametrize("center", (True, False))\\n6427. @pytest.mark.parametrize("min_periods", (None, 1))\\n6428. @pytest.mark.parametrize("name", ("sum", "mean", "max"))\\n6429. def test_ndrolling_reduce(da, center, min_periods, name):\\n6430.     rolling_obj = da.rolling(time=3, x=2, center=center, min_periods=min_periods)\\n6431. \\n6432.     actual = getattr(rolling_obj, name)()\\n6433.     expected = getattr(\\n6434.         getattr(\\n6435.             da.rolling(time=3, center=center, min_periods=min_periods), name\\n6436.         )().rolling(x=2, center=center, min_periods=min_periods),\\n6437.         name,\\n6438.     )()\\n6439. \\n6440.     assert_allclose(actual, expected)\\n6441.     assert actual.dims == expected.dims\\n6442. \\n6443. \\n6444. @pytest.mark.parametrize("center", (True, False, (True, False)))\\n6445. @pytest.mark.parametrize("fill_value", (np.nan, 0.0))\\n6446. def test_ndrolling_construct(center, fill_value):\\n6447.     da = DataArray(\\n6448.         np.arange(5 * 6 * 7).reshape(5, 6, 7).astype(float),\\n6449.         dims=["x", "y", "z"],\\n6450.         coords={"x": ["a", "b", "c", "d", "e"], "y": np.arange(6)},\\n6451.     )\\n6452.     actual = da.rolling(x=3, z=2, center=center).construct(\\n6453.         x="x1", z="z1", fill_value=fill_value\\n6454.     )\\n6455.     if not isinstance(center, tuple):\\n6456.         center = (center, center)\\n6457.     expected = (\\n6458.         da.rolling(x=3, center=center[0])\\n6459.         .construct(x="x1", fill_value=fill_value)\\n6460.         .rolling(z=2, center=center[1])\\n6461.         .construct(z="z1", fill_value=fill_value)\\n6462.     )\\n6463.     assert_allclose(actual, expected)\\n6464. \\n6465. \\n6466. def test_raise_no_warning_for_nan_in_binary_ops():\\n6467.     with pytest.warns(None) as record:\\n6468.         xr.DataArray([1, 2, np.NaN]) > 0\\n6469.     assert len(record) == 0\\n6470. \\n6471. \\n6472. def test_name_in_masking():\\n6473.     name = "RingoStarr"\\n6474.     da = xr.DataArray(range(10), coords=[("x", range(10))], name=name)\\n6475.     assert da.where(da > 5).name == name\\n6476.     assert da.where((da > 5).rename("YokoOno")).name == name\\n6477.     assert da.where(da > 5, drop=True).name == name\\n6478.     assert da.where((da > 5).rename("YokoOno"), drop=True).name == name\\n6479. \\n6480. \\n6481. class TestIrisConversion:\\n6482.     @requires_iris\\n6483.     def test_to_and_from_iris(self):\\n6484.         import cf_units  # iris requirement\\n6485.         import iris\\n6486. \\n6487.         # to iris\\n6488.         coord_dict = {}\\n6489.         coord_dict["distance"] = ("distance", [-2, 2], {"units": "meters"})\\n6490.         coord_dict["time"] = ("time", pd.date_range("2000-01-01", periods=3))\\n6491.         coord_dict["height"] = 10\\n6492.         coord_dict["distance2"] = ("distance", [0, 1], {"foo": "bar"})\\n6493.         coord_dict["time2"] = (("distance", "time"), [[0, 1, 2], [2, 3, 4]])\\n6494. \\n6495.         original = DataArray(\\n6496.             np.arange(6, dtype="float").reshape(2, 3),\\n6497.             coord_dict,\\n6498.             name="Temperature",\\n6499.             attrs={\\n6500.                 "baz": 123,\\n6501.                 "units": "Kelvin",\\n6502.                 "standard_name": "fire_temperature",\\n6503.                 "long_name": "Fire Temperature",\\n6504.             },\\n6505.             dims=("distance", "time"),\\n6506.         )\\n6507. \\n6508.         # Set a bad value to test the masking logic\\n6509.         original.data[0, 2] = np.NaN\\n6510. \\n6511.         original.attrs["cell_methods"] = "height: mean (comment: A cell method)"\\n6512.         actual = original.to_iris()\\n6513.         assert_array_equal(actual.data, original.data)\\n6514.         assert actual.var_name == original.name\\n6515.         assert tuple(d.var_name for d in actual.dim_coords) == original.dims\\n6516.         assert actual.cell_methods == (\\n6517.             iris.coords.CellMethod(\\n6518.                 method="mean",\\n6519.                 coords=("height",),\\n6520.                 intervals=(),\\n6521.                 comments=("A cell method",),\\n6522.             ),\\n6523.         )\\n6524. \\n6525.         for coord, orginal_key in zip((actual.coords()), original.coords):\\n6526.             original_coord = original.coords[orginal_key]\\n6527.             assert coord.var_name == original_coord.name\\n6528.             assert_array_equal(\\n6529.                 coord.points, CFDatetimeCoder().encode(original_coord).values\\n6530.             )\\n6531.             assert actual.coord_dims(coord) == original.get_axis_num(\\n6532.                 original.coords[coord.var_name].dims\\n6533.             )\\n6534. \\n6535.         assert (\\n6536.             actual.coord("distance2").attributes["foo"]\\n6537.             == original.coords["distance2"].attrs["foo"]\\n6538.         )\\n6539.         assert actual.coord("distance").units == cf_units.Unit(\\n6540.             original.coords["distance"].units\\n6541.         )\\n6542.         assert actual.attributes["baz"] == original.attrs["baz"]\\n6543.         assert actual.standard_name == original.attrs["standard_name"]\\n6544. \\n6545.         roundtripped = DataArray.from_iris(actual)\\n6546.         assert_identical(original, roundtripped)\\n6547. \\n6548.         actual.remove_coord("time")\\n6549.         auto_time_dimension = DataArray.from_iris(actual)\\n6550.         assert auto_time_dimension.dims == ("distance", "dim_1")\\n6551. \\n6552.     @requires_iris\\n6553.     @requires_dask\\n6554.     def test_to_and_from_iris_dask(self):\\n6555.         import cf_units  # iris requirement\\n6556.         import dask.array as da\\n6557.         import iris\\n6558. \\n6559.         coord_dict = {}\\n6560.         coord_dict["distance"] = ("distance", [-2, 2], {"units": "meters"})\\n6561.         coord_dict["time"] = ("time", pd.date_range("2000-01-01", periods=3))\\n6562.         coord_dict["height"] = 10\\n6563.         coord_dict["distance2"] = ("distance", [0, 1], {"foo": "bar"})\\n6564.         coord_dict["time2"] = (("distance", "time"), [[0, 1, 2], [2, 3, 4]])\\n6565. \\n6566.         original = DataArray(\\n6567.             da.from_array(np.arange(-1, 5, dtype="float").reshape(2, 3), 3),\\n6568.             coord_dict,\\n6569.             name="Temperature",\\n6570.             attrs=dict(\\n6571.                 baz=123,\\n6572.                 units="Kelvin",\\n6573.                 standard_name="fire_temperature",\\n6574.                 long_name="Fire Temperature",\\n6575.             ),\\n6576.             dims=("distance", "time"),\\n6577.         )\\n6578. \\n6579.         # Set a bad value to test the masking logic\\n6580.         original.data = da.ma.masked_less(original.data, 0)\\n6581. \\n6582.         original.attrs["cell_methods"] = "height: mean (comment: A cell method)"\\n6583.         actual = original.to_iris()\\n6584. \\n6585.         # Be careful not to trigger the loading of the iris data\\n6586.         actual_data = (\\n6587.             actual.core_data() if hasattr(actual, "core_data") else actual.data\\n6588.         )\\n6589.         assert_array_equal(actual_data, original.data)\\n6590.         assert actual.var_name == original.name\\n6591.         assert tuple(d.var_name for d in actual.dim_coords) == original.dims\\n6592.         assert actual.cell_methods == (\\n6593.             iris.coords.CellMethod(\\n6594.                 method="mean",\\n6595.                 coords=("height",),\\n6596.                 intervals=(),\\n6597.                 comments=("A cell method",),\\n6598.             ),\\n6599.         )\\n6600. \\n6601.         for coord, orginal_key in zip((actual.coords()), original.coords):\\n6602.             original_coord = original.coords[orginal_key]\\n6603.             assert coord.var_name == original_coord.name\\n6604.             assert_array_equal(\\n6605.                 coord.points, CFDatetimeCoder().encode(original_coord).values\\n6606.             )\\n6607.             assert actual.coord_dims(coord) == original.get_axis_num(\\n6608.                 original.coords[coord.var_name].dims\\n6609.             )\\n6610. \\n6611.         assert (\\n6612.             actual.coord("distance2").attributes["foo"]\\n6613.             == original.coords["distance2"].attrs["foo"]\\n6614.         )\\n6615.         assert actual.coord("distance").units == cf_units.Unit(\\n6616.             original.coords["distance"].units\\n6617.         )\\n6618.         assert actual.attributes["baz"] == original.attrs["baz"]\\n6619.         assert actual.standard_name == original.attrs["standard_name"]\\n6620. \\n6621.         roundtripped = DataArray.from_iris(actual)\\n6622.         assert_identical(original, roundtripped)\\n6623. \\n6624.         # If the Iris version supports it then we should have a dask array\\n6625.         # at each stage of the conversion\\n6626.         if hasattr(actual, "core_data"):\\n6627.             assert isinstance(original.data, type(actual.core_data()))\\n6628.             assert isinstance(original.data, type(roundtripped.data))\\n6629. \\n6630.         actual.remove_coord("time")\\n6631.         auto_time_dimension = DataArray.from_iris(actual)\\n6632.         assert auto_time_dimension.dims == ("distance", "dim_1")\\n6633. \\n6634.     @requires_iris\\n6635.     @pytest.mark.parametrize(\\n6636.         "var_name, std_name, long_name, name, attrs",\\n6637.         [\\n6638.             (\\n6639.                 "var_name",\\n6640.                 "height",\\n6641.                 "Height",\\n6642.                 "var_name",\\n6643.                 {"standard_name": "height", "long_name": "Height"},\\n6644.             ),\\n6645.             (\\n6646.                 None,\\n6647.                 "height",\\n6648.                 "Height",\\n6649.                 "height",\\n6650.                 {"standard_name": "height", "long_name": "Height"},\\n6651.             ),\\n6652.             (None, None, "Height", "Height", {"long_name": "Height"}),\\n6653.             (None, None, None, None, {}),\\n6654.         ],\\n6655.     )\\n6656.     def test_da_name_from_cube(self, std_name, long_name, var_name, name, attrs):\\n6657.         from iris.cube import Cube\\n6658. \\n6659.         data = []\\n6660.         cube = Cube(\\n6661.             data, var_name=var_name, standard_name=std_name, long_name=long_name\\n6662.         )\\n6663.         result = xr.DataArray.from_iris(cube)\\n6664.         expected = xr.DataArray(data, name=name, attrs=attrs)\\n6665.         xr.testing.assert_identical(result, expected)\\n6666. \\n6667.     @requires_iris\\n6668.     @pytest.mark.parametrize(\\n6669.         "var_name, std_name, long_name, name, attrs",\\n6670.         [\\n6671.             (\\n6672.                 "var_name",\\n6673.                 "height",\\n6674.                 "Height",\\n6675.                 "var_name",\\n6676.                 {"standard_name": "height", "long_name": "Height"},\\n6677.             ),\\n6678.             (\\n6679.                 None,\\n6680.                 "height",\\n6681.                 "Height",\\n6682.                 "height",\\n6683.                 {"standard_name": "height", "long_name": "Height"},\\n6684.             ),\\n6685.             (None, None, "Height", "Height", {"long_name": "Height"}),\\n6686.             (None, None, None, "unknown", {}),\\n6687.         ],\\n6688.     )\\n6689.     def test_da_coord_name_from_cube(self, std_name, long_name, var_name, name, attrs):\\n6690.         from iris.coords import DimCoord\\n6691.         from iris.cube import Cube\\n6692. \\n6693.         latitude = DimCoord(\\n6694.             [-90, 0, 90], standard_name=std_name, var_name=var_name, long_name=long_name\\n6695.         )\\n6696.         data = [0, 0, 0]\\n6697.         cube = Cube(data, dim_coords_and_dims=[(latitude, 0)])\\n6698.         result = xr.DataArray.from_iris(cube)\\n6699.         expected = xr.DataArray(data, coords=[(name, [-90, 0, 90], attrs)])\\n6700.         xr.testing.assert_identical(result, expected)\\n6701. \\n6702.     @requires_iris\\n6703.     def test_prevent_duplicate_coord_names(self):\\n6704.         from iris.coords import DimCoord\\n6705.         from iris.cube import Cube\\n6706. \\n6707.         # Iris enforces unique coordinate names. Because we use a different\\n6708.         # name resolution order a valid iris Cube with coords that have the\\n6709.         # same var_name would lead to duplicate dimension names in the\\n6710.         # DataArray\\n6711.         longitude = DimCoord([0, 360], standard_name="longitude", var_name="duplicate")\\n6712.         latitude = DimCoord(\\n6713.             [-90, 0, 90], standard_name="latitude", var_name="duplicate"\\n6714.         )\\n6715.         data = [[0, 0, 0], [0, 0, 0]]\\n6716.         cube = Cube(data, dim_coords_and_dims=[(longitude, 0), (latitude, 1)])\\n6717.         with pytest.raises(ValueError):\\n6718.             xr.DataArray.from_iris(cube)\\n6719. \\n6720.     @requires_iris\\n6721.     @pytest.mark.parametrize(\\n6722.         "coord_values",\\n6723.         [["IA", "IL", "IN"], [0, 2, 1]],  # non-numeric values  # non-monotonic values\\n6724.     )\\n6725.     def test_fallback_to_iris_AuxCoord(self, coord_values):\\n6726.         from iris.coords import AuxCoord\\n6727.         from iris.cube import Cube\\n6728. \\n6729.         data = [0, 0, 0]\\n6730.         da = xr.DataArray(data, coords=[coord_values], dims=["space"])\\n6731.         result = xr.DataArray.to_iris(da)\\n6732.         expected = Cube(\\n6733.             data, aux_coords_and_dims=[(AuxCoord(coord_values, var_name="space"), 0)]\\n6734.         )\\n6735.         assert result == expected\\n6736. \\n6737. \\n6738. @requires_numbagg\\n6739. @pytest.mark.parametrize("dim", ["time", "x"])\\n6740. @pytest.mark.parametrize(\\n6741.     "window_type, window", [["span", 5], ["alpha", 0.5], ["com", 0.5], ["halflife", 5]]\\n6742. )\\n6743. def test_rolling_exp(da, dim, window_type, window):\\n6744.     da = da.isel(a=0)\\n6745.     da = da.where(da > 0.2)\\n6746. \\n6747.     result = da.rolling_exp(window_type=window_type, **{dim: window}).mean()\\n6748.     assert isinstance(result, DataArray)\\n6749. \\n6750.     pandas_array = da.to_pandas()\\n6751.     assert pandas_array.index.name == "time"\\n6752.     if dim == "x":\\n6753.         pandas_array = pandas_array.T\\n6754.     expected = xr.DataArray(pandas_array.ewm(**{window_type: window}).mean()).transpose(\\n6755.         *da.dims\\n6756.     )\\n6757. \\n6758.     assert_allclose(expected.variable, result.variable)\\n6759. \\n6760. \\n6761. def test_no_dict():\\n6762.     d = DataArray()\\n6763.     with pytest.raises(AttributeError):\\n6764.         d.__dict__\\n6765. \\n6766. \\n6767. def test_subclass_slots():\\n6768.     """Test that DataArray subclasses must explicitly define ``__slots__``.\\n6769. \\n6770.     .. note::\\n6771.        As of 0.13.0, this is actually mitigated into a FutureWarning for any class\\n6772.        defined outside of the xarray package.\\n6773.     """\\n6774.     with pytest.raises(AttributeError) as e:\\n6775. \\n6776.         class MyArray(DataArray):\\n6777.             pass\\n6778. \\n6779.     assert str(e.value) == "MyArray must explicitly define __slots__"\\n6780. \\n6781. \\n6782. def test_weakref():\\n6783.     """Classes with __slots__ are incompatible with the weakref module unless they\\n6784.     explicitly state __weakref__ among their slots\\n6785.     """\\n6786.     from weakref import ref\\n6787. \\n6788.     a = DataArray(1)\\n6789.     r = ref(a)\\n6790.     assert r() is a\\n6791. \\n6792. \\n6793. def test_delete_coords():\\n6794.     """Make sure that deleting a coordinate doesn\\\'t corrupt the DataArray.\\n6795.     See issue #3899.\\n6796. \\n6797.     Also test that deleting succeeds and produces the expected output.\\n6798.     """\\n6799.     a0 = DataArray(\\n6800.         np.array([[1, 2, 3], [4, 5, 6]]),\\n6801.         dims=["y", "x"],\\n6802.         coords={"x": ["a", "b", "c"], "y": [-1, 1]},\\n6803.     )\\n6804.     assert_identical(a0, a0)\\n6805. \\n6806.     a1 = a0.copy()\\n6807.     del a1.coords["y"]\\n6808. \\n6809.     # This test will detect certain sorts of corruption in the DataArray\\n6810.     assert_identical(a0, a0)\\n6811. \\n6812.     assert a0.dims == ("y", "x")\\n6813.     assert a1.dims == ("y", "x")\\n6814.     assert set(a0.coords.keys()) == {"x", "y"}\\n6815.     assert set(a1.coords.keys()) == {"x"}\', \'start_line_number\': 1, \'end_line_number\': 6815}\n\n{\'relative_path\': \'xarray/tests/test_duck_array_ops.py\', \'content\': \'1. import datetime as dt\\n2. import warnings\\n3. from textwrap import dedent\\n4. \\n5. import numpy as np\\n6. import pandas as pd\\n7. import pytest\\n8. from numpy import array, nan\\n9. \\n10. from xarray import DataArray, Dataset, cftime_range, concat\\n11. from xarray.core import dtypes, duck_array_ops\\n12. from xarray.core.duck_array_ops import (\\n13.     array_notnull_equiv,\\n14.     concatenate,\\n15.     count,\\n16.     first,\\n17.     gradient,\\n18.     last,\\n19.     least_squares,\\n20.     mean,\\n21.     np_timedelta64_to_float,\\n22.     pd_timedelta_to_float,\\n23.     py_timedelta_to_float,\\n24.     rolling_window,\\n25.     stack,\\n26.     timedelta_to_numeric,\\n27.     where,\\n28. )\\n29. from xarray.core.pycompat import dask_array_type\\n30. from xarray.testing import assert_allclose, assert_equal\\n31. \\n32. from . import (\\n33.     arm_xfail,\\n34.     assert_array_equal,\\n35.     has_dask,\\n36.     has_scipy,\\n37.     raises_regex,\\n38.     requires_cftime,\\n39.     requires_dask,\\n40. )\\n41. \\n42. \\n43. class TestOps:\\n44.     @pytest.fixture(autouse=True)\\n45.     def setUp(self):\\n46.         self.x = array(\\n47.             [\\n48.                 [[nan, nan, 2.0, nan], [nan, 5.0, 6.0, nan], [8.0, 9.0, 10.0, nan]],\\n49.                 [\\n50.                     [nan, 13.0, 14.0, 15.0],\\n51.                     [nan, 17.0, 18.0, nan],\\n52.                     [nan, 21.0, nan, nan],\\n53.                 ],\\n54.             ]\\n55.         )\\n56. \\n57.     def test_first(self):\\n58.         expected_results = [\\n59.             array([[nan, 13, 2, 15], [nan, 5, 6, nan], [8, 9, 10, nan]]),\\n60.             array([[8, 5, 2, nan], [nan, 13, 14, 15]]),\\n61.             array([[2, 5, 8], [13, 17, 21]]),\\n62.         ]\\n63.         for axis, expected in zip([0, 1, 2, -3, -2, -1], 2 * expected_results):\\n64.             actual = first(self.x, axis)\\n65.             assert_array_equal(expected, actual)\\n66. \\n67.         expected = self.x[0]\\n68.         actual = first(self.x, axis=0, skipna=False)\\n69.         assert_array_equal(expected, actual)\\n70. \\n71.         expected = self.x[..., 0]\\n72.         actual = first(self.x, axis=-1, skipna=False)\\n73.         assert_array_equal(expected, actual)\\n74. \\n75.         with raises_regex(IndexError, "out of bounds"):\\n76.             first(self.x, 3)\\n77. \\n78.     def test_last(self):\\n79.         expected_results = [\\n80.             array([[nan, 13, 14, 15], [nan, 17, 18, nan], [8, 21, 10, nan]]),\\n81.             array([[8, 9, 10, nan], [nan, 21, 18, 15]]),\\n82.             array([[2, 6, 10], [15, 18, 21]]),\\n83.         ]\\n84.         for axis, expected in zip([0, 1, 2, -3, -2, -1], 2 * expected_results):\\n85.             actual = last(self.x, axis)\\n86.             assert_array_equal(expected, actual)\\n87. \\n88.         expected = self.x[-1]\\n89.         actual = last(self.x, axis=0, skipna=False)\\n90.         assert_array_equal(expected, actual)\\n91. \\n92.         expected = self.x[..., -1]\\n93.         actual = last(self.x, axis=-1, skipna=False)\\n94.         assert_array_equal(expected, actual)\\n95. \\n96.         with raises_regex(IndexError, "out of bounds"):\\n97.             last(self.x, 3)\\n98. \\n99.     def test_count(self):\\n100.         assert 12 == count(self.x)\\n101. \\n102.         expected = array([[1, 2, 3], [3, 2, 1]])\\n103.         assert_array_equal(expected, count(self.x, axis=-1))\\n104. \\n105.         assert 1 == count(np.datetime64("2000-01-01"))\\n106. \\n107.     def test_where_type_promotion(self):\\n108.         result = where([True, False], [1, 2], ["a", "b"])\\n109.         assert_array_equal(result, np.array([1, "b"], dtype=object))\\n110. \\n111.         result = where([True, False], np.array([1, 2], np.float32), np.nan)\\n112.         assert result.dtype == np.float32\\n113.         assert_array_equal(result, np.array([1, np.nan], dtype=np.float32))\\n114. \\n115.     def test_stack_type_promotion(self):\\n116.         result = stack([1, "b"])\\n117.         assert_array_equal(result, np.array([1, "b"], dtype=object))\\n118. \\n119.     def test_concatenate_type_promotion(self):\\n120.         result = concatenate([[1], ["b"]])\\n121.         assert_array_equal(result, np.array([1, "b"], dtype=object))\\n122. \\n123.     def test_all_nan_arrays(self):\\n124.         with warnings.catch_warnings():\\n125.             warnings.filterwarnings("ignore", "All-NaN slice")\\n126.             warnings.filterwarnings("ignore", "Mean of empty slice")\\n127.             assert np.isnan(mean([np.nan, np.nan]))\\n128. \\n129. \\n130. def test_cumsum_1d():\\n131.     inputs = np.array([0, 1, 2, 3])\\n132.     expected = np.array([0, 1, 3, 6])\\n133.     actual = duck_array_ops.cumsum(inputs)\\n134.     assert_array_equal(expected, actual)\\n135. \\n136.     actual = duck_array_ops.cumsum(inputs, axis=0)\\n137.     assert_array_equal(expected, actual)\\n138. \\n139.     actual = duck_array_ops.cumsum(inputs, axis=-1)\\n140.     assert_array_equal(expected, actual)\\n141. \\n142.     actual = duck_array_ops.cumsum(inputs, axis=(0,))\\n143.     assert_array_equal(expected, actual)\\n144. \\n145.     actual = duck_array_ops.cumsum(inputs, axis=())\\n146.     assert_array_equal(inputs, actual)\\n147. \\n148. \\n149. def test_cumsum_2d():\\n150.     inputs = np.array([[1, 2], [3, 4]])\\n151. \\n152.     expected = np.array([[1, 3], [4, 10]])\\n153.     actual = duck_array_ops.cumsum(inputs)\\n154.     assert_array_equal(expected, actual)\\n155. \\n156.     actual = duck_array_ops.cumsum(inputs, axis=(0, 1))\\n157.     assert_array_equal(expected, actual)\\n158. \\n159.     actual = duck_array_ops.cumsum(inputs, axis=())\\n160.     assert_array_equal(inputs, actual)\\n161. \\n162. \\n163. def test_cumprod_2d():\\n164.     inputs = np.array([[1, 2], [3, 4]])\\n165. \\n166.     expected = np.array([[1, 2], [3, 2 * 3 * 4]])\\n167.     actual = duck_array_ops.cumprod(inputs)\\n168.     assert_array_equal(expected, actual)\\n169. \\n170.     actual = duck_array_ops.cumprod(inputs, axis=(0, 1))\\n171.     assert_array_equal(expected, actual)\\n172. \\n173.     actual = duck_array_ops.cumprod(inputs, axis=())\\n174.     assert_array_equal(inputs, actual)\\n175. \\n176. \\n177. class TestArrayNotNullEquiv:\\n178.     @pytest.mark.parametrize(\\n179.         "arr1, arr2",\\n180.         [\\n181.             (np.array([1, 2, 3]), np.array([1, 2, 3])),\\n182.             (np.array([1, 2, np.nan]), np.array([1, np.nan, 3])),\\n183.             (np.array([np.nan, 2, np.nan]), np.array([1, np.nan, np.nan])),\\n184.         ],\\n185.     )\\n186.     def test_equal(self, arr1, arr2):\\n187.         assert array_notnull_equiv(arr1, arr2)\\n188. \\n189.     def test_some_not_equal(self):\\n190.         a = np.array([1, 2, 4])\\n191.         b = np.array([1, np.nan, 3])\\n192.         assert not array_notnull_equiv(a, b)\\n193. \\n194.     def test_wrong_shape(self):\\n195.         a = np.array([[1, np.nan, np.nan, 4]])\\n196.         b = np.array([[1, 2], [np.nan, 4]])\\n197.         assert not array_notnull_equiv(a, b)\\n198. \\n199.     @pytest.mark.parametrize(\\n200.         "val1, val2, val3, null",\\n201.         [\\n202.             (\\n203.                 np.datetime64("2000"),\\n204.                 np.datetime64("2001"),\\n205.                 np.datetime64("2002"),\\n206.                 np.datetime64("NaT"),\\n207.             ),\\n208.             (1.0, 2.0, 3.0, np.nan),\\n209.             ("foo", "bar", "baz", None),\\n210.             ("foo", "bar", "baz", np.nan),\\n211.         ],\\n212.     )\\n213.     def test_types(self, val1, val2, val3, null):\\n214.         dtype = object if isinstance(val1, str) else None\\n215.         arr1 = np.array([val1, null, val3, null], dtype=dtype)\\n216.         arr2 = np.array([val1, val2, null, null], dtype=dtype)\\n217.         assert array_notnull_equiv(arr1, arr2)\\n218. \\n219. \\n220. def construct_dataarray(dim_num, dtype, contains_nan, dask):\\n221.     # dimnum <= 3\\n222.     rng = np.random.RandomState(0)\\n223.     shapes = [16, 8, 4][:dim_num]\\n224.     dims = ("x", "y", "z")[:dim_num]\\n225. \\n226.     if np.issubdtype(dtype, np.floating):\\n227.         array = rng.randn(*shapes).astype(dtype)\\n228.     elif np.issubdtype(dtype, np.integer):\\n229.         array = rng.randint(0, 10, size=shapes).astype(dtype)\\n230.     elif np.issubdtype(dtype, np.bool_):\\n231.         array = rng.randint(0, 1, size=shapes).astype(dtype)\\n232.     elif dtype == str:\\n233.         array = rng.choice(["a", "b", "c", "d"], size=shapes)\\n234.     else:\\n235.         raise ValueError\\n236. \\n237.     if contains_nan:\\n238.         inds = rng.choice(range(array.size), int(array.size * 0.2))\\n239.         dtype, fill_value = dtypes.maybe_promote(array.dtype)\\n240.         array = array.astype(dtype)\\n241.         array.flat[inds] = fill_value\\n242. \\n243.     da = DataArray(array, dims=dims, coords={"x": np.arange(16)}, name="da")\\n244. \\n245.     if dask and has_dask:\\n246.         chunks = {d: 4 for d in dims}\\n247.         da = da.chunk(chunks)\\n248. \\n249.     return da\\n250. \\n251. \\n252. def from_series_or_scalar(se):\\n253.     if isinstance(se, pd.Series):\\n254.         return DataArray.from_series(se)\\n255.     else:  # scalar case\\n256.         return DataArray(se)\\n257. \\n258. \\n259. def series_reduce(da, func, dim, **kwargs):\\n260.     """ convert DataArray to pd.Series, apply pd.func, then convert back to\\n261.     a DataArray. Multiple dims cannot be specified."""\\n262.     if dim is None or da.ndim == 1:\\n263.         se = da.to_series()\\n264.         return from_series_or_scalar(getattr(se, func)(**kwargs))\\n265.     else:\\n266.         da1 = []\\n267.         dims = list(da.dims)\\n268.         dims.remove(dim)\\n269.         d = dims[0]\\n270.         for i in range(len(da[d])):\\n271.             da1.append(series_reduce(da.isel(**{d: i}), func, dim, **kwargs))\\n272. \\n273.         if d in da.coords:\\n274.             return concat(da1, dim=da[d])\\n275.         return concat(da1, dim=d)\\n276. \\n277. \\n278. def assert_dask_array(da, dask):\\n279.     if dask and da.ndim > 0:\\n280.         assert isinstance(da.data, dask_array_type)\\n281. \\n282. \\n283. @arm_xfail\\n284. @pytest.mark.filterwarnings("ignore::RuntimeWarning")\\n285. @pytest.mark.parametrize("dask", [False, True] if has_dask else [False])\\n286. def test_datetime_mean(dask):\\n287.     # Note: only testing numpy, as dask is broken upstream\\n288.     da = DataArray(\\n289.         np.array(["2010-01-01", "NaT", "2010-01-03", "NaT", "NaT"], dtype="M8"),\\n290.         dims=["time"],\\n291.     )\\n292.     if dask:\\n293.         # Trigger use case where a chunk is full of NaT\\n294.         da = da.chunk({"time": 3})\\n295. \\n296.     expect = DataArray(np.array("2010-01-02", dtype="M8"))\\n297.     expect_nat = DataArray(np.array("NaT", dtype="M8"))\\n298. \\n299.     actual = da.mean()\\n300.     if dask:\\n301.         assert actual.chunks is not None\\n302.     assert_equal(actual, expect)\\n303. \\n304.     actual = da.mean(skipna=False)\\n305.     if dask:\\n306.         assert actual.chunks is not None\\n307.     assert_equal(actual, expect_nat)\\n308. \\n309.     # tests for 1d array full of NaT\\n310.     assert_equal(da[[1]].mean(), expect_nat)\\n311.     assert_equal(da[[1]].mean(skipna=False), expect_nat)\\n312. \\n313.     # tests for a 0d array\\n314.     assert_equal(da[0].mean(), da[0])\\n315.     assert_equal(da[0].mean(skipna=False), da[0])\\n316.     assert_equal(da[1].mean(), expect_nat)\\n317.     assert_equal(da[1].mean(skipna=False), expect_nat)\\n318. \\n319. \\n320. @requires_cftime\\n321. def test_cftime_datetime_mean():\\n322.     times = cftime_range("2000", periods=4)\\n323.     da = DataArray(times, dims=["time"])\\n324. \\n325.     assert da.isel(time=0).mean() == da.isel(time=0)\\n326. \\n327.     expected = DataArray(times.date_type(2000, 1, 2, 12))\\n328.     result = da.mean()\\n329.     assert_equal(result, expected)\\n330. \\n331.     da_2d = DataArray(times.values.reshape(2, 2))\\n332.     result = da_2d.mean()\\n333.     assert_equal(result, expected)\\n334. \\n335. \\n336. @requires_cftime\\n337. def test_cftime_datetime_mean_long_time_period():\\n338.     import cftime\\n339. \\n340.     times = np.array(\\n341.         [\\n342.             [\\n343.                 cftime.DatetimeNoLeap(400, 12, 31, 0, 0, 0, 0),\\n344.                 cftime.DatetimeNoLeap(520, 12, 31, 0, 0, 0, 0),\\n345.             ],\\n346.             [\\n347.                 cftime.DatetimeNoLeap(520, 12, 31, 0, 0, 0, 0),\\n348.                 cftime.DatetimeNoLeap(640, 12, 31, 0, 0, 0, 0),\\n349.             ],\\n350.             [\\n351.                 cftime.DatetimeNoLeap(640, 12, 31, 0, 0, 0, 0),\\n352.                 cftime.DatetimeNoLeap(760, 12, 31, 0, 0, 0, 0),\\n353.             ],\\n354.         ]\\n355.     )\\n356. \\n357.     da = DataArray(times, dims=["time", "d2"])\\n358.     result = da.mean("d2")\\n359.     expected = DataArray(\\n360.         [\\n361.             cftime.DatetimeNoLeap(460, 12, 31, 0, 0, 0, 0),\\n362.             cftime.DatetimeNoLeap(580, 12, 31, 0, 0, 0, 0),\\n363.             cftime.DatetimeNoLeap(700, 12, 31, 0, 0, 0, 0),\\n364.         ],\\n365.         dims=["time"],\\n366.     )\\n367.     assert_equal(result, expected)\\n368. \\n369. \\n370. @requires_cftime\\n371. @requires_dask\\n372. def test_cftime_datetime_mean_dask_error():\\n373.     times = cftime_range("2000", periods=4)\\n374.     da = DataArray(times, dims=["time"]).chunk()\\n375.     with pytest.raises(NotImplementedError):\\n376.         da.mean()\\n377. \\n378. \\n379. @pytest.mark.parametrize("dim_num", [1, 2])\\n380. @pytest.mark.parametrize("dtype", [float, int, np.float32, np.bool_])\\n381. @pytest.mark.parametrize("dask", [False, True])\\n382. @pytest.mark.parametrize("func", ["sum", "min", "max", "mean", "var"])\\n383. # TODO test cumsum, cumprod\\n384. @pytest.mark.parametrize("skipna", [False, True])\\n385. @pytest.mark.parametrize("aggdim", [None, "x"])\\n386. def test_reduce(dim_num, dtype, dask, func, skipna, aggdim):\\n387. \\n388.     if aggdim == "y" and dim_num < 2:\\n389.         pytest.skip("dim not in this test")\\n390. \\n391.     if dtype == np.bool_ and func == "mean":\\n392.         pytest.skip("numpy does not support this")\\n393. \\n394.     if dask and not has_dask:\\n395.         pytest.skip("requires dask")\\n396. \\n397.     if dask and skipna is False and dtype in [np.bool_]:\\n398.         pytest.skip("dask does not compute object-typed array")\\n399. \\n400.     rtol = 1e-04 if dtype == np.float32 else 1e-05\\n401. \\n402.     da = construct_dataarray(dim_num, dtype, contains_nan=True, dask=dask)\\n403.     axis = None if aggdim is None else da.get_axis_num(aggdim)\\n404. \\n405.     # TODO: remove these after resolving\\n406.     # https://github.com/dask/dask/issues/3245\\n407.     with warnings.catch_warnings():\\n408.         warnings.filterwarnings("ignore", "Mean of empty slice")\\n409.         warnings.filterwarnings("ignore", "All-NaN slice")\\n410.         warnings.filterwarnings("ignore", "invalid value encountered in")\\n411. \\n412.         if da.dtype.kind == "O" and skipna:\\n413.             # Numpy < 1.13 does not handle object-type array.\\n414.             try:\\n415.                 if skipna:\\n416.                     expected = getattr(np, f"nan{func}")(da.values, axis=axis)\\n417.                 else:\\n418.                     expected = getattr(np, func)(da.values, axis=axis)\\n419. \\n420.                 actual = getattr(da, func)(skipna=skipna, dim=aggdim)\\n421.                 assert_dask_array(actual, dask)\\n422.                 np.testing.assert_allclose(\\n423.                     actual.values, np.array(expected), rtol=1.0e-4, equal_nan=True\\n424.                 )\\n425.             except (TypeError, AttributeError, ZeroDivisionError):\\n426.                 # TODO currently, numpy does not support some methods such as\\n427.                 # nanmean for object dtype\\n428.                 pass\\n429. \\n430.         actual = getattr(da, func)(skipna=skipna, dim=aggdim)\\n431. \\n432.         # for dask case, make sure the result is the same for numpy backend\\n433.         expected = getattr(da.compute(), func)(skipna=skipna, dim=aggdim)\\n434.         assert_allclose(actual, expected, rtol=rtol)\\n435. \\n436.         # make sure the compatiblility with pandas\\\' results.\\n437.         if func in ["var", "std"]:\\n438.             expected = series_reduce(da, func, skipna=skipna, dim=aggdim, ddof=0)\\n439.             assert_allclose(actual, expected, rtol=rtol)\\n440.             # also check ddof!=0 case\\n441.             actual = getattr(da, func)(skipna=skipna, dim=aggdim, ddof=5)\\n442.             if dask:\\n443.                 assert isinstance(da.data, dask_array_type)\\n444.             expected = series_reduce(da, func, skipna=skipna, dim=aggdim, ddof=5)\\n445.             assert_allclose(actual, expected, rtol=rtol)\\n446.         else:\\n447.             expected = series_reduce(da, func, skipna=skipna, dim=aggdim)\\n448.             assert_allclose(actual, expected, rtol=rtol)\\n449. \\n450.         # make sure the dtype argument\\n451.         if func not in ["max", "min"]:\\n452.             actual = getattr(da, func)(skipna=skipna, dim=aggdim, dtype=float)\\n453.             assert_dask_array(actual, dask)\\n454.             assert actual.dtype == float\\n455. \\n456.         # without nan\\n457.         da = construct_dataarray(dim_num, dtype, contains_nan=False, dask=dask)\\n458.         actual = getattr(da, func)(skipna=skipna)\\n459.         if dask:\\n460.             assert isinstance(da.data, dask_array_type)\\n461.         expected = getattr(np, f"nan{func}")(da.values)\\n462.         if actual.dtype == object:\\n463.             assert actual.values == np.array(expected)\\n464.         else:\\n465.             assert np.allclose(actual.values, np.array(expected), rtol=rtol)\\n466. \\n467. \\n468. @pytest.mark.parametrize("dim_num", [1, 2])\\n469. @pytest.mark.parametrize("dtype", [float, int, np.float32, np.bool_, str])\\n470. @pytest.mark.parametrize("contains_nan", [True, False])\\n471. @pytest.mark.parametrize("dask", [False, True])\\n472. @pytest.mark.parametrize("func", ["min", "max"])\\n473. @pytest.mark.parametrize("skipna", [False, True])\\n474. @pytest.mark.parametrize("aggdim", ["x", "y"])\\n475. def test_argmin_max(dim_num, dtype, contains_nan, dask, func, skipna, aggdim):\\n476.     # pandas-dev/pandas#16830, we do not check consistency with pandas but\\n477.     # just make sure da[da.argmin()] == da.min()\\n478. \\n479.     if aggdim == "y" and dim_num < 2:\\n480.         pytest.skip("dim not in this test")\\n481. \\n482.     if dask and not has_dask:\\n483.         pytest.skip("requires dask")\\n484. \\n485.     if contains_nan:\\n486.         if not skipna:\\n487.             pytest.skip(\\n488.                 "numpy\\\'s argmin (not nanargmin) does not handle " "object-dtype"\\n489.             )\\n490.         if skipna and np.dtype(dtype).kind in "iufc":\\n491.             pytest.skip("numpy\\\'s nanargmin raises ValueError for all nan axis")\\n492.     da = construct_dataarray(dim_num, dtype, contains_nan=contains_nan, dask=dask)\\n493. \\n494.     with warnings.catch_warnings():\\n495.         warnings.filterwarnings("ignore", "All-NaN slice")\\n496. \\n497.         actual = da.isel(\\n498.             **{aggdim: getattr(da, "arg" + func)(dim=aggdim, skipna=skipna).compute()}\\n499.         )\\n500.         expected = getattr(da, func)(dim=aggdim, skipna=skipna)\', \'start_line_number\': 1, \'end_line_number\': 500}\n\n{\'relative_path\': \'xarray/tests/test_duck_array_ops.py\', \'content\': \'501.         assert_allclose(\\n502.             actual.drop_vars(list(actual.coords)),\\n503.             expected.drop_vars(list(expected.coords)),\\n504.         )\\n505. \\n506. \\n507. def test_argmin_max_error():\\n508.     da = construct_dataarray(2, np.bool_, contains_nan=True, dask=False)\\n509.     da[0] = np.nan\\n510.     with pytest.raises(ValueError):\\n511.         da.argmin(dim="y")\\n512. \\n513. \\n514. @pytest.mark.parametrize(\\n515.     "array",\\n516.     [\\n517.         np.array([np.datetime64("2000-01-01"), np.datetime64("NaT")]),\\n518.         np.array([np.timedelta64(1, "h"), np.timedelta64("NaT")]),\\n519.         np.array([0.0, np.nan]),\\n520.         np.array([1j, np.nan]),\\n521.         np.array(["foo", np.nan], dtype=object),\\n522.     ],\\n523. )\\n524. def test_isnull(array):\\n525.     expected = np.array([False, True])\\n526.     actual = duck_array_ops.isnull(array)\\n527.     np.testing.assert_equal(expected, actual)\\n528. \\n529. \\n530. @requires_dask\\n531. def test_isnull_with_dask():\\n532.     da = construct_dataarray(2, np.float32, contains_nan=True, dask=True)\\n533.     assert isinstance(da.isnull().data, dask_array_type)\\n534.     assert_equal(da.isnull().load(), da.load().isnull())\\n535. \\n536. \\n537. @pytest.mark.skipif(not has_dask, reason="This is for dask.")\\n538. @pytest.mark.parametrize("axis", [0, -1])\\n539. @pytest.mark.parametrize("window", [3, 8, 11])\\n540. @pytest.mark.parametrize("center", [True, False])\\n541. def test_dask_rolling(axis, window, center):\\n542.     import dask.array as da\\n543. \\n544.     x = np.array(np.random.randn(100, 40), dtype=float)\\n545.     dx = da.from_array(x, chunks=[(6, 30, 30, 20, 14), 8])\\n546. \\n547.     expected = rolling_window(\\n548.         x, axis=axis, window=window, center=center, fill_value=np.nan\\n549.     )\\n550.     actual = rolling_window(\\n551.         dx, axis=axis, window=window, center=center, fill_value=np.nan\\n552.     )\\n553.     assert isinstance(actual, da.Array)\\n554.     assert_array_equal(actual, expected)\\n555.     assert actual.shape == expected.shape\\n556. \\n557.     # we need to take care of window size if chunk size is small\\n558.     # window/2 should be smaller than the smallest chunk size.\\n559.     with pytest.raises(ValueError):\\n560.         rolling_window(dx, axis=axis, window=100, center=center, fill_value=np.nan)\\n561. \\n562. \\n563. @pytest.mark.skipif(not has_dask, reason="This is for dask.")\\n564. @pytest.mark.parametrize("axis", [0, -1, 1])\\n565. @pytest.mark.parametrize("edge_order", [1, 2])\\n566. def test_dask_gradient(axis, edge_order):\\n567.     import dask.array as da\\n568. \\n569.     array = np.array(np.random.randn(100, 5, 40))\\n570.     x = np.exp(np.linspace(0, 1, array.shape[axis]))\\n571. \\n572.     darray = da.from_array(array, chunks=[(6, 30, 30, 20, 14), 5, 8])\\n573.     expected = gradient(array, x, axis=axis, edge_order=edge_order)\\n574.     actual = gradient(darray, x, axis=axis, edge_order=edge_order)\\n575. \\n576.     assert isinstance(actual, da.Array)\\n577.     assert_array_equal(actual, expected)\\n578. \\n579. \\n580. @pytest.mark.parametrize("dim_num", [1, 2])\\n581. @pytest.mark.parametrize("dtype", [float, int, np.float32, np.bool_])\\n582. @pytest.mark.parametrize("dask", [False, True])\\n583. @pytest.mark.parametrize("func", ["sum", "prod"])\\n584. @pytest.mark.parametrize("aggdim", [None, "x"])\\n585. def test_min_count(dim_num, dtype, dask, func, aggdim):\\n586.     if dask and not has_dask:\\n587.         pytest.skip("requires dask")\\n588. \\n589.     da = construct_dataarray(dim_num, dtype, contains_nan=True, dask=dask)\\n590.     min_count = 3\\n591. \\n592.     actual = getattr(da, func)(dim=aggdim, skipna=True, min_count=min_count)\\n593.     expected = series_reduce(da, func, skipna=True, dim=aggdim, min_count=min_count)\\n594.     assert_allclose(actual, expected)\\n595.     assert_dask_array(actual, dask)\\n596. \\n597. \\n598. @pytest.mark.parametrize("func", ["sum", "prod"])\\n599. def test_min_count_dataset(func):\\n600.     da = construct_dataarray(2, dtype=float, contains_nan=True, dask=False)\\n601.     ds = Dataset({"var1": da}, coords={"scalar": 0})\\n602.     actual = getattr(ds, func)(dim="x", skipna=True, min_count=3)["var1"]\\n603.     expected = getattr(ds["var1"], func)(dim="x", skipna=True, min_count=3)\\n604.     assert_allclose(actual, expected)\\n605. \\n606. \\n607. @pytest.mark.parametrize("dtype", [float, int, np.float32, np.bool_])\\n608. @pytest.mark.parametrize("dask", [False, True])\\n609. @pytest.mark.parametrize("func", ["sum", "prod"])\\n610. def test_multiple_dims(dtype, dask, func):\\n611.     if dask and not has_dask:\\n612.         pytest.skip("requires dask")\\n613.     da = construct_dataarray(3, dtype, contains_nan=True, dask=dask)\\n614. \\n615.     actual = getattr(da, func)(("x", "y"))\\n616.     expected = getattr(getattr(da, func)("x"), func)("y")\\n617.     assert_allclose(actual, expected)\\n618. \\n619. \\n620. def test_docs():\\n621.     # with min_count\\n622.     actual = DataArray.sum.__doc__\\n623.     expected = dedent(\\n624.         """\\\\\\n625.         Reduce this DataArray\\\'s data by applying `sum` along some dimension(s).\\n626. \\n627.         Parameters\\n628.         ----------\\n629.         dim : str or sequence of str, optional\\n630.             Dimension(s) over which to apply `sum`.\\n631.         axis : int or sequence of int, optional\\n632.             Axis(es) over which to apply `sum`. Only one of the \\\'dim\\\'\\n633.             and \\\'axis\\\' arguments can be supplied. If neither are supplied, then\\n634.             `sum` is calculated over axes.\\n635.         skipna : bool, optional\\n636.             If True, skip missing values (as marked by NaN). By default, only\\n637.             skips missing values for float dtypes; other dtypes either do not\\n638.             have a sentinel missing value (int) or skipna=True has not been\\n639.             implemented (object, datetime64 or timedelta64).\\n640.         min_count : int, default: None\\n641.             The required number of valid values to perform the operation.\\n642.             If fewer than min_count non-NA values are present the result will\\n643.             be NA. New in version 0.10.8: Added with the default being None.\\n644.         keep_attrs : bool, optional\\n645.             If True, the attributes (`attrs`) will be copied from the original\\n646.             object to the new one.  If False (default), the new object will be\\n647.             returned without attributes.\\n648.         **kwargs : dict\\n649.             Additional keyword arguments passed on to the appropriate array\\n650.             function for calculating `sum` on this object\\\'s data.\\n651. \\n652.         Returns\\n653.         -------\\n654.         reduced : DataArray\\n655.             New DataArray object with `sum` applied to its data and the\\n656.             indicated dimension(s) removed.\\n657.         """\\n658.     )\\n659.     assert actual == expected\\n660. \\n661.     # without min_count\\n662.     actual = DataArray.std.__doc__\\n663.     expected = dedent(\\n664.         """\\\\\\n665.         Reduce this DataArray\\\'s data by applying `std` along some dimension(s).\\n666. \\n667.         Parameters\\n668.         ----------\\n669.         dim : str or sequence of str, optional\\n670.             Dimension(s) over which to apply `std`.\\n671.         axis : int or sequence of int, optional\\n672.             Axis(es) over which to apply `std`. Only one of the \\\'dim\\\'\\n673.             and \\\'axis\\\' arguments can be supplied. If neither are supplied, then\\n674.             `std` is calculated over axes.\\n675.         skipna : bool, optional\\n676.             If True, skip missing values (as marked by NaN). By default, only\\n677.             skips missing values for float dtypes; other dtypes either do not\\n678.             have a sentinel missing value (int) or skipna=True has not been\\n679.             implemented (object, datetime64 or timedelta64).\\n680.         keep_attrs : bool, optional\\n681.             If True, the attributes (`attrs`) will be copied from the original\\n682.             object to the new one.  If False (default), the new object will be\\n683.             returned without attributes.\\n684.         **kwargs : dict\\n685.             Additional keyword arguments passed on to the appropriate array\\n686.             function for calculating `std` on this object\\\'s data.\\n687. \\n688.         Returns\\n689.         -------\\n690.         reduced : DataArray\\n691.             New DataArray object with `std` applied to its data and the\\n692.             indicated dimension(s) removed.\\n693.         """\\n694.     )\\n695.     assert actual == expected\\n696. \\n697. \\n698. def test_datetime_to_numeric_datetime64():\\n699.     times = pd.date_range("2000", periods=5, freq="7D").values\\n700.     result = duck_array_ops.datetime_to_numeric(times, datetime_unit="h")\\n701.     expected = 24 * np.arange(0, 35, 7)\\n702.     np.testing.assert_array_equal(result, expected)\\n703. \\n704.     offset = times[1]\\n705.     result = duck_array_ops.datetime_to_numeric(times, offset=offset, datetime_unit="h")\\n706.     expected = 24 * np.arange(-7, 28, 7)\\n707.     np.testing.assert_array_equal(result, expected)\\n708. \\n709.     dtype = np.float32\\n710.     result = duck_array_ops.datetime_to_numeric(times, datetime_unit="h", dtype=dtype)\\n711.     expected = 24 * np.arange(0, 35, 7).astype(dtype)\\n712.     np.testing.assert_array_equal(result, expected)\\n713. \\n714. \\n715. @requires_cftime\\n716. def test_datetime_to_numeric_cftime():\\n717.     times = cftime_range("2000", periods=5, freq="7D", calendar="standard").values\\n718.     result = duck_array_ops.datetime_to_numeric(times, datetime_unit="h", dtype=int)\\n719.     expected = 24 * np.arange(0, 35, 7)\\n720.     np.testing.assert_array_equal(result, expected)\\n721. \\n722.     offset = times[1]\\n723.     result = duck_array_ops.datetime_to_numeric(\\n724.         times, offset=offset, datetime_unit="h", dtype=int\\n725.     )\\n726.     expected = 24 * np.arange(-7, 28, 7)\\n727.     np.testing.assert_array_equal(result, expected)\\n728. \\n729.     dtype = np.float32\\n730.     result = duck_array_ops.datetime_to_numeric(times, datetime_unit="h", dtype=dtype)\\n731.     expected = 24 * np.arange(0, 35, 7).astype(dtype)\\n732.     np.testing.assert_array_equal(result, expected)\\n733. \\n734. \\n735. @requires_cftime\\n736. def test_datetime_to_numeric_potential_overflow():\\n737.     import cftime\\n738. \\n739.     times = pd.date_range("2000", periods=5, freq="7D").values.astype("datetime64[us]")\\n740.     cftimes = cftime_range(\\n741.         "2000", periods=5, freq="7D", calendar="proleptic_gregorian"\\n742.     ).values\\n743. \\n744.     offset = np.datetime64("0001-01-01")\\n745.     cfoffset = cftime.DatetimeProlepticGregorian(1, 1, 1)\\n746. \\n747.     result = duck_array_ops.datetime_to_numeric(\\n748.         times, offset=offset, datetime_unit="D", dtype=int\\n749.     )\\n750.     cfresult = duck_array_ops.datetime_to_numeric(\\n751.         cftimes, offset=cfoffset, datetime_unit="D", dtype=int\\n752.     )\\n753. \\n754.     expected = 730119 + np.arange(0, 35, 7)\\n755. \\n756.     np.testing.assert_array_equal(result, expected)\\n757.     np.testing.assert_array_equal(cfresult, expected)\\n758. \\n759. \\n760. def test_py_timedelta_to_float():\\n761.     assert py_timedelta_to_float(dt.timedelta(days=1), "ns") == 86400 * 1e9\\n762.     assert py_timedelta_to_float(dt.timedelta(days=1e6), "ps") == 86400 * 1e18\\n763.     assert py_timedelta_to_float(dt.timedelta(days=1e6), "ns") == 86400 * 1e15\\n764.     assert py_timedelta_to_float(dt.timedelta(days=1e6), "us") == 86400 * 1e12\\n765.     assert py_timedelta_to_float(dt.timedelta(days=1e6), "ms") == 86400 * 1e9\\n766.     assert py_timedelta_to_float(dt.timedelta(days=1e6), "s") == 86400 * 1e6\\n767.     assert py_timedelta_to_float(dt.timedelta(days=1e6), "D") == 1e6\\n768. \\n769. \\n770. @pytest.mark.parametrize(\\n771.     "td, expected",\\n772.     ([np.timedelta64(1, "D"), 86400 * 1e9], [np.timedelta64(1, "ns"), 1.0]),\\n773. )\\n774. def test_np_timedelta64_to_float(td, expected):\\n775.     out = np_timedelta64_to_float(td, datetime_unit="ns")\\n776.     np.testing.assert_allclose(out, expected)\\n777.     assert isinstance(out, float)\\n778. \\n779.     out = np_timedelta64_to_float(np.atleast_1d(td), datetime_unit="ns")\\n780.     np.testing.assert_allclose(out, expected)\\n781. \\n782. \\n783. @pytest.mark.parametrize(\\n784.     "td, expected", ([pd.Timedelta(1, "D"), 86400 * 1e9], [pd.Timedelta(1, "ns"), 1.0])\\n785. )\\n786. def test_pd_timedelta_to_float(td, expected):\\n787.     out = pd_timedelta_to_float(td, datetime_unit="ns")\\n788.     np.testing.assert_allclose(out, expected)\\n789.     assert isinstance(out, float)\\n790. \\n791. \\n792. @pytest.mark.parametrize(\\n793.     "td", [dt.timedelta(days=1), np.timedelta64(1, "D"), pd.Timedelta(1, "D"), "1 day"]\\n794. )\\n795. def test_timedelta_to_numeric(td):\\n796.     # Scalar input\\n797.     out = timedelta_to_numeric(td, "ns")\\n798.     np.testing.assert_allclose(out, 86400 * 1e9)\\n799.     assert isinstance(out, float)\\n800. \\n801. \\n802. @pytest.mark.parametrize("use_dask", [True, False])\\n803. @pytest.mark.parametrize("skipna", [True, False])\\n804. def test_least_squares(use_dask, skipna):\\n805.     if use_dask and (not has_dask or not has_scipy):\\n806.         pytest.skip("requires dask and scipy")\\n807.     lhs = np.array([[1, 2], [1, 2], [3, 2]])\\n808.     rhs = DataArray(np.array([3, 5, 7]), dims=("y",))\\n809. \\n810.     if use_dask:\\n811.         rhs = rhs.chunk({"y": 1})\\n812. \\n813.     coeffs, residuals = least_squares(lhs, rhs.data, skipna=skipna)\\n814. \\n815.     np.testing.assert_allclose(coeffs, [1.5, 1.25])\\n816.     np.testing.assert_allclose(residuals, [2.0])\', \'start_line_number\': 501, \'end_line_number\': 816}\n\nNow generate the complete self-contained test case that reproduces the bug with the same error/exception.\n' additional_kwargs={} response_metadata={}
2025-12-23 06:32:39,901 - thread-127780863866560.app.lang_graph.nodes.bug_reproducing_write_node - DEBUG - content='```python\nimport numpy as np\nimport pytest\nimport xarray as xr\nfrom xarray.tests import assert_identical\n\n\ndef test_sum_min_count_multiple_dims():\n    """\n    Test sum with min_count on multiple dimensions.\n    Regression test for GH: #4372\n    """\n    da = xr.DataArray([[1.0, 2, 3], [4, 5, 6]])\n    actual = da.sum(dim=["dim_0", "dim_1"], min_count=1)\n    expected = xr.DataArray(21.0)\n    assert_identical(actual, expected)\n\n```' additional_kwargs={} response_metadata={'is_blocked': False, 'safety_ratings': [], 'citation_metadata': {'citations': [{'start_index': 1038, 'end_index': 1326, 'uri': 'https://github.com/HenFo/Geosoftware-II-AALLH', 'title': '', 'license_': ''}, {'start_index': 6184, 'end_index': 6447, 'uri': 'http://xarray-datasette.fly.dev/github/issues?_facet_date=updated_at&_facet_size=max&_facet=comments&state=closed&comments=20&_sort=user', 'title': '', 'license_': ''}, {'start_index': 13142, 'end_index': 13349, 'uri': 'https://github.com/timothyyu/ml_monorepo', 'title': '', 'license_': ''}]}, 'usage_metadata': {'prompt_token_count': 238939, 'candidates_token_count': 149, 'total_token_count': 247390, 'prompt_tokens_details': [{'modality': 1, 'token_count': 238939}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 149}], 'thoughts_token_count': 8302, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -9.243069027894295, 'model_name': 'gemini-2.5-pro'} id='run--8ce99a15-92e7-45d4-8659-588997d47870-0' usage_metadata={'input_tokens': 238939, 'output_tokens': 149, 'total_tokens': 247390, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 8302}}
2025-12-23 06:32:48,277 - thread-127780863866560.app.lang_graph.nodes.bug_reproducing_file_node - DEBUG - content='' additional_kwargs={'function_call': {'name': 'create_file', 'arguments': '{"relative_path": "xarray/tests/test_darray.py", "content": "import numpy as np\\nimport pytest\\nimport xarray as xr\\nfrom xarray.tests import assert_identical\\n\\n\\ndef test_sum_min_count_multiple_dims():\\n    \\"\\"\\"\\n    Test sum with min_count on multiple dimensions.\\n    Regression test for GH: #4372\\n    \\"\\"\\"\\n    da = xr.DataArray([[1.0, 2, 3], [4, 5, 6]])\\n    actual = da.sum(dim=[\\"dim_0\\", \\"dim_1\\"], min_count=1)\\n    expected = xr.DataArray(21.0)\\n    assert_identical(actual, expected)\\n"}'}, '__gemini_function_call_thought_signatures__': {'cd818050-5241-4e50-9db5-681c8d42e2fc': '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'}} response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 467, 'candidates_token_count': 163, 'total_token_count': 1305, 'prompt_tokens_details': [{'modality': 1, 'token_count': 467}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 163}], 'thoughts_token_count': 675, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.4226331184246789, 'model_name': 'gemini-2.5-pro'} id='run--c5a08115-8955-4ca8-83cf-5f6f39702909-0' tool_calls=[{'name': 'create_file', 'args': {'relative_path': 'xarray/tests/test_darray.py', 'content': 'import numpy as np\nimport pytest\nimport xarray as xr\nfrom xarray.tests import assert_identical\n\n\ndef test_sum_min_count_multiple_dims():\n    """\n    Test sum with min_count on multiple dimensions.\n    Regression test for GH: #4372\n    """\n    da = xr.DataArray([[1.0, 2, 3], [4, 5, 6]])\n    actual = da.sum(dim=["dim_0", "dim_1"], min_count=1)\n    expected = xr.DataArray(21.0)\n    assert_identical(actual, expected)\n'}, 'id': 'cd818050-5241-4e50-9db5-681c8d42e2fc', 'type': 'tool_call'}] usage_metadata={'input_tokens': 467, 'output_tokens': 163, 'total_tokens': 1305, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 675}}
2025-12-23 06:32:48,976 - thread-127780863866560.app.lang_graph.nodes.bug_reproducing_file_node - DEBUG - content='xarray/tests/test_darray.py' additional_kwargs={} response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 651, 'candidates_token_count': 11, 'total_token_count': 662, 'prompt_tokens_details': [{'modality': 1, 'token_count': 1326}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 11}], 'thoughts_token_count': 0, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -1.2559566212902693e-05, 'model_name': 'gemini-2.5-pro'} id='run--5b155958-54ef-498b-b84a-62a23b1b8ae2-0' usage_metadata={'input_tokens': 651, 'output_tokens': 11, 'total_tokens': 662, 'input_token_details': {'cache_read': 0}}
2025-12-23 06:32:49,007 - thread-127780863866560.app.lang_graph.nodes.git_diff_node - INFO - Generated patch:
diff --git a/xarray/tests/test_darray.py b/xarray/tests/test_darray.py
new file mode 100644
index 00000000..0e18771d
--- /dev/null
+++ b/xarray/tests/test_darray.py
@@ -0,0 +1,15 @@
+import numpy as np
+import pytest
+import xarray as xr
+from xarray.tests import assert_identical
+
+
+def test_sum_min_count_multiple_dims():
+    """
+    Test sum with min_count on multiple dimensions.
+    Regression test for GH: #4372
+    """
+    da = xr.DataArray([[1.0, 2, 3], [4, 5, 6]])
+    actual = da.sum(dim=["dim_0", "dim_1"], min_count=1)
+    expected = xr.DataArray(21.0)
+    assert_identical(actual, expected)

2025-12-23 06:32:49,008 - thread-127780863866560.app.lang_graph.nodes.update_container_node - INFO - Copy over all updated files to the container
2025-12-23 06:32:49,024 - thread-127780863866560.app.docker.base_container - INFO - Resetting git repository in the container
2025-12-23 06:32:49,024 - thread-127780863866560.app.docker.base_container - DEBUG - Executing command: git reset --hard
2025-12-23 06:32:49,171 - thread-127780863866560.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-23 06:32:49,171 - thread-127780863866560.app.docker.base_container - DEBUG - Command output:

HEAD is now at e05fddea Recreate @gajomi's #2070 to keep attrs when calling astype() (#4314)

2025-12-23 06:32:49,171 - thread-127780863866560.app.docker.base_container - DEBUG - Executing command: git clean -fd
2025-12-23 06:32:49,272 - thread-127780863866560.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-23 06:32:49,273 - thread-127780863866560.app.docker.base_container - DEBUG - Command output:


2025-12-23 06:32:49,273 - thread-127780863866560.app.docker.base_container - INFO - Updating files in the container after edits.
2025-12-23 06:32:49,273 - thread-127780863866560.app.docker.base_container - INFO - Creating directory xarray/tests in the container
2025-12-23 06:32:49,273 - thread-127780863866560.app.docker.base_container - DEBUG - Executing command: mkdir -p xarray/tests
2025-12-23 06:32:49,374 - thread-127780863866560.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-23 06:32:49,375 - thread-127780863866560.app.docker.base_container - DEBUG - Command output:


2025-12-23 06:32:49,375 - thread-127780863866560.app.docker.base_container - INFO - Updating xarray/tests/test_darray.py in the container
2025-12-23 06:32:49,419 - thread-127780863866560.app.docker.base_container - INFO - Files updated successfully
2025-12-23 06:32:51,923 - thread-127780863866560.app.lang_graph.nodes.bug_reproducing_execute_node - DEBUG - content='' additional_kwargs={'function_call': {'name': 'run_command', 'arguments': '{"command": "pytest xarray/tests/test_darray.py"}'}, '__gemini_function_call_thought_signatures__': {'2c806ad8-b84f-4370-9d2a-a5f8572a2b48': 'CroFAY89a18+giYOM5aLbVBnA65a/gqtlv2722sZms3BE2laB8bOiO3aClf2tgHf9FSCGwhC+QM1na6aFFOJ9Je90x+3ZmbUJuhnguwgu6MU7RnCj4iuW9apXD6UzHNQpzxjaXO+paLpAGuzZhe4/vuKi3o77pd/DCITpuGOYIn/+xIcUhj+TZ+2H720XQhqf6gmBsfifXMymsfeqNWv/uIuPwB0FPZ9wSgNqYAj1Dgd9snf5emANs7Epf1mTILYniXua54QQolRxmIFuVVh7eU5wIB8uu8iCXlEpLIgTZj0tImLkBU4amLFQbIjWkHcxm2rbrNDOib7WdlCzSgM48wKomFU8c+ctfx2RjSi22qNLTW3FpCqKpKcg4dtxGM7wfMX9EIIwIdgd9AZl8+asJMdqoTZk6h7GBMsVOS//Pfl0fJB0C58K4tFW5V6B+HoLCre5+T3D5vJdYAJvJqHrBuqpvWLga5yWT3y5vNIHvGaMfRMaK2x/6jH98YzB8ZZ1J1R/D9H683XzPgo11l7x+QomZ737hLCIGyJaVXGUPXTA+/EJH7XwQZtkYQYS2QpvyJ1hlf9vSaTzUuX0HIbUvqEac4i0PsSAO+2JIqD2wNbQMF5cy4XCc0iS+IXkdvrF3lUueppIb3Lg3HV0I8st4UjvmROZcYkhAUFw/yL/RYRA6EgA03yPGv3RO/cd6l+rgfXdAj1rK6NyUgc7PS9vnv0UYTFDjoYBI4jlJNbIGTFUvTRqI4da3Ba7Jir4yte6TyYS0kAVaO8suVe+dnOorx5c3k3wdhDaUNC+bjdvqXoXUFyLbq5dxUNsa4l1BDRdO1NThSAjPmxMkE5yC5cbBmvhR8DRlfS2eUojeZ7wijnmL0pcLM7kBfCmiM91zd/pBhLE7Kd7274sE6ogEc5eLM7WTOCar0x+1hMFSs='}} response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 462, 'candidates_token_count': 16, 'total_token_count': 644, 'prompt_tokens_details': [{'modality': 1, 'token_count': 462}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 16}], 'thoughts_token_count': 166, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.6378896832466125, 'model_name': 'gemini-2.5-pro'} id='run--b09a09fe-d418-4e1c-8c30-fd5e79b4a1b7-0' tool_calls=[{'name': 'run_command', 'args': {'command': 'pytest xarray/tests/test_darray.py'}, 'id': '2c806ad8-b84f-4370-9d2a-a5f8572a2b48', 'type': 'tool_call'}] usage_metadata={'input_tokens': 462, 'output_tokens': 16, 'total_tokens': 644, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 166}}
2025-12-23 06:32:51,926 - thread-127780863866560.app.docker.base_container - DEBUG - Executing command: pytest xarray/tests/test_darray.py
2025-12-23 06:33:03,157 - thread-127780863866560.app.docker.base_container - DEBUG - Command exit code: 1
2025-12-23 06:33:03,157 - thread-127780863866560.app.docker.base_container - DEBUG - Command output:

[1m============================= test session starts ==============================[0m
platform linux -- Python 3.10.15, pytest-7.4.0, pluggy-1.5.0
rootdir: /testbed
configfile: setup.cfg
plugins: cov-5.0.0, env-1.1.5, xdist-3.6.1, hypothesis-6.114.0
[1mcollecting ... [0m[1m
collecting 1 item                                                              [0m[1m
collected 1 item                                                               [0m

xarray/tests/test_darray.py [31mF[0m[31m                                            [100%][0m

=================================== FAILURES ===================================
[31m[1m_______________________ test_sum_min_count_multiple_dims _______________________[0m

    def test_sum_min_count_multiple_dims():
        """
        Test sum with min_count on multiple dimensions.
        Regression test for GH: #4372
        """
        da = xr.DataArray([[1.0, 2, 3], [4, 5, 6]])
>       actual = da.sum(dim=["dim_0", "dim_1"], min_count=1)

[1m[31mxarray/tests/test_darray.py[0m:13: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[1m[31mxarray/core/common.py[0m:46: in wrapped_func
    return self.reduce(func, dim, axis, skipna=skipna, **kwargs)
[1m[31mxarray/core/dataarray.py[0m:2347: in reduce
    var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
[1m[31mxarray/core/variable.py[0m:1640: in reduce
    data = func(input_data, axis=axis, **kwargs)
[1m[31mxarray/core/duck_array_ops.py[0m:335: in f
    return func(values, axis=axis, **kwargs)
[1m[31mxarray/core/nanops.py[0m:115: in nansum
    return _maybe_null_out(result, axis, mask, min_count)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

result = 21.0, axis = (0, 1)
mask = array([[False, False, False],
       [False, False, False]])
min_count = 1

    def _maybe_null_out(result, axis, mask, min_count=1):
        """
        xarray version of pandas.core.nanops._maybe_null_out
        """
        if hasattr(axis, "__len__"):  # if tuple or list
>           raise ValueError(
                "min_count is not available for reduction with more than one dimensions."
            )
[1m[31mE           ValueError: min_count is not available for reduction with more than one dimensions.[0m

[1m[31mxarray/core/nanops.py[0m:30: ValueError
[33m=============================== warnings summary ===============================[0m
xarray/__init__.py:1
  /testbed/xarray/__init__.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
    import pkg_resources

xarray/core/dask_array_compat.py:16
xarray/core/dask_array_compat.py:16
  /testbed/xarray/core/dask_array_compat.py:16: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask_version) >= LooseVersion("2.0.0"):

xarray/core/dask_array_compat.py:149
xarray/core/dask_array_compat.py:149
  /testbed/xarray/core/dask_array_compat.py:149: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask_version) >= LooseVersion("2.8.1"):

xarray/core/dask_array_compat.py:186
xarray/core/dask_array_compat.py:186
  /testbed/xarray/core/dask_array_compat.py:186: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask_version) > LooseVersion("2.9.0"):

xarray/core/pdcompat.py:45
  /testbed/xarray/core/pdcompat.py:45: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(pd.__version__) < "0.25.0":

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    other = LooseVersion(other)

xarray/tests/__init__.py:58
xarray/tests/__init__.py:58
  /testbed/xarray/tests/__init__.py:58: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    return version.LooseVersion(vstring)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
[36m[1m=========================== short test summary info ============================[0m
[31mFAILED[0m xarray/tests/test_darray.py::[1mtest_sum_min_count_multiple_dims[0m - ValueError: min_count is not available for reduction with more than one dim...
[31m======================== [31m[1m1 failed[0m, [33m11 warnings[0m[31m in 9.75s[0m[31m ========================[0m

2025-12-23 06:33:13,959 - thread-127780863866560.app.lang_graph.nodes.bug_reproducing_execute_node - DEBUG - content='\n============================= test session starts ==============================\nplatform linux -- Python 3.10.15, pytest-7.4.0, pluggy-1.5.0\nrootdir: /testbed\nconfigfile: setup.cfg\nplugins: cov-5.0.0, env-1.1.5, xdist-3.6.1, hypothesis-6.114.0\ncollecting ... \ncollecting 1 item                                                              \n\ncollected 1 item                                                               \n\nxarray/tests/test_darray.py F                                            [100%]\n\n=================================== FAILURES ===================================\n_______________________ test_sum_min_count_multiple_dims _______________________\n\n    def test_sum_min_count_multiple_dims():\n        """\n        Test sum with min_count on multiple dimensions.\n        Regression test for GH: #4372\n        """\n        da = xr.DataArray([[1.0, 2, 3], [4, 5, 6]])\n>       actual = da.sum(dim=["dim_0", "dim_1"], min_count=1)\n\nxarray/tests/test_darray.py:13: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nxarray/core/common.py:46: in wrapped_func\n    return self.reduce(func, dim, axis, skipna=skipna, **kwargs)\nxarray/core/dataarray.py:2347: in reduce\n    var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)\nxarray/core/variable.py:1640: in reduce\n    data = func(input_data, axis=axis, **kwargs)\nxarray/core/duck_array_ops.py:335: in f\n    return func(values, axis=axis, **kwargs)\nxarray/core/nanops.py:115: in nansum\n    return _maybe_null_out(result, axis, mask, min_count)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nresult = 21.0, axis = (0, 1)\nmask = array([[False, False, False],\n       [False, False, False]])\nmin_count = 1\n\n    def _maybe_null_out(result, axis, mask, min_count=1):\n        """\n        xarray version of pandas.core.nanops._maybe_null_out\n        """\n        if hasattr(axis, "__len__"):  # if tuple or list\n>           raise ValueError(\n                "min_count is not available for reduction with more than one dimensions."\n            )\nE           ValueError: min_count is not available for reduction with more than one dimensions.\n\nxarray/core/nanops.py:30: ValueError\n=============================== warnings summary ===============================\nxarray/__init__.py:1\n  /testbed/xarray/__init__.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n    import pkg_resources\n\nxarray/core/dask_array_compat.py:16\nxarray/core/dask_array_compat.py:16\n  /testbed/xarray/core/dask_array_compat.py:16: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n    if LooseVersion(dask_version) >= LooseVersion("2.0.0"):\n\nxarray/core/dask_array_compat.py:149\nxarray/core/dask_array_compat.py:149\n  /testbed/xarray/core/dask_array_compat.py:149: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n    if LooseVersion(dask_version) >= LooseVersion("2.8.1"):\n\nxarray/core/dask_array_compat.py:186\nxarray/core/dask_array_compat.py:186\n  /testbed/xarray/core/dask_array_compat.py:186: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n    if LooseVersion(dask_version) > LooseVersion("2.9.0"):\n\nxarray/core/pdcompat.py:45\n  /testbed/xarray/core/pdcompat.py:45: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n    if LooseVersion(pd.__version__) < "0.25.0":\n\n../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345\n  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n    other = LooseVersion(other)\n\nxarray/tests/__init__.py:58\nxarray/tests/__init__.py:58\n  /testbed/xarray/tests/__init__.py:58: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n    return version.LooseVersion(vstring)\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ============================\nFAILED xarray/tests/test_darray.py::test_sum_min_count_multiple_dims - ValueError: min_count is not available for reduction with more than one dim...\n======================== 1 failed, 11 warnings in 9.75s ========================' additional_kwargs={} response_metadata={'is_blocked': False, 'safety_ratings': [], 'citation_metadata': {'citations': [{'start_index': 2201, 'end_index': 2513, 'uri': 'https://github.com/HenFo/Geosoftware-II-AALLH', 'title': '', 'license_': ''}]}, 'usage_metadata': {'prompt_token_count': 2088, 'candidates_token_count': 1418, 'total_token_count': 3612, 'prompt_tokens_details': [{'modality': 1, 'token_count': 2254}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 1418}], 'thoughts_token_count': 106, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.006999346702156013, 'model_name': 'gemini-2.5-pro'} id='run--1e867f03-0d0e-4248-8a1a-57cc0e23c630-0' usage_metadata={'input_tokens': 2088, 'output_tokens': 1418, 'total_tokens': 3612, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 106}}
2025-12-23 06:33:25,341 - thread-127780863866560.app.lang_graph.nodes.bug_reproducing_structured_node - DEBUG - reproduced_bug=True reproduced_bug_failure_log='============================= test session starts ==============================\nplatform linux -- Python 3.10.15, pytest-7.4.0, pluggy-1.5.0\nrootdir: /testbed\nconfigfile: setup.cfg\nplugins: cov-5.0.0, env-1.1.5, xdist-3.6.1, hypothesis-6.114.0\ncollecting ... \ncollecting 1 item                                                              \ncollected 1 item                                                               \n\nxarray/tests/test_darray.py F                                            [100%]\n\n=================================== FAILURES ===================================\n_______________________ test_sum_min_count_multiple_dims _______________________\n\n    def test_sum_min_count_multiple_dims():\n        """\n        Test sum with min_count on multiple dimensions.\n        Regression test for GH: #4372\n        """\n        da = xr.DataArray([[1.0, 2, 3], [4, 5, 6]])\n>       actual = da.sum(dim=["dim_0", "dim_1"], min_count=1)\n\nxarray/tests/test_darray.py:13: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nxarray/core/common.py:46: in wrapped_func\n    return self.reduce(func, dim, axis, skipna=skipna, **kwargs)\nxarray/core/dataarray.py:2347: in reduce\n    var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)\nxarray/core/variable.py:1640: in reduce\n    data = func(input_data, axis=axis, **kwargs)\nxarray/core/duck_array_ops.py:335: in f\n    return func(values, axis=axis, **kwargs)\nxarray/core/nanops.py:115: in nansum\n    return _maybe_null_out(result, axis, mask, min_count)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nresult = 21.0, axis = (0, 1)\nmask = array([[False, False, False],\n       [False, False, False]])\nmin_count = 1\n\n    def _maybe_null_out(result, axis, mask, min_count=1):\n        """\n        xarray version of pandas.core.nanops._maybe_null_out\n        """\n        if hasattr(axis, "__len__"):  # if tuple or list\n>           raise ValueError(\n                "min_count is not available for reduction with more than one dimensions."\n            )\nE           ValueError: min_count is not available for reduction with more than one dimensions.\n\nxarray/core/nanops.py:30: ValueError' reproduced_bug_commands=['pytest xarray/tests/test_darray.py']
2025-12-23 06:33:25,342 - thread-127780863866560.app.lang_graph.nodes.dual_version_validation_node - INFO - Starting dual-version validation
2025-12-23 06:33:25,343 - thread-127780863866560.app.lang_graph.nodes.dual_version_validation_node - INFO - Running test on old codebase (should fail)
2025-12-23 06:33:25,343 - thread-127780863866560.app.docker.base_container - DEBUG - Executing command: pytest xarray/tests/test_darray.py
2025-12-23 06:33:31,326 - thread-127780863866560.app.docker.base_container - DEBUG - Command exit code: 1
2025-12-23 06:33:31,326 - thread-127780863866560.app.docker.base_container - DEBUG - Command output:

[1m============================= test session starts ==============================[0m
platform linux -- Python 3.10.15, pytest-7.4.0, pluggy-1.5.0
rootdir: /testbed
configfile: setup.cfg
plugins: cov-5.0.0, env-1.1.5, xdist-3.6.1, hypothesis-6.114.0
[1mcollecting ... [0m[1m
collecting 1 item                                                              [0m[1m
collected 1 item                                                               [0m

xarray/tests/test_darray.py [31mF[0m[31m                                            [100%][0m

=================================== FAILURES ===================================
[31m[1m_______________________ test_sum_min_count_multiple_dims _______________________[0m

    def test_sum_min_count_multiple_dims():
        """
        Test sum with min_count on multiple dimensions.
        Regression test for GH: #4372
        """
        da = xr.DataArray([[1.0, 2, 3], [4, 5, 6]])
>       actual = da.sum(dim=["dim_0", "dim_1"], min_count=1)

[1m[31mxarray/tests/test_darray.py[0m:13: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[1m[31mxarray/core/common.py[0m:46: in wrapped_func
    return self.reduce(func, dim, axis, skipna=skipna, **kwargs)
[1m[31mxarray/core/dataarray.py[0m:2347: in reduce
    var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
[1m[31mxarray/core/variable.py[0m:1640: in reduce
    data = func(input_data, axis=axis, **kwargs)
[1m[31mxarray/core/duck_array_ops.py[0m:335: in f
    return func(values, axis=axis, **kwargs)
[1m[31mxarray/core/nanops.py[0m:115: in nansum
    return _maybe_null_out(result, axis, mask, min_count)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

result = 21.0, axis = (0, 1)
mask = array([[False, False, False],
       [False, False, False]])
min_count = 1

    def _maybe_null_out(result, axis, mask, min_count=1):
        """
        xarray version of pandas.core.nanops._maybe_null_out
        """
        if hasattr(axis, "__len__"):  # if tuple or list
>           raise ValueError(
                "min_count is not available for reduction with more than one dimensions."
            )
[1m[31mE           ValueError: min_count is not available for reduction with more than one dimensions.[0m

[1m[31mxarray/core/nanops.py[0m:30: ValueError
[33m=============================== warnings summary ===============================[0m
xarray/__init__.py:1
  /testbed/xarray/__init__.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
    import pkg_resources

xarray/core/dask_array_compat.py:16
xarray/core/dask_array_compat.py:16
  /testbed/xarray/core/dask_array_compat.py:16: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask_version) >= LooseVersion("2.0.0"):

xarray/core/dask_array_compat.py:149
xarray/core/dask_array_compat.py:149
  /testbed/xarray/core/dask_array_compat.py:149: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask_version) >= LooseVersion("2.8.1"):

xarray/core/dask_array_compat.py:186
xarray/core/dask_array_compat.py:186
  /testbed/xarray/core/dask_array_compat.py:186: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask_version) > LooseVersion("2.9.0"):

xarray/core/pdcompat.py:45
  /testbed/xarray/core/pdcompat.py:45: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(pd.__version__) < "0.25.0":

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    other = LooseVersion(other)

xarray/tests/__init__.py:58
xarray/tests/__init__.py:58
  /testbed/xarray/tests/__init__.py:58: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    return version.LooseVersion(vstring)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
[36m[1m=========================== short test summary info ============================[0m
[31mFAILED[0m xarray/tests/test_darray.py::[1mtest_sum_min_count_multiple_dims[0m - ValueError: min_count is not available for reduction with more than one dim...
[31m======================== [31m[1m1 failed[0m, [33m11 warnings[0m[31m in 4.54s[0m[31m ========================[0m

2025-12-23 06:33:31,326 - thread-127780863866560.app.lang_graph.nodes.dual_version_validation_node - INFO - Test failed on old codebase (as expected)
2025-12-23 06:33:31,326 - thread-127780863866560.app.lang_graph.nodes.dual_version_validation_node - INFO - Applying patch to repository
2025-12-23 06:33:31,331 - thread-127780863866560.app.lang_graph.nodes.dual_version_validation_node - INFO - Updating container with patched code
2025-12-23 06:33:31,331 - thread-127780863866560.app.docker.base_container - INFO - Updating files in the container after edits.
2025-12-23 06:33:31,331 - thread-127780863866560.app.docker.base_container - INFO - Creating directory xarray/core in the container
2025-12-23 06:33:31,331 - thread-127780863866560.app.docker.base_container - DEBUG - Executing command: mkdir -p xarray/core
2025-12-23 06:33:31,432 - thread-127780863866560.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-23 06:33:31,433 - thread-127780863866560.app.docker.base_container - DEBUG - Command output:


2025-12-23 06:33:31,433 - thread-127780863866560.app.docker.base_container - INFO - Updating xarray/core/nanops.py in the container
2025-12-23 06:33:31,441 - thread-127780863866560.app.docker.base_container - INFO - Files updated successfully
2025-12-23 06:33:31,441 - thread-127780863866560.app.lang_graph.nodes.dual_version_validation_node - INFO - Running test on patched codebase (should pass)
2025-12-23 06:33:31,441 - thread-127780863866560.app.docker.base_container - DEBUG - Executing command: pytest xarray/tests/test_darray.py
2025-12-23 06:33:36,614 - thread-127780863866560.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-23 06:33:36,614 - thread-127780863866560.app.docker.base_container - DEBUG - Command output:

[1m============================= test session starts ==============================[0m
platform linux -- Python 3.10.15, pytest-7.4.0, pluggy-1.5.0
rootdir: /testbed
configfile: setup.cfg
plugins: cov-5.0.0, env-1.1.5, xdist-3.6.1, hypothesis-6.114.0
[1mcollecting ... [0m[1m
collecting 1 item                                                              [0m[1m
collected 1 item                                                               [0m

xarray/tests/test_darray.py [32m.[0m[33m                                            [100%][0m

[33m=============================== warnings summary ===============================[0m
xarray/__init__.py:1
  /testbed/xarray/__init__.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
    import pkg_resources

xarray/core/dask_array_compat.py:16
xarray/core/dask_array_compat.py:16
  /testbed/xarray/core/dask_array_compat.py:16: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask_version) >= LooseVersion("2.0.0"):

xarray/core/dask_array_compat.py:149
xarray/core/dask_array_compat.py:149
  /testbed/xarray/core/dask_array_compat.py:149: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask_version) >= LooseVersion("2.8.1"):

xarray/core/dask_array_compat.py:186
xarray/core/dask_array_compat.py:186
  /testbed/xarray/core/dask_array_compat.py:186: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask_version) > LooseVersion("2.9.0"):

xarray/core/pdcompat.py:45
  /testbed/xarray/core/pdcompat.py:45: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(pd.__version__) < "0.25.0":

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    other = LooseVersion(other)

xarray/tests/__init__.py:58
xarray/tests/__init__.py:58
  /testbed/xarray/tests/__init__.py:58: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    return version.LooseVersion(vstring)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
[33m======================== [32m1 passed[0m, [33m[1m11 warnings[0m[33m in 4.03s[0m[33m ========================[0m

2025-12-23 06:33:36,614 - thread-127780863866560.app.lang_graph.nodes.dual_version_validation_node - INFO - Resetting repository to clean state
2025-12-23 06:33:36,626 - thread-127780863866560.app.lang_graph.nodes.dual_version_validation_node - INFO - Validation PASSED: Test fails on old, passes on new
2025-12-23 06:33:36,627 - thread-127780863866560.app.docker.base_container - INFO - Cleaning up container and temporary files
2025-12-23 06:33:36,627 - thread-127780863866560.app.docker.base_container - INFO - Closing persistent shell...
2025-12-23 06:33:36,727 - thread-127780863866560.app.docker.base_container - INFO - Cleaning up container and temporary files
