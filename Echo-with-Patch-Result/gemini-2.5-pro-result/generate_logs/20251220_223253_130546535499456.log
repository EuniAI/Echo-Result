2025-12-20 22:32:55,247 - thread-130546535499456.app.docker.base_container - INFO - Created temporary project directory: /tmp/tmp64vewp26/django
2025-12-20 22:32:55,247 - thread-130546535499456.app.docker.base_container - DEBUG - Using workdir: /testbed
2025-12-20 22:32:55,247 - thread-130546535499456.app.docker.base_container - INFO - Pulling docker image: swebench/sweb.eval.x86_64.django_1776_django-15561:v1
2025-12-20 22:33:59,790 - thread-130546535499456.app.docker.base_container - INFO - Tagging pulled image as: prometheus_user_defined_container_f72852a865
2025-12-20 22:33:59,798 - thread-130546535499456.app.docker.base_container - INFO - Starting container from image prometheus_user_defined_container_f72852a865
2025-12-20 22:34:00,115 - thread-130546535499456.app.docker.base_container - INFO - Starting persistent shell for interactive mode...
2025-12-20 22:34:01,034 - thread-130546535499456.app.docker.base_container - INFO - Persistent shell is ready
2025-12-20 22:34:01,035 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: . /opt/miniconda3/bin/activate
2025-12-20 22:34:01,265 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-20 22:34:01,266 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:


2025-12-20 22:34:01,266 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: conda activate testbed
2025-12-20 22:34:01,511 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-20 22:34:01,511 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:


2025-12-20 22:34:01,511 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: python -m pip install -e .
2025-12-20 22:34:09,727 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-20 22:34:09,727 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:

Obtaining file:///testbed
  Installing build dependencies ... [?25l- \ | / - done
[?25h  Checking if build backend supports build_editable ... [?25ldone
[?25h  Getting requirements to build editable ... [?25l- \ | / - done
[?25h  Preparing editable metadata (pyproject.toml) ... [?25l- \ | done
[?25hRequirement already satisfied: asgiref>=3.4.1 in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from Django==4.1.dev20220406104243) (3.8.1)
Requirement already satisfied: sqlparse>=0.2.2 in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from Django==4.1.dev20220406104243) (0.5.1)
Requirement already satisfied: typing-extensions>=4 in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from asgiref>=3.4.1->Django==4.1.dev20220406104243) (4.12.2)
Building wheels for collected packages: Django
  Building editable for Django (pyproject.toml) ... [?25l- \ | / - \ | done
[?25h  Created wheel for Django: filename=django-4.1.dev20220406104243-0.editable-py3-none-any.whl size=27020 sha256=95b1d319570b58b736afb3c96e4f77c0aa61dafa1ea7f84c605b32023e45fa00
  Stored in directory: /tmp/pip-ephem-wheel-cache-3u8ehcwz/wheels/7d/66/67/70d1ee2124ccf21d601c352e25cdca10f611f7c8b3f9ffb9e4
Successfully built Django
Installing collected packages: Django
  Attempting uninstall: Django
    Found existing installation: Django 4.1.dev20220406104243
    Uninstalling Django-4.1.dev20220406104243:
      Successfully uninstalled Django-4.1.dev20220406104243
Successfully installed Django-4.1.dev20220406104243
[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.[0m[33m

2025-12-20 22:34:09,758 - thread-130546535499456.app.lang_graph.nodes.focal_code_retrieval_node - INFO - Retrieving focal code context via external CRA
2025-12-20 22:40:50,058 - thread-130546535499456.app.lang_graph.nodes.focal_code_retrieval_node - INFO - Focal code context retrieved: 3 items
2025-12-20 22:40:50,060 - thread-130546535499456.app.lang_graph.nodes.test_case_retrieval_node - INFO - Retrieving test case context via external CRA
2025-12-20 22:48:04,355 - thread-130546535499456.app.lang_graph.nodes.test_case_retrieval_node - INFO - Test case context retrieved: 7 items
2025-12-20 22:48:04,367 - thread-130546535499456.app.lang_graph.nodes.bug_reproducing_write_message_node - DEBUG - Sending message to BugReproducingWriteNode:
content='Issue title:\nAlterField operation should be noop when adding/changing choices on SQLite.\n\nIssue description: \nDescription\n\t\nwhile writing a test case for #33470 i found that for sqlite, even a seemingly db-transparent change like adding choices still generates sql (new table + insert + drop + rename) even though this shouldn\'t be needed. on e.g. postgres the same migration generates no sql\n\nIssue comments:\n\n\nPatch Information:\n```diff\ndiff --git a/django/db/backends/base/schema.py b/django/db/backends/base/schema.py\nindex 9d6952df21..85d53e3b1a 100644\n--- a/django/db/backends/base/schema.py\n+++ b/django/db/backends/base/schema.py\n@@ -1378,6 +1378,7 @@ class BaseDatabaseSchemaEditor:\n         # - adding only a db_column and the column name is not changed\n         non_database_attrs = [\n             "blank",\n+            "choices",\n             "db_column",\n             "editable",\n             "error_messages",\n\n```\n\nFocal Code Context (relevant code that may need fixing):\n{\'relative_path\': \'django/db/backends/base/schema.py\', \'content\': \'69. class BaseDatabaseSchemaEditor:\\n70.     """\\n71.     This class and its subclasses are responsible for emitting schema-changing\\n72.     statements to the databases - model creation/removal/alteration, field\\n73.     renaming, index fiddling, and so on.\\n74.     """\\n75. \\n76.     # Overrideable SQL templates\\n77.     sql_create_table = "CREATE TABLE %(table)s (%(definition)s)"\\n78.     sql_rename_table = "ALTER TABLE %(old_table)s RENAME TO %(new_table)s"\\n79.     sql_retablespace_table = "ALTER TABLE %(table)s SET TABLESPACE %(new_tablespace)s"\\n80.     sql_delete_table = "DROP TABLE %(table)s CASCADE"\\n81. \\n82.     sql_create_column = "ALTER TABLE %(table)s ADD COLUMN %(column)s %(definition)s"\\n83.     sql_alter_column = "ALTER TABLE %(table)s %(changes)s"\\n84.     sql_alter_column_type = "ALTER COLUMN %(column)s TYPE %(type)s"\\n85.     sql_alter_column_null = "ALTER COLUMN %(column)s DROP NOT NULL"\\n86.     sql_alter_column_not_null = "ALTER COLUMN %(column)s SET NOT NULL"\\n87.     sql_alter_column_default = "ALTER COLUMN %(column)s SET DEFAULT %(default)s"\\n88.     sql_alter_column_no_default = "ALTER COLUMN %(column)s DROP DEFAULT"\\n89.     sql_alter_column_no_default_null = sql_alter_column_no_default\\n90.     sql_alter_column_collate = "ALTER COLUMN %(column)s TYPE %(type)s%(collation)s"\\n91.     sql_delete_column = "ALTER TABLE %(table)s DROP COLUMN %(column)s CASCADE"\\n92.     sql_rename_column = (\\n93.         "ALTER TABLE %(table)s RENAME COLUMN %(old_column)s TO %(new_column)s"\\n94.     )\\n95.     sql_update_with_default = (\\n96.         "UPDATE %(table)s SET %(column)s = %(default)s WHERE %(column)s IS NULL"\\n97.     )\\n98. \\n99.     sql_unique_constraint = "UNIQUE (%(columns)s)%(deferrable)s"\\n100.     sql_check_constraint = "CHECK (%(check)s)"\\n101.     sql_delete_constraint = "ALTER TABLE %(table)s DROP CONSTRAINT %(name)s"\\n102.     sql_constraint = "CONSTRAINT %(name)s %(constraint)s"\\n103. \\n104.     sql_create_check = "ALTER TABLE %(table)s ADD CONSTRAINT %(name)s CHECK (%(check)s)"\\n105.     sql_delete_check = sql_delete_constraint\\n106. \\n107.     sql_create_unique = (\\n108.         "ALTER TABLE %(table)s ADD CONSTRAINT %(name)s "\\n109.         "UNIQUE (%(columns)s)%(deferrable)s"\\n110.     )\\n111.     sql_delete_unique = sql_delete_constraint\\n112. \\n113.     sql_create_fk = (\\n114.         "ALTER TABLE %(table)s ADD CONSTRAINT %(name)s FOREIGN KEY (%(column)s) "\\n115.         "REFERENCES %(to_table)s (%(to_column)s)%(deferrable)s"\\n116.     )\\n117.     sql_create_inline_fk = None\\n118.     sql_create_column_inline_fk = None\\n119.     sql_delete_fk = sql_delete_constraint\\n120. \\n121.     sql_create_index = (\\n122.         "CREATE INDEX %(name)s ON %(table)s "\\n123.         "(%(columns)s)%(include)s%(extra)s%(condition)s"\\n124.     )\\n125.     sql_create_unique_index = (\\n126.         "CREATE UNIQUE INDEX %(name)s ON %(table)s "\\n127.         "(%(columns)s)%(include)s%(condition)s"\\n128.     )\\n129.     sql_delete_index = "DROP INDEX %(name)s"\\n130. \\n131.     sql_create_pk = (\\n132.         "ALTER TABLE %(table)s ADD CONSTRAINT %(name)s PRIMARY KEY (%(columns)s)"\\n133.     )\\n134.     sql_delete_pk = sql_delete_constraint\\n135. \\n136.     sql_delete_procedure = "DROP PROCEDURE %(procedure)s"\\n137. \\n138.     def __init__(self, connection, collect_sql=False, atomic=True):\\n139.         self.connection = connection\\n140.         self.collect_sql = collect_sql\\n141.         if self.collect_sql:\\n142.             self.collected_sql = []\\n143.         self.atomic_migration = self.connection.features.can_rollback_ddl and atomic\\n144. \\n145.     # State-managing methods\\n146. \\n147.     def __enter__(self):\\n148.         self.deferred_sql = []\\n149.         if self.atomic_migration:\\n150.             self.atomic = atomic(self.connection.alias)\\n151.             self.atomic.__enter__()\\n152.         return self\\n153. \\n154.     def __exit__(self, exc_type, exc_value, traceback):\\n155.         if exc_type is None:\\n156.             for sql in self.deferred_sql:\\n157.                 self.execute(sql)\\n158.         if self.atomic_migration:\\n159.             self.atomic.__exit__(exc_type, exc_value, traceback)\\n160. \\n161.     # Core utility functions\\n162. \\n163.     def execute(self, sql, params=()):\\n164.         """Execute the given SQL statement, with optional parameters."""\\n165.         # Don\\\'t perform the transactional DDL check if SQL is being collected\\n166.         # as it\\\'s not going to be executed anyway.\\n167.         if (\\n168.             not self.collect_sql\\n169.             and self.connection.in_atomic_block\\n170.             and not self.connection.features.can_rollback_ddl\\n171.         ):\\n172.             raise TransactionManagementError(\\n173.                 "Executing DDL statements while in a transaction on databases "\\n174.                 "that can\\\'t perform a rollback is prohibited."\\n175.             )\\n176.         # Account for non-string statement objects.\\n177.         sql = str(sql)\\n178.         # Log the command we\\\'re running, then run it\\n179.         logger.debug(\\n180.             "%s; (params %r)", sql, params, extra={"params": params, "sql": sql}\\n181.         )\\n182.         if self.collect_sql:\\n183.             ending = "" if sql.rstrip().endswith(";") else ";"\\n184.             if params is not None:\\n185.                 self.collected_sql.append(\\n186.                     (sql % tuple(map(self.quote_value, params))) + ending\\n187.                 )\\n188.             else:\\n189.                 self.collected_sql.append(sql + ending)\\n190.         else:\\n191.             with self.connection.cursor() as cursor:\\n192.                 cursor.execute(sql, params)\\n193. \\n194.     def quote_name(self, name):\\n195.         return self.connection.ops.quote_name(name)\\n196. \\n197.     def table_sql(self, model):\\n198.         """Take a model and return its table definition."""\\n199.         # Add any unique_togethers (always deferred, as some fields might be\\n200.         # created afterward, like geometry fields with some backends).\\n201.         for field_names in model._meta.unique_together:\\n202.             fields = [model._meta.get_field(field) for field in field_names]\\n203.             self.deferred_sql.append(self._create_unique_sql(model, fields))\\n204.         # Create column SQL, add FK deferreds if needed.\\n205.         column_sqls = []\\n206.         params = []\\n207.         for field in model._meta.local_fields:\\n208.             # SQL.\\n209.             definition, extra_params = self.column_sql(model, field)\\n210.             if definition is None:\\n211.                 continue\\n212.             # Check constraints can go on the column SQL here.\\n213.             db_params = field.db_parameters(connection=self.connection)\\n214.             if db_params["check"]:\\n215.                 definition += " " + self.sql_check_constraint % db_params\\n216.             # Autoincrement SQL (for backends with inline variant).\\n217.             col_type_suffix = field.db_type_suffix(connection=self.connection)\\n218.             if col_type_suffix:\\n219.                 definition += " %s" % col_type_suffix\\n220.             params.extend(extra_params)\\n221.             # FK.\\n222.             if field.remote_field and field.db_constraint:\\n223.                 to_table = field.remote_field.model._meta.db_table\\n224.                 to_column = field.remote_field.model._meta.get_field(\\n225.                     field.remote_field.field_name\\n226.                 ).column\\n227.                 if self.sql_create_inline_fk:\\n228.                     definition += " " + self.sql_create_inline_fk % {\\n229.                         "to_table": self.quote_name(to_table),\\n230.                         "to_column": self.quote_name(to_column),\\n231.                     }\\n232.                 elif self.connection.features.supports_foreign_keys:\\n233.                     self.deferred_sql.append(\\n234.                         self._create_fk_sql(\\n235.                             model, field, "_fk_%(to_table)s_%(to_column)s"\\n236.                         )\\n237.                     )\\n238.             # Add the SQL to our big list.\\n239.             column_sqls.append(\\n240.                 "%s %s"\\n241.                 % (\\n242.                     self.quote_name(field.column),\\n243.                     definition,\\n244.                 )\\n245.             )\\n246.             # Autoincrement SQL (for backends with post table definition\\n247.             # variant).\\n248.             if field.get_internal_type() in (\\n249.                 "AutoField",\\n250.                 "BigAutoField",\\n251.                 "SmallAutoField",\\n252.             ):\\n253.                 autoinc_sql = self.connection.ops.autoinc_sql(\\n254.                     model._meta.db_table, field.column\\n255.                 )\\n256.                 if autoinc_sql:\\n257.                     self.deferred_sql.extend(autoinc_sql)\\n258.         constraints = [\\n259.             constraint.constraint_sql(model, self)\\n260.             for constraint in model._meta.constraints\\n261.         ]\\n262.         sql = self.sql_create_table % {\\n263.             "table": self.quote_name(model._meta.db_table),\\n264.             "definition": ", ".join(\\n265.                 constraint for constraint in (*column_sqls, *constraints) if constraint\\n266.             ),\\n267.         }\\n268.         if model._meta.db_tablespace:\\n269.             tablespace_sql = self.connection.ops.tablespace_sql(\\n270.                 model._meta.db_tablespace\\n271.             )\\n272.             if tablespace_sql:\\n273.                 sql += " " + tablespace_sql\\n274.         return sql, params\\n275. \\n276.     # Field <-> database mapping functions\\n277. \\n278.     def _iter_column_sql(self, column_db_type, params, model, field, include_default):\\n279.         yield column_db_type\\n280.         collation = getattr(field, "db_collation", None)\\n281.         if collation:\\n282.             yield self._collate_sql(collation)\\n283.         # Work out nullability.\\n284.         null = field.null\\n285.         # Include a default value, if requested.\\n286.         include_default = (\\n287.             include_default\\n288.             and not self.skip_default(field)\\n289.             and\\n290.             # Don\\\'t include a default value if it\\\'s a nullable field and the\\n291.             # default cannot be dropped in the ALTER COLUMN statement (e.g.\\n292.             # MySQL longtext and longblob).\\n293.             not (null and self.skip_default_on_alter(field))\\n294.         )\\n295.         if include_default:\\n296.             default_value = self.effective_default(field)\\n297.             if default_value is not None:\\n298.                 column_default = "DEFAULT " + self._column_default_sql(field)\\n299.                 if self.connection.features.requires_literal_defaults:\\n300.                     # Some databases can\\\'t take defaults as a parameter (Oracle).\\n301.                     # If this is the case, the individual schema backend should\\n302.                     # implement prepare_default().\\n303.                     yield column_default % self.prepare_default(default_value)\\n304.                 else:\\n305.                     yield column_default\\n306.                     params.append(default_value)\\n307.         # Oracle treats the empty string (\\\'\\\') as null, so coerce the null\\n308.         # option whenever \\\'\\\' is a possible value.\\n309.         if (\\n310.             field.empty_strings_allowed\\n311.             and not field.primary_key\\n312.             and self.connection.features.interprets_empty_strings_as_nulls\\n313.         ):\\n314.             null = True\\n315.         if not null:\\n316.             yield "NOT NULL"\\n317.         elif not self.connection.features.implied_column_null:\\n318.             yield "NULL"\\n319.         if field.primary_key:\\n320.             yield "PRIMARY KEY"\\n321.         elif field.unique:\\n322.             yield "UNIQUE"\\n323.         # Optionally add the tablespace if it\\\'s an implicitly indexed column.\\n324.         tablespace = field.db_tablespace or model._meta.db_tablespace\\n325.         if (\\n326.             tablespace\\n327.             and self.connection.features.supports_tablespaces\\n328.             and field.unique\\n329.         ):\\n330.             yield self.connection.ops.tablespace_sql(tablespace, inline=True)\\n331. \\n332.     def column_sql(self, model, field, include_default=False):\\n333.         """\\n334.         Return the column definition for a field. The field must already have\\n335.         had set_attributes_from_name() called.\\n336.         """\\n337.         # Get the column\\\'s type and use that as the basis of the SQL.\\n338.         db_params = field.db_parameters(connection=self.connection)\\n339.         column_db_type = db_params["type"]\\n340.         # Check for fields that aren\\\'t actually columns (e.g. M2M).\\n341.         if column_db_type is None:\\n342.             return None, None\\n343.         params = []\\n344.         return (\\n345.             " ".join(\\n346.                 # This appends to the params being returned.\\n347.                 self._iter_column_sql(\\n348.                     column_db_type, params, model, field, include_default\\n349.                 )\\n350.             ),\\n351.             params,\\n352.         )\\n353. \\n354.     def skip_default(self, field):\\n355.         """\\n356.         Some backends don\\\'t accept default values for certain columns types\\n357.         (i.e. MySQL longtext and longblob).\\n358.         """\\n359.         return False\\n360. \\n361.     def skip_default_on_alter(self, field):\\n362.         """\\n363.         Some backends don\\\'t accept default values for certain columns types\\n364.         (i.e. MySQL longtext and longblob) in the ALTER COLUMN statement.\\n365.         """\\n366.         return False\\n367. \\n368.     def prepare_default(self, value):\\n369.         """\\n370.         Only used for backends which have requires_literal_defaults feature\\n371.         """\\n372.         raise NotImplementedError(\\n373.             "subclasses of BaseDatabaseSchemaEditor for backends which have "\\n374.             "requires_literal_defaults must provide a prepare_default() method"\\n375.         )\\n376. \\n377.     def _column_default_sql(self, field):\\n378.         """\\n379.         Return the SQL to use in a DEFAULT clause. The resulting string should\\n380.         contain a \\\'%s\\\' placeholder for a default value.\\n381.         """\\n382.         return "%s"\\n383. \\n384.     @staticmethod\\n385.     def _effective_default(field):\\n386.         # This method allows testing its logic without a connection.\\n387.         if field.has_default():\\n388.             default = field.get_default()\\n389.         elif not field.null and field.blank and field.empty_strings_allowed:\\n390.             if field.get_internal_type() == "BinaryField":\\n391.                 default = b""\\n392.             else:\\n393.                 default = ""\\n394.         elif getattr(field, "auto_now", False) or getattr(field, "auto_now_add", False):\\n395.             internal_type = field.get_internal_type()\\n396.             if internal_type == "DateTimeField":\\n397.                 default = timezone.now()\\n398.             else:\\n399.                 default = datetime.now()\\n400.                 if internal_type == "DateField":\\n401.                     default = default.date()\\n402.                 elif internal_type == "TimeField":\\n403.                     default = default.time()\\n404.         else:\\n405.             default = None\\n406.         return default\\n407. \\n408.     def effective_default(self, field):\\n409.         """Return a field\\\'s effective database default value."""\\n410.         return field.get_db_prep_save(self._effective_default(field), self.connection)\\n411. \\n412.     def quote_value(self, value):\\n413.         """\\n414.         Return a quoted version of the value so it\\\'s safe to use in an SQL\\n415.         string. This is not safe against injection from user code; it is\\n416.         intended only for use in making SQL scripts or preparing default values\\n417.         for particularly tricky backends (defaults are not user-defined, though,\\n418.         so this is safe).\\n419.         """\\n420.         raise NotImplementedError()\\n421. \\n422.     # Actions\\n423. \\n424.     def create_model(self, model):\\n425.         """\\n426.         Create a table and any accompanying indexes or unique constraints for\\n427.         the given `model`.\\n428.         """\\n429.         sql, params = self.table_sql(model)\\n430.         # Prevent using [] as params, in the case a literal \\\'%\\\' is used in the\\n431.         # definition.\\n432.         self.execute(sql, params or None)\\n433. \\n434.         # Add any field index and index_together\\\'s (deferred as SQLite\\n435.         # _remake_table needs it).\\n436.         self.deferred_sql.extend(self._model_indexes_sql(model))\\n437. \\n438.         # Make M2M tables\\n439.         for field in model._meta.local_many_to_many:\\n440.             if field.remote_field.through._meta.auto_created:\\n441.                 self.create_model(field.remote_field.through)\\n442. \\n443.     def delete_model(self, model):\\n444.         """Delete a model from the database."""\\n445.         # Handle auto-created intermediary models\\n446.         for field in model._meta.local_many_to_many:\\n447.             if field.remote_field.through._meta.auto_created:\\n448.                 self.delete_model(field.remote_field.through)\\n449. \\n450.         # Delete the table\\n451.         self.execute(\\n452.             self.sql_delete_table\\n453.             % {\\n454.                 "table": self.quote_name(model._meta.db_table),\\n455.             }\\n456.         )\\n457.         # Remove all deferred statements referencing the deleted table.\\n458.         for sql in list(self.deferred_sql):\\n459.             if isinstance(sql, Statement) and sql.references_table(\\n460.                 model._meta.db_table\\n461.             ):\\n462.                 self.deferred_sql.remove(sql)\\n463. \\n464.     def add_index(self, model, index):\\n465.         """Add an index on a model."""\\n466.         if (\\n467.             index.contains_expressions\\n468.             and not self.connection.features.supports_expression_indexes\\n469.         ):\\n470.             return None\\n471.         # Index.create_sql returns interpolated SQL which makes params=None a\\n472.         # necessity to avoid escaping attempts on execution.\\n473.         self.execute(index.create_sql(model, self), params=None)\\n474. \\n475.     def remove_index(self, model, index):\\n476.         """Remove an index from a model."""\\n477.         if (\\n478.             index.contains_expressions\\n479.             and not self.connection.features.supports_expression_indexes\\n480.         ):\\n481.             return None\\n482.         self.execute(index.remove_sql(model, self))\\n483. \\n484.     def add_constraint(self, model, constraint):\\n485.         """Add a constraint to a model."""\\n486.         sql = constraint.create_sql(model, self)\\n487.         if sql:\\n488.             # Constraint.create_sql returns interpolated SQL which makes\\n489.             # params=None a necessity to avoid escaping attempts on execution.\\n490.             self.execute(sql, params=None)\\n491. \\n492.     def remove_constraint(self, model, constraint):\\n493.         """Remove a constraint from a model."""\\n494.         sql = constraint.remove_sql(model, self)\\n495.         if sql:\\n496.             self.execute(sql)\\n497. \\n498.     def alter_unique_together(self, model, old_unique_together, new_unique_together):\\n499.         """\\n500.         Deal with a model changing its unique_together. The input\\n501.         unique_togethers must be doubly-nested, not the single-nested\\n502.         ["foo", "bar"] format.\\n503.         """\\n504.         olds = {tuple(fields) for fields in old_unique_together}\\n505.         news = {tuple(fields) for fields in new_unique_together}\\n506.         # Deleted uniques\\n507.         for fields in olds.difference(news):\\n508.             self._delete_composed_index(\\n509.                 model, fields, {"unique": True}, self.sql_delete_unique\\n510.             )\\n511.         # Created uniques\\n512.         for field_names in news.difference(olds):\\n513.             fields = [model._meta.get_field(field) for field in field_names]\\n514.             self.execute(self._create_unique_sql(model, fields))\\n515. \\n516.     def alter_index_together(self, model, old_index_together, new_index_together):\\n517.         """\\n518.         Deal with a model changing its index_together. The input\\n519.         index_togethers must be doubly-nested, not the single-nested\\n520.         ["foo", "bar"] format.\\n521.         """\\n522.         olds = {tuple(fields) for fields in old_index_together}\\n523.         news = {tuple(fields) for fields in new_index_together}\\n524.         # Deleted indexes\\n525.         for fields in olds.difference(news):\\n526.             self._delete_composed_index(\\n527.                 model,\\n528.                 fields,\\n529.                 {"index": True, "unique": False},\\n530.                 self.sql_delete_index,\\n531.             )\\n532.         # Created indexes\\n533.         for field_names in news.difference(olds):\\n534.             fields = [model._meta.get_field(field) for field in field_names]\\n535.             self.execute(self._create_index_sql(model, fields=fields, suffix="_idx"))\\n536. \\n537.     def _delete_composed_index(self, model, fields, constraint_kwargs, sql):\\n538.         meta_constraint_names = {\\n539.             constraint.name for constraint in model._meta.constraints\\n540.         }\\n541.         meta_index_names = {constraint.name for constraint in model._meta.indexes}\\n542.         columns = [model._meta.get_field(field).column for field in fields]\\n543.         constraint_names = self._constraint_names(\\n544.             model,\\n545.             columns,\\n546.             exclude=meta_constraint_names | meta_index_names,\\n547.             **constraint_kwargs,\\n548.         )\\n549.         if len(constraint_names) != 1:\\n550.             raise ValueError(\\n551.                 "Found wrong number (%s) of constraints for %s(%s)"\\n552.                 % (\\n553.                     len(constraint_names),\\n554.                     model._meta.db_table,\\n555.                     ", ".join(columns),\\n556.                 )\\n557.             )\\n558.         self.execute(self._delete_constraint_sql(sql, model, constraint_names[0]))\\n559. \\n560.     def alter_db_table(self, model, old_db_table, new_db_table):\\n561.         """Rename the table a model points to."""\\n562.         if old_db_table == new_db_table or (\\n563.             self.connection.features.ignores_table_name_case\\n564.             and old_db_table.lower() == new_db_table.lower()\\n565.         ):\\n566.             return\\n567.         self.execute(\\n568.             self.sql_rename_table\\n569.             % {\\n570.                 "old_table": self.quote_name(old_db_table),\\n571.                 "new_table": self.quote_name(new_db_table),\\n572.             }\\n573.         )\\n574.         # Rename all references to the old table name.\\n575.         for sql in self.deferred_sql:\\n576.             if isinstance(sql, Statement):\\n577.                 sql.rename_table_references(old_db_table, new_db_table)\\n578. \\n579.     def alter_db_tablespace(self, model, old_db_tablespace, new_db_tablespace):\\n580.         """Move a model\\\'s table between tablespaces."""\\n581.         self.execute(\\n582.             self.sql_retablespace_table\\n583.             % {\\n584.                 "table": self.quote_name(model._meta.db_table),\\n585.                 "old_tablespace": self.quote_name(old_db_tablespace),\\n586.                 "new_tablespace": self.quote_name(new_db_tablespace),\\n587.             }\\n588.         )\\n589. \\n590.     def add_field(self, model, field):\\n591.         """\\n592.         Create a field on a model. Usually involves adding a column, but may\\n593.         involve adding a table instead (for M2M fields).\\n594.         """\\n595.         # Special-case implicit M2M tables\\n596.         if field.many_to_many and field.remote_field.through._meta.auto_created:\\n597.             return self.create_model(field.remote_field.through)\\n598.         # Get the column\\\'s definition\\n599.         definition, params = self.column_sql(model, field, include_default=True)\\n600.         # It might not actually have a column behind it\\n601.         if definition is None:\\n602.             return\\n603.         # Check constraints can go on the column SQL here\\n604.         db_params = field.db_parameters(connection=self.connection)\\n605.         if db_params["check"]:\\n606.             definition += " " + self.sql_check_constraint % db_params\\n607.         if (\\n608.             field.remote_field\\n609.             and self.connection.features.supports_foreign_keys\\n610.             and field.db_constraint\\n611.         ):\\n612.             constraint_suffix = "_fk_%(to_table)s_%(to_column)s"\\n613.             # Add FK constraint inline, if supported.\\n614.             if self.sql_create_column_inline_fk:\\n615.                 to_table = field.remote_field.model._meta.db_table\\n616.                 to_column = field.remote_field.model._meta.get_field(\\n617.                     field.remote_field.field_name\\n618.                 ).column\\n619.                 namespace, _ = split_identifier(model._meta.db_table)\\n620.                 definition += " " + self.sql_create_column_inline_fk % {\\n621.                     "name": self._fk_constraint_name(model, field, constraint_suffix),\\n622.                     "namespace": "%s." % self.quote_name(namespace)\\n623.                     if namespace\\n624.                     else "",\\n625.                     "column": self.quote_name(field.column),\\n626.                     "to_table": self.quote_name(to_table),\\n627.                     "to_column": self.quote_name(to_column),\\n628.                     "deferrable": self.connection.ops.deferrable_sql(),\\n629.                 }\\n630.             # Otherwise, add FK constraints later.\\n631.             else:\\n632.                 self.deferred_sql.append(\\n633.                     self._create_fk_sql(model, field, constraint_suffix)\\n634.                 )\\n635.         # Build the SQL and run it\\n636.         sql = self.sql_create_column % {\\n637.             "table": self.quote_name(model._meta.db_table),\\n638.             "column": self.quote_name(field.column),\\n639.             "definition": definition,\\n640.         }\\n641.         self.execute(sql, params)\\n642.         # Drop the default if we need to\\n643.         # (Django usually does not use in-database defaults)\\n644.         if (\\n645.             not self.skip_default_on_alter(field)\\n646.             and self.effective_default(field) is not None\\n647.         ):\\n648.             changes_sql, params = self._alter_column_default_sql(\\n649.                 model, None, field, drop=True\\n650.             )\\n651.             sql = self.sql_alter_column % {\\n652.                 "table": self.quote_name(model._meta.db_table),\\n653.                 "changes": changes_sql,\\n654.             }\\n655.             self.execute(sql, params)\\n656.         # Add an index, if required\\n657.         self.deferred_sql.extend(self._field_indexes_sql(model, field))\\n658.         # Reset connection if required\\n659.         if self.connection.features.connection_persists_old_columns:\\n660.             self.connection.close()\\n661. \\n662.     def remove_field(self, model, field):\\n663.         """\\n664.         Remove a field from a model. Usually involves deleting a column,\\n665.         but for M2Ms may involve deleting a table.\\n666.         """\\n667.         # Special-case implicit M2M tables\\n668.         if field.many_to_many and field.remote_field.through._meta.auto_created:\\n669.             return self.delete_model(field.remote_field.through)\\n670.         # It might not actually have a column behind it\\n671.         if field.db_parameters(connection=self.connection)["type"] is None:\\n672.             return\\n673.         # Drop any FK constraints, MySQL requires explicit deletion\\n674.         if field.remote_field:\\n675.             fk_names = self._constraint_names(model, [field.column], foreign_key=True)\\n676.             for fk_name in fk_names:\\n677.                 self.execute(self._delete_fk_sql(model, fk_name))\\n678.         # Delete the column\\n679.         sql = self.sql_delete_column % {\\n680.             "table": self.quote_name(model._meta.db_table),\\n681.             "column": self.quote_name(field.column),\\n682.         }\\n683.         self.execute(sql)\\n684.         # Reset connection if required\\n685.         if self.connection.features.connection_persists_old_columns:\\n686.             self.connection.close()\\n687.         # Remove all deferred statements referencing the deleted column.\\n688.         for sql in list(self.deferred_sql):\\n689.             if isinstance(sql, Statement) and sql.references_column(\\n690.                 model._meta.db_table, field.column\\n691.             ):\\n692.                 self.deferred_sql.remove(sql)\\n693. \\n694.     def alter_field(self, model, old_field, new_field, strict=False):\\n695.         """\\n696.         Allow a field\\\'s type, uniqueness, nullability, default, column,\\n697.         constraints, etc. to be modified.\\n698.         `old_field` is required to compute the necessary changes.\\n699.         If `strict` is True, raise errors if the old column does not match\\n700.         `old_field` precisely.\\n701.         """\\n702.         if not self._field_should_be_altered(old_field, new_field):\\n703.             return\\n704.         # Ensure this field is even column-based\\n705.         old_db_params = old_field.db_parameters(connection=self.connection)\\n706.         old_type = old_db_params["type"]\\n707.         new_db_params = new_field.db_parameters(connection=self.connection)\\n708.         new_type = new_db_params["type"]\\n709.         if (old_type is None and old_field.remote_field is None) or (\\n710.             new_type is None and new_field.remote_field is None\\n711.         ):\\n712.             raise ValueError(\\n713.                 "Cannot alter field %s into %s - they do not properly define "\\n714.                 "db_type (are you using a badly-written custom field?)"\\n715.                 % (old_field, new_field),\\n716.             )\\n717.         elif (\\n718.             old_type is None\\n719.             and new_type is None\\n720.             and (\\n721.                 old_field.remote_field.through\\n722.                 and new_field.remote_field.through\\n723.                 and old_field.remote_field.through._meta.auto_created\\n724.                 and new_field.remote_field.through._meta.auto_created\\n725.             )\\n726.         ):\\n727.             return self._alter_many_to_many(model, old_field, new_field, strict)\\n728.         elif (\\n729.             old_type is None\\n730.             and new_type is None\\n731.             and (\\n732.                 old_field.remote_field.through\\n733.                 and new_field.remote_field.through\\n734.                 and not old_field.remote_field.through._meta.auto_created\\n735.                 and not new_field.remote_field.through._meta.auto_created\\n736.             )\\n737.         ):\\n738.             # Both sides have through models; this is a no-op.\\n739.             return\\n740.         elif old_type is None or new_type is None:\\n741.             raise ValueError(\\n742.                 "Cannot alter field %s into %s - they are not compatible types "\\n743.                 "(you cannot alter to or from M2M fields, or add or remove "\\n744.                 "through= on M2M fields)" % (old_field, new_field)\\n745.             )\\n746. \\n747.         self._alter_field(\\n748.             model,\\n749.             old_field,\\n750.             new_field,\\n751.             old_type,\\n752.             new_type,\\n753.             old_db_params,\\n754.             new_db_params,\\n755.             strict,\\n756.         )\\n757. \\n758.     def _alter_field(\\n759.         self,\\n760.         model,\\n761.         old_field,\\n762.         new_field,\\n763.         old_type,\\n764.         new_type,\\n765.         old_db_params,\\n766.         new_db_params,\\n767.         strict=False,\\n768.     ):\\n769.         """Perform a "physical" (non-ManyToMany) field update."""\\n770.         # Drop any FK constraints, we\\\'ll remake them later\\n771.         fks_dropped = set()\\n772.         if (\\n773.             self.connection.features.supports_foreign_keys\\n774.             and old_field.remote_field\\n775.             and old_field.db_constraint\\n776.         ):\\n777.             fk_names = self._constraint_names(\\n778.                 model, [old_field.column], foreign_key=True\\n779.             )\\n780.             if strict and len(fk_names) != 1:\\n781.                 raise ValueError(\\n782.                     "Found wrong number (%s) of foreign key constraints for %s.%s"\\n783.                     % (\\n784.                         len(fk_names),\\n785.                         model._meta.db_table,\\n786.                         old_field.column,\\n787.                     )\\n788.                 )\\n789.             for fk_name in fk_names:\\n790.                 fks_dropped.add((old_field.column,))\\n791.                 self.execute(self._delete_fk_sql(model, fk_name))\\n792.         # Has unique been removed?\\n793.         if old_field.unique and (\\n794.             not new_field.unique or self._field_became_primary_key(old_field, new_field)\\n795.         ):\\n796.             # Find the unique constraint for this field\\n797.             meta_constraint_names = {\\n798.                 constraint.name for constraint in model._meta.constraints\\n799.             }\\n800.             constraint_names = self._constraint_names(\\n801.                 model,\\n802.                 [old_field.column],\\n803.                 unique=True,\\n804.                 primary_key=False,\\n805.                 exclude=meta_constraint_names,\\n806.             )\\n807.             if strict and len(constraint_names) != 1:\\n808.                 raise ValueError(\\n809.                     "Found wrong number (%s) of unique constraints for %s.%s"\\n810.                     % (\\n811.                         len(constraint_names),\\n812.                         model._meta.db_table,\\n813.                         old_field.column,\\n814.                     )\\n815.                 )\\n816.             for constraint_name in constraint_names:\\n817.                 self.execute(self._delete_unique_sql(model, constraint_name))\\n818.         # Drop incoming FK constraints if the field is a primary key or unique,\\n819.         # which might be a to_field target, and things are going to change.\\n820.         drop_foreign_keys = (\\n821.             self.connection.features.supports_foreign_keys\\n822.             and (\\n823.                 (old_field.primary_key and new_field.primary_key)\\n824.                 or (old_field.unique and new_field.unique)\\n825.             )\\n826.             and old_type != new_type\\n827.         )\\n828.         if drop_foreign_keys:\\n829.             # \\\'_meta.related_field\\\' also contains M2M reverse fields, these\\n830.             # will be filtered out\\n831.             for _old_rel, new_rel in _related_non_m2m_objects(old_field, new_field):\\n832.                 rel_fk_names = self._constraint_names(\\n833.                     new_rel.related_model, [new_rel.field.column], foreign_key=True\\n834.                 )\\n835.                 for fk_name in rel_fk_names:\\n836.                     self.execute(self._delete_fk_sql(new_rel.related_model, fk_name))\\n837.         # Removed an index? (no strict check, as multiple indexes are possible)\\n838.         # Remove indexes if db_index switched to False or a unique constraint\\n839.         # will now be used in lieu of an index. The following lines from the\\n840.         # truth table show all True cases; the rest are False:\\n841.         #\\n842.         # old_field.db_index | old_field.unique | new_field.db_index | new_field.unique\\n843.         # ------------------------------------------------------------------------------\\n844.         # True               | False            | False              | False\\n845.         # True               | False            | False              | True\\n846.         # True               | False            | True               | True\\n847.         if (\\n848.             old_field.db_index\\n849.             and not old_field.unique\\n850.             and (not new_field.db_index or new_field.unique)\\n851.         ):\\n852.             # Find the index for this field\\n853.             meta_index_names = {index.name for index in model._meta.indexes}\\n854.             # Retrieve only BTREE indexes since this is what\\\'s created with\\n855.             # db_index=True.\\n856.             index_names = self._constraint_names(\\n857.                 model,\\n858.                 [old_field.column],\\n859.                 index=True,\\n860.                 type_=Index.suffix,\\n861.                 exclude=meta_index_names,\\n862.             )\\n863.             for index_name in index_names:\\n864.                 # The only way to check if an index was created with\\n865.                 # db_index=True or with Index([\\\'field\\\'], name=\\\'foo\\\')\\n866.                 # is to look at its name (refs #28053).\\n867.                 self.execute(self._delete_index_sql(model, index_name))\\n868.         # Change check constraints?\\n869.         if old_db_params["check"] != new_db_params["check"] and old_db_params["check"]:\\n870.             meta_constraint_names = {\\n871.                 constraint.name for constraint in model._meta.constraints\\n872.             }\\n873.             constraint_names = self._constraint_names(\\n874.                 model,\\n875.                 [old_field.column],\\n876.                 check=True,\\n877.                 exclude=meta_constraint_names,\\n878.             )\\n879.             if strict and len(constraint_names) != 1:\\n880.                 raise ValueError(\\n881.                     "Found wrong number (%s) of check constraints for %s.%s"\\n882.                     % (\\n883.                         len(constraint_names),\\n884.                         model._meta.db_table,\\n885.                         old_field.column,\\n886.                     )\\n887.                 )\\n888.             for constraint_name in constraint_names:\\n889.                 self.execute(self._delete_check_sql(model, constraint_name))\\n890.         # Have they renamed the column?\\n891.         if old_field.column != new_field.column:\\n892.             self.execute(\\n893.                 self._rename_field_sql(\\n894.                     model._meta.db_table, old_field, new_field, new_type\\n895.                 )\\n896.             )\\n897.             # Rename all references to the renamed column.\\n898.             for sql in self.deferred_sql:\\n899.                 if isinstance(sql, Statement):\\n900.                     sql.rename_column_references(\\n901.                         model._meta.db_table, old_field.column, new_field.column\\n902.                     )\\n903.         # Next, start accumulating actions to do\\n904.         actions = []\\n905.         null_actions = []\\n906.         post_actions = []\\n907.         # Collation change?\\n908.         old_collation = getattr(old_field, "db_collation", None)\\n909.         new_collation = getattr(new_field, "db_collation", None)\\n910.         if old_collation != new_collation:\\n911.             # Collation change handles also a type change.\\n912.             fragment = self._alter_column_collation_sql(\\n913.                 model, new_field, new_type, new_collation\\n914.             )\\n915.             actions.append(fragment)\\n916.         # Type change?\\n917.         elif old_type != new_type:\\n918.             fragment, other_actions = self._alter_column_type_sql(\\n919.                 model, old_field, new_field, new_type\\n920.             )\\n921.             actions.append(fragment)\\n922.             post_actions.extend(other_actions)\\n923.         # When changing a column NULL constraint to NOT NULL with a given\\n924.         # default value, we need to perform 4 steps:\\n925.         #  1. Add a default for new incoming writes\\n926.         #  2. Update existing NULL rows with new default\\n927.         #  3. Replace NULL constraint with NOT NULL\\n928.         #  4. Drop the default again.\\n929.         # Default change?\\n930.         needs_database_default = False\\n931.         if old_field.null and not new_field.null:\\n932.             old_default = self.effective_default(old_field)\\n933.             new_default = self.effective_default(new_field)\\n934.             if (\\n935.                 not self.skip_default_on_alter(new_field)\\n936.                 and old_default != new_default\\n937.                 and new_default is not None\\n938.             ):\\n939.                 needs_database_default = True\\n940.                 actions.append(\\n941.                     self._alter_column_default_sql(model, old_field, new_field)\\n942.                 )\\n943.         # Nullability change?\\n944.         if old_field.null != new_field.null:\\n945.             fragment = self._alter_column_null_sql(model, old_field, new_field)\\n946.             if fragment:\\n947.                 null_actions.append(fragment)\\n948.         # Only if we have a default and there is a change from NULL to NOT NULL\\n949.         four_way_default_alteration = new_field.has_default() and (\\n950.             old_field.null and not new_field.null\\n951.         )\\n952.         if actions or null_actions:\\n953.             if not four_way_default_alteration:\\n954.                 # If we don\\\'t have to do a 4-way default alteration we can\\n955.                 # directly run a (NOT) NULL alteration\\n956.                 actions = actions + null_actions\\n957.             # Combine actions together if we can (e.g. postgres)\\n958.             if self.connection.features.supports_combined_alters and actions:\\n959.                 sql, params = tuple(zip(*actions))\\n960.                 actions = [(", ".join(sql), sum(params, []))]\\n961.             # Apply those actions\\n962.             for sql, params in actions:\\n963.                 self.execute(\\n964.                     self.sql_alter_column\\n965.                     % {\\n966.                         "table": self.quote_name(model._meta.db_table),\\n967.                         "changes": sql,\\n968.                     },\\n969.                     params,\\n970.                 )\\n971.             if four_way_default_alteration:\\n972.                 # Update existing rows with default value\\n973.                 self.execute(\\n974.                     self.sql_update_with_default\\n975.                     % {\\n976.                         "table": self.quote_name(model._meta.db_table),\\n977.                         "column": self.quote_name(new_field.column),\\n978.                         "default": "%s",\\n979.                     },\\n980.                     [new_default],\\n981.                 )\\n982.                 # Since we didn\\\'t run a NOT NULL change before we need to do it\\n983.                 # now\\n984.                 for sql, params in null_actions:\\n985.                     self.execute(\\n986.                         self.sql_alter_column\\n987.                         % {\\n988.                             "table": self.quote_name(model._meta.db_table),\\n989.                             "changes": sql,\\n990.                         },\\n991.                         params,\\n992.                     )\\n993.         if post_actions:\\n994.             for sql, params in post_actions:\\n995.                 self.execute(sql, params)\\n996.         # If primary_key changed to False, delete the primary key constraint.\\n997.         if old_field.primary_key and not new_field.primary_key:\\n998.             self._delete_primary_key(model, strict)\\n999.         # Added a unique?\\n1000.         if self._unique_should_be_added(old_field, new_field):\\n1001.             self.execute(self._create_unique_sql(model, [new_field]))\\n1002.         # Added an index? Add an index if db_index switched to True or a unique\\n1003.         # constraint will no longer be used in lieu of an index. The following\\n1004.         # lines from the truth table show all True cases; the rest are False:\\n1005.         #\\n1006.         # old_field.db_index | old_field.unique | new_field.db_index | new_field.unique\\n1007.         # ------------------------------------------------------------------------------\\n1008.         # False              | False            | True               | False\\n1009.         # False              | True             | True               | False\\n1010.         # True               | True             | True               | False\\n1011.         if (\\n1012.             (not old_field.db_index or old_field.unique)\\n1013.             and new_field.db_index\\n1014.             and not new_field.unique\\n1015.         ):\\n1016.             self.execute(self._create_index_sql(model, fields=[new_field]))\\n1017.         # Type alteration on primary key? Then we need to alter the column\\n1018.         # referring to us.\\n1019.         rels_to_update = []\\n1020.         if drop_foreign_keys:\\n1021.             rels_to_update.extend(_related_non_m2m_objects(old_field, new_field))\\n1022.         # Changed to become primary key?\\n1023.         if self._field_became_primary_key(old_field, new_field):\\n1024.             # Make the new one\\n1025.             self.execute(self._create_primary_key_sql(model, new_field))\\n1026.             # Update all referencing columns\\n1027.             rels_to_update.extend(_related_non_m2m_objects(old_field, new_field))\\n1028.         # Handle our type alters on the other end of rels from the PK stuff above\\n1029.         for old_rel, new_rel in rels_to_update:\\n1030.             rel_db_params = new_rel.field.db_parameters(connection=self.connection)\\n1031.             rel_type = rel_db_params["type"]\\n1032.             fragment, other_actions = self._alter_column_type_sql(\\n1033.                 new_rel.related_model, old_rel.field, new_rel.field, rel_type\\n1034.             )\\n1035.             self.execute(\\n1036.                 self.sql_alter_column\\n1037.                 % {\\n1038.                     "table": self.quote_name(new_rel.related_model._meta.db_table),\\n1039.                     "changes": fragment[0],\\n1040.                 },\\n1041.                 fragment[1],\\n1042.             )\\n1043.             for sql, params in other_actions:\\n1044.                 self.execute(sql, params)\\n1045.         # Does it have a foreign key?\\n1046.         if (\\n1047.             self.connection.features.supports_foreign_keys\\n1048.             and new_field.remote_field\\n1049.             and (\\n1050.                 fks_dropped or not old_field.remote_field or not old_field.db_constraint\\n1051.             )\\n1052.             and new_field.db_constraint\\n1053.         ):\\n1054.             self.execute(\\n1055.                 self._create_fk_sql(model, new_field, "_fk_%(to_table)s_%(to_column)s")\\n1056.             )\\n1057.         # Rebuild FKs that pointed to us if we previously had to drop them\\n1058.         if drop_foreign_keys:\\n1059.             for _, rel in rels_to_update:\\n1060.                 if rel.field.db_constraint:\\n1061.                     self.execute(\\n1062.                         self._create_fk_sql(rel.related_model, rel.field, "_fk")\\n1063.                     )\\n1064.         # Does it have check constraints we need to add?\\n1065.         if old_db_params["check"] != new_db_params["check"] and new_db_params["check"]:\\n1066.             constraint_name = self._create_index_name(\\n1067.                 model._meta.db_table, [new_field.column], suffix="_check"\\n1068.             )\\n1069.             self.execute(\\n1070.                 self._create_check_sql(model, constraint_name, new_db_params["check"])\\n1071.             )\\n1072.         # Drop the default if we need to\\n1073.         # (Django usually does not use in-database defaults)\\n1074.         if needs_database_default:\\n1075.             changes_sql, params = self._alter_column_default_sql(\\n1076.                 model, old_field, new_field, drop=True\\n1077.             )\\n1078.             sql = self.sql_alter_column % {\\n1079.                 "table": self.quote_name(model._meta.db_table),\\n1080.                 "changes": changes_sql,\\n1081.             }\\n1082.             self.execute(sql, params)\\n1083.         # Reset connection if required\\n1084.         if self.connection.features.connection_persists_old_columns:\\n1085.             self.connection.close()\\n1086. \\n1087.     def _alter_column_null_sql(self, model, old_field, new_field):\\n1088.         """\\n1089.         Hook to specialize column null alteration.\\n1090. \\n1091.         Return a (sql, params) fragment to set a column to null or non-null\\n1092.         as required by new_field, or None if no changes are required.\\n1093.         """\\n1094.         if (\\n1095.             self.connection.features.interprets_empty_strings_as_nulls\\n1096.             and new_field.empty_strings_allowed\\n1097.         ):\\n1098.             # The field is nullable in the database anyway, leave it alone.\\n1099.             return\\n1100.         else:\\n1101.             new_db_params = new_field.db_parameters(connection=self.connection)\\n1102.             sql = (\\n1103.                 self.sql_alter_column_null\\n1104.                 if new_field.null\\n1105.                 else self.sql_alter_column_not_null\\n1106.             )\\n1107.             return (\\n1108.                 sql\\n1109.                 % {\\n1110.                     "column": self.quote_name(new_field.column),\\n1111.                     "type": new_db_params["type"],\\n1112.                 },\\n1113.                 [],\\n1114.             )\\n1115. \\n1116.     def _alter_column_default_sql(self, model, old_field, new_field, drop=False):\\n1117.         """\\n1118.         Hook to specialize column default alteration.\\n1119. \\n1120.         Return a (sql, params) fragment to add or drop (depending on the drop\\n1121.         argument) a default to new_field\\\'s column.\\n1122.         """\\n1123.         new_default = self.effective_default(new_field)\\n1124.         default = self._column_default_sql(new_field)\\n1125.         params = [new_default]\\n1126. \\n1127.         if drop:\\n1128.             params = []\\n1129.         elif self.connection.features.requires_literal_defaults:\\n1130.             # Some databases (Oracle) can\\\'t take defaults as a parameter\\n1131.             # If this is the case, the SchemaEditor for that database should\\n1132.             # implement prepare_default().\\n1133.             default = self.prepare_default(new_default)\\n1134.             params = []\\n1135. \\n1136.         new_db_params = new_field.db_parameters(connection=self.connection)\\n1137.         if drop:\\n1138.             if new_field.null:\\n1139.                 sql = self.sql_alter_column_no_default_null\\n1140.             else:\\n1141.                 sql = self.sql_alter_column_no_default\\n1142.         else:\\n1143.             sql = self.sql_alter_column_default\\n1144.         return (\\n1145.             sql\\n1146.             % {\\n1147.                 "column": self.quote_name(new_field.column),\\n1148.                 "type": new_db_params["type"],\\n1149.                 "default": default,\\n1150.             },\\n1151.             params,\\n1152.         )\\n1153. \\n1154.     def _alter_column_type_sql(self, model, old_field, new_field, new_type):\\n1155.         """\\n1156.         Hook to specialize column type alteration for different backends,\\n1157.         for cases when a creation type is different to an alteration type\\n1158.         (e.g. SERIAL in PostgreSQL, PostGIS fields).\\n1159. \\n1160.         Return a two-tuple of: an SQL fragment of (sql, params) to insert into\\n1161.         an ALTER TABLE statement and a list of extra (sql, params) tuples to\\n1162.         run once the field is altered.\\n1163.         """\\n1164.         return (\\n1165.             (\\n1166.                 self.sql_alter_column_type\\n1167.                 % {\\n1168.                     "column": self.quote_name(new_field.column),\\n1169.                     "type": new_type,\\n1170.                 },\\n1171.                 [],\\n1172.             ),\\n1173.             [],\\n1174.         )\\n1175. \\n1176.     def _alter_column_collation_sql(self, model, new_field, new_type, new_collation):\\n1177.         return (\\n1178.             self.sql_alter_column_collate\\n1179.             % {\\n1180.                 "column": self.quote_name(new_field.column),\\n1181.                 "type": new_type,\\n1182.                 "collation": " " + self._collate_sql(new_collation)\\n1183.                 if new_collation\\n1184.                 else "",\\n1185.             },\\n1186.             [],\\n1187.         )\\n1188. \\n1189.     def _alter_many_to_many(self, model, old_field, new_field, strict):\\n1190.         """Alter M2Ms to repoint their to= endpoints."""\\n1191.         # Rename the through table\\n1192.         if (\\n1193.             old_field.remote_field.through._meta.db_table\\n1194.             != new_field.remote_field.through._meta.db_table\\n1195.         ):\\n1196.             self.alter_db_table(\\n1197.                 old_field.remote_field.through,\\n1198.                 old_field.remote_field.through._meta.db_table,\\n1199.                 new_field.remote_field.through._meta.db_table,\\n1200.             )\\n1201.         # Repoint the FK to the other side\\n1202.         self.alter_field(\\n1203.             new_field.remote_field.through,\\n1204.             # The field that points to the target model is needed, so we can\\n1205.             # tell alter_field to change it - this is m2m_reverse_field_name()\\n1206.             # (as opposed to m2m_field_name(), which points to our model).\\n1207.             old_field.remote_field.through._meta.get_field(\\n1208.                 old_field.m2m_reverse_field_name()\\n1209.             ),\\n1210.             new_field.remote_field.through._meta.get_field(\\n1211.                 new_field.m2m_reverse_field_name()\\n1212.             ),\\n1213.         )\\n1214.         self.alter_field(\\n1215.             new_field.remote_field.through,\\n1216.             # for self-referential models we need to alter field from the other end too\\n1217.             old_field.remote_field.through._meta.get_field(old_field.m2m_field_name()),\\n1218.             new_field.remote_field.through._meta.get_field(new_field.m2m_field_name()),\\n1219.         )\\n1220. \\n1221.     def _create_index_name(self, table_name, column_names, suffix=""):\\n1222.         """\\n1223.         Generate a unique name for an index/unique constraint.\\n1224. \\n1225.         The name is divided into 3 parts: the table name, the column names,\\n1226.         and a unique digest and suffix.\\n1227.         """\\n1228.         _, table_name = split_identifier(table_name)\\n1229.         hash_suffix_part = "%s%s" % (\\n1230.             names_digest(table_name, *column_names, length=8),\\n1231.             suffix,\\n1232.         )\\n1233.         max_length = self.connection.ops.max_name_length() or 200\\n1234.         # If everything fits into max_length, use that name.\\n1235.         index_name = "%s_%s_%s" % (table_name, "_".join(column_names), hash_suffix_part)\\n1236.         if len(index_name) <= max_length:\\n1237.             return index_name\\n1238.         # Shorten a long suffix.\\n1239.         if len(hash_suffix_part) > max_length / 3:\\n1240.             hash_suffix_part = hash_suffix_part[: max_length // 3]\\n1241.         other_length = (max_length - len(hash_suffix_part)) // 2 - 1\\n1242.         index_name = "%s_%s_%s" % (\\n1243.             table_name[:other_length],\\n1244.             "_".join(column_names)[:other_length],\\n1245.             hash_suffix_part,\\n1246.         )\\n1247.         # Prepend D if needed to prevent the name from starting with an\\n1248.         # underscore or a number (not permitted on Oracle).\\n1249.         if index_name[0] == "_" or index_name[0].isdigit():\\n1250.             index_name = "D%s" % index_name[:-1]\\n1251.         return index_name\\n1252. \\n1253.     def _get_index_tablespace_sql(self, model, fields, db_tablespace=None):\\n1254.         if db_tablespace is None:\\n1255.             if len(fields) == 1 and fields[0].db_tablespace:\\n1256.                 db_tablespace = fields[0].db_tablespace\\n1257.             elif model._meta.db_tablespace:\\n1258.                 db_tablespace = model._meta.db_tablespace\\n1259.         if db_tablespace is not None:\\n1260.             return " " + self.connection.ops.tablespace_sql(db_tablespace)\\n1261.         return ""\\n1262. \\n1263.     def _index_condition_sql(self, condition):\\n1264.         if condition:\\n1265.             return " WHERE " + condition\\n1266.         return ""\\n1267. \\n1268.     def _index_include_sql(self, model, columns):\\n1269.         if not columns or not self.connection.features.supports_covering_indexes:\\n1270.             return ""\\n1271.         return Statement(\\n1272.             " INCLUDE (%(columns)s)",\\n1273.             columns=Columns(model._meta.db_table, columns, self.quote_name),\\n1274.         )\\n1275. \\n1276.     def _create_index_sql(\\n1277.         self,\\n1278.         model,\\n1279.         *,\\n1280.         fields=None,\\n1281.         name=None,\\n1282.         suffix="",\\n1283.         using="",\\n1284.         db_tablespace=None,\\n1285.         col_suffixes=(),\\n1286.         sql=None,\\n1287.         opclasses=(),\\n1288.         condition=None,\\n1289.         include=None,\\n1290.         expressions=None,\\n1291.     ):\\n1292.         """\\n1293.         Return the SQL statement to create the index for one or several fields\\n1294.         or expressions. `sql` can be specified if the syntax differs from the\\n1295.         standard (GIS indexes, ...).\\n1296.         """\\n1297.         fields = fields or []\\n1298.         expressions = expressions or []\\n1299.         compiler = Query(model, alias_cols=False).get_compiler(\\n1300.             connection=self.connection,\\n1301.         )\\n1302.         tablespace_sql = self._get_index_tablespace_sql(\\n1303.             model, fields, db_tablespace=db_tablespace\\n1304.         )\\n1305.         columns = [field.column for field in fields]\\n1306.         sql_create_index = sql or self.sql_create_index\\n1307.         table = model._meta.db_table\\n1308. \\n1309.         def create_index_name(*args, **kwargs):\\n1310.             nonlocal name\\n1311.             if name is None:\\n1312.                 name = self._create_index_name(*args, **kwargs)\\n1313.             return self.quote_name(name)\\n1314. \\n1315.         return Statement(\\n1316.             sql_create_index,\\n1317.             table=Table(table, self.quote_name),\\n1318.             name=IndexName(table, columns, suffix, create_index_name),\\n1319.             using=using,\\n1320.             columns=(\\n1321.                 self._index_columns(table, columns, col_suffixes, opclasses)\\n1322.                 if columns\\n1323.                 else Expressions(table, expressions, compiler, self.quote_value)\\n1324.             ),\\n1325.             extra=tablespace_sql,\\n1326.             condition=self._index_condition_sql(condition),\\n1327.             include=self._index_include_sql(model, include),\\n1328.         )\\n1329. \\n1330.     def _delete_index_sql(self, model, name, sql=None):\\n1331.         return Statement(\\n1332.             sql or self.sql_delete_index,\\n1333.             table=Table(model._meta.db_table, self.quote_name),\\n1334.             name=self.quote_name(name),\\n1335.         )\\n1336. \\n1337.     def _index_columns(self, table, columns, col_suffixes, opclasses):\\n1338.         return Columns(table, columns, self.quote_name, col_suffixes=col_suffixes)\\n1339. \\n1340.     def _model_indexes_sql(self, model):\\n1341.         """\\n1342.         Return a list of all index SQL statements (field indexes,\\n1343.         index_together, Meta.indexes) for the specified model.\\n1344.         """\\n1345.         if not model._meta.managed or model._meta.proxy or model._meta.swapped:\\n1346.             return []\\n1347.         output = []\\n1348.         for field in model._meta.local_fields:\\n1349.             output.extend(self._field_indexes_sql(model, field))\\n1350. \\n1351.         for field_names in model._meta.index_together:\\n1352.             fields = [model._meta.get_field(field) for field in field_names]\\n1353.             output.append(self._create_index_sql(model, fields=fields, suffix="_idx"))\\n1354. \\n1355.         for index in model._meta.indexes:\\n1356.             if (\\n1357.                 not index.contains_expressions\\n1358.                 or self.connection.features.supports_expression_indexes\\n1359.             ):\\n1360.                 output.append(index.create_sql(model, self))\\n1361.         return output\\n1362. \\n1363.     def _field_indexes_sql(self, model, field):\\n1364.         """\\n1365.         Return a list of all index SQL statements for the specified field.\\n1366.         """\\n1367.         output = []\\n1368.         if self._field_should_be_indexed(model, field):\\n1369.             output.append(self._create_index_sql(model, fields=[field]))\\n1370.         return output\\n1371. \\n1372.     def _field_should_be_altered(self, old_field, new_field):\\n1373.         _, old_path, old_args, old_kwargs = old_field.deconstruct()\\n1374.         _, new_path, new_args, new_kwargs = new_field.deconstruct()\\n1375.         # Don\\\'t alter when:\\n1376.         # - changing only a field name\\n1377.         # - changing an attribute that doesn\\\'t affect the schema\\n1378.         # - adding only a db_column and the column name is not changed\\n1379.         non_database_attrs = [\\n1380.             "blank",\\n1381.             "db_column",\\n1382.             "editable",\\n1383.             "error_messages",\\n1384.             "help_text",\\n1385.             "limit_choices_to",\\n1386.             # Database-level options are not supported, see #21961.\\n1387.             "on_delete",\\n1388.             "related_name",\\n1389.             "related_query_name",\\n1390.             "validators",\\n1391.             "verbose_name",\\n1392.         ]\\n1393.         for attr in non_database_attrs:\\n1394.             old_kwargs.pop(attr, None)\\n1395.             new_kwargs.pop(attr, None)\\n1396.         return self.quote_name(old_field.column) != self.quote_name(\\n1397.             new_field.column\\n1398.         ) or (old_path, old_args, old_kwargs) != (new_path, new_args, new_kwargs)\\n1399. \\n1400.     def _field_should_be_indexed(self, model, field):\\n1401.         return field.db_index and not field.unique\\n1402. \\n1403.     def _field_became_primary_key(self, old_field, new_field):\\n1404.         return not old_field.primary_key and new_field.primary_key\\n1405. \\n1406.     def _unique_should_be_added(self, old_field, new_field):\\n1407.         return (\\n1408.             not new_field.primary_key\\n1409.             and new_field.unique\\n1410.             and (not old_field.unique or old_field.primary_key)\\n1411.         )\\n1412. \\n1413.     def _rename_field_sql(self, table, old_field, new_field, new_type):\\n1414.         return self.sql_rename_column % {\\n1415.             "table": self.quote_name(table),\\n1416.             "old_column": self.quote_name(old_field.column),\\n1417.             "new_column": self.quote_name(new_field.column),\\n1418.             "type": new_type,\\n1419.         }\\n1420. \\n1421.     def _create_fk_sql(self, model, field, suffix):\\n1422.         table = Table(model._meta.db_table, self.quote_name)\\n1423.         name = self._fk_constraint_name(model, field, suffix)\\n1424.         column = Columns(model._meta.db_table, [field.column], self.quote_name)\\n1425.         to_table = Table(field.target_field.model._meta.db_table, self.quote_name)\\n1426.         to_column = Columns(\\n1427.             field.target_field.model._meta.db_table,\\n1428.             [field.target_field.column],\\n1429.             self.quote_name,\\n1430.         )\\n1431.         deferrable = self.connection.ops.deferrable_sql()\\n1432.         return Statement(\\n1433.             self.sql_create_fk,\\n1434.             table=table,\\n1435.             name=name,\\n1436.             column=column,\\n1437.             to_table=to_table,\\n1438.             to_column=to_column,\\n1439.             deferrable=deferrable,\\n1440.         )\\n1441. \\n1442.     def _fk_constraint_name(self, model, field, suffix):\\n1443.         def create_fk_name(*args, **kwargs):\\n1444.             return self.quote_name(self._create_index_name(*args, **kwargs))\\n1445. \\n1446.         return ForeignKeyName(\\n1447.             model._meta.db_table,\\n1448.             [field.column],\\n1449.             split_identifier(field.target_field.model._meta.db_table)[1],\\n1450.             [field.target_field.column],\\n1451.             suffix,\\n1452.             create_fk_name,\\n1453.         )\\n1454. \\n1455.     def _delete_fk_sql(self, model, name):\\n1456.         return self._delete_constraint_sql(self.sql_delete_fk, model, name)\\n1457. \\n1458.     def _deferrable_constraint_sql(self, deferrable):\\n1459.         if deferrable is None:\\n1460.             return ""\\n1461.         if deferrable == Deferrable.DEFERRED:\\n1462.             return " DEFERRABLE INITIALLY DEFERRED"\\n1463.         if deferrable == Deferrable.IMMEDIATE:\\n1464.             return " DEFERRABLE INITIALLY IMMEDIATE"\\n1465. \\n1466.     def _unique_sql(\\n1467.         self,\\n1468.         model,\\n1469.         fields,\\n1470.         name,\\n1471.         condition=None,\\n1472.         deferrable=None,\\n1473.         include=None,\\n1474.         opclasses=None,\\n1475.         expressions=None,\\n1476.     ):\\n1477.         if (\\n1478.             deferrable\\n1479.             and not self.connection.features.supports_deferrable_unique_constraints\\n1480.         ):\\n1481.             return None\\n1482.         if condition or include or opclasses or expressions:\\n1483.             # Databases support conditional, covering, and functional unique\\n1484.             # constraints via a unique index.\\n1485.             sql = self._create_unique_sql(\\n1486.                 model,\\n1487.                 fields,\\n1488.                 name=name,\\n1489.                 condition=condition,\\n1490.                 include=include,\\n1491.                 opclasses=opclasses,\\n1492.                 expressions=expressions,\\n1493.             )\\n1494.             if sql:\\n1495.                 self.deferred_sql.append(sql)\\n1496.             return None\\n1497.         constraint = self.sql_unique_constraint % {\\n1498.             "columns": ", ".join([self.quote_name(field.column) for field in fields]),\\n1499.             "deferrable": self._deferrable_constraint_sql(deferrable),\\n1500.         }\\n1501.         return self.sql_constraint % {\\n1502.             "name": self.quote_name(name),\\n1503.             "constraint": constraint,\\n1504.         }\\n1505. \\n1506.     def _create_unique_sql(\\n1507.         self,\\n1508.         model,\\n1509.         fields,\\n1510.         name=None,\\n1511.         condition=None,\\n1512.         deferrable=None,\\n1513.         include=None,\\n1514.         opclasses=None,\\n1515.         expressions=None,\\n1516.     ):\\n1517.         if (\\n1518.             (\\n1519.                 deferrable\\n1520.                 and not self.connection.features.supports_deferrable_unique_constraints\\n1521.             )\\n1522.             or (condition and not self.connection.features.supports_partial_indexes)\\n1523.             or (include and not self.connection.features.supports_covering_indexes)\\n1524.             or (\\n1525.                 expressions and not self.connection.features.supports_expression_indexes\\n1526.             )\\n1527.         ):\\n1528.             return None\\n1529. \\n1530.         def create_unique_name(*args, **kwargs):\\n1531.             return self.quote_name(self._create_index_name(*args, **kwargs))\\n1532. \\n1533.         compiler = Query(model, alias_cols=False).get_compiler(\\n1534.             connection=self.connection\\n1535.         )\\n1536.         table = model._meta.db_table\\n1537.         columns = [field.column for field in fields]\\n1538.         if name is None:\\n1539.             name = IndexName(table, columns, "_uniq", create_unique_name)\\n1540.         else:\\n1541.             name = self.quote_name(name)\\n1542.         if condition or include or opclasses or expressions:\\n1543.             sql = self.sql_create_unique_index\\n1544.         else:\\n1545.             sql = self.sql_create_unique\\n1546.         if columns:\\n1547.             columns = self._index_columns(\\n1548.                 table, columns, col_suffixes=(), opclasses=opclasses\\n1549.             )\\n1550.         else:\\n1551.             columns = Expressions(table, expressions, compiler, self.quote_value)\\n1552.         return Statement(\\n1553.             sql,\\n1554.             table=Table(table, self.quote_name),\\n1555.             name=name,\\n1556.             columns=columns,\\n1557.             condition=self._index_condition_sql(condition),\\n1558.             deferrable=self._deferrable_constraint_sql(deferrable),\\n1559.             include=self._index_include_sql(model, include),\\n1560.         )\\n1561. \\n1562.     def _delete_unique_sql(\\n1563.         self,\\n1564.         model,\\n1565.         name,\\n1566.         condition=None,\\n1567.         deferrable=None,\\n1568.         include=None,\\n1569.         opclasses=None,\\n1570.         expressions=None,\\n1571.     ):\\n1572.         if (\\n1573.             (\\n1574.                 deferrable\\n1575.                 and not self.connection.features.supports_deferrable_unique_constraints\\n1576.             )\\n1577.             or (condition and not self.connection.features.supports_partial_indexes)\\n1578.             or (include and not self.connection.features.supports_covering_indexes)\\n1579.             or (\\n1580.                 expressions and not self.connection.features.supports_expression_indexes\\n1581.             )\\n1582.         ):\\n1583.             return None\\n1584.         if condition or include or opclasses or expressions:\\n1585.             sql = self.sql_delete_index\\n1586.         else:\\n1587.             sql = self.sql_delete_unique\\n1588.         return self._delete_constraint_sql(sql, model, name)\\n1589. \\n1590.     def _check_sql(self, name, check):\\n1591.         return self.sql_constraint % {\\n1592.             "name": self.quote_name(name),\\n1593.             "constraint": self.sql_check_constraint % {"check": check},\\n1594.         }\\n1595. \\n1596.     def _create_check_sql(self, model, name, check):\\n1597.         return Statement(\\n1598.             self.sql_create_check,\\n1599.             table=Table(model._meta.db_table, self.quote_name),\\n1600.             name=self.quote_name(name),\\n1601.             check=check,\\n1602.         )\\n1603. \\n1604.     def _delete_check_sql(self, model, name):\\n1605.         return self._delete_constraint_sql(self.sql_delete_check, model, name)\\n1606. \\n1607.     def _delete_constraint_sql(self, template, model, name):\\n1608.         return Statement(\\n1609.             template,\\n1610.             table=Table(model._meta.db_table, self.quote_name),\\n1611.             name=self.quote_name(name),\\n1612.         )\\n1613. \\n1614.     def _constraint_names(\\n1615.         self,\\n1616.         model,\\n1617.         column_names=None,\\n1618.         unique=None,\\n1619.         primary_key=None,\\n1620.         index=None,\\n1621.         foreign_key=None,\\n1622.         check=None,\\n1623.         type_=None,\\n1624.         exclude=None,\\n1625.     ):\\n1626.         """Return all constraint names matching the columns and conditions."""\\n1627.         if column_names is not None:\\n1628.             column_names = [\\n1629.                 self.connection.introspection.identifier_converter(name)\\n1630.                 for name in column_names\\n1631.             ]\\n1632.         with self.connection.cursor() as cursor:\\n1633.             constraints = self.connection.introspection.get_constraints(\\n1634.                 cursor, model._meta.db_table\\n1635.             )\\n1636.         result = []\\n1637.         for name, infodict in constraints.items():\\n1638.             if column_names is None or column_names == infodict["columns"]:\\n1639.                 if unique is not None and infodict["unique"] != unique:\\n1640.                     continue\\n1641.                 if primary_key is not None and infodict["primary_key"] != primary_key:\\n1642.                     continue\\n1643.                 if index is not None and infodict["index"] != index:\\n1644.                     continue\\n1645.                 if check is not None and infodict["check"] != check:\\n1646.                     continue\\n1647.                 if foreign_key is not None and not infodict["foreign_key"]:\\n1648.                     continue\\n1649.                 if type_ is not None and infodict["type"] != type_:\\n1650.                     continue\\n1651.                 if not exclude or name not in exclude:\\n1652.                     result.append(name)\\n1653.         return result\\n1654. \\n1655.     def _delete_primary_key(self, model, strict=False):\\n1656.         constraint_names = self._constraint_names(model, primary_key=True)\\n1657.         if strict and len(constraint_names) != 1:\\n1658.             raise ValueError(\\n1659.                 "Found wrong number (%s) of PK constraints for %s"\\n1660.                 % (\\n1661.                     len(constraint_names),\\n1662.                     model._meta.db_table,\\n1663.                 )\\n1664.             )\\n1665.         for constraint_name in constraint_names:\\n1666.             self.execute(self._delete_primary_key_sql(model, constraint_name))\\n1667. \\n1668.     def _create_primary_key_sql(self, model, field):\\n1669.         return Statement(\\n1670.             self.sql_create_pk,\\n1671.             table=Table(model._meta.db_table, self.quote_name),\\n1672.             name=self.quote_name(\\n1673.                 self._create_index_name(\\n1674.                     model._meta.db_table, [field.column], suffix="_pk"\\n1675.                 )\\n1676.             ),\\n1677.             columns=Columns(model._meta.db_table, [field.column], self.quote_name),\\n1678.         )\\n1679. \\n1680.     def _delete_primary_key_sql(self, model, name):\\n1681.         return self._delete_constraint_sql(self.sql_delete_pk, model, name)\\n1682. \\n1683.     def _collate_sql(self, collation):\\n1684.         return "COLLATE " + self.quote_name(collation)\\n1685. \\n1686.     def remove_procedure(self, procedure_name, param_types=()):\\n1687.         sql = self.sql_delete_procedure % {\\n1688.             "procedure": self.quote_name(procedure_name),\\n1689.             "param_types": ",".join(param_types),\\n1690.         }\\n1691.         self.execute(sql)\', \'start_line_number\': 69, \'end_line_number\': 1691}\n\n{\'relative_path\': \'django/db/backends/sqlite3/schema.py\', \'content\': \'13. class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):\\n14. \\n15.     sql_delete_table = "DROP TABLE %(table)s"\\n16.     sql_create_fk = None\\n17.     sql_create_inline_fk = (\\n18.         "REFERENCES %(to_table)s (%(to_column)s) DEFERRABLE INITIALLY DEFERRED"\\n19.     )\\n20.     sql_create_column_inline_fk = sql_create_inline_fk\\n21.     sql_delete_column = "ALTER TABLE %(table)s DROP COLUMN %(column)s"\\n22.     sql_create_unique = "CREATE UNIQUE INDEX %(name)s ON %(table)s (%(columns)s)"\\n23.     sql_delete_unique = "DROP INDEX %(name)s"\\n24. \\n25.     def __enter__(self):\\n26.         # Some SQLite schema alterations need foreign key constraints to be\\n27.         # disabled. Enforce it here for the duration of the schema edition.\\n28.         if not self.connection.disable_constraint_checking():\\n29.             raise NotSupportedError(\\n30.                 "SQLite schema editor cannot be used while foreign key "\\n31.                 "constraint checks are enabled. Make sure to disable them "\\n32.                 "before entering a transaction.atomic() context because "\\n33.                 "SQLite does not support disabling them in the middle of "\\n34.                 "a multi-statement transaction."\\n35.             )\\n36.         return super().__enter__()\\n37. \\n38.     def __exit__(self, exc_type, exc_value, traceback):\\n39.         self.connection.check_constraints()\\n40.         super().__exit__(exc_type, exc_value, traceback)\\n41.         self.connection.enable_constraint_checking()\\n42. \\n43.     def quote_value(self, value):\\n44.         # The backend "mostly works" without this function and there are use\\n45.         # cases for compiling Python without the sqlite3 libraries (e.g.\\n46.         # security hardening).\\n47.         try:\\n48.             import sqlite3\\n49. \\n50.             value = sqlite3.adapt(value)\\n51.         except ImportError:\\n52.             pass\\n53.         except sqlite3.ProgrammingError:\\n54.             pass\\n55.         # Manual emulation of SQLite parameter quoting\\n56.         if isinstance(value, bool):\\n57.             return str(int(value))\\n58.         elif isinstance(value, (Decimal, float, int)):\\n59.             return str(value)\\n60.         elif isinstance(value, str):\\n61.             return "\\\'%s\\\'" % value.replace("\\\'", "\\\'\\\'")\\n62.         elif value is None:\\n63.             return "NULL"\\n64.         elif isinstance(value, (bytes, bytearray, memoryview)):\\n65.             # Bytes are only allowed for BLOB fields, encoded as string\\n66.             # literals containing hexadecimal data and preceded by a single "X"\\n67.             # character.\\n68.             return "X\\\'%s\\\'" % value.hex()\\n69.         else:\\n70.             raise ValueError(\\n71.                 "Cannot quote parameter value %r of type %s" % (value, type(value))\\n72.             )\\n73. \\n74.     def prepare_default(self, value):\\n75.         return self.quote_value(value)\\n76. \\n77.     def _is_referenced_by_fk_constraint(\\n78.         self, table_name, column_name=None, ignore_self=False\\n79.     ):\\n80.         """\\n81.         Return whether or not the provided table name is referenced by another\\n82.         one. If `column_name` is specified, only references pointing to that\\n83.         column are considered. If `ignore_self` is True, self-referential\\n84.         constraints are ignored.\\n85.         """\\n86.         with self.connection.cursor() as cursor:\\n87.             for other_table in self.connection.introspection.get_table_list(cursor):\\n88.                 if ignore_self and other_table.name == table_name:\\n89.                     continue\\n90.                 relations = self.connection.introspection.get_relations(\\n91.                     cursor, other_table.name\\n92.                 )\\n93.                 for constraint_column, constraint_table in relations.values():\\n94.                     if constraint_table == table_name and (\\n95.                         column_name is None or constraint_column == column_name\\n96.                     ):\\n97.                         return True\\n98.         return False\\n99. \\n100.     def alter_db_table(\\n101.         self, model, old_db_table, new_db_table, disable_constraints=True\\n102.     ):\\n103.         if (\\n104.             not self.connection.features.supports_atomic_references_rename\\n105.             and disable_constraints\\n106.             and self._is_referenced_by_fk_constraint(old_db_table)\\n107.         ):\\n108.             if self.connection.in_atomic_block:\\n109.                 raise NotSupportedError(\\n110.                     (\\n111.                         "Renaming the %r table while in a transaction is not "\\n112.                         "supported on SQLite < 3.26 because it would break referential "\\n113.                         "integrity. Try adding `atomic = False` to the Migration class."\\n114.                     )\\n115.                     % old_db_table\\n116.                 )\\n117.             self.connection.enable_constraint_checking()\\n118.             super().alter_db_table(model, old_db_table, new_db_table)\\n119.             self.connection.disable_constraint_checking()\\n120.         else:\\n121.             super().alter_db_table(model, old_db_table, new_db_table)\\n122. \\n123.     def alter_field(self, model, old_field, new_field, strict=False):\\n124.         if not self._field_should_be_altered(old_field, new_field):\\n125.             return\\n126.         old_field_name = old_field.name\\n127.         table_name = model._meta.db_table\\n128.         _, old_column_name = old_field.get_attname_column()\\n129.         if (\\n130.             new_field.name != old_field_name\\n131.             and not self.connection.features.supports_atomic_references_rename\\n132.             and self._is_referenced_by_fk_constraint(\\n133.                 table_name, old_column_name, ignore_self=True\\n134.             )\\n135.         ):\\n136.             if self.connection.in_atomic_block:\\n137.                 raise NotSupportedError(\\n138.                     (\\n139.                         "Renaming the %r.%r column while in a transaction is not "\\n140.                         "supported on SQLite < 3.26 because it would break referential "\\n141.                         "integrity. Try adding `atomic = False` to the Migration class."\\n142.                     )\\n143.                     % (model._meta.db_table, old_field_name)\\n144.                 )\\n145.             with atomic(self.connection.alias):\\n146.                 super().alter_field(model, old_field, new_field, strict=strict)\\n147.                 # Follow SQLite\\\'s documented procedure for performing changes\\n148.                 # that don\\\'t affect the on-disk content.\\n149.                 # https://sqlite.org/lang_altertable.html#otheralter\\n150.                 with self.connection.cursor() as cursor:\\n151.                     schema_version = cursor.execute("PRAGMA schema_version").fetchone()[\\n152.                         0\\n153.                     ]\\n154.                     cursor.execute("PRAGMA writable_schema = 1")\\n155.                     references_template = \\\' REFERENCES "%s" ("%%s") \\\' % table_name\\n156.                     new_column_name = new_field.get_attname_column()[1]\\n157.                     search = references_template % old_column_name\\n158.                     replacement = references_template % new_column_name\\n159.                     cursor.execute(\\n160.                         "UPDATE sqlite_master SET sql = replace(sql, %s, %s)",\\n161.                         (search, replacement),\\n162.                     )\\n163.                     cursor.execute("PRAGMA schema_version = %d" % (schema_version + 1))\\n164.                     cursor.execute("PRAGMA writable_schema = 0")\\n165.                     # The integrity check will raise an exception and rollback\\n166.                     # the transaction if the sqlite_master updates corrupt the\\n167.                     # database.\\n168.                     cursor.execute("PRAGMA integrity_check")\\n169.             # Perform a VACUUM to refresh the database representation from\\n170.             # the sqlite_master table.\\n171.             with self.connection.cursor() as cursor:\\n172.                 cursor.execute("VACUUM")\\n173.         else:\\n174.             super().alter_field(model, old_field, new_field, strict=strict)\\n175. \\n176.     def _remake_table(\\n177.         self, model, create_field=None, delete_field=None, alter_field=None\\n178.     ):\\n179.         """\\n180.         Shortcut to transform a model from old_model into new_model\\n181. \\n182.         This follows the correct procedure to perform non-rename or column\\n183.         addition operations based on SQLite\\\'s documentation\\n184. \\n185.         https://www.sqlite.org/lang_altertable.html#caution\\n186. \\n187.         The essential steps are:\\n188.           1. Create a table with the updated definition called "new__app_model"\\n189.           2. Copy the data from the existing "app_model" table to the new table\\n190.           3. Drop the "app_model" table\\n191.           4. Rename the "new__app_model" table to "app_model"\\n192.           5. Restore any index of the previous "app_model" table.\\n193.         """\\n194.         # Self-referential fields must be recreated rather than copied from\\n195.         # the old model to ensure their remote_field.field_name doesn\\\'t refer\\n196.         # to an altered field.\\n197.         def is_self_referential(f):\\n198.             return f.is_relation and f.remote_field.model is model\\n199. \\n200.         # Work out the new fields dict / mapping\\n201.         body = {\\n202.             f.name: f.clone() if is_self_referential(f) else f\\n203.             for f in model._meta.local_concrete_fields\\n204.         }\\n205.         # Since mapping might mix column names and default values,\\n206.         # its values must be already quoted.\\n207.         mapping = {\\n208.             f.column: self.quote_name(f.column)\\n209.             for f in model._meta.local_concrete_fields\\n210.         }\\n211.         # This maps field names (not columns) for things like unique_together\\n212.         rename_mapping = {}\\n213.         # If any of the new or altered fields is introducing a new PK,\\n214.         # remove the old one\\n215.         restore_pk_field = None\\n216.         if getattr(create_field, "primary_key", False) or (\\n217.             alter_field and getattr(alter_field[1], "primary_key", False)\\n218.         ):\\n219.             for name, field in list(body.items()):\\n220.                 if field.primary_key:\\n221.                     field.primary_key = False\\n222.                     restore_pk_field = field\\n223.                     if field.auto_created:\\n224.                         del body[name]\\n225.                         del mapping[field.column]\\n226.         # Add in any created fields\\n227.         if create_field:\\n228.             body[create_field.name] = create_field\\n229.             # Choose a default and insert it into the copy map\\n230.             if not create_field.many_to_many and create_field.concrete:\\n231.                 mapping[create_field.column] = self.prepare_default(\\n232.                     self.effective_default(create_field),\\n233.                 )\\n234.         # Add in any altered fields\\n235.         if alter_field:\\n236.             old_field, new_field = alter_field\\n237.             body.pop(old_field.name, None)\\n238.             mapping.pop(old_field.column, None)\\n239.             body[new_field.name] = new_field\\n240.             if old_field.null and not new_field.null:\\n241.                 case_sql = "coalesce(%(col)s, %(default)s)" % {\\n242.                     "col": self.quote_name(old_field.column),\\n243.                     "default": self.prepare_default(self.effective_default(new_field)),\\n244.                 }\\n245.                 mapping[new_field.column] = case_sql\\n246.             else:\\n247.                 mapping[new_field.column] = self.quote_name(old_field.column)\\n248.             rename_mapping[old_field.name] = new_field.name\\n249.         # Remove any deleted fields\\n250.         if delete_field:\\n251.             del body[delete_field.name]\\n252.             del mapping[delete_field.column]\\n253.             # Remove any implicit M2M tables\\n254.             if (\\n255.                 delete_field.many_to_many\\n256.                 and delete_field.remote_field.through._meta.auto_created\\n257.             ):\\n258.                 return self.delete_model(delete_field.remote_field.through)\\n259.         # Work inside a new app registry\\n260.         apps = Apps()\\n261. \\n262.         # Work out the new value of unique_together, taking renames into\\n263.         # account\\n264.         unique_together = [\\n265.             [rename_mapping.get(n, n) for n in unique]\\n266.             for unique in model._meta.unique_together\\n267.         ]\\n268. \\n269.         # Work out the new value for index_together, taking renames into\\n270.         # account\\n271.         index_together = [\\n272.             [rename_mapping.get(n, n) for n in index]\\n273.             for index in model._meta.index_together\\n274.         ]\\n275. \\n276.         indexes = model._meta.indexes\\n277.         if delete_field:\\n278.             indexes = [\\n279.                 index for index in indexes if delete_field.name not in index.fields\\n280.             ]\\n281. \\n282.         constraints = list(model._meta.constraints)\\n283. \\n284.         # Provide isolated instances of the fields to the new model body so\\n285.         # that the existing model\\\'s internals aren\\\'t interfered with when\\n286.         # the dummy model is constructed.\\n287.         body_copy = copy.deepcopy(body)\\n288. \\n289.         # Construct a new model with the new fields to allow self referential\\n290.         # primary key to resolve to. This model won\\\'t ever be materialized as a\\n291.         # table and solely exists for foreign key reference resolution purposes.\\n292.         # This wouldn\\\'t be required if the schema editor was operating on model\\n293.         # states instead of rendered models.\\n294.         meta_contents = {\\n295.             "app_label": model._meta.app_label,\\n296.             "db_table": model._meta.db_table,\\n297.             "unique_together": unique_together,\\n298.             "index_together": index_together,\\n299.             "indexes": indexes,\\n300.             "constraints": constraints,\\n301.             "apps": apps,\\n302.         }\\n303.         meta = type("Meta", (), meta_contents)\\n304.         body_copy["Meta"] = meta\\n305.         body_copy["__module__"] = model.__module__\\n306.         type(model._meta.object_name, model.__bases__, body_copy)\\n307. \\n308.         # Construct a model with a renamed table name.\\n309.         body_copy = copy.deepcopy(body)\\n310.         meta_contents = {\\n311.             "app_label": model._meta.app_label,\\n312.             "db_table": "new__%s" % strip_quotes(model._meta.db_table),\\n313.             "unique_together": unique_together,\\n314.             "index_together": index_together,\\n315.             "indexes": indexes,\\n316.             "constraints": constraints,\\n317.             "apps": apps,\\n318.         }\\n319.         meta = type("Meta", (), meta_contents)\\n320.         body_copy["Meta"] = meta\\n321.         body_copy["__module__"] = model.__module__\\n322.         new_model = type("New%s" % model._meta.object_name, model.__bases__, body_copy)\\n323. \\n324.         # Create a new table with the updated schema.\\n325.         self.create_model(new_model)\\n326. \\n327.         # Copy data from the old table into the new table\\n328.         self.execute(\\n329.             "INSERT INTO %s (%s) SELECT %s FROM %s"\\n330.             % (\\n331.                 self.quote_name(new_model._meta.db_table),\\n332.                 ", ".join(self.quote_name(x) for x in mapping),\\n333.                 ", ".join(mapping.values()),\\n334.                 self.quote_name(model._meta.db_table),\\n335.             )\\n336.         )\\n337. \\n338.         # Delete the old table to make way for the new\\n339.         self.delete_model(model, handle_autom2m=False)\\n340. \\n341.         # Rename the new table to take way for the old\\n342.         self.alter_db_table(\\n343.             new_model,\\n344.             new_model._meta.db_table,\\n345.             model._meta.db_table,\\n346.             disable_constraints=False,\\n347.         )\\n348. \\n349.         # Run deferred SQL on correct table\\n350.         for sql in self.deferred_sql:\\n351.             self.execute(sql)\\n352.         self.deferred_sql = []\\n353.         # Fix any PK-removed field\\n354.         if restore_pk_field:\\n355.             restore_pk_field.primary_key = True\\n356. \\n357.     def delete_model(self, model, handle_autom2m=True):\\n358.         if handle_autom2m:\\n359.             super().delete_model(model)\\n360.         else:\\n361.             # Delete the table (and only that)\\n362.             self.execute(\\n363.                 self.sql_delete_table\\n364.                 % {\\n365.                     "table": self.quote_name(model._meta.db_table),\\n366.                 }\\n367.             )\\n368.             # Remove all deferred statements referencing the deleted table.\\n369.             for sql in list(self.deferred_sql):\\n370.                 if isinstance(sql, Statement) and sql.references_table(\\n371.                     model._meta.db_table\\n372.                 ):\\n373.                     self.deferred_sql.remove(sql)\\n374. \\n375.     def add_field(self, model, field):\\n376.         """Create a field on a model."""\\n377.         if (\\n378.             # Primary keys and unique fields are not supported in ALTER TABLE\\n379.             # ADD COLUMN.\\n380.             field.primary_key\\n381.             or field.unique\\n382.             or\\n383.             # Fields with default values cannot by handled by ALTER TABLE ADD\\n384.             # COLUMN statement because DROP DEFAULT is not supported in\\n385.             # ALTER TABLE.\\n386.             not field.null\\n387.             or self.effective_default(field) is not None\\n388.         ):\\n389.             self._remake_table(model, create_field=field)\\n390.         else:\\n391.             super().add_field(model, field)\\n392. \\n393.     def remove_field(self, model, field):\\n394.         """\\n395.         Remove a field from a model. Usually involves deleting a column,\\n396.         but for M2Ms may involve deleting a table.\\n397.         """\\n398.         # M2M fields are a special case\\n399.         if field.many_to_many:\\n400.             # For implicit M2M tables, delete the auto-created table\\n401.             if field.remote_field.through._meta.auto_created:\\n402.                 self.delete_model(field.remote_field.through)\\n403.             # For explicit "through" M2M fields, do nothing\\n404.         elif (\\n405.             self.connection.features.can_alter_table_drop_column\\n406.             # Primary keys, unique fields, and foreign keys are not\\n407.             # supported in ALTER TABLE DROP COLUMN.\\n408.             and not field.primary_key\\n409.             and not field.unique\\n410.             and not (field.remote_field and field.db_constraint)\\n411.         ):\\n412.             super().remove_field(model, field)\\n413.         # For everything else, remake.\\n414.         else:\\n415.             # It might not actually have a column behind it\\n416.             if field.db_parameters(connection=self.connection)["type"] is None:\\n417.                 return\\n418.             self._remake_table(model, delete_field=field)\\n419. \\n420.     def _alter_field(\\n421.         self,\\n422.         model,\\n423.         old_field,\\n424.         new_field,\\n425.         old_type,\\n426.         new_type,\\n427.         old_db_params,\\n428.         new_db_params,\\n429.         strict=False,\\n430.     ):\\n431.         """Perform a "physical" (non-ManyToMany) field update."""\\n432.         # Use "ALTER TABLE ... RENAME COLUMN" if only the column name\\n433.         # changed and there aren\\\'t any constraints.\\n434.         if (\\n435.             self.connection.features.can_alter_table_rename_column\\n436.             and old_field.column != new_field.column\\n437.             and self.column_sql(model, old_field) == self.column_sql(model, new_field)\\n438.             and not (\\n439.                 old_field.remote_field\\n440.                 and old_field.db_constraint\\n441.                 or new_field.remote_field\\n442.                 and new_field.db_constraint\\n443.             )\\n444.         ):\\n445.             return self.execute(\\n446.                 self._rename_field_sql(\\n447.                     model._meta.db_table, old_field, new_field, new_type\\n448.                 )\\n449.             )\\n450.         # Alter by remaking table\\n451.         self._remake_table(model, alter_field=(old_field, new_field))\\n452.         # Rebuild tables with FKs pointing to this field.\\n453.         if new_field.unique and old_type != new_type:\\n454.             related_models = set()\\n455.             opts = new_field.model._meta\\n456.             for remote_field in opts.related_objects:\\n457.                 # Ignore self-relationship since the table was already rebuilt.\\n458.                 if remote_field.related_model == model:\\n459.                     continue\\n460.                 if not remote_field.many_to_many:\\n461.                     if remote_field.field_name == new_field.name:\\n462.                         related_models.add(remote_field.related_model)\\n463.                 elif new_field.primary_key and remote_field.through._meta.auto_created:\\n464.                     related_models.add(remote_field.through)\\n465.             if new_field.primary_key:\\n466.                 for many_to_many in opts.many_to_many:\\n467.                     # Ignore self-relationship since the table was already rebuilt.\\n468.                     if many_to_many.related_model == model:\\n469.                         continue\\n470.                     if many_to_many.remote_field.through._meta.auto_created:\\n471.                         related_models.add(many_to_many.remote_field.through)\\n472.             for related_model in related_models:\\n473.                 self._remake_table(related_model)\\n474. \\n475.     def _alter_many_to_many(self, model, old_field, new_field, strict):\\n476.         """Alter M2Ms to repoint their to= endpoints."""\\n477.         if (\\n478.             old_field.remote_field.through._meta.db_table\\n479.             == new_field.remote_field.through._meta.db_table\\n480.         ):\\n481.             # The field name didn\\\'t change, but some options did, so we have to\\n482.             # propagate this altering.\\n483.             self._remake_table(\\n484.                 old_field.remote_field.through,\\n485.                 alter_field=(\\n486.                     # The field that points to the target model is needed, so\\n487.                     # we can tell alter_field to change it - this is\\n488.                     # m2m_reverse_field_name() (as opposed to m2m_field_name(),\\n489.                     # which points to our model).\\n490.                     old_field.remote_field.through._meta.get_field(\\n491.                         old_field.m2m_reverse_field_name()\\n492.                     ),\\n493.                     new_field.remote_field.through._meta.get_field(\\n494.                         new_field.m2m_reverse_field_name()\\n495.                     ),\\n496.                 ),\\n497.             )\\n498.             return\\n499. \\n500.         # Make a new through table\\n501.         self.create_model(new_field.remote_field.through)\\n502.         # Copy the data across\\n503.         self.execute(\\n504.             "INSERT INTO %s (%s) SELECT %s FROM %s"\\n505.             % (\\n506.                 self.quote_name(new_field.remote_field.through._meta.db_table),\\n507.                 ", ".join(\\n508.                     [\\n509.                         "id",\\n510.                         new_field.m2m_column_name(),\\n511.                         new_field.m2m_reverse_name(),\\n512.                     ]\\n513.                 ),\\n514.                 ", ".join(\\n515.                     [\\n516.                         "id",\\n517.                         old_field.m2m_column_name(),\\n518.                         old_field.m2m_reverse_name(),\\n519.                     ]\\n520.                 ),\\n521.                 self.quote_name(old_field.remote_field.through._meta.db_table),\\n522.             )\\n523.         )\\n524.         # Delete the old through table\\n525.         self.delete_model(old_field.remote_field.through)\\n526. \\n527.     def add_constraint(self, model, constraint):\\n528.         if isinstance(constraint, UniqueConstraint) and (\\n529.             constraint.condition\\n530.             or constraint.contains_expressions\\n531.             or constraint.include\\n532.             or constraint.deferrable\\n533.         ):\\n534.             super().add_constraint(model, constraint)\\n535.         else:\\n536.             self._remake_table(model)\\n537. \\n538.     def remove_constraint(self, model, constraint):\\n539.         if isinstance(constraint, UniqueConstraint) and (\\n540.             constraint.condition\\n541.             or constraint.contains_expressions\\n542.             or constraint.include\\n543.             or constraint.deferrable\\n544.         ):\\n545.             super().remove_constraint(model, constraint)\\n546.         else:\\n547.             self._remake_table(model)\\n548. \\n549.     def _collate_sql(self, collation):\\n550.         return "COLLATE " + collation\', \'start_line_number\': 13, \'end_line_number\': 550}\n\n{\'relative_path\': \'django/db/migrations/operations/fields.py\', \'content\': \'198. class AlterField(FieldOperation):\\n199.     """\\n200.     Alter a field\\\'s database column (e.g. null, max_length) to the provided\\n201.     new field.\\n202.     """\\n203. \\n204.     def __init__(self, model_name, name, field, preserve_default=True):\\n205.         self.preserve_default = preserve_default\\n206.         super().__init__(model_name, name, field)\\n207. \\n208.     def deconstruct(self):\\n209.         kwargs = {\\n210.             "model_name": self.model_name,\\n211.             "name": self.name,\\n212.             "field": self.field,\\n213.         }\\n214.         if self.preserve_default is not True:\\n215.             kwargs["preserve_default"] = self.preserve_default\\n216.         return (self.__class__.__name__, [], kwargs)\\n217. \\n218.     def state_forwards(self, app_label, state):\\n219.         state.alter_field(\\n220.             app_label,\\n221.             self.model_name_lower,\\n222.             self.name,\\n223.             self.field,\\n224.             self.preserve_default,\\n225.         )\\n226. \\n227.     def database_forwards(self, app_label, schema_editor, from_state, to_state):\\n228.         to_model = to_state.apps.get_model(app_label, self.model_name)\\n229.         if self.allow_migrate_model(schema_editor.connection.alias, to_model):\\n230.             from_model = from_state.apps.get_model(app_label, self.model_name)\\n231.             from_field = from_model._meta.get_field(self.name)\\n232.             to_field = to_model._meta.get_field(self.name)\\n233.             if not self.preserve_default:\\n234.                 to_field.default = self.field.default\\n235.             schema_editor.alter_field(from_model, from_field, to_field)\\n236.             if not self.preserve_default:\\n237.                 to_field.default = NOT_PROVIDED\\n238. \\n239.     def database_backwards(self, app_label, schema_editor, from_state, to_state):\\n240.         self.database_forwards(app_label, schema_editor, from_state, to_state)\\n241. \\n242.     def describe(self):\\n243.         return "Alter field %s on %s" % (self.name, self.model_name)\\n244. \\n245.     @property\\n246.     def migration_name_fragment(self):\\n247.         return "alter_%s_%s" % (self.model_name_lower, self.name_lower)\\n248. \\n249.     def reduce(self, operation, app_label):\\n250.         if isinstance(operation, RemoveField) and self.is_same_field_operation(\\n251.             operation\\n252.         ):\\n253.             return [operation]\\n254.         elif (\\n255.             isinstance(operation, RenameField)\\n256.             and self.is_same_field_operation(operation)\\n257.             and self.field.db_column is None\\n258.         ):\\n259.             return [\\n260.                 operation,\\n261.                 AlterField(\\n262.                     model_name=self.model_name,\\n263.                     name=operation.new_name,\\n264.                     field=self.field,\\n265.                 ),\\n266.             ]\\n267.         return super().reduce(operation, app_label)\', \'start_line_number\': 198, \'end_line_number\': 267}\n\nTest Case Context (similar existing tests for reference):\n{\'relative_path\': \'django/db/backends/base/schema.py\', \'content\': \'1. import logging\\n2. from datetime import datetime\\n3. \\n4. from django.db.backends.ddl_references import (\\n5.     Columns,\\n6.     Expressions,\\n7.     ForeignKeyName,\\n8.     IndexName,\\n9.     Statement,\\n10.     Table,\\n11. )\\n12. from django.db.backends.utils import names_digest, split_identifier\\n13. from django.db.models import Deferrable, Index\\n14. from django.db.models.sql import Query\\n15. from django.db.transaction import TransactionManagementError, atomic\\n16. from django.utils import timezone\\n17. \\n18. logger = logging.getLogger("django.db.backends.schema")\\n19. \\n20. \\n21. def _is_relevant_relation(relation, altered_field):\\n22.     """\\n23.     When altering the given field, must constraints on its model from the given\\n24.     relation be temporarily dropped?\\n25.     """\\n26.     field = relation.field\\n27.     if field.many_to_many:\\n28.         # M2M reverse field\\n29.         return False\\n30.     if altered_field.primary_key and field.to_fields == [None]:\\n31.         # Foreign key constraint on the primary key, which is being altered.\\n32.         return True\\n33.     # Is the constraint targeting the field being altered?\\n34.     return altered_field.name in field.to_fields\\n35. \\n36. \\n37. def _all_related_fields(model):\\n38.     return model._meta._get_fields(\\n39.         forward=False,\\n40.         reverse=True,\\n41.         include_hidden=True,\\n42.         include_parents=False,\\n43.     )\\n44. \\n45. \\n46. def _related_non_m2m_objects(old_field, new_field):\\n47.     # Filter out m2m objects from reverse relations.\\n48.     # Return (old_relation, new_relation) tuples.\\n49.     related_fields = zip(\\n50.         (\\n51.             obj\\n52.             for obj in _all_related_fields(old_field.model)\\n53.             if _is_relevant_relation(obj, old_field)\\n54.         ),\\n55.         (\\n56.             obj\\n57.             for obj in _all_related_fields(new_field.model)\\n58.             if _is_relevant_relation(obj, new_field)\\n59.         ),\\n60.     )\\n61.     for old_rel, new_rel in related_fields:\\n62.         yield old_rel, new_rel\\n63.         yield from _related_non_m2m_objects(\\n64.             old_rel.remote_field,\\n65.             new_rel.remote_field,\\n66.         )\\n67. \\n68. \\n69. class BaseDatabaseSchemaEditor:\\n70.     """\\n71.     This class and its subclasses are responsible for emitting schema-changing\\n72.     statements to the databases - model creation/removal/alteration, field\\n73.     renaming, index fiddling, and so on.\\n74.     """\\n75. \\n76.     # Overrideable SQL templates\\n77.     sql_create_table = "CREATE TABLE %(table)s (%(definition)s)"\\n78.     sql_rename_table = "ALTER TABLE %(old_table)s RENAME TO %(new_table)s"\\n79.     sql_retablespace_table = "ALTER TABLE %(table)s SET TABLESPACE %(new_tablespace)s"\\n80.     sql_delete_table = "DROP TABLE %(table)s CASCADE"\\n81. \\n82.     sql_create_column = "ALTER TABLE %(table)s ADD COLUMN %(column)s %(definition)s"\\n83.     sql_alter_column = "ALTER TABLE %(table)s %(changes)s"\\n84.     sql_alter_column_type = "ALTER COLUMN %(column)s TYPE %(type)s"\\n85.     sql_alter_column_null = "ALTER COLUMN %(column)s DROP NOT NULL"\\n86.     sql_alter_column_not_null = "ALTER COLUMN %(column)s SET NOT NULL"\\n87.     sql_alter_column_default = "ALTER COLUMN %(column)s SET DEFAULT %(default)s"\\n88.     sql_alter_column_no_default = "ALTER COLUMN %(column)s DROP DEFAULT"\\n89.     sql_alter_column_no_default_null = sql_alter_column_no_default\\n90.     sql_alter_column_collate = "ALTER COLUMN %(column)s TYPE %(type)s%(collation)s"\\n91.     sql_delete_column = "ALTER TABLE %(table)s DROP COLUMN %(column)s CASCADE"\\n92.     sql_rename_column = (\\n93.         "ALTER TABLE %(table)s RENAME COLUMN %(old_column)s TO %(new_column)s"\\n94.     )\\n95.     sql_update_with_default = (\\n96.         "UPDATE %(table)s SET %(column)s = %(default)s WHERE %(column)s IS NULL"\\n97.     )\\n98. \\n99.     sql_unique_constraint = "UNIQUE (%(columns)s)%(deferrable)s"\\n100.     sql_check_constraint = "CHECK (%(check)s)"\\n101.     sql_delete_constraint = "ALTER TABLE %(table)s DROP CONSTRAINT %(name)s"\\n102.     sql_constraint = "CONSTRAINT %(name)s %(constraint)s"\\n103. \\n104.     sql_create_check = "ALTER TABLE %(table)s ADD CONSTRAINT %(name)s CHECK (%(check)s)"\\n105.     sql_delete_check = sql_delete_constraint\\n106. \\n107.     sql_create_unique = (\\n108.         "ALTER TABLE %(table)s ADD CONSTRAINT %(name)s "\\n109.         "UNIQUE (%(columns)s)%(deferrable)s"\\n110.     )\\n111.     sql_delete_unique = sql_delete_constraint\\n112. \\n113.     sql_create_fk = (\\n114.         "ALTER TABLE %(table)s ADD CONSTRAINT %(name)s FOREIGN KEY (%(column)s) "\\n115.         "REFERENCES %(to_table)s (%(to_column)s)%(deferrable)s"\\n116.     )\\n117.     sql_create_inline_fk = None\\n118.     sql_create_column_inline_fk = None\\n119.     sql_delete_fk = sql_delete_constraint\\n120. \\n121.     sql_create_index = (\\n122.         "CREATE INDEX %(name)s ON %(table)s "\\n123.         "(%(columns)s)%(include)s%(extra)s%(condition)s"\\n124.     )\\n125.     sql_create_unique_index = (\\n126.         "CREATE UNIQUE INDEX %(name)s ON %(table)s "\\n127.         "(%(columns)s)%(include)s%(condition)s"\\n128.     )\\n129.     sql_delete_index = "DROP INDEX %(name)s"\\n130. \\n131.     sql_create_pk = (\\n132.         "ALTER TABLE %(table)s ADD CONSTRAINT %(name)s PRIMARY KEY (%(columns)s)"\\n133.     )\\n134.     sql_delete_pk = sql_delete_constraint\\n135. \\n136.     sql_delete_procedure = "DROP PROCEDURE %(procedure)s"\\n137. \\n138.     def __init__(self, connection, collect_sql=False, atomic=True):\\n139.         self.connection = connection\\n140.         self.collect_sql = collect_sql\\n141.         if self.collect_sql:\\n142.             self.collected_sql = []\\n143.         self.atomic_migration = self.connection.features.can_rollback_ddl and atomic\\n144. \\n145.     # State-managing methods\\n146. \\n147.     def __enter__(self):\\n148.         self.deferred_sql = []\\n149.         if self.atomic_migration:\\n150.             self.atomic = atomic(self.connection.alias)\\n151.             self.atomic.__enter__()\\n152.         return self\\n153. \\n154.     def __exit__(self, exc_type, exc_value, traceback):\\n155.         if exc_type is None:\\n156.             for sql in self.deferred_sql:\\n157.                 self.execute(sql)\\n158.         if self.atomic_migration:\\n159.             self.atomic.__exit__(exc_type, exc_value, traceback)\\n160. \\n161.     # Core utility functions\\n162. \\n163.     def execute(self, sql, params=()):\\n164.         """Execute the given SQL statement, with optional parameters."""\\n165.         # Don\\\'t perform the transactional DDL check if SQL is being collected\\n166.         # as it\\\'s not going to be executed anyway.\\n167.         if (\\n168.             not self.collect_sql\\n169.             and self.connection.in_atomic_block\\n170.             and not self.connection.features.can_rollback_ddl\\n171.         ):\\n172.             raise TransactionManagementError(\\n173.                 "Executing DDL statements while in a transaction on databases "\\n174.                 "that can\\\'t perform a rollback is prohibited."\\n175.             )\\n176.         # Account for non-string statement objects.\\n177.         sql = str(sql)\\n178.         # Log the command we\\\'re running, then run it\\n179.         logger.debug(\\n180.             "%s; (params %r)", sql, params, extra={"params": params, "sql": sql}\\n181.         )\\n182.         if self.collect_sql:\\n183.             ending = "" if sql.rstrip().endswith(";") else ";"\\n184.             if params is not None:\\n185.                 self.collected_sql.append(\\n186.                     (sql % tuple(map(self.quote_value, params))) + ending\\n187.                 )\\n188.             else:\\n189.                 self.collected_sql.append(sql + ending)\\n190.         else:\\n191.             with self.connection.cursor() as cursor:\\n192.                 cursor.execute(sql, params)\\n193. \\n194.     def quote_name(self, name):\\n195.         return self.connection.ops.quote_name(name)\\n196. \\n197.     def table_sql(self, model):\\n198.         """Take a model and return its table definition."""\\n199.         # Add any unique_togethers (always deferred, as some fields might be\\n200.         # created afterward, like geometry fields with some backends).\\n201.         for field_names in model._meta.unique_together:\\n202.             fields = [model._meta.get_field(field) for field in field_names]\\n203.             self.deferred_sql.append(self._create_unique_sql(model, fields))\\n204.         # Create column SQL, add FK deferreds if needed.\\n205.         column_sqls = []\\n206.         params = []\\n207.         for field in model._meta.local_fields:\\n208.             # SQL.\\n209.             definition, extra_params = self.column_sql(model, field)\\n210.             if definition is None:\\n211.                 continue\\n212.             # Check constraints can go on the column SQL here.\\n213.             db_params = field.db_parameters(connection=self.connection)\\n214.             if db_params["check"]:\\n215.                 definition += " " + self.sql_check_constraint % db_params\\n216.             # Autoincrement SQL (for backends with inline variant).\\n217.             col_type_suffix = field.db_type_suffix(connection=self.connection)\\n218.             if col_type_suffix:\\n219.                 definition += " %s" % col_type_suffix\\n220.             params.extend(extra_params)\\n221.             # FK.\\n222.             if field.remote_field and field.db_constraint:\\n223.                 to_table = field.remote_field.model._meta.db_table\\n224.                 to_column = field.remote_field.model._meta.get_field(\\n225.                     field.remote_field.field_name\\n226.                 ).column\\n227.                 if self.sql_create_inline_fk:\\n228.                     definition += " " + self.sql_create_inline_fk % {\\n229.                         "to_table": self.quote_name(to_table),\\n230.                         "to_column": self.quote_name(to_column),\\n231.                     }\\n232.                 elif self.connection.features.supports_foreign_keys:\\n233.                     self.deferred_sql.append(\\n234.                         self._create_fk_sql(\\n235.                             model, field, "_fk_%(to_table)s_%(to_column)s"\\n236.                         )\\n237.                     )\\n238.             # Add the SQL to our big list.\\n239.             column_sqls.append(\\n240.                 "%s %s"\\n241.                 % (\\n242.                     self.quote_name(field.column),\\n243.                     definition,\\n244.                 )\\n245.             )\\n246.             # Autoincrement SQL (for backends with post table definition\\n247.             # variant).\\n248.             if field.get_internal_type() in (\\n249.                 "AutoField",\\n250.                 "BigAutoField",\\n251.                 "SmallAutoField",\\n252.             ):\\n253.                 autoinc_sql = self.connection.ops.autoinc_sql(\\n254.                     model._meta.db_table, field.column\\n255.                 )\\n256.                 if autoinc_sql:\\n257.                     self.deferred_sql.extend(autoinc_sql)\\n258.         constraints = [\\n259.             constraint.constraint_sql(model, self)\\n260.             for constraint in model._meta.constraints\\n261.         ]\\n262.         sql = self.sql_create_table % {\\n263.             "table": self.quote_name(model._meta.db_table),\\n264.             "definition": ", ".join(\\n265.                 constraint for constraint in (*column_sqls, *constraints) if constraint\\n266.             ),\\n267.         }\\n268.         if model._meta.db_tablespace:\\n269.             tablespace_sql = self.connection.ops.tablespace_sql(\\n270.                 model._meta.db_tablespace\\n271.             )\\n272.             if tablespace_sql:\\n273.                 sql += " " + tablespace_sql\\n274.         return sql, params\\n275. \\n276.     # Field <-> database mapping functions\\n277. \\n278.     def _iter_column_sql(self, column_db_type, params, model, field, include_default):\\n279.         yield column_db_type\\n280.         collation = getattr(field, "db_collation", None)\\n281.         if collation:\\n282.             yield self._collate_sql(collation)\\n283.         # Work out nullability.\\n284.         null = field.null\\n285.         # Include a default value, if requested.\\n286.         include_default = (\\n287.             include_default\\n288.             and not self.skip_default(field)\\n289.             and\\n290.             # Don\\\'t include a default value if it\\\'s a nullable field and the\\n291.             # default cannot be dropped in the ALTER COLUMN statement (e.g.\\n292.             # MySQL longtext and longblob).\\n293.             not (null and self.skip_default_on_alter(field))\\n294.         )\\n295.         if include_default:\\n296.             default_value = self.effective_default(field)\\n297.             if default_value is not None:\\n298.                 column_default = "DEFAULT " + self._column_default_sql(field)\\n299.                 if self.connection.features.requires_literal_defaults:\\n300.                     # Some databases can\\\'t take defaults as a parameter (Oracle).\\n301.                     # If this is the case, the individual schema backend should\\n302.                     # implement prepare_default().\\n303.                     yield column_default % self.prepare_default(default_value)\\n304.                 else:\\n305.                     yield column_default\\n306.                     params.append(default_value)\\n307.         # Oracle treats the empty string (\\\'\\\') as null, so coerce the null\\n308.         # option whenever \\\'\\\' is a possible value.\\n309.         if (\\n310.             field.empty_strings_allowed\\n311.             and not field.primary_key\\n312.             and self.connection.features.interprets_empty_strings_as_nulls\\n313.         ):\\n314.             null = True\\n315.         if not null:\\n316.             yield "NOT NULL"\\n317.         elif not self.connection.features.implied_column_null:\\n318.             yield "NULL"\\n319.         if field.primary_key:\\n320.             yield "PRIMARY KEY"\\n321.         elif field.unique:\\n322.             yield "UNIQUE"\\n323.         # Optionally add the tablespace if it\\\'s an implicitly indexed column.\\n324.         tablespace = field.db_tablespace or model._meta.db_tablespace\\n325.         if (\\n326.             tablespace\\n327.             and self.connection.features.supports_tablespaces\\n328.             and field.unique\\n329.         ):\\n330.             yield self.connection.ops.tablespace_sql(tablespace, inline=True)\\n331. \\n332.     def column_sql(self, model, field, include_default=False):\\n333.         """\\n334.         Return the column definition for a field. The field must already have\\n335.         had set_attributes_from_name() called.\\n336.         """\\n337.         # Get the column\\\'s type and use that as the basis of the SQL.\\n338.         db_params = field.db_parameters(connection=self.connection)\\n339.         column_db_type = db_params["type"]\\n340.         # Check for fields that aren\\\'t actually columns (e.g. M2M).\\n341.         if column_db_type is None:\\n342.             return None, None\\n343.         params = []\\n344.         return (\\n345.             " ".join(\\n346.                 # This appends to the params being returned.\\n347.                 self._iter_column_sql(\\n348.                     column_db_type, params, model, field, include_default\\n349.                 )\\n350.             ),\\n351.             params,\\n352.         )\\n353. \\n354.     def skip_default(self, field):\\n355.         """\\n356.         Some backends don\\\'t accept default values for certain columns types\\n357.         (i.e. MySQL longtext and longblob).\\n358.         """\\n359.         return False\\n360. \\n361.     def skip_default_on_alter(self, field):\\n362.         """\\n363.         Some backends don\\\'t accept default values for certain columns types\\n364.         (i.e. MySQL longtext and longblob) in the ALTER COLUMN statement.\\n365.         """\\n366.         return False\\n367. \\n368.     def prepare_default(self, value):\\n369.         """\\n370.         Only used for backends which have requires_literal_defaults feature\\n371.         """\\n372.         raise NotImplementedError(\\n373.             "subclasses of BaseDatabaseSchemaEditor for backends which have "\\n374.             "requires_literal_defaults must provide a prepare_default() method"\\n375.         )\\n376. \\n377.     def _column_default_sql(self, field):\\n378.         """\\n379.         Return the SQL to use in a DEFAULT clause. The resulting string should\\n380.         contain a \\\'%s\\\' placeholder for a default value.\\n381.         """\\n382.         return "%s"\\n383. \\n384.     @staticmethod\\n385.     def _effective_default(field):\\n386.         # This method allows testing its logic without a connection.\\n387.         if field.has_default():\\n388.             default = field.get_default()\\n389.         elif not field.null and field.blank and field.empty_strings_allowed:\\n390.             if field.get_internal_type() == "BinaryField":\\n391.                 default = b""\\n392.             else:\\n393.                 default = ""\\n394.         elif getattr(field, "auto_now", False) or getattr(field, "auto_now_add", False):\\n395.             internal_type = field.get_internal_type()\\n396.             if internal_type == "DateTimeField":\\n397.                 default = timezone.now()\\n398.             else:\\n399.                 default = datetime.now()\\n400.                 if internal_type == "DateField":\\n401.                     default = default.date()\\n402.                 elif internal_type == "TimeField":\\n403.                     default = default.time()\\n404.         else:\\n405.             default = None\\n406.         return default\\n407. \\n408.     def effective_default(self, field):\\n409.         """Return a field\\\'s effective database default value."""\\n410.         return field.get_db_prep_save(self._effective_default(field), self.connection)\\n411. \\n412.     def quote_value(self, value):\\n413.         """\\n414.         Return a quoted version of the value so it\\\'s safe to use in an SQL\\n415.         string. This is not safe against injection from user code; it is\\n416.         intended only for use in making SQL scripts or preparing default values\\n417.         for particularly tricky backends (defaults are not user-defined, though,\\n418.         so this is safe).\\n419.         """\\n420.         raise NotImplementedError()\\n421. \\n422.     # Actions\\n423. \\n424.     def create_model(self, model):\\n425.         """\\n426.         Create a table and any accompanying indexes or unique constraints for\\n427.         the given `model`.\\n428.         """\\n429.         sql, params = self.table_sql(model)\\n430.         # Prevent using [] as params, in the case a literal \\\'%\\\' is used in the\\n431.         # definition.\\n432.         self.execute(sql, params or None)\\n433. \\n434.         # Add any field index and index_together\\\'s (deferred as SQLite\\n435.         # _remake_table needs it).\\n436.         self.deferred_sql.extend(self._model_indexes_sql(model))\\n437. \\n438.         # Make M2M tables\\n439.         for field in model._meta.local_many_to_many:\\n440.             if field.remote_field.through._meta.auto_created:\\n441.                 self.create_model(field.remote_field.through)\\n442. \\n443.     def delete_model(self, model):\\n444.         """Delete a model from the database."""\\n445.         # Handle auto-created intermediary models\\n446.         for field in model._meta.local_many_to_many:\\n447.             if field.remote_field.through._meta.auto_created:\\n448.                 self.delete_model(field.remote_field.through)\\n449. \\n450.         # Delete the table\\n451.         self.execute(\\n452.             self.sql_delete_table\\n453.             % {\\n454.                 "table": self.quote_name(model._meta.db_table),\\n455.             }\\n456.         )\\n457.         # Remove all deferred statements referencing the deleted table.\\n458.         for sql in list(self.deferred_sql):\\n459.             if isinstance(sql, Statement) and sql.references_table(\\n460.                 model._meta.db_table\\n461.             ):\\n462.                 self.deferred_sql.remove(sql)\\n463. \\n464.     def add_index(self, model, index):\\n465.         """Add an index on a model."""\\n466.         if (\\n467.             index.contains_expressions\\n468.             and not self.connection.features.supports_expression_indexes\\n469.         ):\\n470.             return None\\n471.         # Index.create_sql returns interpolated SQL which makes params=None a\\n472.         # necessity to avoid escaping attempts on execution.\\n473.         self.execute(index.create_sql(model, self), params=None)\\n474. \\n475.     def remove_index(self, model, index):\\n476.         """Remove an index from a model."""\\n477.         if (\\n478.             index.contains_expressions\\n479.             and not self.connection.features.supports_expression_indexes\\n480.         ):\\n481.             return None\\n482.         self.execute(index.remove_sql(model, self))\\n483. \\n484.     def add_constraint(self, model, constraint):\\n485.         """Add a constraint to a model."""\\n486.         sql = constraint.create_sql(model, self)\\n487.         if sql:\\n488.             # Constraint.create_sql returns interpolated SQL which makes\\n489.             # params=None a necessity to avoid escaping attempts on execution.\\n490.             self.execute(sql, params=None)\\n491. \\n492.     def remove_constraint(self, model, constraint):\\n493.         """Remove a constraint from a model."""\\n494.         sql = constraint.remove_sql(model, self)\\n495.         if sql:\\n496.             self.execute(sql)\\n497. \\n498.     def alter_unique_together(self, model, old_unique_together, new_unique_together):\\n499.         """\\n500.         Deal with a model changing its unique_together. The input\', \'start_line_number\': 1, \'end_line_number\': 500}\n\n{\'relative_path\': \'django/db/backends/sqlite3/schema.py\', \'content\': \'1. import copy\\n2. from decimal import Decimal\\n3. \\n4. from django.apps.registry import Apps\\n5. from django.db import NotSupportedError\\n6. from django.db.backends.base.schema import BaseDatabaseSchemaEditor\\n7. from django.db.backends.ddl_references import Statement\\n8. from django.db.backends.utils import strip_quotes\\n9. from django.db.models import UniqueConstraint\\n10. from django.db.transaction import atomic\\n11. \\n12. \\n13. class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):\\n14. \\n15.     sql_delete_table = "DROP TABLE %(table)s"\\n16.     sql_create_fk = None\\n17.     sql_create_inline_fk = (\\n18.         "REFERENCES %(to_table)s (%(to_column)s) DEFERRABLE INITIALLY DEFERRED"\\n19.     )\\n20.     sql_create_column_inline_fk = sql_create_inline_fk\\n21.     sql_delete_column = "ALTER TABLE %(table)s DROP COLUMN %(column)s"\\n22.     sql_create_unique = "CREATE UNIQUE INDEX %(name)s ON %(table)s (%(columns)s)"\\n23.     sql_delete_unique = "DROP INDEX %(name)s"\\n24. \\n25.     def __enter__(self):\\n26.         # Some SQLite schema alterations need foreign key constraints to be\\n27.         # disabled. Enforce it here for the duration of the schema edition.\\n28.         if not self.connection.disable_constraint_checking():\\n29.             raise NotSupportedError(\\n30.                 "SQLite schema editor cannot be used while foreign key "\\n31.                 "constraint checks are enabled. Make sure to disable them "\\n32.                 "before entering a transaction.atomic() context because "\\n33.                 "SQLite does not support disabling them in the middle of "\\n34.                 "a multi-statement transaction."\\n35.             )\\n36.         return super().__enter__()\\n37. \\n38.     def __exit__(self, exc_type, exc_value, traceback):\\n39.         self.connection.check_constraints()\\n40.         super().__exit__(exc_type, exc_value, traceback)\\n41.         self.connection.enable_constraint_checking()\\n42. \\n43.     def quote_value(self, value):\\n44.         # The backend "mostly works" without this function and there are use\\n45.         # cases for compiling Python without the sqlite3 libraries (e.g.\\n46.         # security hardening).\\n47.         try:\\n48.             import sqlite3\\n49. \\n50.             value = sqlite3.adapt(value)\\n51.         except ImportError:\\n52.             pass\\n53.         except sqlite3.ProgrammingError:\\n54.             pass\\n55.         # Manual emulation of SQLite parameter quoting\\n56.         if isinstance(value, bool):\\n57.             return str(int(value))\\n58.         elif isinstance(value, (Decimal, float, int)):\\n59.             return str(value)\\n60.         elif isinstance(value, str):\\n61.             return "\\\'%s\\\'" % value.replace("\\\'", "\\\'\\\'")\\n62.         elif value is None:\\n63.             return "NULL"\\n64.         elif isinstance(value, (bytes, bytearray, memoryview)):\\n65.             # Bytes are only allowed for BLOB fields, encoded as string\\n66.             # literals containing hexadecimal data and preceded by a single "X"\\n67.             # character.\\n68.             return "X\\\'%s\\\'" % value.hex()\\n69.         else:\\n70.             raise ValueError(\\n71.                 "Cannot quote parameter value %r of type %s" % (value, type(value))\\n72.             )\\n73. \\n74.     def prepare_default(self, value):\\n75.         return self.quote_value(value)\\n76. \\n77.     def _is_referenced_by_fk_constraint(\\n78.         self, table_name, column_name=None, ignore_self=False\\n79.     ):\\n80.         """\\n81.         Return whether or not the provided table name is referenced by another\\n82.         one. If `column_name` is specified, only references pointing to that\\n83.         column are considered. If `ignore_self` is True, self-referential\\n84.         constraints are ignored.\\n85.         """\\n86.         with self.connection.cursor() as cursor:\\n87.             for other_table in self.connection.introspection.get_table_list(cursor):\\n88.                 if ignore_self and other_table.name == table_name:\\n89.                     continue\\n90.                 relations = self.connection.introspection.get_relations(\\n91.                     cursor, other_table.name\\n92.                 )\\n93.                 for constraint_column, constraint_table in relations.values():\\n94.                     if constraint_table == table_name and (\\n95.                         column_name is None or constraint_column == column_name\\n96.                     ):\\n97.                         return True\\n98.         return False\\n99. \\n100.     def alter_db_table(\\n101.         self, model, old_db_table, new_db_table, disable_constraints=True\\n102.     ):\\n103.         if (\\n104.             not self.connection.features.supports_atomic_references_rename\\n105.             and disable_constraints\\n106.             and self._is_referenced_by_fk_constraint(old_db_table)\\n107.         ):\\n108.             if self.connection.in_atomic_block:\\n109.                 raise NotSupportedError(\\n110.                     (\\n111.                         "Renaming the %r table while in a transaction is not "\\n112.                         "supported on SQLite < 3.26 because it would break referential "\\n113.                         "integrity. Try adding `atomic = False` to the Migration class."\\n114.                     )\\n115.                     % old_db_table\\n116.                 )\\n117.             self.connection.enable_constraint_checking()\\n118.             super().alter_db_table(model, old_db_table, new_db_table)\\n119.             self.connection.disable_constraint_checking()\\n120.         else:\\n121.             super().alter_db_table(model, old_db_table, new_db_table)\\n122. \\n123.     def alter_field(self, model, old_field, new_field, strict=False):\\n124.         if not self._field_should_be_altered(old_field, new_field):\\n125.             return\\n126.         old_field_name = old_field.name\\n127.         table_name = model._meta.db_table\\n128.         _, old_column_name = old_field.get_attname_column()\\n129.         if (\\n130.             new_field.name != old_field_name\\n131.             and not self.connection.features.supports_atomic_references_rename\\n132.             and self._is_referenced_by_fk_constraint(\\n133.                 table_name, old_column_name, ignore_self=True\\n134.             )\\n135.         ):\\n136.             if self.connection.in_atomic_block:\\n137.                 raise NotSupportedError(\\n138.                     (\\n139.                         "Renaming the %r.%r column while in a transaction is not "\\n140.                         "supported on SQLite < 3.26 because it would break referential "\\n141.                         "integrity. Try adding `atomic = False` to the Migration class."\\n142.                     )\\n143.                     % (model._meta.db_table, old_field_name)\\n144.                 )\\n145.             with atomic(self.connection.alias):\\n146.                 super().alter_field(model, old_field, new_field, strict=strict)\\n147.                 # Follow SQLite\\\'s documented procedure for performing changes\\n148.                 # that don\\\'t affect the on-disk content.\\n149.                 # https://sqlite.org/lang_altertable.html#otheralter\\n150.                 with self.connection.cursor() as cursor:\\n151.                     schema_version = cursor.execute("PRAGMA schema_version").fetchone()[\\n152.                         0\\n153.                     ]\\n154.                     cursor.execute("PRAGMA writable_schema = 1")\\n155.                     references_template = \\\' REFERENCES "%s" ("%%s") \\\' % table_name\\n156.                     new_column_name = new_field.get_attname_column()[1]\\n157.                     search = references_template % old_column_name\\n158.                     replacement = references_template % new_column_name\\n159.                     cursor.execute(\\n160.                         "UPDATE sqlite_master SET sql = replace(sql, %s, %s)",\\n161.                         (search, replacement),\\n162.                     )\\n163.                     cursor.execute("PRAGMA schema_version = %d" % (schema_version + 1))\\n164.                     cursor.execute("PRAGMA writable_schema = 0")\\n165.                     # The integrity check will raise an exception and rollback\\n166.                     # the transaction if the sqlite_master updates corrupt the\\n167.                     # database.\\n168.                     cursor.execute("PRAGMA integrity_check")\\n169.             # Perform a VACUUM to refresh the database representation from\\n170.             # the sqlite_master table.\\n171.             with self.connection.cursor() as cursor:\\n172.                 cursor.execute("VACUUM")\\n173.         else:\\n174.             super().alter_field(model, old_field, new_field, strict=strict)\\n175. \\n176.     def _remake_table(\\n177.         self, model, create_field=None, delete_field=None, alter_field=None\\n178.     ):\\n179.         """\\n180.         Shortcut to transform a model from old_model into new_model\\n181. \\n182.         This follows the correct procedure to perform non-rename or column\\n183.         addition operations based on SQLite\\\'s documentation\\n184. \\n185.         https://www.sqlite.org/lang_altertable.html#caution\\n186. \\n187.         The essential steps are:\\n188.           1. Create a table with the updated definition called "new__app_model"\\n189.           2. Copy the data from the existing "app_model" table to the new table\\n190.           3. Drop the "app_model" table\\n191.           4. Rename the "new__app_model" table to "app_model"\\n192.           5. Restore any index of the previous "app_model" table.\\n193.         """\\n194.         # Self-referential fields must be recreated rather than copied from\\n195.         # the old model to ensure their remote_field.field_name doesn\\\'t refer\\n196.         # to an altered field.\\n197.         def is_self_referential(f):\\n198.             return f.is_relation and f.remote_field.model is model\\n199. \\n200.         # Work out the new fields dict / mapping\\n201.         body = {\\n202.             f.name: f.clone() if is_self_referential(f) else f\\n203.             for f in model._meta.local_concrete_fields\\n204.         }\\n205.         # Since mapping might mix column names and default values,\\n206.         # its values must be already quoted.\\n207.         mapping = {\\n208.             f.column: self.quote_name(f.column)\\n209.             for f in model._meta.local_concrete_fields\\n210.         }\\n211.         # This maps field names (not columns) for things like unique_together\\n212.         rename_mapping = {}\\n213.         # If any of the new or altered fields is introducing a new PK,\\n214.         # remove the old one\\n215.         restore_pk_field = None\\n216.         if getattr(create_field, "primary_key", False) or (\\n217.             alter_field and getattr(alter_field[1], "primary_key", False)\\n218.         ):\\n219.             for name, field in list(body.items()):\\n220.                 if field.primary_key:\\n221.                     field.primary_key = False\\n222.                     restore_pk_field = field\\n223.                     if field.auto_created:\\n224.                         del body[name]\\n225.                         del mapping[field.column]\\n226.         # Add in any created fields\\n227.         if create_field:\\n228.             body[create_field.name] = create_field\\n229.             # Choose a default and insert it into the copy map\\n230.             if not create_field.many_to_many and create_field.concrete:\\n231.                 mapping[create_field.column] = self.prepare_default(\\n232.                     self.effective_default(create_field),\\n233.                 )\\n234.         # Add in any altered fields\\n235.         if alter_field:\\n236.             old_field, new_field = alter_field\\n237.             body.pop(old_field.name, None)\\n238.             mapping.pop(old_field.column, None)\\n239.             body[new_field.name] = new_field\\n240.             if old_field.null and not new_field.null:\\n241.                 case_sql = "coalesce(%(col)s, %(default)s)" % {\\n242.                     "col": self.quote_name(old_field.column),\\n243.                     "default": self.prepare_default(self.effective_default(new_field)),\\n244.                 }\\n245.                 mapping[new_field.column] = case_sql\\n246.             else:\\n247.                 mapping[new_field.column] = self.quote_name(old_field.column)\\n248.             rename_mapping[old_field.name] = new_field.name\\n249.         # Remove any deleted fields\\n250.         if delete_field:\\n251.             del body[delete_field.name]\\n252.             del mapping[delete_field.column]\\n253.             # Remove any implicit M2M tables\\n254.             if (\\n255.                 delete_field.many_to_many\\n256.                 and delete_field.remote_field.through._meta.auto_created\\n257.             ):\\n258.                 return self.delete_model(delete_field.remote_field.through)\\n259.         # Work inside a new app registry\\n260.         apps = Apps()\\n261. \\n262.         # Work out the new value of unique_together, taking renames into\\n263.         # account\\n264.         unique_together = [\\n265.             [rename_mapping.get(n, n) for n in unique]\\n266.             for unique in model._meta.unique_together\\n267.         ]\\n268. \\n269.         # Work out the new value for index_together, taking renames into\\n270.         # account\\n271.         index_together = [\\n272.             [rename_mapping.get(n, n) for n in index]\\n273.             for index in model._meta.index_together\\n274.         ]\\n275. \\n276.         indexes = model._meta.indexes\\n277.         if delete_field:\\n278.             indexes = [\\n279.                 index for index in indexes if delete_field.name not in index.fields\\n280.             ]\\n281. \\n282.         constraints = list(model._meta.constraints)\\n283. \\n284.         # Provide isolated instances of the fields to the new model body so\\n285.         # that the existing model\\\'s internals aren\\\'t interfered with when\\n286.         # the dummy model is constructed.\\n287.         body_copy = copy.deepcopy(body)\\n288. \\n289.         # Construct a new model with the new fields to allow self referential\\n290.         # primary key to resolve to. This model won\\\'t ever be materialized as a\\n291.         # table and solely exists for foreign key reference resolution purposes.\\n292.         # This wouldn\\\'t be required if the schema editor was operating on model\\n293.         # states instead of rendered models.\\n294.         meta_contents = {\\n295.             "app_label": model._meta.app_label,\\n296.             "db_table": model._meta.db_table,\\n297.             "unique_together": unique_together,\\n298.             "index_together": index_together,\\n299.             "indexes": indexes,\\n300.             "constraints": constraints,\\n301.             "apps": apps,\\n302.         }\\n303.         meta = type("Meta", (), meta_contents)\\n304.         body_copy["Meta"] = meta\\n305.         body_copy["__module__"] = model.__module__\\n306.         type(model._meta.object_name, model.__bases__, body_copy)\\n307. \\n308.         # Construct a model with a renamed table name.\\n309.         body_copy = copy.deepcopy(body)\\n310.         meta_contents = {\\n311.             "app_label": model._meta.app_label,\\n312.             "db_table": "new__%s" % strip_quotes(model._meta.db_table),\\n313.             "unique_together": unique_together,\\n314.             "index_together": index_together,\\n315.             "indexes": indexes,\\n316.             "constraints": constraints,\\n317.             "apps": apps,\\n318.         }\\n319.         meta = type("Meta", (), meta_contents)\\n320.         body_copy["Meta"] = meta\\n321.         body_copy["__module__"] = model.__module__\\n322.         new_model = type("New%s" % model._meta.object_name, model.__bases__, body_copy)\\n323. \\n324.         # Create a new table with the updated schema.\\n325.         self.create_model(new_model)\\n326. \\n327.         # Copy data from the old table into the new table\\n328.         self.execute(\\n329.             "INSERT INTO %s (%s) SELECT %s FROM %s"\\n330.             % (\\n331.                 self.quote_name(new_model._meta.db_table),\\n332.                 ", ".join(self.quote_name(x) for x in mapping),\\n333.                 ", ".join(mapping.values()),\\n334.                 self.quote_name(model._meta.db_table),\\n335.             )\\n336.         )\\n337. \\n338.         # Delete the old table to make way for the new\\n339.         self.delete_model(model, handle_autom2m=False)\\n340. \\n341.         # Rename the new table to take way for the old\\n342.         self.alter_db_table(\\n343.             new_model,\\n344.             new_model._meta.db_table,\\n345.             model._meta.db_table,\\n346.             disable_constraints=False,\\n347.         )\\n348. \\n349.         # Run deferred SQL on correct table\\n350.         for sql in self.deferred_sql:\\n351.             self.execute(sql)\\n352.         self.deferred_sql = []\\n353.         # Fix any PK-removed field\\n354.         if restore_pk_field:\\n355.             restore_pk_field.primary_key = True\\n356. \\n357.     def delete_model(self, model, handle_autom2m=True):\\n358.         if handle_autom2m:\\n359.             super().delete_model(model)\\n360.         else:\\n361.             # Delete the table (and only that)\\n362.             self.execute(\\n363.                 self.sql_delete_table\\n364.                 % {\\n365.                     "table": self.quote_name(model._meta.db_table),\\n366.                 }\\n367.             )\\n368.             # Remove all deferred statements referencing the deleted table.\\n369.             for sql in list(self.deferred_sql):\\n370.                 if isinstance(sql, Statement) and sql.references_table(\\n371.                     model._meta.db_table\\n372.                 ):\\n373.                     self.deferred_sql.remove(sql)\\n374. \\n375.     def add_field(self, model, field):\\n376.         """Create a field on a model."""\\n377.         if (\\n378.             # Primary keys and unique fields are not supported in ALTER TABLE\\n379.             # ADD COLUMN.\\n380.             field.primary_key\\n381.             or field.unique\\n382.             or\\n383.             # Fields with default values cannot by handled by ALTER TABLE ADD\\n384.             # COLUMN statement because DROP DEFAULT is not supported in\\n385.             # ALTER TABLE.\\n386.             not field.null\\n387.             or self.effective_default(field) is not None\\n388.         ):\\n389.             self._remake_table(model, create_field=field)\\n390.         else:\\n391.             super().add_field(model, field)\\n392. \\n393.     def remove_field(self, model, field):\\n394.         """\\n395.         Remove a field from a model. Usually involves deleting a column,\\n396.         but for M2Ms may involve deleting a table.\\n397.         """\\n398.         # M2M fields are a special case\\n399.         if field.many_to_many:\\n400.             # For implicit M2M tables, delete the auto-created table\\n401.             if field.remote_field.through._meta.auto_created:\\n402.                 self.delete_model(field.remote_field.through)\\n403.             # For explicit "through" M2M fields, do nothing\\n404.         elif (\\n405.             self.connection.features.can_alter_table_drop_column\\n406.             # Primary keys, unique fields, and foreign keys are not\\n407.             # supported in ALTER TABLE DROP COLUMN.\\n408.             and not field.primary_key\\n409.             and not field.unique\\n410.             and not (field.remote_field and field.db_constraint)\\n411.         ):\\n412.             super().remove_field(model, field)\\n413.         # For everything else, remake.\\n414.         else:\\n415.             # It might not actually have a column behind it\\n416.             if field.db_parameters(connection=self.connection)["type"] is None:\\n417.                 return\\n418.             self._remake_table(model, delete_field=field)\\n419. \\n420.     def _alter_field(\\n421.         self,\\n422.         model,\\n423.         old_field,\\n424.         new_field,\\n425.         old_type,\\n426.         new_type,\\n427.         old_db_params,\\n428.         new_db_params,\\n429.         strict=False,\\n430.     ):\\n431.         """Perform a "physical" (non-ManyToMany) field update."""\\n432.         # Use "ALTER TABLE ... RENAME COLUMN" if only the column name\\n433.         # changed and there aren\\\'t any constraints.\\n434.         if (\\n435.             self.connection.features.can_alter_table_rename_column\\n436.             and old_field.column != new_field.column\\n437.             and self.column_sql(model, old_field) == self.column_sql(model, new_field)\\n438.             and not (\\n439.                 old_field.remote_field\\n440.                 and old_field.db_constraint\\n441.                 or new_field.remote_field\\n442.                 and new_field.db_constraint\\n443.             )\\n444.         ):\\n445.             return self.execute(\\n446.                 self._rename_field_sql(\\n447.                     model._meta.db_table, old_field, new_field, new_type\\n448.                 )\\n449.             )\\n450.         # Alter by remaking table\\n451.         self._remake_table(model, alter_field=(old_field, new_field))\\n452.         # Rebuild tables with FKs pointing to this field.\\n453.         if new_field.unique and old_type != new_type:\\n454.             related_models = set()\\n455.             opts = new_field.model._meta\\n456.             for remote_field in opts.related_objects:\\n457.                 # Ignore self-relationship since the table was already rebuilt.\\n458.                 if remote_field.related_model == model:\\n459.                     continue\\n460.                 if not remote_field.many_to_many:\\n461.                     if remote_field.field_name == new_field.name:\\n462.                         related_models.add(remote_field.related_model)\\n463.                 elif new_field.primary_key and remote_field.through._meta.auto_created:\\n464.                     related_models.add(remote_field.through)\\n465.             if new_field.primary_key:\\n466.                 for many_to_many in opts.many_to_many:\\n467.                     # Ignore self-relationship since the table was already rebuilt.\\n468.                     if many_to_many.related_model == model:\\n469.                         continue\\n470.                     if many_to_many.remote_field.through._meta.auto_created:\\n471.                         related_models.add(many_to_many.remote_field.through)\\n472.             for related_model in related_models:\\n473.                 self._remake_table(related_model)\\n474. \\n475.     def _alter_many_to_many(self, model, old_field, new_field, strict):\\n476.         """Alter M2Ms to repoint their to= endpoints."""\\n477.         if (\\n478.             old_field.remote_field.through._meta.db_table\\n479.             == new_field.remote_field.through._meta.db_table\\n480.         ):\\n481.             # The field name didn\\\'t change, but some options did, so we have to\\n482.             # propagate this altering.\\n483.             self._remake_table(\\n484.                 old_field.remote_field.through,\\n485.                 alter_field=(\\n486.                     # The field that points to the target model is needed, so\\n487.                     # we can tell alter_field to change it - this is\\n488.                     # m2m_reverse_field_name() (as opposed to m2m_field_name(),\\n489.                     # which points to our model).\\n490.                     old_field.remote_field.through._meta.get_field(\\n491.                         old_field.m2m_reverse_field_name()\\n492.                     ),\\n493.                     new_field.remote_field.through._meta.get_field(\\n494.                         new_field.m2m_reverse_field_name()\\n495.                     ),\\n496.                 ),\\n497.             )\\n498.             return\\n499. \\n500.         # Make a new through table\', \'start_line_number\': 1, \'end_line_number\': 500}\n\n{\'relative_path\': \'tests/migrations/test_base.py\', \'content\': \'1. import os\\n2. import shutil\\n3. import tempfile\\n4. from contextlib import contextmanager\\n5. from importlib import import_module\\n6. \\n7. from django.apps import apps\\n8. from django.db import connection, connections, migrations, models\\n9. from django.db.migrations.migration import Migration\\n10. from django.db.migrations.recorder import MigrationRecorder\\n11. from django.db.migrations.state import ProjectState\\n12. from django.test import TransactionTestCase\\n13. from django.test.utils import extend_sys_path\\n14. from django.utils.module_loading import module_dir\\n15. \\n16. \\n17. class MigrationTestBase(TransactionTestCase):\\n18.     """\\n19.     Contains an extended set of asserts for testing migrations and schema operations.\\n20.     """\\n21. \\n22.     available_apps = ["migrations"]\\n23.     databases = {"default", "other"}\\n24. \\n25.     def tearDown(self):\\n26.         # Reset applied-migrations state.\\n27.         for db in self.databases:\\n28.             recorder = MigrationRecorder(connections[db])\\n29.             recorder.migration_qs.filter(app="migrations").delete()\\n30. \\n31.     def get_table_description(self, table, using="default"):\\n32.         with connections[using].cursor() as cursor:\\n33.             return connections[using].introspection.get_table_description(cursor, table)\\n34. \\n35.     def assertTableExists(self, table, using="default"):\\n36.         with connections[using].cursor() as cursor:\\n37.             self.assertIn(table, connections[using].introspection.table_names(cursor))\\n38. \\n39.     def assertTableNotExists(self, table, using="default"):\\n40.         with connections[using].cursor() as cursor:\\n41.             self.assertNotIn(\\n42.                 table, connections[using].introspection.table_names(cursor)\\n43.             )\\n44. \\n45.     def assertColumnExists(self, table, column, using="default"):\\n46.         self.assertIn(\\n47.             column, [c.name for c in self.get_table_description(table, using=using)]\\n48.         )\\n49. \\n50.     def assertColumnNotExists(self, table, column, using="default"):\\n51.         self.assertNotIn(\\n52.             column, [c.name for c in self.get_table_description(table, using=using)]\\n53.         )\\n54. \\n55.     def _get_column_allows_null(self, table, column, using):\\n56.         return [\\n57.             c.null_ok\\n58.             for c in self.get_table_description(table, using=using)\\n59.             if c.name == column\\n60.         ][0]\\n61. \\n62.     def assertColumnNull(self, table, column, using="default"):\\n63.         self.assertTrue(self._get_column_allows_null(table, column, using))\\n64. \\n65.     def assertColumnNotNull(self, table, column, using="default"):\\n66.         self.assertFalse(self._get_column_allows_null(table, column, using))\\n67. \\n68.     def assertIndexExists(\\n69.         self, table, columns, value=True, using="default", index_type=None\\n70.     ):\\n71.         with connections[using].cursor() as cursor:\\n72.             self.assertEqual(\\n73.                 value,\\n74.                 any(\\n75.                     c["index"]\\n76.                     for c in connections[using]\\n77.                     .introspection.get_constraints(cursor, table)\\n78.                     .values()\\n79.                     if (\\n80.                         c["columns"] == list(columns)\\n81.                         and (index_type is None or c["type"] == index_type)\\n82.                         and not c["unique"]\\n83.                     )\\n84.                 ),\\n85.             )\\n86. \\n87.     def assertIndexNotExists(self, table, columns):\\n88.         return self.assertIndexExists(table, columns, False)\\n89. \\n90.     def assertIndexNameExists(self, table, index, using="default"):\\n91.         with connections[using].cursor() as cursor:\\n92.             self.assertIn(\\n93.                 index,\\n94.                 connection.introspection.get_constraints(cursor, table),\\n95.             )\\n96. \\n97.     def assertIndexNameNotExists(self, table, index, using="default"):\\n98.         with connections[using].cursor() as cursor:\\n99.             self.assertNotIn(\\n100.                 index,\\n101.                 connection.introspection.get_constraints(cursor, table),\\n102.             )\\n103. \\n104.     def assertConstraintExists(self, table, name, value=True, using="default"):\\n105.         with connections[using].cursor() as cursor:\\n106.             constraints = (\\n107.                 connections[using].introspection.get_constraints(cursor, table).items()\\n108.             )\\n109.             self.assertEqual(\\n110.                 value,\\n111.                 any(c["check"] for n, c in constraints if n == name),\\n112.             )\\n113. \\n114.     def assertConstraintNotExists(self, table, name):\\n115.         return self.assertConstraintExists(table, name, False)\\n116. \\n117.     def assertUniqueConstraintExists(self, table, columns, value=True, using="default"):\\n118.         with connections[using].cursor() as cursor:\\n119.             constraints = (\\n120.                 connections[using].introspection.get_constraints(cursor, table).values()\\n121.             )\\n122.             self.assertEqual(\\n123.                 value,\\n124.                 any(c["unique"] for c in constraints if c["columns"] == list(columns)),\\n125.             )\\n126. \\n127.     def assertFKExists(self, table, columns, to, value=True, using="default"):\\n128.         with connections[using].cursor() as cursor:\\n129.             self.assertEqual(\\n130.                 value,\\n131.                 any(\\n132.                     c["foreign_key"] == to\\n133.                     for c in connections[using]\\n134.                     .introspection.get_constraints(cursor, table)\\n135.                     .values()\\n136.                     if c["columns"] == list(columns)\\n137.                 ),\\n138.             )\\n139. \\n140.     def assertFKNotExists(self, table, columns, to):\\n141.         return self.assertFKExists(table, columns, to, False)\\n142. \\n143.     @contextmanager\\n144.     def temporary_migration_module(self, app_label="migrations", module=None):\\n145.         """\\n146.         Allows testing management commands in a temporary migrations module.\\n147. \\n148.         Wrap all invocations to makemigrations and squashmigrations with this\\n149.         context manager in order to avoid creating migration files in your\\n150.         source tree inadvertently.\\n151. \\n152.         Takes the application label that will be passed to makemigrations or\\n153.         squashmigrations and the Python path to a migrations module.\\n154. \\n155.         The migrations module is used as a template for creating the temporary\\n156.         migrations module. If it isn\\\'t provided, the application\\\'s migrations\\n157.         module is used, if it exists.\\n158. \\n159.         Returns the filesystem path to the temporary migrations module.\\n160.         """\\n161.         with tempfile.TemporaryDirectory() as temp_dir:\\n162.             target_dir = tempfile.mkdtemp(dir=temp_dir)\\n163.             with open(os.path.join(target_dir, "__init__.py"), "w"):\\n164.                 pass\\n165.             target_migrations_dir = os.path.join(target_dir, "migrations")\\n166. \\n167.             if module is None:\\n168.                 module = apps.get_app_config(app_label).name + ".migrations"\\n169. \\n170.             try:\\n171.                 source_migrations_dir = module_dir(import_module(module))\\n172.             except (ImportError, ValueError):\\n173.                 pass\\n174.             else:\\n175.                 shutil.copytree(source_migrations_dir, target_migrations_dir)\\n176. \\n177.             with extend_sys_path(temp_dir):\\n178.                 new_module = os.path.basename(target_dir) + ".migrations"\\n179.                 with self.settings(MIGRATION_MODULES={app_label: new_module}):\\n180.                     yield target_migrations_dir\\n181. \\n182. \\n183. class OperationTestBase(MigrationTestBase):\\n184.     """Common functions to help test operations."""\\n185. \\n186.     @classmethod\\n187.     def setUpClass(cls):\\n188.         super().setUpClass()\\n189.         cls._initial_table_names = frozenset(connection.introspection.table_names())\\n190. \\n191.     def tearDown(self):\\n192.         self.cleanup_test_tables()\\n193.         super().tearDown()\\n194. \\n195.     def cleanup_test_tables(self):\\n196.         table_names = (\\n197.             frozenset(connection.introspection.table_names())\\n198.             - self._initial_table_names\\n199.         )\\n200.         with connection.schema_editor() as editor:\\n201.             with connection.constraint_checks_disabled():\\n202.                 for table_name in table_names:\\n203.                     editor.execute(\\n204.                         editor.sql_delete_table\\n205.                         % {\\n206.                             "table": editor.quote_name(table_name),\\n207.                         }\\n208.                     )\\n209. \\n210.     def apply_operations(self, app_label, project_state, operations, atomic=True):\\n211.         migration = Migration("name", app_label)\\n212.         migration.operations = operations\\n213.         with connection.schema_editor(atomic=atomic) as editor:\\n214.             return migration.apply(project_state, editor)\\n215. \\n216.     def unapply_operations(self, app_label, project_state, operations, atomic=True):\\n217.         migration = Migration("name", app_label)\\n218.         migration.operations = operations\\n219.         with connection.schema_editor(atomic=atomic) as editor:\\n220.             return migration.unapply(project_state, editor)\\n221. \\n222.     def make_test_state(self, app_label, operation, **kwargs):\\n223.         """\\n224.         Makes a test state using set_up_test_model and returns the\\n225.         original state and the state after the migration is applied.\\n226.         """\\n227.         project_state = self.set_up_test_model(app_label, **kwargs)\\n228.         new_state = project_state.clone()\\n229.         operation.state_forwards(app_label, new_state)\\n230.         return project_state, new_state\\n231. \\n232.     def set_up_test_model(\\n233.         self,\\n234.         app_label,\\n235.         second_model=False,\\n236.         third_model=False,\\n237.         index=False,\\n238.         multicol_index=False,\\n239.         related_model=False,\\n240.         mti_model=False,\\n241.         proxy_model=False,\\n242.         manager_model=False,\\n243.         unique_together=False,\\n244.         options=False,\\n245.         db_table=None,\\n246.         index_together=False,\\n247.         constraints=None,\\n248.         indexes=None,\\n249.     ):\\n250.         """Creates a test model state and database table."""\\n251.         # Make the "current" state.\\n252.         model_options = {\\n253.             "swappable": "TEST_SWAP_MODEL",\\n254.             "index_together": [["weight", "pink"]] if index_together else [],\\n255.             "unique_together": [["pink", "weight"]] if unique_together else [],\\n256.         }\\n257.         if options:\\n258.             model_options["permissions"] = [("can_groom", "Can groom")]\\n259.         if db_table:\\n260.             model_options["db_table"] = db_table\\n261.         operations = [\\n262.             migrations.CreateModel(\\n263.                 "Pony",\\n264.                 [\\n265.                     ("id", models.AutoField(primary_key=True)),\\n266.                     ("pink", models.IntegerField(default=3)),\\n267.                     ("weight", models.FloatField()),\\n268.                 ],\\n269.                 options=model_options,\\n270.             )\\n271.         ]\\n272.         if index:\\n273.             operations.append(\\n274.                 migrations.AddIndex(\\n275.                     "Pony",\\n276.                     models.Index(fields=["pink"], name="pony_pink_idx"),\\n277.                 )\\n278.             )\\n279.         if multicol_index:\\n280.             operations.append(\\n281.                 migrations.AddIndex(\\n282.                     "Pony",\\n283.                     models.Index(fields=["pink", "weight"], name="pony_test_idx"),\\n284.                 )\\n285.             )\\n286.         if indexes:\\n287.             for index in indexes:\\n288.                 operations.append(migrations.AddIndex("Pony", index))\\n289.         if constraints:\\n290.             for constraint in constraints:\\n291.                 operations.append(migrations.AddConstraint("Pony", constraint))\\n292.         if second_model:\\n293.             operations.append(\\n294.                 migrations.CreateModel(\\n295.                     "Stable",\\n296.                     [\\n297.                         ("id", models.AutoField(primary_key=True)),\\n298.                     ],\\n299.                 )\\n300.             )\\n301.         if third_model:\\n302.             operations.append(\\n303.                 migrations.CreateModel(\\n304.                     "Van",\\n305.                     [\\n306.                         ("id", models.AutoField(primary_key=True)),\\n307.                     ],\\n308.                 )\\n309.             )\\n310.         if related_model:\\n311.             operations.append(\\n312.                 migrations.CreateModel(\\n313.                     "Rider",\\n314.                     [\\n315.                         ("id", models.AutoField(primary_key=True)),\\n316.                         ("pony", models.ForeignKey("Pony", models.CASCADE)),\\n317.                         (\\n318.                             "friend",\\n319.                             models.ForeignKey("self", models.CASCADE, null=True),\\n320.                         ),\\n321.                     ],\\n322.                 )\\n323.             )\\n324.         if mti_model:\\n325.             operations.append(\\n326.                 migrations.CreateModel(\\n327.                     "ShetlandPony",\\n328.                     fields=[\\n329.                         (\\n330.                             "pony_ptr",\\n331.                             models.OneToOneField(\\n332.                                 "Pony",\\n333.                                 models.CASCADE,\\n334.                                 auto_created=True,\\n335.                                 parent_link=True,\\n336.                                 primary_key=True,\\n337.                                 to_field="id",\\n338.                                 serialize=False,\\n339.                             ),\\n340.                         ),\\n341.                         ("cuteness", models.IntegerField(default=1)),\\n342.                     ],\\n343.                     bases=["%s.Pony" % app_label],\\n344.                 )\\n345.             )\\n346.         if proxy_model:\\n347.             operations.append(\\n348.                 migrations.CreateModel(\\n349.                     "ProxyPony",\\n350.                     fields=[],\\n351.                     options={"proxy": True},\\n352.                     bases=["%s.Pony" % app_label],\\n353.                 )\\n354.             )\\n355.         if manager_model:\\n356.             from .models import FoodManager, FoodQuerySet\\n357. \\n358.             operations.append(\\n359.                 migrations.CreateModel(\\n360.                     "Food",\\n361.                     fields=[\\n362.                         ("id", models.AutoField(primary_key=True)),\\n363.                     ],\\n364.                     managers=[\\n365.                         ("food_qs", FoodQuerySet.as_manager()),\\n366.                         ("food_mgr", FoodManager("a", "b")),\\n367.                         ("food_mgr_kwargs", FoodManager("x", "y", 3, 4)),\\n368.                     ],\\n369.                 )\\n370.             )\\n371.         return self.apply_operations(app_label, ProjectState(), operations)\', \'start_line_number\': 1, \'end_line_number\': 371}\n\n{\'relative_path\': \'tests/migrations/test_operations.py\', \'content\': \'1. from django.core.exceptions import FieldDoesNotExist\\n2. from django.db import IntegrityError, connection, migrations, models, transaction\\n3. from django.db.migrations.migration import Migration\\n4. from django.db.migrations.operations.fields import FieldOperation\\n5. from django.db.migrations.state import ModelState, ProjectState\\n6. from django.db.models.functions import Abs\\n7. from django.db.transaction import atomic\\n8. from django.test import SimpleTestCase, override_settings, skipUnlessDBFeature\\n9. from django.test.utils import CaptureQueriesContext\\n10. \\n11. from .models import FoodManager, FoodQuerySet, UnicodeModel\\n12. from .test_base import OperationTestBase\\n13. \\n14. \\n15. class Mixin:\\n16.     pass\\n17. \\n18. \', \'start_line_number\': 1, \'end_line_number\': 18}\n\n{\'relative_path\': \'tests/migrations/test_operations.py\', \'content\': \'19. class OperationTests(OperationTestBase):\\n20.     """\\n21.     Tests running the operations and making sure they do what they say they do.\\n22.     Each test looks at their state changing, and then their database operation -\\n23.     both forwards and backwards.\\n24.     """\\n25. \\n26.     def test_create_model(self):\\n27.         """\\n28.         Tests the CreateModel operation.\\n29.         Most other tests use this operation as part of setup, so check failures\\n30.         here first.\\n31.         """\\n32.         operation = migrations.CreateModel(\\n33.             "Pony",\\n34.             [\\n35.                 ("id", models.AutoField(primary_key=True)),\\n36.                 ("pink", models.IntegerField(default=1)),\\n37.             ],\\n38.         )\\n39.         self.assertEqual(operation.describe(), "Create model Pony")\\n40.         self.assertEqual(operation.migration_name_fragment, "pony")\\n41.         # Test the state alteration\\n42.         project_state = ProjectState()\\n43.         new_state = project_state.clone()\\n44.         operation.state_forwards("test_crmo", new_state)\\n45.         self.assertEqual(new_state.models["test_crmo", "pony"].name, "Pony")\\n46.         self.assertEqual(len(new_state.models["test_crmo", "pony"].fields), 2)\\n47.         # Test the database alteration\\n48.         self.assertTableNotExists("test_crmo_pony")\\n49.         with connection.schema_editor() as editor:\\n50.             operation.database_forwards("test_crmo", editor, project_state, new_state)\\n51.         self.assertTableExists("test_crmo_pony")\\n52.         # And test reversal\\n53.         with connection.schema_editor() as editor:\\n54.             operation.database_backwards("test_crmo", editor, new_state, project_state)\\n55.         self.assertTableNotExists("test_crmo_pony")\\n56.         # And deconstruction\\n57.         definition = operation.deconstruct()\\n58.         self.assertEqual(definition[0], "CreateModel")\\n59.         self.assertEqual(definition[1], [])\\n60.         self.assertEqual(sorted(definition[2]), ["fields", "name"])\\n61.         # And default manager not in set\\n62.         operation = migrations.CreateModel(\\n63.             "Foo", fields=[], managers=[("objects", models.Manager())]\\n64.         )\\n65.         definition = operation.deconstruct()\\n66.         self.assertNotIn("managers", definition[2])\\n67. \\n68.     def test_create_model_with_duplicate_field_name(self):\\n69.         with self.assertRaisesMessage(\\n70.             ValueError, "Found duplicate value pink in CreateModel fields argument."\\n71.         ):\\n72.             migrations.CreateModel(\\n73.                 "Pony",\\n74.                 [\\n75.                     ("id", models.AutoField(primary_key=True)),\\n76.                     ("pink", models.TextField()),\\n77.                     ("pink", models.IntegerField(default=1)),\\n78.                 ],\\n79.             )\\n80. \\n81.     def test_create_model_with_duplicate_base(self):\\n82.         message = "Found duplicate value test_crmo.pony in CreateModel bases argument."\\n83.         with self.assertRaisesMessage(ValueError, message):\\n84.             migrations.CreateModel(\\n85.                 "Pony",\\n86.                 fields=[],\\n87.                 bases=(\\n88.                     "test_crmo.Pony",\\n89.                     "test_crmo.Pony",\\n90.                 ),\\n91.             )\\n92.         with self.assertRaisesMessage(ValueError, message):\\n93.             migrations.CreateModel(\\n94.                 "Pony",\\n95.                 fields=[],\\n96.                 bases=(\\n97.                     "test_crmo.Pony",\\n98.                     "test_crmo.pony",\\n99.                 ),\\n100.             )\\n101.         message = (\\n102.             "Found duplicate value migrations.unicodemodel in CreateModel bases "\\n103.             "argument."\\n104.         )\\n105.         with self.assertRaisesMessage(ValueError, message):\\n106.             migrations.CreateModel(\\n107.                 "Pony",\\n108.                 fields=[],\\n109.                 bases=(\\n110.                     UnicodeModel,\\n111.                     UnicodeModel,\\n112.                 ),\\n113.             )\\n114.         with self.assertRaisesMessage(ValueError, message):\\n115.             migrations.CreateModel(\\n116.                 "Pony",\\n117.                 fields=[],\\n118.                 bases=(\\n119.                     UnicodeModel,\\n120.                     "migrations.unicodemodel",\\n121.                 ),\\n122.             )\\n123.         with self.assertRaisesMessage(ValueError, message):\\n124.             migrations.CreateModel(\\n125.                 "Pony",\\n126.                 fields=[],\\n127.                 bases=(\\n128.                     UnicodeModel,\\n129.                     "migrations.UnicodeModel",\\n130.                 ),\\n131.             )\\n132.         message = (\\n133.             "Found duplicate value <class \\\'django.db.models.base.Model\\\'> in "\\n134.             "CreateModel bases argument."\\n135.         )\\n136.         with self.assertRaisesMessage(ValueError, message):\\n137.             migrations.CreateModel(\\n138.                 "Pony",\\n139.                 fields=[],\\n140.                 bases=(\\n141.                     models.Model,\\n142.                     models.Model,\\n143.                 ),\\n144.             )\\n145.         message = (\\n146.             "Found duplicate value <class \\\'migrations.test_operations.Mixin\\\'> in "\\n147.             "CreateModel bases argument."\\n148.         )\\n149.         with self.assertRaisesMessage(ValueError, message):\\n150.             migrations.CreateModel(\\n151.                 "Pony",\\n152.                 fields=[],\\n153.                 bases=(\\n154.                     Mixin,\\n155.                     Mixin,\\n156.                 ),\\n157.             )\\n158. \\n159.     def test_create_model_with_duplicate_manager_name(self):\\n160.         with self.assertRaisesMessage(\\n161.             ValueError,\\n162.             "Found duplicate value objects in CreateModel managers argument.",\\n163.         ):\\n164.             migrations.CreateModel(\\n165.                 "Pony",\\n166.                 fields=[],\\n167.                 managers=[\\n168.                     ("objects", models.Manager()),\\n169.                     ("objects", models.Manager()),\\n170.                 ],\\n171.             )\\n172. \\n173.     def test_create_model_with_unique_after(self):\\n174.         """\\n175.         Tests the CreateModel operation directly followed by an\\n176.         AlterUniqueTogether (bug #22844 - sqlite remake issues)\\n177.         """\\n178.         operation1 = migrations.CreateModel(\\n179.             "Pony",\\n180.             [\\n181.                 ("id", models.AutoField(primary_key=True)),\\n182.                 ("pink", models.IntegerField(default=1)),\\n183.             ],\\n184.         )\\n185.         operation2 = migrations.CreateModel(\\n186.             "Rider",\\n187.             [\\n188.                 ("id", models.AutoField(primary_key=True)),\\n189.                 ("number", models.IntegerField(default=1)),\\n190.                 ("pony", models.ForeignKey("test_crmoua.Pony", models.CASCADE)),\\n191.             ],\\n192.         )\\n193.         operation3 = migrations.AlterUniqueTogether(\\n194.             "Rider",\\n195.             [\\n196.                 ("number", "pony"),\\n197.             ],\\n198.         )\\n199.         # Test the database alteration\\n200.         project_state = ProjectState()\\n201.         self.assertTableNotExists("test_crmoua_pony")\\n202.         self.assertTableNotExists("test_crmoua_rider")\\n203.         with connection.schema_editor() as editor:\\n204.             new_state = project_state.clone()\\n205.             operation1.state_forwards("test_crmoua", new_state)\\n206.             operation1.database_forwards(\\n207.                 "test_crmoua", editor, project_state, new_state\\n208.             )\\n209.             project_state, new_state = new_state, new_state.clone()\\n210.             operation2.state_forwards("test_crmoua", new_state)\\n211.             operation2.database_forwards(\\n212.                 "test_crmoua", editor, project_state, new_state\\n213.             )\\n214.             project_state, new_state = new_state, new_state.clone()\\n215.             operation3.state_forwards("test_crmoua", new_state)\\n216.             operation3.database_forwards(\\n217.                 "test_crmoua", editor, project_state, new_state\\n218.             )\\n219.         self.assertTableExists("test_crmoua_pony")\\n220.         self.assertTableExists("test_crmoua_rider")\\n221. \\n222.     def test_create_model_m2m(self):\\n223.         """\\n224.         Test the creation of a model with a ManyToMany field and the\\n225.         auto-created "through" model.\\n226.         """\\n227.         project_state = self.set_up_test_model("test_crmomm")\\n228.         operation = migrations.CreateModel(\\n229.             "Stable",\\n230.             [\\n231.                 ("id", models.AutoField(primary_key=True)),\\n232.                 ("ponies", models.ManyToManyField("Pony", related_name="stables")),\\n233.             ],\\n234.         )\\n235.         # Test the state alteration\\n236.         new_state = project_state.clone()\\n237.         operation.state_forwards("test_crmomm", new_state)\\n238.         # Test the database alteration\\n239.         self.assertTableNotExists("test_crmomm_stable_ponies")\\n240.         with connection.schema_editor() as editor:\\n241.             operation.database_forwards("test_crmomm", editor, project_state, new_state)\\n242.         self.assertTableExists("test_crmomm_stable")\\n243.         self.assertTableExists("test_crmomm_stable_ponies")\\n244.         self.assertColumnNotExists("test_crmomm_stable", "ponies")\\n245.         # Make sure the M2M field actually works\\n246.         with atomic():\\n247.             Pony = new_state.apps.get_model("test_crmomm", "Pony")\\n248.             Stable = new_state.apps.get_model("test_crmomm", "Stable")\\n249.             stable = Stable.objects.create()\\n250.             p1 = Pony.objects.create(pink=False, weight=4.55)\\n251.             p2 = Pony.objects.create(pink=True, weight=5.43)\\n252.             stable.ponies.add(p1, p2)\\n253.             self.assertEqual(stable.ponies.count(), 2)\\n254.             stable.ponies.all().delete()\\n255.         # And test reversal\\n256.         with connection.schema_editor() as editor:\\n257.             operation.database_backwards(\\n258.                 "test_crmomm", editor, new_state, project_state\\n259.             )\\n260.         self.assertTableNotExists("test_crmomm_stable")\\n261.         self.assertTableNotExists("test_crmomm_stable_ponies")\\n262. \\n263.     def test_create_model_inheritance(self):\\n264.         """\\n265.         Tests the CreateModel operation on a multi-table inheritance setup.\\n266.         """\\n267.         project_state = self.set_up_test_model("test_crmoih")\\n268.         # Test the state alteration\\n269.         operation = migrations.CreateModel(\\n270.             "ShetlandPony",\\n271.             [\\n272.                 (\\n273.                     "pony_ptr",\\n274.                     models.OneToOneField(\\n275.                         "test_crmoih.Pony",\\n276.                         models.CASCADE,\\n277.                         auto_created=True,\\n278.                         primary_key=True,\\n279.                         to_field="id",\\n280.                         serialize=False,\\n281.                     ),\\n282.                 ),\\n283.                 ("cuteness", models.IntegerField(default=1)),\\n284.             ],\\n285.         )\\n286.         new_state = project_state.clone()\\n287.         operation.state_forwards("test_crmoih", new_state)\\n288.         self.assertIn(("test_crmoih", "shetlandpony"), new_state.models)\\n289.         # Test the database alteration\\n290.         self.assertTableNotExists("test_crmoih_shetlandpony")\\n291.         with connection.schema_editor() as editor:\\n292.             operation.database_forwards("test_crmoih", editor, project_state, new_state)\\n293.         self.assertTableExists("test_crmoih_shetlandpony")\\n294.         # And test reversal\\n295.         with connection.schema_editor() as editor:\\n296.             operation.database_backwards(\\n297.                 "test_crmoih", editor, new_state, project_state\\n298.             )\\n299.         self.assertTableNotExists("test_crmoih_shetlandpony")\\n300. \\n301.     def test_create_proxy_model(self):\\n302.         """\\n303.         CreateModel ignores proxy models.\\n304.         """\\n305.         project_state = self.set_up_test_model("test_crprmo")\\n306.         # Test the state alteration\\n307.         operation = migrations.CreateModel(\\n308.             "ProxyPony",\\n309.             [],\\n310.             options={"proxy": True},\\n311.             bases=("test_crprmo.Pony",),\\n312.         )\\n313.         self.assertEqual(operation.describe(), "Create proxy model ProxyPony")\\n314.         new_state = project_state.clone()\\n315.         operation.state_forwards("test_crprmo", new_state)\\n316.         self.assertIn(("test_crprmo", "proxypony"), new_state.models)\\n317.         # Test the database alteration\\n318.         self.assertTableNotExists("test_crprmo_proxypony")\\n319.         self.assertTableExists("test_crprmo_pony")\\n320.         with connection.schema_editor() as editor:\\n321.             operation.database_forwards("test_crprmo", editor, project_state, new_state)\\n322.         self.assertTableNotExists("test_crprmo_proxypony")\\n323.         self.assertTableExists("test_crprmo_pony")\\n324.         # And test reversal\\n325.         with connection.schema_editor() as editor:\\n326.             operation.database_backwards(\\n327.                 "test_crprmo", editor, new_state, project_state\\n328.             )\\n329.         self.assertTableNotExists("test_crprmo_proxypony")\\n330.         self.assertTableExists("test_crprmo_pony")\\n331.         # And deconstruction\\n332.         definition = operation.deconstruct()\\n333.         self.assertEqual(definition[0], "CreateModel")\\n334.         self.assertEqual(definition[1], [])\\n335.         self.assertEqual(sorted(definition[2]), ["bases", "fields", "name", "options"])\\n336. \\n337.     def test_create_unmanaged_model(self):\\n338.         """\\n339.         CreateModel ignores unmanaged models.\\n340.         """\\n341.         project_state = self.set_up_test_model("test_crummo")\\n342.         # Test the state alteration\\n343.         operation = migrations.CreateModel(\\n344.             "UnmanagedPony",\\n345.             [],\\n346.             options={"proxy": True},\\n347.             bases=("test_crummo.Pony",),\\n348.         )\\n349.         self.assertEqual(operation.describe(), "Create proxy model UnmanagedPony")\\n350.         new_state = project_state.clone()\\n351.         operation.state_forwards("test_crummo", new_state)\\n352.         self.assertIn(("test_crummo", "unmanagedpony"), new_state.models)\\n353.         # Test the database alteration\\n354.         self.assertTableNotExists("test_crummo_unmanagedpony")\\n355.         self.assertTableExists("test_crummo_pony")\\n356.         with connection.schema_editor() as editor:\\n357.             operation.database_forwards("test_crummo", editor, project_state, new_state)\\n358.         self.assertTableNotExists("test_crummo_unmanagedpony")\\n359.         self.assertTableExists("test_crummo_pony")\\n360.         # And test reversal\\n361.         with connection.schema_editor() as editor:\\n362.             operation.database_backwards(\\n363.                 "test_crummo", editor, new_state, project_state\\n364.             )\\n365.         self.assertTableNotExists("test_crummo_unmanagedpony")\\n366.         self.assertTableExists("test_crummo_pony")\\n367. \\n368.     @skipUnlessDBFeature("supports_table_check_constraints")\\n369.     def test_create_model_with_constraint(self):\\n370.         where = models.Q(pink__gt=2)\\n371.         check_constraint = models.CheckConstraint(\\n372.             check=where, name="test_constraint_pony_pink_gt_2"\\n373.         )\\n374.         operation = migrations.CreateModel(\\n375.             "Pony",\\n376.             [\\n377.                 ("id", models.AutoField(primary_key=True)),\\n378.                 ("pink", models.IntegerField(default=3)),\\n379.             ],\\n380.             options={"constraints": [check_constraint]},\\n381.         )\\n382. \\n383.         # Test the state alteration\\n384.         project_state = ProjectState()\\n385.         new_state = project_state.clone()\\n386.         operation.state_forwards("test_crmo", new_state)\\n387.         self.assertEqual(\\n388.             len(new_state.models["test_crmo", "pony"].options["constraints"]), 1\\n389.         )\\n390. \\n391.         # Test database alteration\\n392.         self.assertTableNotExists("test_crmo_pony")\\n393.         with connection.schema_editor() as editor:\\n394.             operation.database_forwards("test_crmo", editor, project_state, new_state)\\n395.         self.assertTableExists("test_crmo_pony")\\n396.         with connection.cursor() as cursor:\\n397.             with self.assertRaises(IntegrityError):\\n398.                 cursor.execute("INSERT INTO test_crmo_pony (id, pink) VALUES (1, 1)")\\n399. \\n400.         # Test reversal\\n401.         with connection.schema_editor() as editor:\\n402.             operation.database_backwards("test_crmo", editor, new_state, project_state)\\n403.         self.assertTableNotExists("test_crmo_pony")\\n404. \\n405.         # Test deconstruction\\n406.         definition = operation.deconstruct()\\n407.         self.assertEqual(definition[0], "CreateModel")\\n408.         self.assertEqual(definition[1], [])\\n409.         self.assertEqual(definition[2]["options"]["constraints"], [check_constraint])\\n410. \\n411.     def test_create_model_with_partial_unique_constraint(self):\\n412.         partial_unique_constraint = models.UniqueConstraint(\\n413.             fields=["pink"],\\n414.             condition=models.Q(weight__gt=5),\\n415.             name="test_constraint_pony_pink_for_weight_gt_5_uniq",\\n416.         )\\n417.         operation = migrations.CreateModel(\\n418.             "Pony",\\n419.             [\\n420.                 ("id", models.AutoField(primary_key=True)),\\n421.                 ("pink", models.IntegerField(default=3)),\\n422.                 ("weight", models.FloatField()),\\n423.             ],\\n424.             options={"constraints": [partial_unique_constraint]},\\n425.         )\\n426.         # Test the state alteration\\n427.         project_state = ProjectState()\\n428.         new_state = project_state.clone()\\n429.         operation.state_forwards("test_crmo", new_state)\\n430.         self.assertEqual(\\n431.             len(new_state.models["test_crmo", "pony"].options["constraints"]), 1\\n432.         )\\n433.         # Test database alteration\\n434.         self.assertTableNotExists("test_crmo_pony")\\n435.         with connection.schema_editor() as editor:\\n436.             operation.database_forwards("test_crmo", editor, project_state, new_state)\\n437.         self.assertTableExists("test_crmo_pony")\\n438.         # Test constraint works\\n439.         Pony = new_state.apps.get_model("test_crmo", "Pony")\\n440.         Pony.objects.create(pink=1, weight=4.0)\\n441.         Pony.objects.create(pink=1, weight=4.0)\\n442.         Pony.objects.create(pink=1, weight=6.0)\\n443.         if connection.features.supports_partial_indexes:\\n444.             with self.assertRaises(IntegrityError):\\n445.                 Pony.objects.create(pink=1, weight=7.0)\\n446.         else:\\n447.             Pony.objects.create(pink=1, weight=7.0)\\n448.         # Test reversal\\n449.         with connection.schema_editor() as editor:\\n450.             operation.database_backwards("test_crmo", editor, new_state, project_state)\\n451.         self.assertTableNotExists("test_crmo_pony")\\n452.         # Test deconstruction\\n453.         definition = operation.deconstruct()\\n454.         self.assertEqual(definition[0], "CreateModel")\\n455.         self.assertEqual(definition[1], [])\\n456.         self.assertEqual(\\n457.             definition[2]["options"]["constraints"], [partial_unique_constraint]\\n458.         )\\n459. \\n460.     def test_create_model_with_deferred_unique_constraint(self):\\n461.         deferred_unique_constraint = models.UniqueConstraint(\\n462.             fields=["pink"],\\n463.             name="deferrable_pink_constraint",\\n464.             deferrable=models.Deferrable.DEFERRED,\\n465.         )\\n466.         operation = migrations.CreateModel(\\n467.             "Pony",\\n468.             [\\n469.                 ("id", models.AutoField(primary_key=True)),\\n470.                 ("pink", models.IntegerField(default=3)),\\n471.             ],\\n472.             options={"constraints": [deferred_unique_constraint]},\\n473.         )\\n474.         project_state = ProjectState()\\n475.         new_state = project_state.clone()\\n476.         operation.state_forwards("test_crmo", new_state)\\n477.         self.assertEqual(\\n478.             len(new_state.models["test_crmo", "pony"].options["constraints"]), 1\\n479.         )\\n480.         self.assertTableNotExists("test_crmo_pony")\\n481.         # Create table.\\n482.         with connection.schema_editor() as editor:\\n483.             operation.database_forwards("test_crmo", editor, project_state, new_state)\\n484.         self.assertTableExists("test_crmo_pony")\\n485.         Pony = new_state.apps.get_model("test_crmo", "Pony")\\n486.         Pony.objects.create(pink=1)\\n487.         if connection.features.supports_deferrable_unique_constraints:\\n488.             # Unique constraint is deferred.\\n489.             with transaction.atomic():\\n490.                 obj = Pony.objects.create(pink=1)\\n491.                 obj.pink = 2\\n492.                 obj.save()\\n493.             # Constraint behavior can be changed with SET CONSTRAINTS.\\n494.             with self.assertRaises(IntegrityError):\\n495.                 with transaction.atomic(), connection.cursor() as cursor:\\n496.                     quoted_name = connection.ops.quote_name(\\n497.                         deferred_unique_constraint.name\\n498.                     )\\n499.                     cursor.execute("SET CONSTRAINTS %s IMMEDIATE" % quoted_name)\\n500.                     obj = Pony.objects.create(pink=1)\\n501.                     obj.pink = 3\\n502.                     obj.save()\\n503.         else:\\n504.             Pony.objects.create(pink=1)\\n505.         # Reversal.\\n506.         with connection.schema_editor() as editor:\\n507.             operation.database_backwards("test_crmo", editor, new_state, project_state)\\n508.         self.assertTableNotExists("test_crmo_pony")\\n509.         # Deconstruction.\\n510.         definition = operation.deconstruct()\\n511.         self.assertEqual(definition[0], "CreateModel")\\n512.         self.assertEqual(definition[1], [])\\n513.         self.assertEqual(\\n514.             definition[2]["options"]["constraints"],\\n515.             [deferred_unique_constraint],\\n516.         )\\n517. \\n518.     @skipUnlessDBFeature("supports_covering_indexes")\\n519.     def test_create_model_with_covering_unique_constraint(self):\\n520.         covering_unique_constraint = models.UniqueConstraint(\\n521.             fields=["pink"],\\n522.             include=["weight"],\\n523.             name="test_constraint_pony_pink_covering_weight",\\n524.         )\\n525.         operation = migrations.CreateModel(\\n526.             "Pony",\\n527.             [\\n528.                 ("id", models.AutoField(primary_key=True)),\\n529.                 ("pink", models.IntegerField(default=3)),\\n530.                 ("weight", models.FloatField()),\\n531.             ],\\n532.             options={"constraints": [covering_unique_constraint]},\\n533.         )\\n534.         project_state = ProjectState()\\n535.         new_state = project_state.clone()\\n536.         operation.state_forwards("test_crmo", new_state)\\n537.         self.assertEqual(\\n538.             len(new_state.models["test_crmo", "pony"].options["constraints"]), 1\\n539.         )\\n540.         self.assertTableNotExists("test_crmo_pony")\\n541.         # Create table.\\n542.         with connection.schema_editor() as editor:\\n543.             operation.database_forwards("test_crmo", editor, project_state, new_state)\\n544.         self.assertTableExists("test_crmo_pony")\\n545.         Pony = new_state.apps.get_model("test_crmo", "Pony")\\n546.         Pony.objects.create(pink=1, weight=4.0)\\n547.         with self.assertRaises(IntegrityError):\\n548.             Pony.objects.create(pink=1, weight=7.0)\\n549.         # Reversal.\\n550.         with connection.schema_editor() as editor:\\n551.             operation.database_backwards("test_crmo", editor, new_state, project_state)\\n552.         self.assertTableNotExists("test_crmo_pony")\\n553.         # Deconstruction.\\n554.         definition = operation.deconstruct()\\n555.         self.assertEqual(definition[0], "CreateModel")\\n556.         self.assertEqual(definition[1], [])\\n557.         self.assertEqual(\\n558.             definition[2]["options"]["constraints"],\\n559.             [covering_unique_constraint],\\n560.         )\\n561. \\n562.     def test_create_model_managers(self):\\n563.         """\\n564.         The managers on a model are set.\\n565.         """\\n566.         project_state = self.set_up_test_model("test_cmoma")\\n567.         # Test the state alteration\\n568.         operation = migrations.CreateModel(\\n569.             "Food",\\n570.             fields=[\\n571.                 ("id", models.AutoField(primary_key=True)),\\n572.             ],\\n573.             managers=[\\n574.                 ("food_qs", FoodQuerySet.as_manager()),\\n575.                 ("food_mgr", FoodManager("a", "b")),\\n576.                 ("food_mgr_kwargs", FoodManager("x", "y", 3, 4)),\\n577.             ],\\n578.         )\\n579.         self.assertEqual(operation.describe(), "Create model Food")\\n580.         new_state = project_state.clone()\\n581.         operation.state_forwards("test_cmoma", new_state)\\n582.         self.assertIn(("test_cmoma", "food"), new_state.models)\\n583.         managers = new_state.models["test_cmoma", "food"].managers\\n584.         self.assertEqual(managers[0][0], "food_qs")\\n585.         self.assertIsInstance(managers[0][1], models.Manager)\\n586.         self.assertEqual(managers[1][0], "food_mgr")\\n587.         self.assertIsInstance(managers[1][1], FoodManager)\\n588.         self.assertEqual(managers[1][1].args, ("a", "b", 1, 2))\\n589.         self.assertEqual(managers[2][0], "food_mgr_kwargs")\\n590.         self.assertIsInstance(managers[2][1], FoodManager)\\n591.         self.assertEqual(managers[2][1].args, ("x", "y", 3, 4))\\n592. \\n593.     def test_delete_model(self):\\n594.         """\\n595.         Tests the DeleteModel operation.\\n596.         """\\n597.         project_state = self.set_up_test_model("test_dlmo")\\n598.         # Test the state alteration\\n599.         operation = migrations.DeleteModel("Pony")\\n600.         self.assertEqual(operation.describe(), "Delete model Pony")\\n601.         self.assertEqual(operation.migration_name_fragment, "delete_pony")\\n602.         new_state = project_state.clone()\\n603.         operation.state_forwards("test_dlmo", new_state)\\n604.         self.assertNotIn(("test_dlmo", "pony"), new_state.models)\\n605.         # Test the database alteration\\n606.         self.assertTableExists("test_dlmo_pony")\\n607.         with connection.schema_editor() as editor:\\n608.             operation.database_forwards("test_dlmo", editor, project_state, new_state)\\n609.         self.assertTableNotExists("test_dlmo_pony")\\n610.         # And test reversal\\n611.         with connection.schema_editor() as editor:\\n612.             operation.database_backwards("test_dlmo", editor, new_state, project_state)\\n613.         self.assertTableExists("test_dlmo_pony")\\n614.         # And deconstruction\\n615.         definition = operation.deconstruct()\\n616.         self.assertEqual(definition[0], "DeleteModel")\\n617.         self.assertEqual(definition[1], [])\\n618.         self.assertEqual(list(definition[2]), ["name"])\\n619. \\n620.     def test_delete_proxy_model(self):\\n621.         """\\n622.         Tests the DeleteModel operation ignores proxy models.\\n623.         """\\n624.         project_state = self.set_up_test_model("test_dlprmo", proxy_model=True)\\n625.         # Test the state alteration\\n626.         operation = migrations.DeleteModel("ProxyPony")\\n627.         new_state = project_state.clone()\\n628.         operation.state_forwards("test_dlprmo", new_state)\\n629.         self.assertIn(("test_dlprmo", "proxypony"), project_state.models)\\n630.         self.assertNotIn(("test_dlprmo", "proxypony"), new_state.models)\\n631.         # Test the database alteration\\n632.         self.assertTableExists("test_dlprmo_pony")\\n633.         self.assertTableNotExists("test_dlprmo_proxypony")\\n634.         with connection.schema_editor() as editor:\\n635.             operation.database_forwards("test_dlprmo", editor, project_state, new_state)\\n636.         self.assertTableExists("test_dlprmo_pony")\\n637.         self.assertTableNotExists("test_dlprmo_proxypony")\\n638.         # And test reversal\\n639.         with connection.schema_editor() as editor:\\n640.             operation.database_backwards(\\n641.                 "test_dlprmo", editor, new_state, project_state\\n642.             )\\n643.         self.assertTableExists("test_dlprmo_pony")\\n644.         self.assertTableNotExists("test_dlprmo_proxypony")\\n645. \\n646.     def test_delete_mti_model(self):\\n647.         project_state = self.set_up_test_model("test_dlmtimo", mti_model=True)\\n648.         # Test the state alteration\\n649.         operation = migrations.DeleteModel("ShetlandPony")\\n650.         new_state = project_state.clone()\\n651.         operation.state_forwards("test_dlmtimo", new_state)\\n652.         self.assertIn(("test_dlmtimo", "shetlandpony"), project_state.models)\\n653.         self.assertNotIn(("test_dlmtimo", "shetlandpony"), new_state.models)\\n654.         # Test the database alteration\\n655.         self.assertTableExists("test_dlmtimo_pony")\\n656.         self.assertTableExists("test_dlmtimo_shetlandpony")\\n657.         self.assertColumnExists("test_dlmtimo_shetlandpony", "pony_ptr_id")\\n658.         with connection.schema_editor() as editor:\\n659.             operation.database_forwards(\\n660.                 "test_dlmtimo", editor, project_state, new_state\\n661.             )\\n662.         self.assertTableExists("test_dlmtimo_pony")\\n663.         self.assertTableNotExists("test_dlmtimo_shetlandpony")\\n664.         # And test reversal\\n665.         with connection.schema_editor() as editor:\\n666.             operation.database_backwards(\\n667.                 "test_dlmtimo", editor, new_state, project_state\\n668.             )\\n669.         self.assertTableExists("test_dlmtimo_pony")\\n670.         self.assertTableExists("test_dlmtimo_shetlandpony")\\n671.         self.assertColumnExists("test_dlmtimo_shetlandpony", "pony_ptr_id")\\n672. \\n673.     def test_rename_model(self):\\n674.         """\\n675.         Tests the RenameModel operation.\\n676.         """\\n677.         project_state = self.set_up_test_model("test_rnmo", related_model=True)\\n678.         # Test the state alteration\\n679.         operation = migrations.RenameModel("Pony", "Horse")\\n680.         self.assertEqual(operation.describe(), "Rename model Pony to Horse")\\n681.         self.assertEqual(operation.migration_name_fragment, "rename_pony_horse")\\n682.         # Test initial state and database\\n683.         self.assertIn(("test_rnmo", "pony"), project_state.models)\\n684.         self.assertNotIn(("test_rnmo", "horse"), project_state.models)\\n685.         self.assertTableExists("test_rnmo_pony")\\n686.         self.assertTableNotExists("test_rnmo_horse")\\n687.         if connection.features.supports_foreign_keys:\\n688.             self.assertFKExists(\\n689.                 "test_rnmo_rider", ["pony_id"], ("test_rnmo_pony", "id")\\n690.             )\\n691.             self.assertFKNotExists(\\n692.                 "test_rnmo_rider", ["pony_id"], ("test_rnmo_horse", "id")\\n693.             )\\n694.         # Migrate forwards\\n695.         new_state = project_state.clone()\\n696.         atomic_rename = connection.features.supports_atomic_references_rename\\n697.         new_state = self.apply_operations(\\n698.             "test_rnmo", new_state, [operation], atomic=atomic_rename\\n699.         )\\n700.         # Test new state and database\\n701.         self.assertNotIn(("test_rnmo", "pony"), new_state.models)\\n702.         self.assertIn(("test_rnmo", "horse"), new_state.models)\\n703.         # RenameModel also repoints all incoming FKs and M2Ms\\n704.         self.assertEqual(\\n705.             new_state.models["test_rnmo", "rider"].fields["pony"].remote_field.model,\\n706.             "test_rnmo.Horse",\\n707.         )\\n708.         self.assertTableNotExists("test_rnmo_pony")\\n709.         self.assertTableExists("test_rnmo_horse")\\n710.         if connection.features.supports_foreign_keys:\\n711.             self.assertFKNotExists(\\n712.                 "test_rnmo_rider", ["pony_id"], ("test_rnmo_pony", "id")\\n713.             )\\n714.             self.assertFKExists(\\n715.                 "test_rnmo_rider", ["pony_id"], ("test_rnmo_horse", "id")\\n716.             )\\n717.         # Migrate backwards\\n718.         original_state = self.unapply_operations(\\n719.             "test_rnmo", project_state, [operation], atomic=atomic_rename\\n720.         )\\n721.         # Test original state and database\\n722.         self.assertIn(("test_rnmo", "pony"), original_state.models)\\n723.         self.assertNotIn(("test_rnmo", "horse"), original_state.models)\\n724.         self.assertEqual(\\n725.             original_state.models["test_rnmo", "rider"]\\n726.             .fields["pony"]\\n727.             .remote_field.model,\\n728.             "Pony",\\n729.         )\\n730.         self.assertTableExists("test_rnmo_pony")\\n731.         self.assertTableNotExists("test_rnmo_horse")\\n732.         if connection.features.supports_foreign_keys:\\n733.             self.assertFKExists(\\n734.                 "test_rnmo_rider", ["pony_id"], ("test_rnmo_pony", "id")\\n735.             )\\n736.             self.assertFKNotExists(\\n737.                 "test_rnmo_rider", ["pony_id"], ("test_rnmo_horse", "id")\\n738.             )\\n739.         # And deconstruction\\n740.         definition = operation.deconstruct()\\n741.         self.assertEqual(definition[0], "RenameModel")\\n742.         self.assertEqual(definition[1], [])\\n743.         self.assertEqual(definition[2], {"old_name": "Pony", "new_name": "Horse"})\\n744. \\n745.     def test_rename_model_state_forwards(self):\\n746.         """\\n747.         RenameModel operations shouldn\\\'t trigger the caching of rendered apps\\n748.         on state without prior apps.\\n749.         """\\n750.         state = ProjectState()\\n751.         state.add_model(ModelState("migrations", "Foo", []))\\n752.         operation = migrations.RenameModel("Foo", "Bar")\\n753.         operation.state_forwards("migrations", state)\\n754.         self.assertNotIn("apps", state.__dict__)\\n755.         self.assertNotIn(("migrations", "foo"), state.models)\\n756.         self.assertIn(("migrations", "bar"), state.models)\\n757.         # Now with apps cached.\\n758.         apps = state.apps\\n759.         operation = migrations.RenameModel("Bar", "Foo")\\n760.         operation.state_forwards("migrations", state)\\n761.         self.assertIs(state.apps, apps)\\n762.         self.assertNotIn(("migrations", "bar"), state.models)\\n763.         self.assertIn(("migrations", "foo"), state.models)\\n764. \\n765.     def test_rename_model_with_self_referential_fk(self):\\n766.         """\\n767.         Tests the RenameModel operation on model with self referential FK.\\n768.         """\\n769.         project_state = self.set_up_test_model("test_rmwsrf", related_model=True)\\n770.         # Test the state alteration\\n771.         operation = migrations.RenameModel("Rider", "HorseRider")\\n772.         self.assertEqual(operation.describe(), "Rename model Rider to HorseRider")\\n773.         new_state = project_state.clone()\\n774.         operation.state_forwards("test_rmwsrf", new_state)\\n775.         self.assertNotIn(("test_rmwsrf", "rider"), new_state.models)\\n776.         self.assertIn(("test_rmwsrf", "horserider"), new_state.models)\\n777.         # Remember, RenameModel also repoints all incoming FKs and M2Ms\\n778.         self.assertEqual(\\n779.             "self",\\n780.             new_state.models["test_rmwsrf", "horserider"]\\n781.             .fields["friend"]\\n782.             .remote_field.model,\\n783.         )\\n784.         HorseRider = new_state.apps.get_model("test_rmwsrf", "horserider")\\n785.         self.assertIs(\\n786.             HorseRider._meta.get_field("horserider").remote_field.model, HorseRider\\n787.         )\\n788.         # Test the database alteration\\n789.         self.assertTableExists("test_rmwsrf_rider")\\n790.         self.assertTableNotExists("test_rmwsrf_horserider")\\n791.         if connection.features.supports_foreign_keys:\\n792.             self.assertFKExists(\\n793.                 "test_rmwsrf_rider", ["friend_id"], ("test_rmwsrf_rider", "id")\\n794.             )\\n795.             self.assertFKNotExists(\\n796.                 "test_rmwsrf_rider", ["friend_id"], ("test_rmwsrf_horserider", "id")\\n797.             )\\n798.         atomic_rename = connection.features.supports_atomic_references_rename\\n799.         with connection.schema_editor(atomic=atomic_rename) as editor:\\n800.             operation.database_forwards("test_rmwsrf", editor, project_state, new_state)\\n801.         self.assertTableNotExists("test_rmwsrf_rider")\\n802.         self.assertTableExists("test_rmwsrf_horserider")\\n803.         if connection.features.supports_foreign_keys:\\n804.             self.assertFKNotExists(\\n805.                 "test_rmwsrf_horserider", ["friend_id"], ("test_rmwsrf_rider", "id")\\n806.             )\\n807.             self.assertFKExists(\\n808.                 "test_rmwsrf_horserider",\\n809.                 ["friend_id"],\\n810.                 ("test_rmwsrf_horserider", "id"),\\n811.             )\\n812.         # And test reversal\\n813.         with connection.schema_editor(atomic=atomic_rename) as editor:\\n814.             operation.database_backwards(\\n815.                 "test_rmwsrf", editor, new_state, project_state\\n816.             )\\n817.         self.assertTableExists("test_rmwsrf_rider")\\n818.         self.assertTableNotExists("test_rmwsrf_horserider")\\n819.         if connection.features.supports_foreign_keys:\\n820.             self.assertFKExists(\\n821.                 "test_rmwsrf_rider", ["friend_id"], ("test_rmwsrf_rider", "id")\\n822.             )\\n823.             self.assertFKNotExists(\\n824.                 "test_rmwsrf_rider", ["friend_id"], ("test_rmwsrf_horserider", "id")\\n825.             )\\n826. \\n827.     def test_rename_model_with_superclass_fk(self):\\n828.         """\\n829.         Tests the RenameModel operation on a model which has a superclass that\\n830.         has a foreign key.\\n831.         """\\n832.         project_state = self.set_up_test_model(\\n833.             "test_rmwsc", related_model=True, mti_model=True\\n834.         )\\n835.         # Test the state alteration\\n836.         operation = migrations.RenameModel("ShetlandPony", "LittleHorse")\\n837.         self.assertEqual(\\n838.             operation.describe(), "Rename model ShetlandPony to LittleHorse"\\n839.         )\\n840.         new_state = project_state.clone()\\n841.         operation.state_forwards("test_rmwsc", new_state)\\n842.         self.assertNotIn(("test_rmwsc", "shetlandpony"), new_state.models)\\n843.         self.assertIn(("test_rmwsc", "littlehorse"), new_state.models)\\n844.         # RenameModel shouldn\\\'t repoint the superclass\\\'s relations, only local ones\\n845.         self.assertEqual(\\n846.             project_state.models["test_rmwsc", "rider"]\\n847.             .fields["pony"]\\n848.             .remote_field.model,\\n849.             new_state.models["test_rmwsc", "rider"].fields["pony"].remote_field.model,\\n850.         )\\n851.         # Before running the migration we have a table for Shetland Pony, not\\n852.         # Little Horse.\\n853.         self.assertTableExists("test_rmwsc_shetlandpony")\\n854.         self.assertTableNotExists("test_rmwsc_littlehorse")\\n855.         if connection.features.supports_foreign_keys:\\n856.             # and the foreign key on rider points to pony, not shetland pony\\n857.             self.assertFKExists(\\n858.                 "test_rmwsc_rider", ["pony_id"], ("test_rmwsc_pony", "id")\\n859.             )\\n860.             self.assertFKNotExists(\\n861.                 "test_rmwsc_rider", ["pony_id"], ("test_rmwsc_shetlandpony", "id")\\n862.             )\\n863.         with connection.schema_editor(\\n864.             atomic=connection.features.supports_atomic_references_rename\\n865.         ) as editor:\\n866.             operation.database_forwards("test_rmwsc", editor, project_state, new_state)\\n867.         # Now we have a little horse table, not shetland pony\\n868.         self.assertTableNotExists("test_rmwsc_shetlandpony")\\n869.         self.assertTableExists("test_rmwsc_littlehorse")\\n870.         if connection.features.supports_foreign_keys:\\n871.             # but the Foreign keys still point at pony, not little horse\\n872.             self.assertFKExists(\\n873.                 "test_rmwsc_rider", ["pony_id"], ("test_rmwsc_pony", "id")\\n874.             )\\n875.             self.assertFKNotExists(\\n876.                 "test_rmwsc_rider", ["pony_id"], ("test_rmwsc_littlehorse", "id")\\n877.             )\\n878. \\n879.     def test_rename_model_with_self_referential_m2m(self):\\n880.         app_label = "test_rename_model_with_self_referential_m2m"\\n881. \\n882.         project_state = self.apply_operations(\\n883.             app_label,\\n884.             ProjectState(),\\n885.             operations=[\\n886.                 migrations.CreateModel(\\n887.                     "ReflexivePony",\\n888.                     fields=[\\n889.                         ("id", models.AutoField(primary_key=True)),\\n890.                         ("ponies", models.ManyToManyField("self")),\\n891.                     ],\\n892.                 ),\\n893.             ],\\n894.         )\\n895.         project_state = self.apply_operations(\\n896.             app_label,\\n897.             project_state,\\n898.             operations=[\\n899.                 migrations.RenameModel("ReflexivePony", "ReflexivePony2"),\\n900.             ],\\n901.             atomic=connection.features.supports_atomic_references_rename,\\n902.         )\\n903.         Pony = project_state.apps.get_model(app_label, "ReflexivePony2")\\n904.         pony = Pony.objects.create()\\n905.         pony.ponies.add(pony)\\n906. \\n907.     def test_rename_model_with_m2m(self):\\n908.         app_label = "test_rename_model_with_m2m"\\n909.         project_state = self.apply_operations(\\n910.             app_label,\\n911.             ProjectState(),\\n912.             operations=[\\n913.                 migrations.CreateModel(\\n914.                     "Rider",\\n915.                     fields=[\\n916.                         ("id", models.AutoField(primary_key=True)),\\n917.                     ],\\n918.                 ),\\n919.                 migrations.CreateModel(\\n920.                     "Pony",\\n921.                     fields=[\\n922.                         ("id", models.AutoField(primary_key=True)),\\n923.                         ("riders", models.ManyToManyField("Rider")),\\n924.                     ],\\n925.                 ),\\n926.             ],\\n927.         )\\n928.         Pony = project_state.apps.get_model(app_label, "Pony")\\n929.         Rider = project_state.apps.get_model(app_label, "Rider")\\n930.         pony = Pony.objects.create()\\n931.         rider = Rider.objects.create()\\n932.         pony.riders.add(rider)\\n933. \\n934.         project_state = self.apply_operations(\\n935.             app_label,\\n936.             project_state,\\n937.             operations=[\\n938.                 migrations.RenameModel("Pony", "Pony2"),\\n939.             ],\\n940.             atomic=connection.features.supports_atomic_references_rename,\\n941.         )\\n942.         Pony = project_state.apps.get_model(app_label, "Pony2")\\n943.         Rider = project_state.apps.get_model(app_label, "Rider")\\n944.         pony = Pony.objects.create()\\n945.         rider = Rider.objects.create()\\n946.         pony.riders.add(rider)\\n947.         self.assertEqual(Pony.objects.count(), 2)\\n948.         self.assertEqual(Rider.objects.count(), 2)\\n949.         self.assertEqual(\\n950.             Pony._meta.get_field("riders").remote_field.through.objects.count(), 2\\n951.         )\\n952. \\n953.     def test_rename_model_with_db_table_noop(self):\\n954.         app_label = "test_rmwdbtn"\\n955.         project_state = self.apply_operations(\\n956.             app_label,\\n957.             ProjectState(),\\n958.             operations=[\\n959.                 migrations.CreateModel(\\n960.                     "Rider",\\n961.                     fields=[\\n962.                         ("id", models.AutoField(primary_key=True)),\\n963.                     ],\\n964.                     options={"db_table": "rider"},\\n965.                 ),\\n966.                 migrations.CreateModel(\\n967.                     "Pony",\\n968.                     fields=[\\n969.                         ("id", models.AutoField(primary_key=True)),\\n970.                         (\\n971.                             "rider",\\n972.                             models.ForeignKey("%s.Rider" % app_label, models.CASCADE),\\n973.                         ),\\n974.                     ],\\n975.                 ),\\n976.             ],\\n977.         )\\n978.         new_state = project_state.clone()\\n979.         operation = migrations.RenameModel("Rider", "Runner")\\n980.         operation.state_forwards(app_label, new_state)\\n981. \\n982.         with connection.schema_editor() as editor:\\n983.             with self.assertNumQueries(0):\\n984.                 operation.database_forwards(app_label, editor, project_state, new_state)\\n985.         with connection.schema_editor() as editor:\\n986.             with self.assertNumQueries(0):\\n987.                 operation.database_backwards(\\n988.                     app_label, editor, new_state, project_state\\n989.                 )\\n990. \\n991.     def test_rename_m2m_target_model(self):\\n992.         app_label = "test_rename_m2m_target_model"\\n993.         project_state = self.apply_operations(\\n994.             app_label,\\n995.             ProjectState(),\\n996.             operations=[\\n997.                 migrations.CreateModel(\\n998.                     "Rider",\\n999.                     fields=[\\n1000.                         ("id", models.AutoField(primary_key=True)),\\n1001.                     ],\\n1002.                 ),\\n1003.                 migrations.CreateModel(\\n1004.                     "Pony",\\n1005.                     fields=[\\n1006.                         ("id", models.AutoField(primary_key=True)),\\n1007.                         ("riders", models.ManyToManyField("Rider")),\\n1008.                     ],\\n1009.                 ),\\n1010.             ],\\n1011.         )\\n1012.         Pony = project_state.apps.get_model(app_label, "Pony")\\n1013.         Rider = project_state.apps.get_model(app_label, "Rider")\\n1014.         pony = Pony.objects.create()\\n1015.         rider = Rider.objects.create()\\n1016.         pony.riders.add(rider)\\n1017. \\n1018.         project_state = self.apply_operations(\\n1019.             app_label,\\n1020.             project_state,\\n1021.             operations=[\\n1022.                 migrations.RenameModel("Rider", "Rider2"),\\n1023.             ],\\n1024.             atomic=connection.features.supports_atomic_references_rename,\\n1025.         )\\n1026.         Pony = project_state.apps.get_model(app_label, "Pony")\\n1027.         Rider = project_state.apps.get_model(app_label, "Rider2")\\n1028.         pony = Pony.objects.create()\\n1029.         rider = Rider.objects.create()\\n1030.         pony.riders.add(rider)\\n1031.         self.assertEqual(Pony.objects.count(), 2)\\n1032.         self.assertEqual(Rider.objects.count(), 2)\\n1033.         self.assertEqual(\\n1034.             Pony._meta.get_field("riders").remote_field.through.objects.count(), 2\\n1035.         )\\n1036. \\n1037.     def test_rename_m2m_through_model(self):\\n1038.         app_label = "test_rename_through"\\n1039.         project_state = self.apply_operations(\\n1040.             app_label,\\n1041.             ProjectState(),\\n1042.             operations=[\\n1043.                 migrations.CreateModel(\\n1044.                     "Rider",\\n1045.                     fields=[\\n1046.                         ("id", models.AutoField(primary_key=True)),\\n1047.                     ],\\n1048.                 ),\\n1049.                 migrations.CreateModel(\\n1050.                     "Pony",\\n1051.                     fields=[\\n1052.                         ("id", models.AutoField(primary_key=True)),\\n1053.                     ],\\n1054.                 ),\\n1055.                 migrations.CreateModel(\\n1056.                     "PonyRider",\\n1057.                     fields=[\\n1058.                         ("id", models.AutoField(primary_key=True)),\\n1059.                         (\\n1060.                             "rider",\\n1061.                             models.ForeignKey(\\n1062.                                 "test_rename_through.Rider", models.CASCADE\\n1063.                             ),\\n1064.                         ),\\n1065.                         (\\n1066.                             "pony",\\n1067.                             models.ForeignKey(\\n1068.                                 "test_rename_through.Pony", models.CASCADE\\n1069.                             ),\\n1070.                         ),\\n1071.                     ],\\n1072.                 ),\\n1073.                 migrations.AddField(\\n1074.                     "Pony",\\n1075.                     "riders",\\n1076.                     models.ManyToManyField(\\n1077.                         "test_rename_through.Rider",\\n1078.                         through="test_rename_through.PonyRider",\\n1079.                     ),\\n1080.                 ),\\n1081.             ],\\n1082.         )\\n1083.         Pony = project_state.apps.get_model(app_label, "Pony")\\n1084.         Rider = project_state.apps.get_model(app_label, "Rider")\\n1085.         PonyRider = project_state.apps.get_model(app_label, "PonyRider")\\n1086.         pony = Pony.objects.create()\\n1087.         rider = Rider.objects.create()\\n1088.         PonyRider.objects.create(pony=pony, rider=rider)\\n1089. \\n1090.         project_state = self.apply_operations(\\n1091.             app_label,\\n1092.             project_state,\\n1093.             operations=[\\n1094.                 migrations.RenameModel("PonyRider", "PonyRider2"),\\n1095.             ],\\n1096.         )\\n1097.         Pony = project_state.apps.get_model(app_label, "Pony")\\n1098.         Rider = project_state.apps.get_model(app_label, "Rider")\\n1099.         PonyRider = project_state.apps.get_model(app_label, "PonyRider2")\\n1100.         pony = Pony.objects.first()\\n1101.         rider = Rider.objects.create()\\n1102.         PonyRider.objects.create(pony=pony, rider=rider)\\n1103.         self.assertEqual(Pony.objects.count(), 1)\\n1104.         self.assertEqual(Rider.objects.count(), 2)\\n1105.         self.assertEqual(PonyRider.objects.count(), 2)\\n1106.         self.assertEqual(pony.riders.count(), 2)\\n1107. \\n1108.     def test_rename_m2m_model_after_rename_field(self):\\n1109.         """RenameModel renames a many-to-many column after a RenameField."""\\n1110.         app_label = "test_rename_multiple"\\n1111.         project_state = self.apply_operations(\\n1112.             app_label,\\n1113.             ProjectState(),\\n1114.             operations=[\\n1115.                 migrations.CreateModel(\\n1116.                     "Pony",\\n1117.                     fields=[\\n1118.                         ("id", models.AutoField(primary_key=True)),\\n1119.                         ("name", models.CharField(max_length=20)),\\n1120.                     ],\\n1121.                 ),\\n1122.                 migrations.CreateModel(\\n1123.                     "Rider",\\n1124.                     fields=[\\n1125.                         ("id", models.AutoField(primary_key=True)),\\n1126.                         (\\n1127.                             "pony",\\n1128.                             models.ForeignKey(\\n1129.                                 "test_rename_multiple.Pony", models.CASCADE\\n1130.                             ),\\n1131.                         ),\\n1132.                     ],\\n1133.                 ),\\n1134.                 migrations.CreateModel(\\n1135.                     "PonyRider",\\n1136.                     fields=[\\n1137.                         ("id", models.AutoField(primary_key=True)),\\n1138.                         ("riders", models.ManyToManyField("Rider")),\\n1139.                     ],\\n1140.                 ),\\n1141.                 migrations.RenameField(\\n1142.                     model_name="pony", old_name="name", new_name="fancy_name"\\n1143.                 ),\\n1144.                 migrations.RenameModel(old_name="Rider", new_name="Jockey"),\\n1145.             ],\\n1146.             atomic=connection.features.supports_atomic_references_rename,\\n1147.         )\\n1148.         Pony = project_state.apps.get_model(app_label, "Pony")\\n1149.         Jockey = project_state.apps.get_model(app_label, "Jockey")\\n1150.         PonyRider = project_state.apps.get_model(app_label, "PonyRider")\\n1151.         # No "no such column" error means the column was renamed correctly.\\n1152.         pony = Pony.objects.create(fancy_name="a good name")\\n1153.         jockey = Jockey.objects.create(pony=pony)\\n1154.         ponyrider = PonyRider.objects.create()\\n1155.         ponyrider.riders.add(jockey)\\n1156. \\n1157.     def test_add_field(self):\\n1158.         """\\n1159.         Tests the AddField operation.\\n1160.         """\\n1161.         # Test the state alteration\\n1162.         operation = migrations.AddField(\\n1163.             "Pony",\\n1164.             "height",\\n1165.             models.FloatField(null=True, default=5),\\n1166.         )\\n1167.         self.assertEqual(operation.describe(), "Add field height to Pony")\\n1168.         self.assertEqual(operation.migration_name_fragment, "pony_height")\\n1169.         project_state, new_state = self.make_test_state("test_adfl", operation)\\n1170.         self.assertEqual(len(new_state.models["test_adfl", "pony"].fields), 4)\\n1171.         field = new_state.models["test_adfl", "pony"].fields["height"]\\n1172.         self.assertEqual(field.default, 5)\\n1173.         # Test the database alteration\\n1174.         self.assertColumnNotExists("test_adfl_pony", "height")\\n1175.         with connection.schema_editor() as editor:\\n1176.             operation.database_forwards("test_adfl", editor, project_state, new_state)\\n1177.         self.assertColumnExists("test_adfl_pony", "height")\\n1178.         # And test reversal\\n1179.         with connection.schema_editor() as editor:\\n1180.             operation.database_backwards("test_adfl", editor, new_state, project_state)\\n1181.         self.assertColumnNotExists("test_adfl_pony", "height")\\n1182.         # And deconstruction\\n1183.         definition = operation.deconstruct()\\n1184.         self.assertEqual(definition[0], "AddField")\\n1185.         self.assertEqual(definition[1], [])\\n1186.         self.assertEqual(sorted(definition[2]), ["field", "model_name", "name"])\\n1187. \\n1188.     def test_add_charfield(self):\\n1189.         """\\n1190.         Tests the AddField operation on TextField.\\n1191.         """\\n1192.         project_state = self.set_up_test_model("test_adchfl")\\n1193. \\n1194.         Pony = project_state.apps.get_model("test_adchfl", "Pony")\\n1195.         pony = Pony.objects.create(weight=42)\\n1196. \\n1197.         new_state = self.apply_operations(\\n1198.             "test_adchfl",\\n1199.             project_state,\\n1200.             [\\n1201.                 migrations.AddField(\\n1202.                     "Pony",\\n1203.                     "text",\\n1204.                     models.CharField(max_length=10, default="some text"),\\n1205.                 ),\\n1206.                 migrations.AddField(\\n1207.                     "Pony",\\n1208.                     "empty",\\n1209.                     models.CharField(max_length=10, default=""),\\n1210.                 ),\\n1211.                 # If not properly quoted digits would be interpreted as an int.\\n1212.                 migrations.AddField(\\n1213.                     "Pony",\\n1214.                     "digits",\\n1215.                     models.CharField(max_length=10, default="42"),\\n1216.                 ),\\n1217.                 # Manual quoting is fragile and could trip on quotes. Refs #xyz.\\n1218.                 migrations.AddField(\\n1219.                     "Pony",\\n1220.                     "quotes",\\n1221.                     models.CharField(max_length=10, default=\\\'"\\\\\\\'"\\\'),\\n1222.                 ),\\n1223.             ],\\n1224.         )\\n1225. \\n1226.         Pony = new_state.apps.get_model("test_adchfl", "Pony")\\n1227.         pony = Pony.objects.get(pk=pony.pk)\\n1228.         self.assertEqual(pony.text, "some text")\\n1229.         self.assertEqual(pony.empty, "")\\n1230.         self.assertEqual(pony.digits, "42")\\n1231.         self.assertEqual(pony.quotes, \\\'"\\\\\\\'"\\\')\\n1232. \\n1233.     def test_add_textfield(self):\\n1234.         """\\n1235.         Tests the AddField operation on TextField.\\n1236.         """\\n1237.         project_state = self.set_up_test_model("test_adtxtfl")\\n1238. \\n1239.         Pony = project_state.apps.get_model("test_adtxtfl", "Pony")\\n1240.         pony = Pony.objects.create(weight=42)\\n1241. \\n1242.         new_state = self.apply_operations(\\n1243.             "test_adtxtfl",\\n1244.             project_state,\\n1245.             [\\n1246.                 migrations.AddField(\\n1247.                     "Pony",\\n1248.                     "text",\\n1249.                     models.TextField(default="some text"),\\n1250.                 ),\\n1251.                 migrations.AddField(\\n1252.                     "Pony",\\n1253.                     "empty",\\n1254.                     models.TextField(default=""),\\n1255.                 ),\\n1256.                 # If not properly quoted digits would be interpreted as an int.\\n1257.                 migrations.AddField(\\n1258.                     "Pony",\\n1259.                     "digits",\\n1260.                     models.TextField(default="42"),\\n1261.                 ),\\n1262.                 # Manual quoting is fragile and could trip on quotes. Refs #xyz.\\n1263.                 migrations.AddField(\\n1264.                     "Pony",\\n1265.                     "quotes",\\n1266.                     models.TextField(default=\\\'"\\\\\\\'"\\\'),\\n1267.                 ),\\n1268.             ],\\n1269.         )\\n1270. \\n1271.         Pony = new_state.apps.get_model("test_adtxtfl", "Pony")\\n1272.         pony = Pony.objects.get(pk=pony.pk)\\n1273.         self.assertEqual(pony.text, "some text")\\n1274.         self.assertEqual(pony.empty, "")\\n1275.         self.assertEqual(pony.digits, "42")\\n1276.         self.assertEqual(pony.quotes, \\\'"\\\\\\\'"\\\')\\n1277. \\n1278.     def test_add_binaryfield(self):\\n1279.         """\\n1280.         Tests the AddField operation on TextField/BinaryField.\\n1281.         """\\n1282.         project_state = self.set_up_test_model("test_adbinfl")\\n1283. \\n1284.         Pony = project_state.apps.get_model("test_adbinfl", "Pony")\\n1285.         pony = Pony.objects.create(weight=42)\\n1286. \\n1287.         new_state = self.apply_operations(\\n1288.             "test_adbinfl",\\n1289.             project_state,\\n1290.             [\\n1291.                 migrations.AddField(\\n1292.                     "Pony",\\n1293.                     "blob",\\n1294.                     models.BinaryField(default=b"some text"),\\n1295.                 ),\\n1296.                 migrations.AddField(\\n1297.                     "Pony",\\n1298.                     "empty",\\n1299.                     models.BinaryField(default=b""),\\n1300.                 ),\\n1301.                 # If not properly quoted digits would be interpreted as an int.\\n1302.                 migrations.AddField(\\n1303.                     "Pony",\\n1304.                     "digits",\\n1305.                     models.BinaryField(default=b"42"),\\n1306.                 ),\\n1307.                 # Manual quoting is fragile and could trip on quotes. Refs #xyz.\\n1308.                 migrations.AddField(\\n1309.                     "Pony",\\n1310.                     "quotes",\\n1311.                     models.BinaryField(default=b\\\'"\\\\\\\'"\\\'),\\n1312.                 ),\\n1313.             ],\\n1314.         )\\n1315. \\n1316.         Pony = new_state.apps.get_model("test_adbinfl", "Pony")\\n1317.         pony = Pony.objects.get(pk=pony.pk)\\n1318.         # SQLite returns buffer/memoryview, cast to bytes for checking.\\n1319.         self.assertEqual(bytes(pony.blob), b"some text")\\n1320.         self.assertEqual(bytes(pony.empty), b"")\\n1321.         self.assertEqual(bytes(pony.digits), b"42")\\n1322.         self.assertEqual(bytes(pony.quotes), b\\\'"\\\\\\\'"\\\')\\n1323. \\n1324.     def test_column_name_quoting(self):\\n1325.         """\\n1326.         Column names that are SQL keywords shouldn\\\'t cause problems when used\\n1327.         in migrations (#22168).\\n1328.         """\\n1329.         project_state = self.set_up_test_model("test_regr22168")\\n1330.         operation = migrations.AddField(\\n1331.             "Pony",\\n1332.             "order",\\n1333.             models.IntegerField(default=0),\\n1334.         )\\n1335.         new_state = project_state.clone()\\n1336.         operation.state_forwards("test_regr22168", new_state)\\n1337.         with connection.schema_editor() as editor:\\n1338.             operation.database_forwards(\\n1339.                 "test_regr22168", editor, project_state, new_state\\n1340.             )\\n1341.         self.assertColumnExists("test_regr22168_pony", "order")\\n1342. \\n1343.     def test_add_field_preserve_default(self):\\n1344.         """\\n1345.         Tests the AddField operation\\\'s state alteration\\n1346.         when preserve_default = False.\\n1347.         """\\n1348.         project_state = self.set_up_test_model("test_adflpd")\\n1349.         # Test the state alteration\\n1350.         operation = migrations.AddField(\\n1351.             "Pony",\\n1352.             "height",\\n1353.             models.FloatField(null=True, default=4),\\n1354.             preserve_default=False,\\n1355.         )\\n1356.         new_state = project_state.clone()\\n1357.         operation.state_forwards("test_adflpd", new_state)\\n1358.         self.assertEqual(len(new_state.models["test_adflpd", "pony"].fields), 4)\\n1359.         field = new_state.models["test_adflpd", "pony"].fields["height"]\\n1360.         self.assertEqual(field.default, models.NOT_PROVIDED)\\n1361.         # Test the database alteration\\n1362.         project_state.apps.get_model("test_adflpd", "pony").objects.create(\\n1363.             weight=4,\\n1364.         )\\n1365.         self.assertColumnNotExists("test_adflpd_pony", "height")\\n1366.         with connection.schema_editor() as editor:\\n1367.             operation.database_forwards("test_adflpd", editor, project_state, new_state)\\n1368.         self.assertColumnExists("test_adflpd_pony", "height")\\n1369.         # And deconstruction\\n1370.         definition = operation.deconstruct()\\n1371.         self.assertEqual(definition[0], "AddField")\\n1372.         self.assertEqual(definition[1], [])\\n1373.         self.assertEqual(\\n1374.             sorted(definition[2]), ["field", "model_name", "name", "preserve_default"]\\n1375.         )\\n1376. \\n1377.     def test_add_field_m2m(self):\\n1378.         """\\n1379.         Tests the AddField operation with a ManyToManyField.\\n1380.         """\\n1381.         project_state = self.set_up_test_model("test_adflmm", second_model=True)\\n1382.         # Test the state alteration\\n1383.         operation = migrations.AddField(\\n1384.             "Pony", "stables", models.ManyToManyField("Stable", related_name="ponies")\\n1385.         )\\n1386.         new_state = project_state.clone()\\n1387.         operation.state_forwards("test_adflmm", new_state)\\n1388.         self.assertEqual(len(new_state.models["test_adflmm", "pony"].fields), 4)\\n1389.         # Test the database alteration\\n1390.         self.assertTableNotExists("test_adflmm_pony_stables")\\n1391.         with connection.schema_editor() as editor:\\n1392.             operation.database_forwards("test_adflmm", editor, project_state, new_state)\\n1393.         self.assertTableExists("test_adflmm_pony_stables")\\n1394.         self.assertColumnNotExists("test_adflmm_pony", "stables")\\n1395.         # Make sure the M2M field actually works\\n1396.         with atomic():\\n1397.             Pony = new_state.apps.get_model("test_adflmm", "Pony")\\n1398.             p = Pony.objects.create(pink=False, weight=4.55)\\n1399.             p.stables.create()\\n1400.             self.assertEqual(p.stables.count(), 1)\\n1401.             p.stables.all().delete()\\n1402.         # And test reversal\\n1403.         with connection.schema_editor() as editor:\\n1404.             operation.database_backwards(\\n1405.                 "test_adflmm", editor, new_state, project_state\\n1406.             )\\n1407.         self.assertTableNotExists("test_adflmm_pony_stables")\\n1408. \\n1409.     def test_alter_field_m2m(self):\\n1410.         project_state = self.set_up_test_model("test_alflmm", second_model=True)\\n1411. \\n1412.         project_state = self.apply_operations(\\n1413.             "test_alflmm",\\n1414.             project_state,\\n1415.             operations=[\\n1416.                 migrations.AddField(\\n1417.                     "Pony",\\n1418.                     "stables",\\n1419.                     models.ManyToManyField("Stable", related_name="ponies"),\\n1420.                 )\\n1421.             ],\\n1422.         )\\n1423.         Pony = project_state.apps.get_model("test_alflmm", "Pony")\\n1424.         self.assertFalse(Pony._meta.get_field("stables").blank)\\n1425. \\n1426.         project_state = self.apply_operations(\\n1427.             "test_alflmm",\\n1428.             project_state,\\n1429.             operations=[\\n1430.                 migrations.AlterField(\\n1431.                     "Pony",\\n1432.                     "stables",\\n1433.                     models.ManyToManyField(\\n1434.                         to="Stable", related_name="ponies", blank=True\\n1435.                     ),\\n1436.                 )\\n1437.             ],\\n1438.         )\\n1439.         Pony = project_state.apps.get_model("test_alflmm", "Pony")\\n1440.         self.assertTrue(Pony._meta.get_field("stables").blank)\\n1441. \\n1442.     def test_repoint_field_m2m(self):\\n1443.         project_state = self.set_up_test_model(\\n1444.             "test_alflmm", second_model=True, third_model=True\\n1445.         )\\n1446. \\n1447.         project_state = self.apply_operations(\\n1448.             "test_alflmm",\\n1449.             project_state,\\n1450.             operations=[\\n1451.                 migrations.AddField(\\n1452.                     "Pony",\\n1453.                     "places",\\n1454.                     models.ManyToManyField("Stable", related_name="ponies"),\\n1455.                 )\\n1456.             ],\\n1457.         )\\n1458.         Pony = project_state.apps.get_model("test_alflmm", "Pony")\\n1459. \\n1460.         project_state = self.apply_operations(\\n1461.             "test_alflmm",\\n1462.             project_state,\\n1463.             operations=[\\n1464.                 migrations.AlterField(\\n1465.                     "Pony",\\n1466.                     "places",\\n1467.                     models.ManyToManyField(to="Van", related_name="ponies"),\\n1468.                 )\\n1469.             ],\\n1470.         )\\n1471. \\n1472.         # Ensure the new field actually works\\n1473.         Pony = project_state.apps.get_model("test_alflmm", "Pony")\\n1474.         p = Pony.objects.create(pink=False, weight=4.55)\\n1475.         p.places.create()\\n1476.         self.assertEqual(p.places.count(), 1)\\n1477.         p.places.all().delete()\\n1478. \\n1479.     def test_remove_field_m2m(self):\\n1480.         project_state = self.set_up_test_model("test_rmflmm", second_model=True)\\n1481. \\n1482.         project_state = self.apply_operations(\\n1483.             "test_rmflmm",\\n1484.             project_state,\\n1485.             operations=[\\n1486.                 migrations.AddField(\\n1487.                     "Pony",\\n1488.                     "stables",\\n1489.                     models.ManyToManyField("Stable", related_name="ponies"),\\n1490.                 )\\n1491.             ],\\n1492.         )\\n1493.         self.assertTableExists("test_rmflmm_pony_stables")\\n1494. \\n1495.         with_field_state = project_state.clone()\\n1496.         operations = [migrations.RemoveField("Pony", "stables")]\\n1497.         project_state = self.apply_operations(\\n1498.             "test_rmflmm", project_state, operations=operations\\n1499.         )\\n1500.         self.assertTableNotExists("test_rmflmm_pony_stables")\\n1501. \\n1502.         # And test reversal\\n1503.         self.unapply_operations("test_rmflmm", with_field_state, operations=operations)\\n1504.         self.assertTableExists("test_rmflmm_pony_stables")\\n1505. \\n1506.     def test_remove_field_m2m_with_through(self):\\n1507.         project_state = self.set_up_test_model("test_rmflmmwt", second_model=True)\\n1508. \\n1509.         self.assertTableNotExists("test_rmflmmwt_ponystables")\\n1510.         project_state = self.apply_operations(\\n1511.             "test_rmflmmwt",\\n1512.             project_state,\\n1513.             operations=[\\n1514.                 migrations.CreateModel(\\n1515.                     "PonyStables",\\n1516.                     fields=[\\n1517.                         (\\n1518.                             "pony",\\n1519.                             models.ForeignKey("test_rmflmmwt.Pony", models.CASCADE),\\n1520.                         ),\\n1521.                         (\\n1522.                             "stable",\\n1523.                             models.ForeignKey("test_rmflmmwt.Stable", models.CASCADE),\\n1524.                         ),\\n1525.                     ],\\n1526.                 ),\\n1527.                 migrations.AddField(\\n1528.                     "Pony",\\n1529.                     "stables",\\n1530.                     models.ManyToManyField(\\n1531.                         "Stable",\\n1532.                         related_name="ponies",\\n1533.                         through="test_rmflmmwt.PonyStables",\\n1534.                     ),\\n1535.                 ),\\n1536.             ],\\n1537.         )\\n1538.         self.assertTableExists("test_rmflmmwt_ponystables")\\n1539. \\n1540.         operations = [\\n1541.             migrations.RemoveField("Pony", "stables"),\\n1542.             migrations.DeleteModel("PonyStables"),\\n1543.         ]\\n1544.         self.apply_operations("test_rmflmmwt", project_state, operations=operations)\\n1545. \\n1546.     def test_remove_field(self):\\n1547.         """\\n1548.         Tests the RemoveField operation.\\n1549.         """\\n1550.         project_state = self.set_up_test_model("test_rmfl")\\n1551.         # Test the state alteration\\n1552.         operation = migrations.RemoveField("Pony", "pink")\\n1553.         self.assertEqual(operation.describe(), "Remove field pink from Pony")\\n1554.         self.assertEqual(operation.migration_name_fragment, "remove_pony_pink")\\n1555.         new_state = project_state.clone()\\n1556.         operation.state_forwards("test_rmfl", new_state)\\n1557.         self.assertEqual(len(new_state.models["test_rmfl", "pony"].fields), 2)\\n1558.         # Test the database alteration\\n1559.         self.assertColumnExists("test_rmfl_pony", "pink")\\n1560.         with connection.schema_editor() as editor:\\n1561.             operation.database_forwards("test_rmfl", editor, project_state, new_state)\\n1562.         self.assertColumnNotExists("test_rmfl_pony", "pink")\\n1563.         # And test reversal\\n1564.         with connection.schema_editor() as editor:\\n1565.             operation.database_backwards("test_rmfl", editor, new_state, project_state)\\n1566.         self.assertColumnExists("test_rmfl_pony", "pink")\\n1567.         # And deconstruction\\n1568.         definition = operation.deconstruct()\\n1569.         self.assertEqual(definition[0], "RemoveField")\\n1570.         self.assertEqual(definition[1], [])\\n1571.         self.assertEqual(definition[2], {"model_name": "Pony", "name": "pink"})\\n1572. \\n1573.     def test_remove_fk(self):\\n1574.         """\\n1575.         Tests the RemoveField operation on a foreign key.\\n1576.         """\\n1577.         project_state = self.set_up_test_model("test_rfk", related_model=True)\\n1578.         self.assertColumnExists("test_rfk_rider", "pony_id")\\n1579.         operation = migrations.RemoveField("Rider", "pony")\\n1580. \\n1581.         new_state = project_state.clone()\\n1582.         operation.state_forwards("test_rfk", new_state)\\n1583.         with connection.schema_editor() as editor:\\n1584.             operation.database_forwards("test_rfk", editor, project_state, new_state)\\n1585.         self.assertColumnNotExists("test_rfk_rider", "pony_id")\\n1586.         with connection.schema_editor() as editor:\\n1587.             operation.database_backwards("test_rfk", editor, new_state, project_state)\\n1588.         self.assertColumnExists("test_rfk_rider", "pony_id")\\n1589. \\n1590.     def test_alter_model_table(self):\\n1591.         """\\n1592.         Tests the AlterModelTable operation.\\n1593.         """\\n1594.         project_state = self.set_up_test_model("test_almota")\\n1595.         # Test the state alteration\\n1596.         operation = migrations.AlterModelTable("Pony", "test_almota_pony_2")\\n1597.         self.assertEqual(\\n1598.             operation.describe(), "Rename table for Pony to test_almota_pony_2"\\n1599.         )\\n1600.         self.assertEqual(operation.migration_name_fragment, "alter_pony_table")\\n1601.         new_state = project_state.clone()\\n1602.         operation.state_forwards("test_almota", new_state)\\n1603.         self.assertEqual(\\n1604.             new_state.models["test_almota", "pony"].options["db_table"],\\n1605.             "test_almota_pony_2",\\n1606.         )\\n1607.         # Test the database alteration\\n1608.         self.assertTableExists("test_almota_pony")\\n1609.         self.assertTableNotExists("test_almota_pony_2")\\n1610.         with connection.schema_editor() as editor:\\n1611.             operation.database_forwards("test_almota", editor, project_state, new_state)\\n1612.         self.assertTableNotExists("test_almota_pony")\\n1613.         self.assertTableExists("test_almota_pony_2")\\n1614.         # And test reversal\\n1615.         with connection.schema_editor() as editor:\\n1616.             operation.database_backwards(\\n1617.                 "test_almota", editor, new_state, project_state\\n1618.             )\\n1619.         self.assertTableExists("test_almota_pony")\\n1620.         self.assertTableNotExists("test_almota_pony_2")\\n1621.         # And deconstruction\\n1622.         definition = operation.deconstruct()\\n1623.         self.assertEqual(definition[0], "AlterModelTable")\\n1624.         self.assertEqual(definition[1], [])\\n1625.         self.assertEqual(definition[2], {"name": "Pony", "table": "test_almota_pony_2"})\\n1626. \\n1627.     def test_alter_model_table_none(self):\\n1628.         """\\n1629.         Tests the AlterModelTable operation if the table name is set to None.\\n1630.         """\\n1631.         operation = migrations.AlterModelTable("Pony", None)\\n1632.         self.assertEqual(operation.describe(), "Rename table for Pony to (default)")\\n1633. \\n1634.     def test_alter_model_table_noop(self):\\n1635.         """\\n1636.         Tests the AlterModelTable operation if the table name is not changed.\\n1637.         """\\n1638.         project_state = self.set_up_test_model("test_almota")\\n1639.         # Test the state alteration\\n1640.         operation = migrations.AlterModelTable("Pony", "test_almota_pony")\\n1641.         new_state = project_state.clone()\\n1642.         operation.state_forwards("test_almota", new_state)\\n1643.         self.assertEqual(\\n1644.             new_state.models["test_almota", "pony"].options["db_table"],\\n1645.             "test_almota_pony",\\n1646.         )\\n1647.         # Test the database alteration\\n1648.         self.assertTableExists("test_almota_pony")\\n1649.         with connection.schema_editor() as editor:\\n1650.             operation.database_forwards("test_almota", editor, project_state, new_state)\\n1651.         self.assertTableExists("test_almota_pony")\\n1652.         # And test reversal\\n1653.         with connection.schema_editor() as editor:\\n1654.             operation.database_backwards(\\n1655.                 "test_almota", editor, new_state, project_state\\n1656.             )\\n1657.         self.assertTableExists("test_almota_pony")\\n1658. \\n1659.     def test_alter_model_table_m2m(self):\\n1660.         """\\n1661.         AlterModelTable should rename auto-generated M2M tables.\\n1662.         """\\n1663.         app_label = "test_talflmltlm2m"\\n1664.         pony_db_table = "pony_foo"\\n1665.         project_state = self.set_up_test_model(\\n1666.             app_label, second_model=True, db_table=pony_db_table\\n1667.         )\\n1668.         # Add the M2M field\\n1669.         first_state = project_state.clone()\\n1670.         operation = migrations.AddField(\\n1671.             "Pony", "stables", models.ManyToManyField("Stable")\\n1672.         )\\n1673.         operation.state_forwards(app_label, first_state)\\n1674.         with connection.schema_editor() as editor:\\n1675.             operation.database_forwards(app_label, editor, project_state, first_state)\\n1676.         original_m2m_table = "%s_%s" % (pony_db_table, "stables")\\n1677.         new_m2m_table = "%s_%s" % (app_label, "pony_stables")\\n1678.         self.assertTableExists(original_m2m_table)\\n1679.         self.assertTableNotExists(new_m2m_table)\\n1680.         # Rename the Pony db_table which should also rename the m2m table.\\n1681.         second_state = first_state.clone()\\n1682.         operation = migrations.AlterModelTable(name="pony", table=None)\\n1683.         operation.state_forwards(app_label, second_state)\\n1684.         atomic_rename = connection.features.supports_atomic_references_rename\\n1685.         with connection.schema_editor(atomic=atomic_rename) as editor:\\n1686.             operation.database_forwards(app_label, editor, first_state, second_state)\\n1687.         self.assertTableExists(new_m2m_table)\\n1688.         self.assertTableNotExists(original_m2m_table)\\n1689.         # And test reversal\\n1690.         with connection.schema_editor(atomic=atomic_rename) as editor:\\n1691.             operation.database_backwards(app_label, editor, second_state, first_state)\\n1692.         self.assertTableExists(original_m2m_table)\\n1693.         self.assertTableNotExists(new_m2m_table)\\n1694. \\n1695.     def test_alter_field(self):\\n1696.         """\\n1697.         Tests the AlterField operation.\\n1698.         """\\n1699.         project_state = self.set_up_test_model("test_alfl")\\n1700.         # Test the state alteration\\n1701.         operation = migrations.AlterField(\\n1702.             "Pony", "pink", models.IntegerField(null=True)\\n1703.         )\\n1704.         self.assertEqual(operation.describe(), "Alter field pink on Pony")\\n1705.         self.assertEqual(operation.migration_name_fragment, "alter_pony_pink")\\n1706.         new_state = project_state.clone()\\n1707.         operation.state_forwards("test_alfl", new_state)\\n1708.         self.assertIs(\\n1709.             project_state.models["test_alfl", "pony"].fields["pink"].null, False\\n1710.         )\\n1711.         self.assertIs(new_state.models["test_alfl", "pony"].fields["pink"].null, True)\\n1712.         # Test the database alteration\\n1713.         self.assertColumnNotNull("test_alfl_pony", "pink")\\n1714.         with connection.schema_editor() as editor:\\n1715.             operation.database_forwards("test_alfl", editor, project_state, new_state)\\n1716.         self.assertColumnNull("test_alfl_pony", "pink")\\n1717.         # And test reversal\\n1718.         with connection.schema_editor() as editor:\\n1719.             operation.database_backwards("test_alfl", editor, new_state, project_state)\\n1720.         self.assertColumnNotNull("test_alfl_pony", "pink")\\n1721.         # And deconstruction\\n1722.         definition = operation.deconstruct()\\n1723.         self.assertEqual(definition[0], "AlterField")\\n1724.         self.assertEqual(definition[1], [])\\n1725.         self.assertEqual(sorted(definition[2]), ["field", "model_name", "name"])\\n1726. \\n1727.     def test_alter_field_add_db_column_noop(self):\\n1728.         """\\n1729.         AlterField operation is a noop when adding only a db_column and the\\n1730.         column name is not changed.\\n1731.         """\\n1732.         app_label = "test_afadbn"\\n1733.         project_state = self.set_up_test_model(app_label, related_model=True)\\n1734.         pony_table = "%s_pony" % app_label\\n1735.         new_state = project_state.clone()\\n1736.         operation = migrations.AlterField(\\n1737.             "Pony", "weight", models.FloatField(db_column="weight")\\n1738.         )\\n1739.         operation.state_forwards(app_label, new_state)\\n1740.         self.assertIsNone(\\n1741.             project_state.models[app_label, "pony"].fields["weight"].db_column,\\n1742.         )\\n1743.         self.assertEqual(\\n1744.             new_state.models[app_label, "pony"].fields["weight"].db_column,\\n1745.             "weight",\\n1746.         )\\n1747.         self.assertColumnExists(pony_table, "weight")\\n1748.         with connection.schema_editor() as editor:\\n1749.             with self.assertNumQueries(0):\\n1750.                 operation.database_forwards(app_label, editor, project_state, new_state)\\n1751.         self.assertColumnExists(pony_table, "weight")\\n1752.         with connection.schema_editor() as editor:\\n1753.             with self.assertNumQueries(0):\\n1754.                 operation.database_backwards(\\n1755.                     app_label, editor, new_state, project_state\\n1756.                 )\\n1757.         self.assertColumnExists(pony_table, "weight")\\n1758. \\n1759.         rider_table = "%s_rider" % app_label\\n1760.         new_state = project_state.clone()\\n1761.         operation = migrations.AlterField(\\n1762.             "Rider",\\n1763.             "pony",\\n1764.             models.ForeignKey("Pony", models.CASCADE, db_column="pony_id"),\\n1765.         )\\n1766.         operation.state_forwards(app_label, new_state)\\n1767.         self.assertIsNone(\\n1768.             project_state.models[app_label, "rider"].fields["pony"].db_column,\\n1769.         )\\n1770.         self.assertIs(\\n1771.             new_state.models[app_label, "rider"].fields["pony"].db_column,\\n1772.             "pony_id",\\n1773.         )\\n1774.         self.assertColumnExists(rider_table, "pony_id")\\n1775.         with connection.schema_editor() as editor:\\n1776.             with self.assertNumQueries(0):\\n1777.                 operation.database_forwards(app_label, editor, project_state, new_state)\\n1778.         self.assertColumnExists(rider_table, "pony_id")\\n1779.         with connection.schema_editor() as editor:\\n1780.             with self.assertNumQueries(0):\\n1781.                 operation.database_forwards(app_label, editor, new_state, project_state)\\n1782.         self.assertColumnExists(rider_table, "pony_id")\\n1783. \\n1784.     def test_alter_field_pk(self):\\n1785.         """\\n1786.         The AlterField operation on primary keys (things like PostgreSQL\\\'s\\n1787.         SERIAL weirdness).\\n1788.         """\\n1789.         project_state = self.set_up_test_model("test_alflpk")\\n1790.         # Test the state alteration\\n1791.         operation = migrations.AlterField(\\n1792.             "Pony", "id", models.IntegerField(primary_key=True)\\n1793.         )\\n1794.         new_state = project_state.clone()\\n1795.         operation.state_forwards("test_alflpk", new_state)\\n1796.         self.assertIsInstance(\\n1797.             project_state.models["test_alflpk", "pony"].fields["id"],\\n1798.             models.AutoField,\\n1799.         )\\n1800.         self.assertIsInstance(\\n1801.             new_state.models["test_alflpk", "pony"].fields["id"],\\n1802.             models.IntegerField,\\n1803.         )\\n1804.         # Test the database alteration\\n1805.         with connection.schema_editor() as editor:\\n1806.             operation.database_forwards("test_alflpk", editor, project_state, new_state)\\n1807.         # And test reversal\\n1808.         with connection.schema_editor() as editor:\\n1809.             operation.database_backwards(\\n1810.                 "test_alflpk", editor, new_state, project_state\\n1811.             )\\n1812. \\n1813.     @skipUnlessDBFeature("supports_foreign_keys")\\n1814.     def test_alter_field_pk_fk(self):\\n1815.         """\\n1816.         Tests the AlterField operation on primary keys changes any FKs pointing to it.\\n1817.         """\\n1818.         project_state = self.set_up_test_model("test_alflpkfk", related_model=True)\\n1819.         project_state = self.apply_operations(\\n1820.             "test_alflpkfk",\\n1821.             project_state,\\n1822.             [\\n1823.                 migrations.CreateModel(\\n1824.                     "Stable",\\n1825.                     fields=[\\n1826.                         ("ponies", models.ManyToManyField("Pony")),\\n1827.                     ],\\n1828.                 ),\\n1829.                 migrations.AddField(\\n1830.                     "Pony",\\n1831.                     "stables",\\n1832.                     models.ManyToManyField("Stable"),\\n1833.                 ),\\n1834.             ],\\n1835.         )\\n1836.         # Test the state alteration\\n1837.         operation = migrations.AlterField(\\n1838.             "Pony", "id", models.FloatField(primary_key=True)\\n1839.         )\\n1840.         new_state = project_state.clone()\\n1841.         operation.state_forwards("test_alflpkfk", new_state)\\n1842.         self.assertIsInstance(\\n1843.             project_state.models["test_alflpkfk", "pony"].fields["id"],\\n1844.             models.AutoField,\\n1845.         )\\n1846.         self.assertIsInstance(\\n1847.             new_state.models["test_alflpkfk", "pony"].fields["id"],\\n1848.             models.FloatField,\\n1849.         )\\n1850. \\n1851.         def assertIdTypeEqualsFkType():\\n1852.             with connection.cursor() as cursor:\\n1853.                 id_type, id_null = [\\n1854.                     (c.type_code, c.null_ok)\\n1855.                     for c in connection.introspection.get_table_description(\\n1856.                         cursor, "test_alflpkfk_pony"\\n1857.                     )\\n1858.                     if c.name == "id"\\n1859.                 ][0]\\n1860.                 fk_type, fk_null = [\\n1861.                     (c.type_code, c.null_ok)\\n1862.                     for c in connection.introspection.get_table_description(\\n1863.                         cursor, "test_alflpkfk_rider"\\n1864.                     )\\n1865.                     if c.name == "pony_id"\\n1866.                 ][0]\\n1867.                 m2m_fk_type, m2m_fk_null = [\\n1868.                     (c.type_code, c.null_ok)\\n1869.                     for c in connection.introspection.get_table_description(\\n1870.                         cursor,\\n1871.                         "test_alflpkfk_pony_stables",\\n1872.                     )\\n1873.                     if c.name == "pony_id"\\n1874.                 ][0]\\n1875.                 remote_m2m_fk_type, remote_m2m_fk_null = [\\n1876.                     (c.type_code, c.null_ok)\\n1877.                     for c in connection.introspection.get_table_description(\\n1878.                         cursor,\\n1879.                         "test_alflpkfk_stable_ponies",\\n1880.                     )\\n1881.                     if c.name == "pony_id"\\n1882.                 ][0]\\n1883.             self.assertEqual(id_type, fk_type)\\n1884.             self.assertEqual(id_type, m2m_fk_type)\\n1885.             self.assertEqual(id_type, remote_m2m_fk_type)\\n1886.             self.assertEqual(id_null, fk_null)\\n1887.             self.assertEqual(id_null, m2m_fk_null)\\n1888.             self.assertEqual(id_null, remote_m2m_fk_null)\\n1889. \\n1890.         assertIdTypeEqualsFkType()\\n1891.         # Test the database alteration\\n1892.         with connection.schema_editor() as editor:\\n1893.             operation.database_forwards(\\n1894.                 "test_alflpkfk", editor, project_state, new_state\\n1895.             )\\n1896.         assertIdTypeEqualsFkType()\\n1897.         if connection.features.supports_foreign_keys:\\n1898.             self.assertFKExists(\\n1899.                 "test_alflpkfk_pony_stables",\\n1900.                 ["pony_id"],\\n1901.                 ("test_alflpkfk_pony", "id"),\\n1902.             )\\n1903.             self.assertFKExists(\\n1904.                 "test_alflpkfk_stable_ponies",\\n1905.                 ["pony_id"],\\n1906.                 ("test_alflpkfk_pony", "id"),\\n1907.             )\\n1908.         # And test reversal\\n1909.         with connection.schema_editor() as editor:\\n1910.             operation.database_backwards(\\n1911.                 "test_alflpkfk", editor, new_state, project_state\\n1912.             )\\n1913.         assertIdTypeEqualsFkType()\\n1914.         if connection.features.supports_foreign_keys:\\n1915.             self.assertFKExists(\\n1916.                 "test_alflpkfk_pony_stables",\\n1917.                 ["pony_id"],\\n1918.                 ("test_alflpkfk_pony", "id"),\\n1919.             )\\n1920.             self.assertFKExists(\\n1921.                 "test_alflpkfk_stable_ponies",\\n1922.                 ["pony_id"],\\n1923.                 ("test_alflpkfk_pony", "id"),\\n1924.             )\\n1925. \\n1926.     def test_alter_field_pk_mti_fk(self):\\n1927.         app_label = "test_alflpkmtifk"\\n1928.         project_state = self.set_up_test_model(app_label, mti_model=True)\\n1929.         project_state = self.apply_operations(\\n1930.             app_label,\\n1931.             project_state,\\n1932.             [\\n1933.                 migrations.CreateModel(\\n1934.                     "ShetlandRider",\\n1935.                     fields=[\\n1936.                         (\\n1937.                             "pony",\\n1938.                             models.ForeignKey(\\n1939.                                 f"{app_label}.ShetlandPony", models.CASCADE\\n1940.                             ),\\n1941.                         ),\\n1942.                     ],\\n1943.                 ),\\n1944.             ],\\n1945.         )\\n1946.         operation = migrations.AlterField(\\n1947.             "Pony",\\n1948.             "id",\\n1949.             models.BigAutoField(primary_key=True),\\n1950.         )\\n1951.         new_state = project_state.clone()\\n1952.         operation.state_forwards(app_label, new_state)\\n1953.         self.assertIsInstance(\\n1954.             new_state.models[app_label, "pony"].fields["id"],\\n1955.             models.BigAutoField,\\n1956.         )\\n1957. \\n1958.         def _get_column_id_type(cursor, table, column):\\n1959.             return [\\n1960.                 c.type_code\\n1961.                 for c in connection.introspection.get_table_description(\\n1962.                     cursor,\\n1963.                     f"{app_label}_{table}",\\n1964.                 )\\n1965.                 if c.name == column\\n1966.             ][0]\\n1967. \\n1968.         def assertIdTypeEqualsMTIFkType():\\n1969.             with connection.cursor() as cursor:\\n1970.                 parent_id_type = _get_column_id_type(cursor, "pony", "id")\\n1971.                 child_id_type = _get_column_id_type(\\n1972.                     cursor, "shetlandpony", "pony_ptr_id"\\n1973.                 )\\n1974.                 mti_id_type = _get_column_id_type(cursor, "shetlandrider", "pony_id")\\n1975.             self.assertEqual(parent_id_type, child_id_type)\\n1976.             self.assertEqual(parent_id_type, mti_id_type)\\n1977. \\n1978.         assertIdTypeEqualsMTIFkType()\\n1979.         # Alter primary key.\\n1980.         with connection.schema_editor() as editor:\\n1981.             operation.database_forwards(app_label, editor, project_state, new_state)\\n1982.         assertIdTypeEqualsMTIFkType()\\n1983.         if connection.features.supports_foreign_keys:\\n1984.             self.assertFKExists(\\n1985.                 f"{app_label}_shetlandpony",\\n1986.                 ["pony_ptr_id"],\\n1987.                 (f"{app_label}_pony", "id"),\\n1988.             )\\n1989.             self.assertFKExists(\\n1990.                 f"{app_label}_shetlandrider",\\n1991.                 ["pony_id"],\\n1992.                 (f"{app_label}_shetlandpony", "pony_ptr_id"),\\n1993.             )\\n1994.         # Reversal.\\n1995.         with connection.schema_editor() as editor:\\n1996.             operation.database_backwards(app_label, editor, new_state, project_state)\\n1997.         assertIdTypeEqualsMTIFkType()\\n1998.         if connection.features.supports_foreign_keys:\\n1999.             self.assertFKExists(\\n2000.                 f"{app_label}_shetlandpony",\\n2001.                 ["pony_ptr_id"],\\n2002.                 (f"{app_label}_pony", "id"),\\n2003.             )\\n2004.             self.assertFKExists(\\n2005.                 f"{app_label}_shetlandrider",\\n2006.                 ["pony_id"],\\n2007.                 (f"{app_label}_shetlandpony", "pony_ptr_id"),\\n2008.             )\\n2009. \\n2010.     def test_alter_field_pk_mti_and_fk_to_base(self):\\n2011.         app_label = "test_alflpkmtiftb"\\n2012.         project_state = self.set_up_test_model(\\n2013.             app_label,\\n2014.             mti_model=True,\\n2015.             related_model=True,\\n2016.         )\\n2017.         operation = migrations.AlterField(\\n2018.             "Pony",\\n2019.             "id",\\n2020.             models.BigAutoField(primary_key=True),\\n2021.         )\\n2022.         new_state = project_state.clone()\\n2023.         operation.state_forwards(app_label, new_state)\\n2024.         self.assertIsInstance(\\n2025.             new_state.models[app_label, "pony"].fields["id"],\\n2026.             models.BigAutoField,\\n2027.         )\\n2028. \\n2029.         def _get_column_id_type(cursor, table, column):\\n2030.             return [\\n2031.                 c.type_code\\n2032.                 for c in connection.introspection.get_table_description(\\n2033.                     cursor,\\n2034.                     f"{app_label}_{table}",\\n2035.                 )\\n2036.                 if c.name == column\\n2037.             ][0]\\n2038. \\n2039.         def assertIdTypeEqualsMTIFkType():\\n2040.             with connection.cursor() as cursor:\\n2041.                 parent_id_type = _get_column_id_type(cursor, "pony", "id")\\n2042.                 fk_id_type = _get_column_id_type(cursor, "rider", "pony_id")\\n2043.                 child_id_type = _get_column_id_type(\\n2044.                     cursor, "shetlandpony", "pony_ptr_id"\\n2045.                 )\\n2046.             self.assertEqual(parent_id_type, child_id_type)\\n2047.             self.assertEqual(parent_id_type, fk_id_type)\\n2048. \\n2049.         assertIdTypeEqualsMTIFkType()\\n2050.         # Alter primary key.\\n2051.         with connection.schema_editor() as editor:\\n2052.             operation.database_forwards(app_label, editor, project_state, new_state)\\n2053.         assertIdTypeEqualsMTIFkType()\\n2054.         if connection.features.supports_foreign_keys:\\n2055.             self.assertFKExists(\\n2056.                 f"{app_label}_shetlandpony",\\n2057.                 ["pony_ptr_id"],\\n2058.                 (f"{app_label}_pony", "id"),\\n2059.             )\\n2060.             self.assertFKExists(\\n2061.                 f"{app_label}_rider",\\n2062.                 ["pony_id"],\\n2063.                 (f"{app_label}_pony", "id"),\\n2064.             )\\n2065.         # Reversal.\\n2066.         with connection.schema_editor() as editor:\\n2067.             operation.database_backwards(app_label, editor, new_state, project_state)\\n2068.         assertIdTypeEqualsMTIFkType()\\n2069.         if connection.features.supports_foreign_keys:\\n2070.             self.assertFKExists(\\n2071.                 f"{app_label}_shetlandpony",\\n2072.                 ["pony_ptr_id"],\\n2073.                 (f"{app_label}_pony", "id"),\\n2074.             )\\n2075.             self.assertFKExists(\\n2076.                 f"{app_label}_rider",\\n2077.                 ["pony_id"],\\n2078.                 (f"{app_label}_pony", "id"),\\n2079.             )\\n2080. \\n2081.     @skipUnlessDBFeature("supports_foreign_keys")\\n2082.     def test_alter_field_reloads_state_on_fk_with_to_field_target_type_change(self):\\n2083.         app_label = "test_alflrsfkwtflttc"\\n2084.         project_state = self.apply_operations(\\n2085.             app_label,\\n2086.             ProjectState(),\\n2087.             operations=[\\n2088.                 migrations.CreateModel(\\n2089.                     "Rider",\\n2090.                     fields=[\\n2091.                         ("id", models.AutoField(primary_key=True)),\\n2092.                         ("code", models.IntegerField(unique=True)),\\n2093.                     ],\\n2094.                 ),\\n2095.                 migrations.CreateModel(\\n2096.                     "Pony",\\n2097.                     fields=[\\n2098.                         ("id", models.AutoField(primary_key=True)),\\n2099.                         (\\n2100.                             "rider",\\n2101.                             models.ForeignKey(\\n2102.                                 "%s.Rider" % app_label, models.CASCADE, to_field="code"\\n2103.                             ),\\n2104.                         ),\\n2105.                     ],\\n2106.                 ),\\n2107.             ],\\n2108.         )\\n2109.         operation = migrations.AlterField(\\n2110.             "Rider",\\n2111.             "code",\\n2112.             models.CharField(max_length=100, unique=True),\\n2113.         )\\n2114.         self.apply_operations(app_label, project_state, operations=[operation])\\n2115.         id_type, id_null = [\\n2116.             (c.type_code, c.null_ok)\\n2117.             for c in self.get_table_description("%s_rider" % app_label)\\n2118.             if c.name == "code"\\n2119.         ][0]\\n2120.         fk_type, fk_null = [\\n2121.             (c.type_code, c.null_ok)\\n2122.             for c in self.get_table_description("%s_pony" % app_label)\\n2123.             if c.name == "rider_id"\\n2124.         ][0]\\n2125.         self.assertEqual(id_type, fk_type)\\n2126.         self.assertEqual(id_null, fk_null)\\n2127. \\n2128.     @skipUnlessDBFeature("supports_foreign_keys")\\n2129.     def test_alter_field_reloads_state_fk_with_to_field_related_name_target_type_change(\\n2130.         self,\\n2131.     ):\\n2132.         app_label = "test_alflrsfkwtflrnttc"\\n2133.         project_state = self.apply_operations(\\n2134.             app_label,\\n2135.             ProjectState(),\\n2136.             operations=[\\n2137.                 migrations.CreateModel(\\n2138.                     "Rider",\\n2139.                     fields=[\\n2140.                         ("id", models.AutoField(primary_key=True)),\\n2141.                         ("code", models.PositiveIntegerField(unique=True)),\\n2142.                     ],\\n2143.                 ),\\n2144.                 migrations.CreateModel(\\n2145.                     "Pony",\\n2146.                     fields=[\\n2147.                         ("id", models.AutoField(primary_key=True)),\\n2148.                         (\\n2149.                             "rider",\\n2150.                             models.ForeignKey(\\n2151.                                 "%s.Rider" % app_label,\\n2152.                                 models.CASCADE,\\n2153.                                 to_field="code",\\n2154.                                 related_name="+",\\n2155.                             ),\\n2156.                         ),\\n2157.                     ],\\n2158.                 ),\\n2159.             ],\\n2160.         )\\n2161.         operation = migrations.AlterField(\\n2162.             "Rider",\\n2163.             "code",\\n2164.             models.CharField(max_length=100, unique=True),\\n2165.         )\\n2166.         self.apply_operations(app_label, project_state, operations=[operation])\\n2167. \\n2168.     def test_alter_field_reloads_state_on_fk_target_changes(self):\\n2169.         """\\n2170.         If AlterField doesn\\\'t reload state appropriately, the second AlterField\\n2171.         crashes on MySQL due to not dropping the PonyRider.pony foreign key\\n2172.         constraint before modifying the column.\\n2173.         """\\n2174.         app_label = "alter_alter_field_reloads_state_on_fk_target_changes"\\n2175.         project_state = self.apply_operations(\\n2176.             app_label,\\n2177.             ProjectState(),\\n2178.             operations=[\\n2179.                 migrations.CreateModel(\\n2180.                     "Rider",\\n2181.                     fields=[\\n2182.                         ("id", models.CharField(primary_key=True, max_length=100)),\\n2183.                     ],\\n2184.                 ),\\n2185.                 migrations.CreateModel(\\n2186.                     "Pony",\\n2187.                     fields=[\\n2188.                         ("id", models.CharField(primary_key=True, max_length=100)),\\n2189.                         (\\n2190.                             "rider",\\n2191.                             models.ForeignKey("%s.Rider" % app_label, models.CASCADE),\\n2192.                         ),\\n2193.                     ],\\n2194.                 ),\\n2195.                 migrations.CreateModel(\\n2196.                     "PonyRider",\\n2197.                     fields=[\\n2198.                         ("id", models.AutoField(primary_key=True)),\\n2199.                         (\\n2200.                             "pony",\\n2201.                             models.ForeignKey("%s.Pony" % app_label, models.CASCADE),\\n2202.                         ),\\n2203.                     ],\\n2204.                 ),\\n2205.             ],\\n2206.         )\\n2207.         project_state = self.apply_operations(\\n2208.             app_label,\\n2209.             project_state,\\n2210.             operations=[\\n2211.                 migrations.AlterField(\\n2212.                     "Rider", "id", models.CharField(primary_key=True, max_length=99)\\n2213.                 ),\\n2214.                 migrations.AlterField(\\n2215.                     "Pony", "id", models.CharField(primary_key=True, max_length=99)\\n2216.                 ),\\n2217.             ],\\n2218.         )\\n2219. \\n2220.     def test_alter_field_reloads_state_on_fk_with_to_field_target_changes(self):\\n2221.         """\\n2222.         If AlterField doesn\\\'t reload state appropriately, the second AlterField\\n2223.         crashes on MySQL due to not dropping the PonyRider.pony foreign key\\n2224.         constraint before modifying the column.\\n2225.         """\\n2226.         app_label = "alter_alter_field_reloads_state_on_fk_with_to_field_target_changes"\\n2227.         project_state = self.apply_operations(\\n2228.             app_label,\\n2229.             ProjectState(),\\n2230.             operations=[\\n2231.                 migrations.CreateModel(\\n2232.                     "Rider",\\n2233.                     fields=[\\n2234.                         ("id", models.CharField(primary_key=True, max_length=100)),\\n2235.                         ("slug", models.CharField(unique=True, max_length=100)),\\n2236.                     ],\\n2237.                 ),\\n2238.                 migrations.CreateModel(\\n2239.                     "Pony",\\n2240.                     fields=[\\n2241.                         ("id", models.CharField(primary_key=True, max_length=100)),\\n2242.                         (\\n2243.                             "rider",\\n2244.                             models.ForeignKey(\\n2245.                                 "%s.Rider" % app_label, models.CASCADE, to_field="slug"\\n2246.                             ),\\n2247.                         ),\\n2248.                         ("slug", models.CharField(unique=True, max_length=100)),\\n2249.                     ],\\n2250.                 ),\\n2251.                 migrations.CreateModel(\\n2252.                     "PonyRider",\\n2253.                     fields=[\\n2254.                         ("id", models.AutoField(primary_key=True)),\\n2255.                         (\\n2256.                             "pony",\\n2257.                             models.ForeignKey(\\n2258.                                 "%s.Pony" % app_label, models.CASCADE, to_field="slug"\\n2259.                             ),\\n2260.                         ),\\n2261.                     ],\\n2262.                 ),\\n2263.             ],\\n2264.         )\\n2265.         project_state = self.apply_operations(\\n2266.             app_label,\\n2267.             project_state,\\n2268.             operations=[\\n2269.                 migrations.AlterField(\\n2270.                     "Rider", "slug", models.CharField(unique=True, max_length=99)\\n2271.                 ),\\n2272.                 migrations.AlterField(\\n2273.                     "Pony", "slug", models.CharField(unique=True, max_length=99)\\n2274.                 ),\\n2275.             ],\\n2276.         )\\n2277. \\n2278.     def test_rename_field_reloads_state_on_fk_target_changes(self):\\n2279.         """\\n2280.         If RenameField doesn\\\'t reload state appropriately, the AlterField\\n2281.         crashes on MySQL due to not dropping the PonyRider.pony foreign key\\n2282.         constraint before modifying the column.\\n2283.         """\\n2284.         app_label = "alter_rename_field_reloads_state_on_fk_target_changes"\\n2285.         project_state = self.apply_operations(\\n2286.             app_label,\\n2287.             ProjectState(),\\n2288.             operations=[\\n2289.                 migrations.CreateModel(\\n2290.                     "Rider",\\n2291.                     fields=[\\n2292.                         ("id", models.CharField(primary_key=True, max_length=100)),\\n2293.                     ],\\n2294.                 ),\\n2295.                 migrations.CreateModel(\\n2296.                     "Pony",\\n2297.                     fields=[\\n2298.                         ("id", models.CharField(primary_key=True, max_length=100)),\\n2299.                         (\\n2300.                             "rider",\\n2301.                             models.ForeignKey("%s.Rider" % app_label, models.CASCADE),\\n2302.                         ),\\n2303.                     ],\\n2304.                 ),\\n2305.                 migrations.CreateModel(\\n2306.                     "PonyRider",\\n2307.                     fields=[\\n2308.                         ("id", models.AutoField(primary_key=True)),\\n2309.                         (\\n2310.                             "pony",\\n2311.                             models.ForeignKey("%s.Pony" % app_label, models.CASCADE),\\n2312.                         ),\\n2313.                     ],\\n2314.                 ),\\n2315.             ],\\n2316.         )\\n2317.         project_state = self.apply_operations(\\n2318.             app_label,\\n2319.             project_state,\\n2320.             operations=[\\n2321.                 migrations.RenameField("Rider", "id", "id2"),\\n2322.                 migrations.AlterField(\\n2323.                     "Pony", "id", models.CharField(primary_key=True, max_length=99)\\n2324.                 ),\\n2325.             ],\\n2326.             atomic=connection.features.supports_atomic_references_rename,\\n2327.         )\\n2328. \\n2329.     def test_rename_field(self):\\n2330.         """\\n2331.         Tests the RenameField operation.\\n2332.         """\\n2333.         project_state = self.set_up_test_model(\\n2334.             "test_rnfl", unique_together=True, index_together=True\\n2335.         )\\n2336.         # Test the state alteration\\n2337.         operation = migrations.RenameField("Pony", "pink", "blue")\\n2338.         self.assertEqual(operation.describe(), "Rename field pink on Pony to blue")\\n2339.         self.assertEqual(operation.migration_name_fragment, "rename_pink_pony_blue")\\n2340.         new_state = project_state.clone()\\n2341.         operation.state_forwards("test_rnfl", new_state)\\n2342.         self.assertIn("blue", new_state.models["test_rnfl", "pony"].fields)\\n2343.         self.assertNotIn("pink", new_state.models["test_rnfl", "pony"].fields)\\n2344.         # Make sure the unique_together has the renamed column too\\n2345.         self.assertIn(\\n2346.             "blue", new_state.models["test_rnfl", "pony"].options["unique_together"][0]\\n2347.         )\\n2348.         self.assertNotIn(\\n2349.             "pink", new_state.models["test_rnfl", "pony"].options["unique_together"][0]\\n2350.         )\\n2351.         # Make sure the index_together has the renamed column too\\n2352.         self.assertIn(\\n2353.             "blue", new_state.models["test_rnfl", "pony"].options["index_together"][0]\\n2354.         )\\n2355.         self.assertNotIn(\\n2356.             "pink", new_state.models["test_rnfl", "pony"].options["index_together"][0]\\n2357.         )\\n2358.         # Test the database alteration\\n2359.         self.assertColumnExists("test_rnfl_pony", "pink")\\n2360.         self.assertColumnNotExists("test_rnfl_pony", "blue")\\n2361.         with connection.schema_editor() as editor:\\n2362.             operation.database_forwards("test_rnfl", editor, project_state, new_state)\\n2363.         self.assertColumnExists("test_rnfl_pony", "blue")\\n2364.         self.assertColumnNotExists("test_rnfl_pony", "pink")\\n2365.         # Ensure the unique constraint has been ported over\\n2366.         with connection.cursor() as cursor:\\n2367.             cursor.execute("INSERT INTO test_rnfl_pony (blue, weight) VALUES (1, 1)")\\n2368.             with self.assertRaises(IntegrityError):\\n2369.                 with atomic():\\n2370.                     cursor.execute(\\n2371.                         "INSERT INTO test_rnfl_pony (blue, weight) VALUES (1, 1)"\\n2372.                     )\\n2373.             cursor.execute("DELETE FROM test_rnfl_pony")\\n2374.         # Ensure the index constraint has been ported over\\n2375.         self.assertIndexExists("test_rnfl_pony", ["weight", "blue"])\\n2376.         # And test reversal\\n2377.         with connection.schema_editor() as editor:\\n2378.             operation.database_backwards("test_rnfl", editor, new_state, project_state)\\n2379.         self.assertColumnExists("test_rnfl_pony", "pink")\\n2380.         self.assertColumnNotExists("test_rnfl_pony", "blue")\\n2381.         # Ensure the index constraint has been reset\\n2382.         self.assertIndexExists("test_rnfl_pony", ["weight", "pink"])\\n2383.         # And deconstruction\\n2384.         definition = operation.deconstruct()\\n2385.         self.assertEqual(definition[0], "RenameField")\\n2386.         self.assertEqual(definition[1], [])\\n2387.         self.assertEqual(\\n2388.             definition[2],\\n2389.             {"model_name": "Pony", "old_name": "pink", "new_name": "blue"},\\n2390.         )\\n2391. \\n2392.     def test_rename_field_with_db_column(self):\\n2393.         project_state = self.apply_operations(\\n2394.             "test_rfwdbc",\\n2395.             ProjectState(),\\n2396.             operations=[\\n2397.                 migrations.CreateModel(\\n2398.                     "Pony",\\n2399.                     fields=[\\n2400.                         ("id", models.AutoField(primary_key=True)),\\n2401.                         ("field", models.IntegerField(db_column="db_field")),\\n2402.                         (\\n2403.                             "fk_field",\\n2404.                             models.ForeignKey(\\n2405.                                 "Pony",\\n2406.                                 models.CASCADE,\\n2407.                                 db_column="db_fk_field",\\n2408.                             ),\\n2409.                         ),\\n2410.                     ],\\n2411.                 ),\\n2412.             ],\\n2413.         )\\n2414.         new_state = project_state.clone()\\n2415.         operation = migrations.RenameField("Pony", "field", "renamed_field")\\n2416.         operation.state_forwards("test_rfwdbc", new_state)\\n2417.         self.assertIn("renamed_field", new_state.models["test_rfwdbc", "pony"].fields)\\n2418.         self.assertNotIn("field", new_state.models["test_rfwdbc", "pony"].fields)\\n2419.         self.assertColumnExists("test_rfwdbc_pony", "db_field")\\n2420.         with connection.schema_editor() as editor:\\n2421.             with self.assertNumQueries(0):\\n2422.                 operation.database_forwards(\\n2423.                     "test_rfwdbc", editor, project_state, new_state\\n2424.                 )\\n2425.         self.assertColumnExists("test_rfwdbc_pony", "db_field")\\n2426.         with connection.schema_editor() as editor:\\n2427.             with self.assertNumQueries(0):\\n2428.                 operation.database_backwards(\\n2429.                     "test_rfwdbc", editor, new_state, project_state\\n2430.                 )\\n2431.         self.assertColumnExists("test_rfwdbc_pony", "db_field")\\n2432. \\n2433.         new_state = project_state.clone()\\n2434.         operation = migrations.RenameField("Pony", "fk_field", "renamed_fk_field")\\n2435.         operation.state_forwards("test_rfwdbc", new_state)\\n2436.         self.assertIn(\\n2437.             "renamed_fk_field", new_state.models["test_rfwdbc", "pony"].fields\\n2438.         )\\n2439.         self.assertNotIn("fk_field", new_state.models["test_rfwdbc", "pony"].fields)\\n2440.         self.assertColumnExists("test_rfwdbc_pony", "db_fk_field")\\n2441.         with connection.schema_editor() as editor:\\n2442.             with self.assertNumQueries(0):\\n2443.                 operation.database_forwards(\\n2444.                     "test_rfwdbc", editor, project_state, new_state\\n2445.                 )\\n2446.         self.assertColumnExists("test_rfwdbc_pony", "db_fk_field")\\n2447.         with connection.schema_editor() as editor:\\n2448.             with self.assertNumQueries(0):\\n2449.                 operation.database_backwards(\\n2450.                     "test_rfwdbc", editor, new_state, project_state\\n2451.                 )\\n2452.         self.assertColumnExists("test_rfwdbc_pony", "db_fk_field")\\n2453. \\n2454.     def test_rename_field_case(self):\\n2455.         project_state = self.apply_operations(\\n2456.             "test_rfmx",\\n2457.             ProjectState(),\\n2458.             operations=[\\n2459.                 migrations.CreateModel(\\n2460.                     "Pony",\\n2461.                     fields=[\\n2462.                         ("id", models.AutoField(primary_key=True)),\\n2463.                         ("field", models.IntegerField()),\\n2464.                     ],\\n2465.                 ),\\n2466.             ],\\n2467.         )\\n2468.         new_state = project_state.clone()\\n2469.         operation = migrations.RenameField("Pony", "field", "FiElD")\\n2470.         operation.state_forwards("test_rfmx", new_state)\\n2471.         self.assertIn("FiElD", new_state.models["test_rfmx", "pony"].fields)\\n2472.         self.assertColumnExists("test_rfmx_pony", "field")\\n2473.         with connection.schema_editor() as editor:\\n2474.             operation.database_forwards("test_rfmx", editor, project_state, new_state)\\n2475.         self.assertColumnExists(\\n2476.             "test_rfmx_pony",\\n2477.             connection.introspection.identifier_converter("FiElD"),\\n2478.         )\\n2479.         with connection.schema_editor() as editor:\\n2480.             operation.database_backwards("test_rfmx", editor, new_state, project_state)\\n2481.         self.assertColumnExists("test_rfmx_pony", "field")\\n2482. \\n2483.     def test_rename_missing_field(self):\\n2484.         state = ProjectState()\\n2485.         state.add_model(ModelState("app", "model", []))\\n2486.         with self.assertRaisesMessage(\\n2487.             FieldDoesNotExist, "app.model has no field named \\\'field\\\'"\\n2488.         ):\\n2489.             migrations.RenameField("model", "field", "new_field").state_forwards(\\n2490.                 "app", state\\n2491.             )\\n2492. \\n2493.     def test_rename_referenced_field_state_forward(self):\\n2494.         state = ProjectState()\\n2495.         state.add_model(\\n2496.             ModelState(\\n2497.                 "app",\\n2498.                 "Model",\\n2499.                 [\\n2500.                     ("id", models.AutoField(primary_key=True)),\\n2501.                     ("field", models.IntegerField(unique=True)),\\n2502.                 ],\\n2503.             )\\n2504.         )\\n2505.         state.add_model(\\n2506.             ModelState(\\n2507.                 "app",\\n2508.                 "OtherModel",\\n2509.                 [\\n2510.                     ("id", models.AutoField(primary_key=True)),\\n2511.                     (\\n2512.                         "fk",\\n2513.                         models.ForeignKey("Model", models.CASCADE, to_field="field"),\\n2514.                     ),\\n2515.                     (\\n2516.                         "fo",\\n2517.                         models.ForeignObject(\\n2518.                             "Model",\\n2519.                             models.CASCADE,\\n2520.                             from_fields=("fk",),\\n2521.                             to_fields=("field",),\\n2522.                         ),\\n2523.                     ),\\n2524.                 ],\\n2525.             )\\n2526.         )\\n2527.         operation = migrations.RenameField("Model", "field", "renamed")\\n2528.         new_state = state.clone()\\n2529.         operation.state_forwards("app", new_state)\\n2530.         self.assertEqual(\\n2531.             new_state.models["app", "othermodel"].fields["fk"].remote_field.field_name,\\n2532.             "renamed",\\n2533.         )\\n2534.         self.assertEqual(\\n2535.             new_state.models["app", "othermodel"].fields["fk"].from_fields, ["self"]\\n2536.         )\\n2537.         self.assertEqual(\\n2538.             new_state.models["app", "othermodel"].fields["fk"].to_fields, ("renamed",)\\n2539.         )\\n2540.         self.assertEqual(\\n2541.             new_state.models["app", "othermodel"].fields["fo"].from_fields, ("fk",)\\n2542.         )\\n2543.         self.assertEqual(\\n2544.             new_state.models["app", "othermodel"].fields["fo"].to_fields, ("renamed",)\\n2545.         )\\n2546.         operation = migrations.RenameField("OtherModel", "fk", "renamed_fk")\\n2547.         new_state = state.clone()\\n2548.         operation.state_forwards("app", new_state)\\n2549.         self.assertEqual(\\n2550.             new_state.models["app", "othermodel"]\\n2551.             .fields["renamed_fk"]\\n2552.             .remote_field.field_name,\\n2553.             "renamed",\\n2554.         )\\n2555.         self.assertEqual(\\n2556.             new_state.models["app", "othermodel"].fields["renamed_fk"].from_fields,\\n2557.             ("self",),\\n2558.         )\\n2559.         self.assertEqual(\\n2560.             new_state.models["app", "othermodel"].fields["renamed_fk"].to_fields,\\n2561.             ("renamed",),\\n2562.         )\\n2563.         self.assertEqual(\\n2564.             new_state.models["app", "othermodel"].fields["fo"].from_fields,\\n2565.             ("renamed_fk",),\\n2566.         )\\n2567.         self.assertEqual(\\n2568.             new_state.models["app", "othermodel"].fields["fo"].to_fields, ("renamed",)\\n2569.         )\\n2570. \\n2571.     def test_alter_unique_together(self):\\n2572.         """\\n2573.         Tests the AlterUniqueTogether operation.\\n2574.         """\\n2575.         project_state = self.set_up_test_model("test_alunto")\\n2576.         # Test the state alteration\\n2577.         operation = migrations.AlterUniqueTogether("Pony", [("pink", "weight")])\\n2578.         self.assertEqual(\\n2579.             operation.describe(), "Alter unique_together for Pony (1 constraint(s))"\\n2580.         )\\n2581.         self.assertEqual(\\n2582.             operation.migration_name_fragment,\\n2583.             "alter_pony_unique_together",\\n2584.         )\\n2585.         new_state = project_state.clone()\\n2586.         operation.state_forwards("test_alunto", new_state)\\n2587.         self.assertEqual(\\n2588.             len(\\n2589.                 project_state.models["test_alunto", "pony"].options.get(\\n2590.                     "unique_together", set()\\n2591.                 )\\n2592.             ),\\n2593.             0,\\n2594.         )\\n2595.         self.assertEqual(\\n2596.             len(\\n2597.                 new_state.models["test_alunto", "pony"].options.get(\\n2598.                     "unique_together", set()\\n2599.                 )\\n2600.             ),\\n2601.             1,\\n2602.         )\\n2603.         # Make sure we can insert duplicate rows\\n2604.         with connection.cursor() as cursor:\\n2605.             cursor.execute("INSERT INTO test_alunto_pony (pink, weight) VALUES (1, 1)")\\n2606.             cursor.execute("INSERT INTO test_alunto_pony (pink, weight) VALUES (1, 1)")\\n2607.             cursor.execute("DELETE FROM test_alunto_pony")\\n2608.             # Test the database alteration\\n2609.             with connection.schema_editor() as editor:\\n2610.                 operation.database_forwards(\\n2611.                     "test_alunto", editor, project_state, new_state\\n2612.                 )\\n2613.             cursor.execute("INSERT INTO test_alunto_pony (pink, weight) VALUES (1, 1)")\\n2614.             with self.assertRaises(IntegrityError):\\n2615.                 with atomic():\\n2616.                     cursor.execute(\\n2617.                         "INSERT INTO test_alunto_pony (pink, weight) VALUES (1, 1)"\\n2618.                     )\\n2619.             cursor.execute("DELETE FROM test_alunto_pony")\\n2620.             # And test reversal\\n2621.             with connection.schema_editor() as editor:\\n2622.                 operation.database_backwards(\\n2623.                     "test_alunto", editor, new_state, project_state\\n2624.                 )\\n2625.             cursor.execute("INSERT INTO test_alunto_pony (pink, weight) VALUES (1, 1)")\\n2626.             cursor.execute("INSERT INTO test_alunto_pony (pink, weight) VALUES (1, 1)")\\n2627.             cursor.execute("DELETE FROM test_alunto_pony")\\n2628.         # Test flat unique_together\\n2629.         operation = migrations.AlterUniqueTogether("Pony", ("pink", "weight"))\\n2630.         operation.state_forwards("test_alunto", new_state)\\n2631.         self.assertEqual(\\n2632.             len(\\n2633.                 new_state.models["test_alunto", "pony"].options.get(\\n2634.                     "unique_together", set()\\n2635.                 )\\n2636.             ),\\n2637.             1,\\n2638.         )\\n2639.         # And deconstruction\\n2640.         definition = operation.deconstruct()\\n2641.         self.assertEqual(definition[0], "AlterUniqueTogether")\\n2642.         self.assertEqual(definition[1], [])\\n2643.         self.assertEqual(\\n2644.             definition[2], {"name": "Pony", "unique_together": {("pink", "weight")}}\\n2645.         )\\n2646. \\n2647.     def test_alter_unique_together_remove(self):\\n2648.         operation = migrations.AlterUniqueTogether("Pony", None)\\n2649.         self.assertEqual(\\n2650.             operation.describe(), "Alter unique_together for Pony (0 constraint(s))"\\n2651.         )\\n2652. \\n2653.     def test_add_index(self):\\n2654.         """\\n2655.         Test the AddIndex operation.\\n2656.         """\\n2657.         project_state = self.set_up_test_model("test_adin")\\n2658.         msg = (\\n2659.             "Indexes passed to AddIndex operations require a name argument. "\\n2660.             "<Index: fields=[\\\'pink\\\']> doesn\\\'t have one."\\n2661.         )\\n2662.         with self.assertRaisesMessage(ValueError, msg):\\n2663.             migrations.AddIndex("Pony", models.Index(fields=["pink"]))\\n2664.         index = models.Index(fields=["pink"], name="test_adin_pony_pink_idx")\\n2665.         operation = migrations.AddIndex("Pony", index)\\n2666.         self.assertEqual(\\n2667.             operation.describe(),\\n2668.             "Create index test_adin_pony_pink_idx on field(s) pink of model Pony",\\n2669.         )\\n2670.         self.assertEqual(\\n2671.             operation.migration_name_fragment,\\n2672.             "pony_test_adin_pony_pink_idx",\\n2673.         )\\n2674.         new_state = project_state.clone()\\n2675.         operation.state_forwards("test_adin", new_state)\\n2676.         # Test the database alteration\\n2677.         self.assertEqual(\\n2678.             len(new_state.models["test_adin", "pony"].options["indexes"]), 1\\n2679.         )\\n2680.         self.assertIndexNotExists("test_adin_pony", ["pink"])\\n2681.         with connection.schema_editor() as editor:\\n2682.             operation.database_forwards("test_adin", editor, project_state, new_state)\\n2683.         self.assertIndexExists("test_adin_pony", ["pink"])\\n2684.         # And test reversal\\n2685.         with connection.schema_editor() as editor:\\n2686.             operation.database_backwards("test_adin", editor, new_state, project_state)\\n2687.         self.assertIndexNotExists("test_adin_pony", ["pink"])\\n2688.         # And deconstruction\\n2689.         definition = operation.deconstruct()\\n2690.         self.assertEqual(definition[0], "AddIndex")\\n2691.         self.assertEqual(definition[1], [])\\n2692.         self.assertEqual(definition[2], {"model_name": "Pony", "index": index})\\n2693. \\n2694.     def test_remove_index(self):\\n2695.         """\\n2696.         Test the RemoveIndex operation.\\n2697.         """\\n2698.         project_state = self.set_up_test_model("test_rmin", multicol_index=True)\\n2699.         self.assertTableExists("test_rmin_pony")\\n2700.         self.assertIndexExists("test_rmin_pony", ["pink", "weight"])\\n2701.         operation = migrations.RemoveIndex("Pony", "pony_test_idx")\\n2702.         self.assertEqual(operation.describe(), "Remove index pony_test_idx from Pony")\\n2703.         self.assertEqual(\\n2704.             operation.migration_name_fragment,\\n2705.             "remove_pony_pony_test_idx",\\n2706.         )\\n2707.         new_state = project_state.clone()\\n2708.         operation.state_forwards("test_rmin", new_state)\\n2709.         # Test the state alteration\\n2710.         self.assertEqual(\\n2711.             len(new_state.models["test_rmin", "pony"].options["indexes"]), 0\\n2712.         )\\n2713.         self.assertIndexExists("test_rmin_pony", ["pink", "weight"])\\n2714.         # Test the database alteration\\n2715.         with connection.schema_editor() as editor:\\n2716.             operation.database_forwards("test_rmin", editor, project_state, new_state)\\n2717.         self.assertIndexNotExists("test_rmin_pony", ["pink", "weight"])\\n2718.         # And test reversal\\n2719.         with connection.schema_editor() as editor:\\n2720.             operation.database_backwards("test_rmin", editor, new_state, project_state)\\n2721.         self.assertIndexExists("test_rmin_pony", ["pink", "weight"])\\n2722.         # And deconstruction\\n2723.         definition = operation.deconstruct()\\n2724.         self.assertEqual(definition[0], "RemoveIndex")\\n2725.         self.assertEqual(definition[1], [])\\n2726.         self.assertEqual(definition[2], {"model_name": "Pony", "name": "pony_test_idx"})\\n2727. \\n2728.         # Also test a field dropped with index - sqlite remake issue\\n2729.         operations = [\\n2730.             migrations.RemoveIndex("Pony", "pony_test_idx"),\\n2731.             migrations.RemoveField("Pony", "pink"),\\n2732.         ]\\n2733.         self.assertColumnExists("test_rmin_pony", "pink")\\n2734.         self.assertIndexExists("test_rmin_pony", ["pink", "weight"])\\n2735.         # Test database alteration\\n2736.         new_state = project_state.clone()\\n2737.         self.apply_operations("test_rmin", new_state, operations=operations)\\n2738.         self.assertColumnNotExists("test_rmin_pony", "pink")\\n2739.         self.assertIndexNotExists("test_rmin_pony", ["pink", "weight"])\\n2740.         # And test reversal\\n2741.         self.unapply_operations("test_rmin", project_state, operations=operations)\\n2742.         self.assertIndexExists("test_rmin_pony", ["pink", "weight"])\\n2743. \\n2744.     def test_add_index_state_forwards(self):\\n2745.         project_state = self.set_up_test_model("test_adinsf")\\n2746.         index = models.Index(fields=["pink"], name="test_adinsf_pony_pink_idx")\\n2747.         old_model = project_state.apps.get_model("test_adinsf", "Pony")\\n2748.         new_state = project_state.clone()\\n2749. \\n2750.         operation = migrations.AddIndex("Pony", index)\\n2751.         operation.state_forwards("test_adinsf", new_state)\\n2752.         new_model = new_state.apps.get_model("test_adinsf", "Pony")\\n2753.         self.assertIsNot(old_model, new_model)\\n2754. \\n2755.     def test_remove_index_state_forwards(self):\\n2756.         project_state = self.set_up_test_model("test_rminsf")\\n2757.         index = models.Index(fields=["pink"], name="test_rminsf_pony_pink_idx")\\n2758.         migrations.AddIndex("Pony", index).state_forwards("test_rminsf", project_state)\\n2759.         old_model = project_state.apps.get_model("test_rminsf", "Pony")\\n2760.         new_state = project_state.clone()\\n2761. \\n2762.         operation = migrations.RemoveIndex("Pony", "test_rminsf_pony_pink_idx")\\n2763.         operation.state_forwards("test_rminsf", new_state)\\n2764.         new_model = new_state.apps.get_model("test_rminsf", "Pony")\\n2765.         self.assertIsNot(old_model, new_model)\\n2766. \\n2767.     @skipUnlessDBFeature("supports_expression_indexes")\\n2768.     def test_add_func_index(self):\\n2769.         app_label = "test_addfuncin"\\n2770.         index_name = f"{app_label}_pony_abs_idx"\\n2771.         table_name = f"{app_label}_pony"\\n2772.         project_state = self.set_up_test_model(app_label)\\n2773.         index = models.Index(Abs("weight"), name=index_name)\\n2774.         operation = migrations.AddIndex("Pony", index)\\n2775.         self.assertEqual(\\n2776.             operation.describe(),\\n2777.             "Create index test_addfuncin_pony_abs_idx on Abs(F(weight)) on model Pony",\\n2778.         )\\n2779.         self.assertEqual(\\n2780.             operation.migration_name_fragment,\\n2781.             "pony_test_addfuncin_pony_abs_idx",\\n2782.         )\\n2783.         new_state = project_state.clone()\\n2784.         operation.state_forwards(app_label, new_state)\\n2785.         self.assertEqual(len(new_state.models[app_label, "pony"].options["indexes"]), 1)\\n2786.         self.assertIndexNameNotExists(table_name, index_name)\\n2787.         # Add index.\\n2788.         with connection.schema_editor() as editor:\\n2789.             operation.database_forwards(app_label, editor, project_state, new_state)\\n2790.         self.assertIndexNameExists(table_name, index_name)\\n2791.         # Reversal.\\n2792.         with connection.schema_editor() as editor:\\n2793.             operation.database_backwards(app_label, editor, new_state, project_state)\\n2794.         self.assertIndexNameNotExists(table_name, index_name)\\n2795.         # Deconstruction.\\n2796.         definition = operation.deconstruct()\\n2797.         self.assertEqual(definition[0], "AddIndex")\\n2798.         self.assertEqual(definition[1], [])\\n2799.         self.assertEqual(definition[2], {"model_name": "Pony", "index": index})\\n2800. \\n2801.     @skipUnlessDBFeature("supports_expression_indexes")\\n2802.     def test_remove_func_index(self):\\n2803.         app_label = "test_rmfuncin"\\n2804.         index_name = f"{app_label}_pony_abs_idx"\\n2805.         table_name = f"{app_label}_pony"\\n2806.         project_state = self.set_up_test_model(\\n2807.             app_label,\\n2808.             indexes=[\\n2809.                 models.Index(Abs("weight"), name=index_name),\\n2810.             ],\\n2811.         )\\n2812.         self.assertTableExists(table_name)\\n2813.         self.assertIndexNameExists(table_name, index_name)\\n2814.         operation = migrations.RemoveIndex("Pony", index_name)\\n2815.         self.assertEqual(\\n2816.             operation.describe(),\\n2817.             "Remove index test_rmfuncin_pony_abs_idx from Pony",\\n2818.         )\\n2819.         self.assertEqual(\\n2820.             operation.migration_name_fragment,\\n2821.             "remove_pony_test_rmfuncin_pony_abs_idx",\\n2822.         )\\n2823.         new_state = project_state.clone()\\n2824.         operation.state_forwards(app_label, new_state)\\n2825.         self.assertEqual(len(new_state.models[app_label, "pony"].options["indexes"]), 0)\\n2826.         # Remove index.\\n2827.         with connection.schema_editor() as editor:\\n2828.             operation.database_forwards(app_label, editor, project_state, new_state)\\n2829.         self.assertIndexNameNotExists(table_name, index_name)\\n2830.         # Reversal.\\n2831.         with connection.schema_editor() as editor:\\n2832.             operation.database_backwards(app_label, editor, new_state, project_state)\\n2833.         self.assertIndexNameExists(table_name, index_name)\\n2834.         # Deconstruction.\\n2835.         definition = operation.deconstruct()\\n2836.         self.assertEqual(definition[0], "RemoveIndex")\\n2837.         self.assertEqual(definition[1], [])\\n2838.         self.assertEqual(definition[2], {"model_name": "Pony", "name": index_name})\\n2839. \\n2840.     @skipUnlessDBFeature("supports_expression_indexes")\\n2841.     def test_alter_field_with_func_index(self):\\n2842.         app_label = "test_alfuncin"\\n2843.         index_name = f"{app_label}_pony_idx"\\n2844.         table_name = f"{app_label}_pony"\\n2845.         project_state = self.set_up_test_model(\\n2846.             app_label,\\n2847.             indexes=[models.Index(Abs("pink"), name=index_name)],\\n2848.         )\\n2849.         operation = migrations.AlterField(\\n2850.             "Pony", "pink", models.IntegerField(null=True)\\n2851.         )\\n2852.         new_state = project_state.clone()\\n2853.         operation.state_forwards(app_label, new_state)\\n2854.         with connection.schema_editor() as editor:\\n2855.             operation.database_forwards(app_label, editor, project_state, new_state)\\n2856.         self.assertIndexNameExists(table_name, index_name)\\n2857.         with connection.schema_editor() as editor:\\n2858.             operation.database_backwards(app_label, editor, new_state, project_state)\\n2859.         self.assertIndexNameExists(table_name, index_name)\\n2860. \\n2861.     def test_alter_field_with_index(self):\\n2862.         """\\n2863.         Test AlterField operation with an index to ensure indexes created via\\n2864.         Meta.indexes don\\\'t get dropped with sqlite3 remake.\\n2865.         """\\n2866.         project_state = self.set_up_test_model("test_alflin", index=True)\\n2867.         operation = migrations.AlterField(\\n2868.             "Pony", "pink", models.IntegerField(null=True)\\n2869.         )\\n2870.         new_state = project_state.clone()\\n2871.         operation.state_forwards("test_alflin", new_state)\\n2872.         # Test the database alteration\\n2873.         self.assertColumnNotNull("test_alflin_pony", "pink")\\n2874.         with connection.schema_editor() as editor:\\n2875.             operation.database_forwards("test_alflin", editor, project_state, new_state)\\n2876.         # Index hasn\\\'t been dropped\\n2877.         self.assertIndexExists("test_alflin_pony", ["pink"])\\n2878.         # And test reversal\\n2879.         with connection.schema_editor() as editor:\\n2880.             operation.database_backwards(\\n2881.                 "test_alflin", editor, new_state, project_state\\n2882.             )\\n2883.         # Ensure the index is still there\\n2884.         self.assertIndexExists("test_alflin_pony", ["pink"])\\n2885. \\n2886.     def test_alter_index_together(self):\\n2887.         """\\n2888.         Tests the AlterIndexTogether operation.\\n2889.         """\\n2890.         project_state = self.set_up_test_model("test_alinto")\\n2891.         # Test the state alteration\\n2892.         operation = migrations.AlterIndexTogether("Pony", [("pink", "weight")])\\n2893.         self.assertEqual(\\n2894.             operation.describe(), "Alter index_together for Pony (1 constraint(s))"\\n2895.         )\\n2896.         self.assertEqual(\\n2897.             operation.migration_name_fragment,\\n2898.             "alter_pony_index_together",\\n2899.         )\\n2900.         new_state = project_state.clone()\\n2901.         operation.state_forwards("test_alinto", new_state)\\n2902.         self.assertEqual(\\n2903.             len(\\n2904.                 project_state.models["test_alinto", "pony"].options.get(\\n2905.                     "index_together", set()\\n2906.                 )\\n2907.             ),\\n2908.             0,\\n2909.         )\\n2910.         self.assertEqual(\\n2911.             len(\\n2912.                 new_state.models["test_alinto", "pony"].options.get(\\n2913.                     "index_together", set()\\n2914.                 )\\n2915.             ),\\n2916.             1,\\n2917.         )\\n2918.         # Make sure there\\\'s no matching index\\n2919.         self.assertIndexNotExists("test_alinto_pony", ["pink", "weight"])\\n2920.         # Test the database alteration\\n2921.         with connection.schema_editor() as editor:\\n2922.             operation.database_forwards("test_alinto", editor, project_state, new_state)\\n2923.         self.assertIndexExists("test_alinto_pony", ["pink", "weight"])\\n2924.         # And test reversal\\n2925.         with connection.schema_editor() as editor:\\n2926.             operation.database_backwards(\\n2927.                 "test_alinto", editor, new_state, project_state\\n2928.             )\\n2929.         self.assertIndexNotExists("test_alinto_pony", ["pink", "weight"])\\n2930.         # And deconstruction\\n2931.         definition = operation.deconstruct()\\n2932.         self.assertEqual(definition[0], "AlterIndexTogether")\\n2933.         self.assertEqual(definition[1], [])\\n2934.         self.assertEqual(\\n2935.             definition[2], {"name": "Pony", "index_together": {("pink", "weight")}}\\n2936.         )\\n2937. \\n2938.     def test_alter_index_together_remove(self):\\n2939.         operation = migrations.AlterIndexTogether("Pony", None)\\n2940.         self.assertEqual(\\n2941.             operation.describe(), "Alter index_together for Pony (0 constraint(s))"\\n2942.         )\\n2943. \\n2944.     @skipUnlessDBFeature("allows_multiple_constraints_on_same_fields")\\n2945.     def test_alter_index_together_remove_with_unique_together(self):\\n2946.         app_label = "test_alintoremove_wunto"\\n2947.         table_name = "%s_pony" % app_label\\n2948.         project_state = self.set_up_test_model(app_label, unique_together=True)\\n2949.         self.assertUniqueConstraintExists(table_name, ["pink", "weight"])\\n2950.         # Add index together.\\n2951.         new_state = project_state.clone()\\n2952.         operation = migrations.AlterIndexTogether("Pony", [("pink", "weight")])\\n2953.         operation.state_forwards(app_label, new_state)\\n2954.         with connection.schema_editor() as editor:\\n2955.             operation.database_forwards(app_label, editor, project_state, new_state)\\n2956.         self.assertIndexExists(table_name, ["pink", "weight"])\\n2957.         # Remove index together.\\n2958.         project_state = new_state\\n2959.         new_state = project_state.clone()\\n2960.         operation = migrations.AlterIndexTogether("Pony", set())\\n2961.         operation.state_forwards(app_label, new_state)\\n2962.         with connection.schema_editor() as editor:\\n2963.             operation.database_forwards(app_label, editor, project_state, new_state)\\n2964.         self.assertIndexNotExists(table_name, ["pink", "weight"])\\n2965.         self.assertUniqueConstraintExists(table_name, ["pink", "weight"])\\n2966. \\n2967.     @skipUnlessDBFeature("supports_table_check_constraints")\\n2968.     def test_add_constraint(self):\\n2969.         project_state = self.set_up_test_model("test_addconstraint")\\n2970.         gt_check = models.Q(pink__gt=2)\\n2971.         gt_constraint = models.CheckConstraint(\\n2972.             check=gt_check, name="test_add_constraint_pony_pink_gt_2"\\n2973.         )\\n2974.         gt_operation = migrations.AddConstraint("Pony", gt_constraint)\\n2975.         self.assertEqual(\\n2976.             gt_operation.describe(),\\n2977.             "Create constraint test_add_constraint_pony_pink_gt_2 on model Pony",\\n2978.         )\\n2979.         self.assertEqual(\\n2980.             gt_operation.migration_name_fragment,\\n2981.             "pony_test_add_constraint_pony_pink_gt_2",\\n2982.         )\\n2983.         # Test the state alteration\\n2984.         new_state = project_state.clone()\\n2985.         gt_operation.state_forwards("test_addconstraint", new_state)\\n2986.         self.assertEqual(\\n2987.             len(new_state.models["test_addconstraint", "pony"].options["constraints"]),\\n2988.             1,\\n2989.         )\\n2990.         Pony = new_state.apps.get_model("test_addconstraint", "Pony")\\n2991.         self.assertEqual(len(Pony._meta.constraints), 1)\\n2992.         # Test the database alteration\\n2993.         with connection.schema_editor() as editor:\\n2994.             gt_operation.database_forwards(\\n2995.                 "test_addconstraint", editor, project_state, new_state\\n2996.             )\\n2997.         with self.assertRaises(IntegrityError), transaction.atomic():\\n2998.             Pony.objects.create(pink=1, weight=1.0)\\n2999.         # Add another one.\\n3000.         lt_check = models.Q(pink__lt=100)\\n3001.         lt_constraint = models.CheckConstraint(\\n3002.             check=lt_check, name="test_add_constraint_pony_pink_lt_100"\\n3003.         )\\n3004.         lt_operation = migrations.AddConstraint("Pony", lt_constraint)\\n3005.         lt_operation.state_forwards("test_addconstraint", new_state)\\n3006.         self.assertEqual(\\n3007.             len(new_state.models["test_addconstraint", "pony"].options["constraints"]),\\n3008.             2,\\n3009.         )\\n3010.         Pony = new_state.apps.get_model("test_addconstraint", "Pony")\\n3011.         self.assertEqual(len(Pony._meta.constraints), 2)\\n3012.         with connection.schema_editor() as editor:\\n3013.             lt_operation.database_forwards(\\n3014.                 "test_addconstraint", editor, project_state, new_state\\n3015.             )\\n3016.         with self.assertRaises(IntegrityError), transaction.atomic():\\n3017.             Pony.objects.create(pink=100, weight=1.0)\\n3018.         # Test reversal\\n3019.         with connection.schema_editor() as editor:\\n3020.             gt_operation.database_backwards(\\n3021.                 "test_addconstraint", editor, new_state, project_state\\n3022.             )\\n3023.         Pony.objects.create(pink=1, weight=1.0)\\n3024.         # Test deconstruction\\n3025.         definition = gt_operation.deconstruct()\\n3026.         self.assertEqual(definition[0], "AddConstraint")\\n3027.         self.assertEqual(definition[1], [])\\n3028.         self.assertEqual(\\n3029.             definition[2], {"model_name": "Pony", "constraint": gt_constraint}\\n3030.         )\\n3031. \\n3032.     @skipUnlessDBFeature("supports_table_check_constraints")\\n3033.     def test_add_constraint_percent_escaping(self):\\n3034.         app_label = "add_constraint_string_quoting"\\n3035.         operations = [\\n3036.             migrations.CreateModel(\\n3037.                 "Author",\\n3038.                 fields=[\\n3039.                     ("id", models.AutoField(primary_key=True)),\\n3040.                     ("name", models.CharField(max_length=100)),\\n3041.                     ("surname", models.CharField(max_length=100, default="")),\\n3042.                     ("rebate", models.CharField(max_length=100)),\\n3043.                 ],\\n3044.             ),\\n3045.         ]\\n3046.         from_state = self.apply_operations(app_label, ProjectState(), operations)\\n3047.         # "%" generated in startswith lookup should be escaped in a way that is\\n3048.         # considered a leading wildcard.\\n3049.         check = models.Q(name__startswith="Albert")\\n3050.         constraint = models.CheckConstraint(check=check, name="name_constraint")\\n3051.         operation = migrations.AddConstraint("Author", constraint)\\n3052.         to_state = from_state.clone()\\n3053.         operation.state_forwards(app_label, to_state)\\n3054.         with connection.schema_editor() as editor:\\n3055.             operation.database_forwards(app_label, editor, from_state, to_state)\\n3056.         Author = to_state.apps.get_model(app_label, "Author")\\n3057.         with self.assertRaises(IntegrityError), transaction.atomic():\\n3058.             Author.objects.create(name="Artur")\\n3059.         # Literal "%" should be escaped in a way that is not a considered a\\n3060.         # wildcard.\\n3061.         check = models.Q(rebate__endswith="%")\\n3062.         constraint = models.CheckConstraint(check=check, name="rebate_constraint")\\n3063.         operation = migrations.AddConstraint("Author", constraint)\\n3064.         from_state = to_state\\n3065.         to_state = from_state.clone()\\n3066.         operation.state_forwards(app_label, to_state)\\n3067.         Author = to_state.apps.get_model(app_label, "Author")\\n3068.         with connection.schema_editor() as editor:\\n3069.             operation.database_forwards(app_label, editor, from_state, to_state)\\n3070.         Author = to_state.apps.get_model(app_label, "Author")\\n3071.         with self.assertRaises(IntegrityError), transaction.atomic():\\n3072.             Author.objects.create(name="Albert", rebate="10$")\\n3073.         author = Author.objects.create(name="Albert", rebate="10%")\\n3074.         self.assertEqual(Author.objects.get(), author)\\n3075.         # Right-hand-side baked "%" literals should not be used for parameters\\n3076.         # interpolation.\\n3077.         check = ~models.Q(surname__startswith=models.F("name"))\\n3078.         constraint = models.CheckConstraint(check=check, name="name_constraint_rhs")\\n3079.         operation = migrations.AddConstraint("Author", constraint)\\n3080.         from_state = to_state\\n3081.         to_state = from_state.clone()\\n3082.         operation.state_forwards(app_label, to_state)\\n3083.         with connection.schema_editor() as editor:\\n3084.             operation.database_forwards(app_label, editor, from_state, to_state)\\n3085.         Author = to_state.apps.get_model(app_label, "Author")\\n3086.         with self.assertRaises(IntegrityError), transaction.atomic():\\n3087.             Author.objects.create(name="Albert", surname="Alberto")\\n3088. \\n3089.     @skipUnlessDBFeature("supports_table_check_constraints")\\n3090.     def test_add_or_constraint(self):\\n3091.         app_label = "test_addorconstraint"\\n3092.         constraint_name = "add_constraint_or"\\n3093.         from_state = self.set_up_test_model(app_label)\\n3094.         check = models.Q(pink__gt=2, weight__gt=2) | models.Q(weight__lt=0)\\n3095.         constraint = models.CheckConstraint(check=check, name=constraint_name)\\n3096.         operation = migrations.AddConstraint("Pony", constraint)\\n3097.         to_state = from_state.clone()\\n3098.         operation.state_forwards(app_label, to_state)\\n3099.         with connection.schema_editor() as editor:\\n3100.             operation.database_forwards(app_label, editor, from_state, to_state)\\n3101.         Pony = to_state.apps.get_model(app_label, "Pony")\\n3102.         with self.assertRaises(IntegrityError), transaction.atomic():\\n3103.             Pony.objects.create(pink=2, weight=3.0)\\n3104.         with self.assertRaises(IntegrityError), transaction.atomic():\\n3105.             Pony.objects.create(pink=3, weight=1.0)\\n3106.         Pony.objects.bulk_create(\\n3107.             [\\n3108.                 Pony(pink=3, weight=-1.0),\\n3109.                 Pony(pink=1, weight=-1.0),\\n3110.                 Pony(pink=3, weight=3.0),\\n3111.             ]\\n3112.         )\\n3113. \\n3114.     @skipUnlessDBFeature("supports_table_check_constraints")\\n3115.     def test_add_constraint_combinable(self):\\n3116.         app_label = "test_addconstraint_combinable"\\n3117.         operations = [\\n3118.             migrations.CreateModel(\\n3119.                 "Book",\\n3120.                 fields=[\\n3121.                     ("id", models.AutoField(primary_key=True)),\\n3122.                     ("read", models.PositiveIntegerField()),\\n3123.                     ("unread", models.PositiveIntegerField()),\\n3124.                 ],\\n3125.             ),\\n3126.         ]\\n3127.         from_state = self.apply_operations(app_label, ProjectState(), operations)\\n3128.         constraint = models.CheckConstraint(\\n3129.             check=models.Q(read=(100 - models.F("unread"))),\\n3130.             name="test_addconstraint_combinable_sum_100",\\n3131.         )\\n3132.         operation = migrations.AddConstraint("Book", constraint)\\n3133.         to_state = from_state.clone()\\n3134.         operation.state_forwards(app_label, to_state)\\n3135.         with connection.schema_editor() as editor:\\n3136.             operation.database_forwards(app_label, editor, from_state, to_state)\\n3137.         Book = to_state.apps.get_model(app_label, "Book")\\n3138.         with self.assertRaises(IntegrityError), transaction.atomic():\\n3139.             Book.objects.create(read=70, unread=10)\\n3140.         Book.objects.create(read=70, unread=30)\\n3141. \\n3142.     @skipUnlessDBFeature("supports_table_check_constraints")\\n3143.     def test_remove_constraint(self):\\n3144.         project_state = self.set_up_test_model(\\n3145.             "test_removeconstraint",\\n3146.             constraints=[\\n3147.                 models.CheckConstraint(\\n3148.                     check=models.Q(pink__gt=2),\\n3149.                     name="test_remove_constraint_pony_pink_gt_2",\\n3150.                 ),\\n3151.                 models.CheckConstraint(\\n3152.                     check=models.Q(pink__lt=100),\\n3153.                     name="test_remove_constraint_pony_pink_lt_100",\\n3154.                 ),\\n3155.             ],\\n3156.         )\\n3157.         gt_operation = migrations.RemoveConstraint(\\n3158.             "Pony", "test_remove_constraint_pony_pink_gt_2"\\n3159.         )\\n3160.         self.assertEqual(\\n3161.             gt_operation.describe(),\\n3162.             "Remove constraint test_remove_constraint_pony_pink_gt_2 from model Pony",\\n3163.         )\\n3164.         self.assertEqual(\\n3165.             gt_operation.migration_name_fragment,\\n3166.             "remove_pony_test_remove_constraint_pony_pink_gt_2",\\n3167.         )\\n3168.         # Test state alteration\\n3169.         new_state = project_state.clone()\\n3170.         gt_operation.state_forwards("test_removeconstraint", new_state)\\n3171.         self.assertEqual(\\n3172.             len(\\n3173.                 new_state.models["test_removeconstraint", "pony"].options["constraints"]\\n3174.             ),\\n3175.             1,\\n3176.         )\\n3177.         Pony = new_state.apps.get_model("test_removeconstraint", "Pony")\\n3178.         self.assertEqual(len(Pony._meta.constraints), 1)\\n3179.         # Test database alteration\\n3180.         with connection.schema_editor() as editor:\\n3181.             gt_operation.database_forwards(\\n3182.                 "test_removeconstraint", editor, project_state, new_state\\n3183.             )\\n3184.         Pony.objects.create(pink=1, weight=1.0).delete()\\n3185.         with self.assertRaises(IntegrityError), transaction.atomic():\\n3186.             Pony.objects.create(pink=100, weight=1.0)\\n3187.         # Remove the other one.\\n3188.         lt_operation = migrations.RemoveConstraint(\\n3189.             "Pony", "test_remove_constraint_pony_pink_lt_100"\\n3190.         )\\n3191.         lt_operation.state_forwards("test_removeconstraint", new_state)\\n3192.         self.assertEqual(\\n3193.             len(\\n3194.                 new_state.models["test_removeconstraint", "pony"].options["constraints"]\\n3195.             ),\\n3196.             0,\\n3197.         )\\n3198.         Pony = new_state.apps.get_model("test_removeconstraint", "Pony")\\n3199.         self.assertEqual(len(Pony._meta.constraints), 0)\\n3200.         with connection.schema_editor() as editor:\\n3201.             lt_operation.database_forwards(\\n3202.                 "test_removeconstraint", editor, project_state, new_state\\n3203.             )\\n3204.         Pony.objects.create(pink=100, weight=1.0).delete()\\n3205.         # Test reversal\\n3206.         with connection.schema_editor() as editor:\\n3207.             gt_operation.database_backwards(\\n3208.                 "test_removeconstraint", editor, new_state, project_state\\n3209.             )\\n3210.         with self.assertRaises(IntegrityError), transaction.atomic():\\n3211.             Pony.objects.create(pink=1, weight=1.0)\\n3212.         # Test deconstruction\\n3213.         definition = gt_operation.deconstruct()\\n3214.         self.assertEqual(definition[0], "RemoveConstraint")\\n3215.         self.assertEqual(definition[1], [])\\n3216.         self.assertEqual(\\n3217.             definition[2],\\n3218.             {"model_name": "Pony", "name": "test_remove_constraint_pony_pink_gt_2"},\\n3219.         )\\n3220. \\n3221.     def test_add_partial_unique_constraint(self):\\n3222.         project_state = self.set_up_test_model("test_addpartialuniqueconstraint")\\n3223.         partial_unique_constraint = models.UniqueConstraint(\\n3224.             fields=["pink"],\\n3225.             condition=models.Q(weight__gt=5),\\n3226.             name="test_constraint_pony_pink_for_weight_gt_5_uniq",\\n3227.         )\\n3228.         operation = migrations.AddConstraint("Pony", partial_unique_constraint)\\n3229.         self.assertEqual(\\n3230.             operation.describe(),\\n3231.             "Create constraint test_constraint_pony_pink_for_weight_gt_5_uniq "\\n3232.             "on model Pony",\\n3233.         )\\n3234.         # Test the state alteration\\n3235.         new_state = project_state.clone()\\n3236.         operation.state_forwards("test_addpartialuniqueconstraint", new_state)\\n3237.         self.assertEqual(\\n3238.             len(\\n3239.                 new_state.models["test_addpartialuniqueconstraint", "pony"].options[\\n3240.                     "constraints"\\n3241.                 ]\\n3242.             ),\\n3243.             1,\\n3244.         )\\n3245.         Pony = new_state.apps.get_model("test_addpartialuniqueconstraint", "Pony")\\n3246.         self.assertEqual(len(Pony._meta.constraints), 1)\\n3247.         # Test the database alteration\\n3248.         with connection.schema_editor() as editor:\\n3249.             operation.database_forwards(\\n3250.                 "test_addpartialuniqueconstraint", editor, project_state, new_state\\n3251.             )\\n3252.         # Test constraint works\\n3253.         Pony.objects.create(pink=1, weight=4.0)\\n3254.         Pony.objects.create(pink=1, weight=4.0)\\n3255.         Pony.objects.create(pink=1, weight=6.0)\\n3256.         if connection.features.supports_partial_indexes:\\n3257.             with self.assertRaises(IntegrityError), transaction.atomic():\\n3258.                 Pony.objects.create(pink=1, weight=7.0)\\n3259.         else:\\n3260.             Pony.objects.create(pink=1, weight=7.0)\\n3261.         # Test reversal\\n3262.         with connection.schema_editor() as editor:\\n3263.             operation.database_backwards(\\n3264.                 "test_addpartialuniqueconstraint", editor, new_state, project_state\\n3265.             )\\n3266.         # Test constraint doesn\\\'t work\\n3267.         Pony.objects.create(pink=1, weight=7.0)\\n3268.         # Test deconstruction\\n3269.         definition = operation.deconstruct()\\n3270.         self.assertEqual(definition[0], "AddConstraint")\\n3271.         self.assertEqual(definition[1], [])\\n3272.         self.assertEqual(\\n3273.             definition[2],\\n3274.             {"model_name": "Pony", "constraint": partial_unique_constraint},\\n3275.         )\\n3276. \\n3277.     def test_remove_partial_unique_constraint(self):\\n3278.         project_state = self.set_up_test_model(\\n3279.             "test_removepartialuniqueconstraint",\\n3280.             constraints=[\\n3281.                 models.UniqueConstraint(\\n3282.                     fields=["pink"],\\n3283.                     condition=models.Q(weight__gt=5),\\n3284.                     name="test_constraint_pony_pink_for_weight_gt_5_uniq",\\n3285.                 ),\\n3286.             ],\\n3287.         )\\n3288.         gt_operation = migrations.RemoveConstraint(\\n3289.             "Pony", "test_constraint_pony_pink_for_weight_gt_5_uniq"\\n3290.         )\\n3291.         self.assertEqual(\\n3292.             gt_operation.describe(),\\n3293.             "Remove constraint test_constraint_pony_pink_for_weight_gt_5_uniq from "\\n3294.             "model Pony",\\n3295.         )\\n3296.         # Test state alteration\\n3297.         new_state = project_state.clone()\\n3298.         gt_operation.state_forwards("test_removepartialuniqueconstraint", new_state)\\n3299.         self.assertEqual(\\n3300.             len(\\n3301.                 new_state.models["test_removepartialuniqueconstraint", "pony"].options[\\n3302.                     "constraints"\\n3303.                 ]\\n3304.             ),\\n3305.             0,\\n3306.         )\\n3307.         Pony = new_state.apps.get_model("test_removepartialuniqueconstraint", "Pony")\\n3308.         self.assertEqual(len(Pony._meta.constraints), 0)\\n3309.         # Test database alteration\\n3310.         with connection.schema_editor() as editor:\\n3311.             gt_operation.database_forwards(\\n3312.                 "test_removepartialuniqueconstraint", editor, project_state, new_state\\n3313.             )\\n3314.         # Test constraint doesn\\\'t work\\n3315.         Pony.objects.create(pink=1, weight=4.0)\\n3316.         Pony.objects.create(pink=1, weight=4.0)\\n3317.         Pony.objects.create(pink=1, weight=6.0)\\n3318.         Pony.objects.create(pink=1, weight=7.0).delete()\\n3319.         # Test reversal\\n3320.         with connection.schema_editor() as editor:\\n3321.             gt_operation.database_backwards(\\n3322.                 "test_removepartialuniqueconstraint", editor, new_state, project_state\\n3323.             )\\n3324.         # Test constraint works\\n3325.         if connection.features.supports_partial_indexes:\\n3326.             with self.assertRaises(IntegrityError), transaction.atomic():\\n3327.                 Pony.objects.create(pink=1, weight=7.0)\\n3328.         else:\\n3329.             Pony.objects.create(pink=1, weight=7.0)\\n3330.         # Test deconstruction\\n3331.         definition = gt_operation.deconstruct()\\n3332.         self.assertEqual(definition[0], "RemoveConstraint")\\n3333.         self.assertEqual(definition[1], [])\\n3334.         self.assertEqual(\\n3335.             definition[2],\\n3336.             {\\n3337.                 "model_name": "Pony",\\n3338.                 "name": "test_constraint_pony_pink_for_weight_gt_5_uniq",\\n3339.             },\\n3340.         )\\n3341. \\n3342.     def test_add_deferred_unique_constraint(self):\\n3343.         app_label = "test_adddeferred_uc"\\n3344.         project_state = self.set_up_test_model(app_label)\\n3345.         deferred_unique_constraint = models.UniqueConstraint(\\n3346.             fields=["pink"],\\n3347.             name="deferred_pink_constraint_add",\\n3348.             deferrable=models.Deferrable.DEFERRED,\\n3349.         )\\n3350.         operation = migrations.AddConstraint("Pony", deferred_unique_constraint)\\n3351.         self.assertEqual(\\n3352.             operation.describe(),\\n3353.             "Create constraint deferred_pink_constraint_add on model Pony",\\n3354.         )\\n3355.         # Add constraint.\\n3356.         new_state = project_state.clone()\\n3357.         operation.state_forwards(app_label, new_state)\\n3358.         self.assertEqual(\\n3359.             len(new_state.models[app_label, "pony"].options["constraints"]), 1\\n3360.         )\\n3361.         Pony = new_state.apps.get_model(app_label, "Pony")\\n3362.         self.assertEqual(len(Pony._meta.constraints), 1)\\n3363.         with connection.schema_editor() as editor, CaptureQueriesContext(\\n3364.             connection\\n3365.         ) as ctx:\\n3366.             operation.database_forwards(app_label, editor, project_state, new_state)\\n3367.         Pony.objects.create(pink=1, weight=4.0)\\n3368.         if connection.features.supports_deferrable_unique_constraints:\\n3369.             # Unique constraint is deferred.\\n3370.             with transaction.atomic():\\n3371.                 obj = Pony.objects.create(pink=1, weight=4.0)\\n3372.                 obj.pink = 2\\n3373.                 obj.save()\\n3374.             # Constraint behavior can be changed with SET CONSTRAINTS.\\n3375.             with self.assertRaises(IntegrityError):\\n3376.                 with transaction.atomic(), connection.cursor() as cursor:\\n3377.                     quoted_name = connection.ops.quote_name(\\n3378.                         deferred_unique_constraint.name\\n3379.                     )\\n3380.                     cursor.execute("SET CONSTRAINTS %s IMMEDIATE" % quoted_name)\\n3381.                     obj = Pony.objects.create(pink=1, weight=4.0)\\n3382.                     obj.pink = 3\\n3383.                     obj.save()\\n3384.         else:\\n3385.             self.assertEqual(len(ctx), 0)\\n3386.             Pony.objects.create(pink=1, weight=4.0)\\n3387.         # Reversal.\\n3388.         with connection.schema_editor() as editor:\\n3389.             operation.database_backwards(app_label, editor, new_state, project_state)\\n3390.         # Constraint doesn\\\'t work.\\n3391.         Pony.objects.create(pink=1, weight=4.0)\\n3392.         # Deconstruction.\\n3393.         definition = operation.deconstruct()\\n3394.         self.assertEqual(definition[0], "AddConstraint")\\n3395.         self.assertEqual(definition[1], [])\\n3396.         self.assertEqual(\\n3397.             definition[2],\\n3398.             {"model_name": "Pony", "constraint": deferred_unique_constraint},\\n3399.         )\\n3400. \\n3401.     def test_remove_deferred_unique_constraint(self):\\n3402.         app_label = "test_removedeferred_uc"\\n3403.         deferred_unique_constraint = models.UniqueConstraint(\\n3404.             fields=["pink"],\\n3405.             name="deferred_pink_constraint_rm",\\n3406.             deferrable=models.Deferrable.DEFERRED,\\n3407.         )\\n3408.         project_state = self.set_up_test_model(\\n3409.             app_label, constraints=[deferred_unique_constraint]\\n3410.         )\\n3411.         operation = migrations.RemoveConstraint("Pony", deferred_unique_constraint.name)\\n3412.         self.assertEqual(\\n3413.             operation.describe(),\\n3414.             "Remove constraint deferred_pink_constraint_rm from model Pony",\\n3415.         )\\n3416.         # Remove constraint.\\n3417.         new_state = project_state.clone()\\n3418.         operation.state_forwards(app_label, new_state)\\n3419.         self.assertEqual(\\n3420.             len(new_state.models[app_label, "pony"].options["constraints"]), 0\\n3421.         )\\n3422.         Pony = new_state.apps.get_model(app_label, "Pony")\\n3423.         self.assertEqual(len(Pony._meta.constraints), 0)\\n3424.         with connection.schema_editor() as editor, CaptureQueriesContext(\\n3425.             connection\\n3426.         ) as ctx:\\n3427.             operation.database_forwards(app_label, editor, project_state, new_state)\\n3428.         # Constraint doesn\\\'t work.\\n3429.         Pony.objects.create(pink=1, weight=4.0)\\n3430.         Pony.objects.create(pink=1, weight=4.0).delete()\\n3431.         if not connection.features.supports_deferrable_unique_constraints:\\n3432.             self.assertEqual(len(ctx), 0)\\n3433.         # Reversal.\\n3434.         with connection.schema_editor() as editor:\\n3435.             operation.database_backwards(app_label, editor, new_state, project_state)\\n3436.         if connection.features.supports_deferrable_unique_constraints:\\n3437.             # Unique constraint is deferred.\\n3438.             with transaction.atomic():\\n3439.                 obj = Pony.objects.create(pink=1, weight=4.0)\\n3440.                 obj.pink = 2\\n3441.                 obj.save()\\n3442.             # Constraint behavior can be changed with SET CONSTRAINTS.\\n3443.             with self.assertRaises(IntegrityError):\\n3444.                 with transaction.atomic(), connection.cursor() as cursor:\\n3445.                     quoted_name = connection.ops.quote_name(\\n3446.                         deferred_unique_constraint.name\\n3447.                     )\\n3448.                     cursor.execute("SET CONSTRAINTS %s IMMEDIATE" % quoted_name)\\n3449.                     obj = Pony.objects.create(pink=1, weight=4.0)\\n3450.                     obj.pink = 3\\n3451.                     obj.save()\\n3452.         else:\\n3453.             Pony.objects.create(pink=1, weight=4.0)\\n3454.         # Deconstruction.\\n3455.         definition = operation.deconstruct()\\n3456.         self.assertEqual(definition[0], "RemoveConstraint")\\n3457.         self.assertEqual(definition[1], [])\\n3458.         self.assertEqual(\\n3459.             definition[2],\\n3460.             {\\n3461.                 "model_name": "Pony",\\n3462.                 "name": "deferred_pink_constraint_rm",\\n3463.             },\\n3464.         )\\n3465. \\n3466.     def test_add_covering_unique_constraint(self):\\n3467.         app_label = "test_addcovering_uc"\\n3468.         project_state = self.set_up_test_model(app_label)\\n3469.         covering_unique_constraint = models.UniqueConstraint(\\n3470.             fields=["pink"],\\n3471.             name="covering_pink_constraint_add",\\n3472.             include=["weight"],\\n3473.         )\\n3474.         operation = migrations.AddConstraint("Pony", covering_unique_constraint)\\n3475.         self.assertEqual(\\n3476.             operation.describe(),\\n3477.             "Create constraint covering_pink_constraint_add on model Pony",\\n3478.         )\\n3479.         # Add constraint.\\n3480.         new_state = project_state.clone()\\n3481.         operation.state_forwards(app_label, new_state)\\n3482.         self.assertEqual(\\n3483.             len(new_state.models[app_label, "pony"].options["constraints"]), 1\\n3484.         )\\n3485.         Pony = new_state.apps.get_model(app_label, "Pony")\\n3486.         self.assertEqual(len(Pony._meta.constraints), 1)\\n3487.         with connection.schema_editor() as editor, CaptureQueriesContext(\\n3488.             connection\\n3489.         ) as ctx:\\n3490.             operation.database_forwards(app_label, editor, project_state, new_state)\\n3491.         Pony.objects.create(pink=1, weight=4.0)\\n3492.         if connection.features.supports_covering_indexes:\\n3493.             with self.assertRaises(IntegrityError):\\n3494.                 Pony.objects.create(pink=1, weight=4.0)\\n3495.         else:\\n3496.             self.assertEqual(len(ctx), 0)\\n3497.             Pony.objects.create(pink=1, weight=4.0)\\n3498.         # Reversal.\\n3499.         with connection.schema_editor() as editor:\\n3500.             operation.database_backwards(app_label, editor, new_state, project_state)\\n3501.         # Constraint doesn\\\'t work.\\n3502.         Pony.objects.create(pink=1, weight=4.0)\\n3503.         # Deconstruction.\\n3504.         definition = operation.deconstruct()\\n3505.         self.assertEqual(definition[0], "AddConstraint")\\n3506.         self.assertEqual(definition[1], [])\\n3507.         self.assertEqual(\\n3508.             definition[2],\\n3509.             {"model_name": "Pony", "constraint": covering_unique_constraint},\\n3510.         )\\n3511. \\n3512.     def test_remove_covering_unique_constraint(self):\\n3513.         app_label = "test_removecovering_uc"\\n3514.         covering_unique_constraint = models.UniqueConstraint(\\n3515.             fields=["pink"],\\n3516.             name="covering_pink_constraint_rm",\\n3517.             include=["weight"],\\n3518.         )\\n3519.         project_state = self.set_up_test_model(\\n3520.             app_label, constraints=[covering_unique_constraint]\\n3521.         )\\n3522.         operation = migrations.RemoveConstraint("Pony", covering_unique_constraint.name)\\n3523.         self.assertEqual(\\n3524.             operation.describe(),\\n3525.             "Remove constraint covering_pink_constraint_rm from model Pony",\\n3526.         )\\n3527.         # Remove constraint.\\n3528.         new_state = project_state.clone()\\n3529.         operation.state_forwards(app_label, new_state)\\n3530.         self.assertEqual(\\n3531.             len(new_state.models[app_label, "pony"].options["constraints"]), 0\\n3532.         )\\n3533.         Pony = new_state.apps.get_model(app_label, "Pony")\\n3534.         self.assertEqual(len(Pony._meta.constraints), 0)\\n3535.         with connection.schema_editor() as editor, CaptureQueriesContext(\\n3536.             connection\\n3537.         ) as ctx:\\n3538.             operation.database_forwards(app_label, editor, project_state, new_state)\\n3539.         # Constraint doesn\\\'t work.\\n3540.         Pony.objects.create(pink=1, weight=4.0)\\n3541.         Pony.objects.create(pink=1, weight=4.0).delete()\\n3542.         if not connection.features.supports_covering_indexes:\\n3543.             self.assertEqual(len(ctx), 0)\\n3544.         # Reversal.\\n3545.         with connection.schema_editor() as editor:\\n3546.             operation.database_backwards(app_label, editor, new_state, project_state)\\n3547.         if connection.features.supports_covering_indexes:\\n3548.             with self.assertRaises(IntegrityError):\\n3549.                 Pony.objects.create(pink=1, weight=4.0)\\n3550.         else:\\n3551.             Pony.objects.create(pink=1, weight=4.0)\\n3552.         # Deconstruction.\\n3553.         definition = operation.deconstruct()\\n3554.         self.assertEqual(definition[0], "RemoveConstraint")\\n3555.         self.assertEqual(definition[1], [])\\n3556.         self.assertEqual(\\n3557.             definition[2],\\n3558.             {\\n3559.                 "model_name": "Pony",\\n3560.                 "name": "covering_pink_constraint_rm",\\n3561.             },\\n3562.         )\\n3563. \\n3564.     def test_alter_field_with_func_unique_constraint(self):\\n3565.         app_label = "test_alfuncuc"\\n3566.         constraint_name = f"{app_label}_pony_uq"\\n3567.         table_name = f"{app_label}_pony"\\n3568.         project_state = self.set_up_test_model(\\n3569.             app_label,\\n3570.             constraints=[\\n3571.                 models.UniqueConstraint("pink", "weight", name=constraint_name)\\n3572.             ],\\n3573.         )\\n3574.         operation = migrations.AlterField(\\n3575.             "Pony", "pink", models.IntegerField(null=True)\\n3576.         )\\n3577.         new_state = project_state.clone()\\n3578.         operation.state_forwards(app_label, new_state)\\n3579.         with connection.schema_editor() as editor:\\n3580.             operation.database_forwards(app_label, editor, project_state, new_state)\\n3581.         if connection.features.supports_expression_indexes:\\n3582.             self.assertIndexNameExists(table_name, constraint_name)\\n3583.         with connection.schema_editor() as editor:\\n3584.             operation.database_backwards(app_label, editor, new_state, project_state)\\n3585.         if connection.features.supports_expression_indexes:\\n3586.             self.assertIndexNameExists(table_name, constraint_name)\\n3587. \\n3588.     def test_add_func_unique_constraint(self):\\n3589.         app_label = "test_adfuncuc"\\n3590.         constraint_name = f"{app_label}_pony_abs_uq"\\n3591.         table_name = f"{app_label}_pony"\\n3592.         project_state = self.set_up_test_model(app_label)\\n3593.         constraint = models.UniqueConstraint(Abs("weight"), name=constraint_name)\\n3594.         operation = migrations.AddConstraint("Pony", constraint)\\n3595.         self.assertEqual(\\n3596.             operation.describe(),\\n3597.             "Create constraint test_adfuncuc_pony_abs_uq on model Pony",\\n3598.         )\\n3599.         self.assertEqual(\\n3600.             operation.migration_name_fragment,\\n3601.             "pony_test_adfuncuc_pony_abs_uq",\\n3602.         )\\n3603.         new_state = project_state.clone()\\n3604.         operation.state_forwards(app_label, new_state)\\n3605.         self.assertEqual(\\n3606.             len(new_state.models[app_label, "pony"].options["constraints"]), 1\\n3607.         )\\n3608.         self.assertIndexNameNotExists(table_name, constraint_name)\\n3609.         # Add constraint.\\n3610.         with connection.schema_editor() as editor:\\n3611.             operation.database_forwards(app_label, editor, project_state, new_state)\\n3612.         Pony = new_state.apps.get_model(app_label, "Pony")\\n3613.         Pony.objects.create(weight=4.0)\\n3614.         if connection.features.supports_expression_indexes:\\n3615.             self.assertIndexNameExists(table_name, constraint_name)\\n3616.             with self.assertRaises(IntegrityError):\\n3617.                 Pony.objects.create(weight=-4.0)\\n3618.         else:\\n3619.             self.assertIndexNameNotExists(table_name, constraint_name)\\n3620.             Pony.objects.create(weight=-4.0)\\n3621.         # Reversal.\\n3622.         with connection.schema_editor() as editor:\\n3623.             operation.database_backwards(app_label, editor, new_state, project_state)\\n3624.         self.assertIndexNameNotExists(table_name, constraint_name)\\n3625.         # Constraint doesn\\\'t work.\\n3626.         Pony.objects.create(weight=-4.0)\\n3627.         # Deconstruction.\\n3628.         definition = operation.deconstruct()\\n3629.         self.assertEqual(definition[0], "AddConstraint")\\n3630.         self.assertEqual(definition[1], [])\\n3631.         self.assertEqual(\\n3632.             definition[2],\\n3633.             {"model_name": "Pony", "constraint": constraint},\\n3634.         )\\n3635. \\n3636.     def test_remove_func_unique_constraint(self):\\n3637.         app_label = "test_rmfuncuc"\\n3638.         constraint_name = f"{app_label}_pony_abs_uq"\\n3639.         table_name = f"{app_label}_pony"\\n3640.         project_state = self.set_up_test_model(\\n3641.             app_label,\\n3642.             constraints=[\\n3643.                 models.UniqueConstraint(Abs("weight"), name=constraint_name),\\n3644.             ],\\n3645.         )\\n3646.         self.assertTableExists(table_name)\\n3647.         if connection.features.supports_expression_indexes:\\n3648.             self.assertIndexNameExists(table_name, constraint_name)\\n3649.         operation = migrations.RemoveConstraint("Pony", constraint_name)\\n3650.         self.assertEqual(\\n3651.             operation.describe(),\\n3652.             "Remove constraint test_rmfuncuc_pony_abs_uq from model Pony",\\n3653.         )\\n3654.         self.assertEqual(\\n3655.             operation.migration_name_fragment,\\n3656.             "remove_pony_test_rmfuncuc_pony_abs_uq",\\n3657.         )\\n3658.         new_state = project_state.clone()\\n3659.         operation.state_forwards(app_label, new_state)\\n3660.         self.assertEqual(\\n3661.             len(new_state.models[app_label, "pony"].options["constraints"]), 0\\n3662.         )\\n3663.         Pony = new_state.apps.get_model(app_label, "Pony")\\n3664.         self.assertEqual(len(Pony._meta.constraints), 0)\\n3665.         # Remove constraint.\\n3666.         with connection.schema_editor() as editor:\\n3667.             operation.database_forwards(app_label, editor, project_state, new_state)\\n3668.         self.assertIndexNameNotExists(table_name, constraint_name)\\n3669.         # Constraint doesn\\\'t work.\\n3670.         Pony.objects.create(pink=1, weight=4.0)\\n3671.         Pony.objects.create(pink=1, weight=-4.0).delete()\\n3672.         # Reversal.\\n3673.         with connection.schema_editor() as editor:\\n3674.             operation.database_backwards(app_label, editor, new_state, project_state)\\n3675.         if connection.features.supports_expression_indexes:\\n3676.             self.assertIndexNameExists(table_name, constraint_name)\\n3677.             with self.assertRaises(IntegrityError):\\n3678.                 Pony.objects.create(weight=-4.0)\\n3679.         else:\\n3680.             self.assertIndexNameNotExists(table_name, constraint_name)\\n3681.             Pony.objects.create(weight=-4.0)\\n3682.         # Deconstruction.\\n3683.         definition = operation.deconstruct()\\n3684.         self.assertEqual(definition[0], "RemoveConstraint")\\n3685.         self.assertEqual(definition[1], [])\\n3686.         self.assertEqual(definition[2], {"model_name": "Pony", "name": constraint_name})\\n3687. \\n3688.     def test_alter_model_options(self):\\n3689.         """\\n3690.         Tests the AlterModelOptions operation.\\n3691.         """\\n3692.         project_state = self.set_up_test_model("test_almoop")\\n3693.         # Test the state alteration (no DB alteration to test)\\n3694.         operation = migrations.AlterModelOptions(\\n3695.             "Pony", {"permissions": [("can_groom", "Can groom")]}\\n3696.         )\\n3697.         self.assertEqual(operation.describe(), "Change Meta options on Pony")\\n3698.         self.assertEqual(operation.migration_name_fragment, "alter_pony_options")\\n3699.         new_state = project_state.clone()\\n3700.         operation.state_forwards("test_almoop", new_state)\\n3701.         self.assertEqual(\\n3702.             len(\\n3703.                 project_state.models["test_almoop", "pony"].options.get(\\n3704.                     "permissions", []\\n3705.                 )\\n3706.             ),\\n3707.             0,\\n3708.         )\\n3709.         self.assertEqual(\\n3710.             len(new_state.models["test_almoop", "pony"].options.get("permissions", [])),\\n3711.             1,\\n3712.         )\\n3713.         self.assertEqual(\\n3714.             new_state.models["test_almoop", "pony"].options["permissions"][0][0],\\n3715.             "can_groom",\\n3716.         )\\n3717.         # And deconstruction\\n3718.         definition = operation.deconstruct()\\n3719.         self.assertEqual(definition[0], "AlterModelOptions")\\n3720.         self.assertEqual(definition[1], [])\\n3721.         self.assertEqual(\\n3722.             definition[2],\\n3723.             {"name": "Pony", "options": {"permissions": [("can_groom", "Can groom")]}},\\n3724.         )\\n3725. \\n3726.     def test_alter_model_options_emptying(self):\\n3727.         """\\n3728.         The AlterModelOptions operation removes keys from the dict (#23121)\\n3729.         """\\n3730.         project_state = self.set_up_test_model("test_almoop", options=True)\\n3731.         # Test the state alteration (no DB alteration to test)\\n3732.         operation = migrations.AlterModelOptions("Pony", {})\\n3733.         self.assertEqual(operation.describe(), "Change Meta options on Pony")\\n3734.         new_state = project_state.clone()\\n3735.         operation.state_forwards("test_almoop", new_state)\\n3736.         self.assertEqual(\\n3737.             len(\\n3738.                 project_state.models["test_almoop", "pony"].options.get(\\n3739.                     "permissions", []\\n3740.                 )\\n3741.             ),\\n3742.             1,\\n3743.         )\\n3744.         self.assertEqual(\\n3745.             len(new_state.models["test_almoop", "pony"].options.get("permissions", [])),\\n3746.             0,\\n3747.         )\\n3748.         # And deconstruction\\n3749.         definition = operation.deconstruct()\\n3750.         self.assertEqual(definition[0], "AlterModelOptions")\\n3751.         self.assertEqual(definition[1], [])\\n3752.         self.assertEqual(definition[2], {"name": "Pony", "options": {}})\\n3753. \\n3754.     def test_alter_order_with_respect_to(self):\\n3755.         """\\n3756.         Tests the AlterOrderWithRespectTo operation.\\n3757.         """\\n3758.         project_state = self.set_up_test_model("test_alorwrtto", related_model=True)\\n3759.         # Test the state alteration\\n3760.         operation = migrations.AlterOrderWithRespectTo("Rider", "pony")\\n3761.         self.assertEqual(\\n3762.             operation.describe(), "Set order_with_respect_to on Rider to pony"\\n3763.         )\\n3764.         self.assertEqual(\\n3765.             operation.migration_name_fragment,\\n3766.             "alter_rider_order_with_respect_to",\\n3767.         )\\n3768.         new_state = project_state.clone()\\n3769.         operation.state_forwards("test_alorwrtto", new_state)\\n3770.         self.assertIsNone(\\n3771.             project_state.models["test_alorwrtto", "rider"].options.get(\\n3772.                 "order_with_respect_to", None\\n3773.             )\\n3774.         )\\n3775.         self.assertEqual(\\n3776.             new_state.models["test_alorwrtto", "rider"].options.get(\\n3777.                 "order_with_respect_to", None\\n3778.             ),\\n3779.             "pony",\\n3780.         )\\n3781.         # Make sure there\\\'s no matching index\\n3782.         self.assertColumnNotExists("test_alorwrtto_rider", "_order")\\n3783.         # Create some rows before alteration\\n3784.         rendered_state = project_state.apps\\n3785.         pony = rendered_state.get_model("test_alorwrtto", "Pony").objects.create(\\n3786.             weight=50\\n3787.         )\\n3788.         rider1 = rendered_state.get_model("test_alorwrtto", "Rider").objects.create(\\n3789.             pony=pony\\n3790.         )\\n3791.         rider1.friend = rider1\\n3792.         rider1.save()\\n3793.         rider2 = rendered_state.get_model("test_alorwrtto", "Rider").objects.create(\\n3794.             pony=pony\\n3795.         )\\n3796.         rider2.friend = rider2\\n3797.         rider2.save()\\n3798.         # Test the database alteration\\n3799.         with connection.schema_editor() as editor:\\n3800.             operation.database_forwards(\\n3801.                 "test_alorwrtto", editor, project_state, new_state\\n3802.             )\\n3803.         self.assertColumnExists("test_alorwrtto_rider", "_order")\\n3804.         # Check for correct value in rows\\n3805.         updated_riders = new_state.apps.get_model(\\n3806.             "test_alorwrtto", "Rider"\\n3807.         ).objects.all()\\n3808.         self.assertEqual(updated_riders[0]._order, 0)\\n3809.         self.assertEqual(updated_riders[1]._order, 0)\\n3810.         # And test reversal\\n3811.         with connection.schema_editor() as editor:\\n3812.             operation.database_backwards(\\n3813.                 "test_alorwrtto", editor, new_state, project_state\\n3814.             )\\n3815.         self.assertColumnNotExists("test_alorwrtto_rider", "_order")\\n3816.         # And deconstruction\\n3817.         definition = operation.deconstruct()\\n3818.         self.assertEqual(definition[0], "AlterOrderWithRespectTo")\\n3819.         self.assertEqual(definition[1], [])\\n3820.         self.assertEqual(\\n3821.             definition[2], {"name": "Rider", "order_with_respect_to": "pony"}\\n3822.         )\\n3823. \\n3824.     def test_alter_model_managers(self):\\n3825.         """\\n3826.         The managers on a model are set.\\n3827.         """\\n3828.         project_state = self.set_up_test_model("test_almoma")\\n3829.         # Test the state alteration\\n3830.         operation = migrations.AlterModelManagers(\\n3831.             "Pony",\\n3832.             managers=[\\n3833.                 ("food_qs", FoodQuerySet.as_manager()),\\n3834.                 ("food_mgr", FoodManager("a", "b")),\\n3835.                 ("food_mgr_kwargs", FoodManager("x", "y", 3, 4)),\\n3836.             ],\\n3837.         )\\n3838.         self.assertEqual(operation.describe(), "Change managers on Pony")\\n3839.         self.assertEqual(operation.migration_name_fragment, "alter_pony_managers")\\n3840.         managers = project_state.models["test_almoma", "pony"].managers\\n3841.         self.assertEqual(managers, [])\\n3842. \\n3843.         new_state = project_state.clone()\\n3844.         operation.state_forwards("test_almoma", new_state)\\n3845.         self.assertIn(("test_almoma", "pony"), new_state.models)\\n3846.         managers = new_state.models["test_almoma", "pony"].managers\\n3847.         self.assertEqual(managers[0][0], "food_qs")\\n3848.         self.assertIsInstance(managers[0][1], models.Manager)\\n3849.         self.assertEqual(managers[1][0], "food_mgr")\\n3850.         self.assertIsInstance(managers[1][1], FoodManager)\\n3851.         self.assertEqual(managers[1][1].args, ("a", "b", 1, 2))\\n3852.         self.assertEqual(managers[2][0], "food_mgr_kwargs")\\n3853.         self.assertIsInstance(managers[2][1], FoodManager)\\n3854.         self.assertEqual(managers[2][1].args, ("x", "y", 3, 4))\\n3855.         rendered_state = new_state.apps\\n3856.         model = rendered_state.get_model("test_almoma", "pony")\\n3857.         self.assertIsInstance(model.food_qs, models.Manager)\\n3858.         self.assertIsInstance(model.food_mgr, FoodManager)\\n3859.         self.assertIsInstance(model.food_mgr_kwargs, FoodManager)\\n3860. \\n3861.     def test_alter_model_managers_emptying(self):\\n3862.         """\\n3863.         The managers on a model are set.\\n3864.         """\\n3865.         project_state = self.set_up_test_model("test_almomae", manager_model=True)\\n3866.         # Test the state alteration\\n3867.         operation = migrations.AlterModelManagers("Food", managers=[])\\n3868.         self.assertEqual(operation.describe(), "Change managers on Food")\\n3869.         self.assertIn(("test_almomae", "food"), project_state.models)\\n3870.         managers = project_state.models["test_almomae", "food"].managers\\n3871.         self.assertEqual(managers[0][0], "food_qs")\\n3872.         self.assertIsInstance(managers[0][1], models.Manager)\\n3873.         self.assertEqual(managers[1][0], "food_mgr")\\n3874.         self.assertIsInstance(managers[1][1], FoodManager)\\n3875.         self.assertEqual(managers[1][1].args, ("a", "b", 1, 2))\\n3876.         self.assertEqual(managers[2][0], "food_mgr_kwargs")\\n3877.         self.assertIsInstance(managers[2][1], FoodManager)\\n3878.         self.assertEqual(managers[2][1].args, ("x", "y", 3, 4))\\n3879. \\n3880.         new_state = project_state.clone()\\n3881.         operation.state_forwards("test_almomae", new_state)\\n3882.         managers = new_state.models["test_almomae", "food"].managers\\n3883.         self.assertEqual(managers, [])\\n3884. \\n3885.     def test_alter_fk(self):\\n3886.         """\\n3887.         Creating and then altering an FK works correctly\\n3888.         and deals with the pending SQL (#23091)\\n3889.         """\\n3890.         project_state = self.set_up_test_model("test_alfk")\\n3891.         # Test adding and then altering the FK in one go\\n3892.         create_operation = migrations.CreateModel(\\n3893.             name="Rider",\\n3894.             fields=[\\n3895.                 ("id", models.AutoField(primary_key=True)),\\n3896.                 ("pony", models.ForeignKey("Pony", models.CASCADE)),\\n3897.             ],\\n3898.         )\\n3899.         create_state = project_state.clone()\\n3900.         create_operation.state_forwards("test_alfk", create_state)\\n3901.         alter_operation = migrations.AlterField(\\n3902.             model_name="Rider",\\n3903.             name="pony",\\n3904.             field=models.ForeignKey("Pony", models.CASCADE, editable=False),\\n3905.         )\\n3906.         alter_state = create_state.clone()\\n3907.         alter_operation.state_forwards("test_alfk", alter_state)\\n3908.         with connection.schema_editor() as editor:\\n3909.             create_operation.database_forwards(\\n3910.                 "test_alfk", editor, project_state, create_state\\n3911.             )\\n3912.             alter_operation.database_forwards(\\n3913.                 "test_alfk", editor, create_state, alter_state\\n3914.             )\\n3915. \\n3916.     def test_alter_fk_non_fk(self):\\n3917.         """\\n3918.         Altering an FK to a non-FK works (#23244)\\n3919.         """\\n3920.         # Test the state alteration\\n3921.         operation = migrations.AlterField(\\n3922.             model_name="Rider",\\n3923.             name="pony",\\n3924.             field=models.FloatField(),\\n3925.         )\\n3926.         project_state, new_state = self.make_test_state(\\n3927.             "test_afknfk", operation, related_model=True\\n3928.         )\\n3929.         # Test the database alteration\\n3930.         self.assertColumnExists("test_afknfk_rider", "pony_id")\\n3931.         self.assertColumnNotExists("test_afknfk_rider", "pony")\\n3932.         with connection.schema_editor() as editor:\\n3933.             operation.database_forwards("test_afknfk", editor, project_state, new_state)\\n3934.         self.assertColumnExists("test_afknfk_rider", "pony")\\n3935.         self.assertColumnNotExists("test_afknfk_rider", "pony_id")\\n3936.         # And test reversal\\n3937.         with connection.schema_editor() as editor:\\n3938.             operation.database_backwards(\\n3939.                 "test_afknfk", editor, new_state, project_state\\n3940.             )\\n3941.         self.assertColumnExists("test_afknfk_rider", "pony_id")\\n3942.         self.assertColumnNotExists("test_afknfk_rider", "pony")\\n3943. \\n3944.     def test_run_sql(self):\\n3945.         """\\n3946.         Tests the RunSQL operation.\\n3947.         """\\n3948.         project_state = self.set_up_test_model("test_runsql")\\n3949.         # Create the operation\\n3950.         operation = migrations.RunSQL(\\n3951.             # Use a multi-line string with a comment to test splitting on\\n3952.             # SQLite and MySQL respectively.\\n3953.             "CREATE TABLE i_love_ponies (id int, special_thing varchar(15));\\\\n"\\n3954.             "INSERT INTO i_love_ponies (id, special_thing) "\\n3955.             "VALUES (1, \\\'i love ponies\\\'); -- this is magic!\\\\n"\\n3956.             "INSERT INTO i_love_ponies (id, special_thing) "\\n3957.             "VALUES (2, \\\'i love django\\\');\\\\n"\\n3958.             "UPDATE i_love_ponies SET special_thing = \\\'Ponies\\\' "\\n3959.             "WHERE special_thing LIKE \\\'%%ponies\\\';"\\n3960.             "UPDATE i_love_ponies SET special_thing = \\\'Django\\\' "\\n3961.             "WHERE special_thing LIKE \\\'%django\\\';",\\n3962.             # Run delete queries to test for parameter substitution failure\\n3963.             # reported in #23426\\n3964.             "DELETE FROM i_love_ponies WHERE special_thing LIKE \\\'%Django%\\\';"\\n3965.             "DELETE FROM i_love_ponies WHERE special_thing LIKE \\\'%%Ponies%%\\\';"\\n3966.             "DROP TABLE i_love_ponies",\\n3967.             state_operations=[\\n3968.                 migrations.CreateModel(\\n3969.                     "SomethingElse", [("id", models.AutoField(primary_key=True))]\\n3970.                 )\\n3971.             ],\\n3972.         )\\n3973.         self.assertEqual(operation.describe(), "Raw SQL operation")\\n3974.         # Test the state alteration\\n3975.         new_state = project_state.clone()\\n3976.         operation.state_forwards("test_runsql", new_state)\\n3977.         self.assertEqual(\\n3978.             len(new_state.models["test_runsql", "somethingelse"].fields), 1\\n3979.         )\\n3980.         # Make sure there\\\'s no table\\n3981.         self.assertTableNotExists("i_love_ponies")\\n3982.         # Test SQL collection\\n3983.         with connection.schema_editor(collect_sql=True) as editor:\\n3984.             operation.database_forwards("test_runsql", editor, project_state, new_state)\\n3985.             self.assertIn("LIKE \\\'%%ponies\\\';", "\\\\n".join(editor.collected_sql))\\n3986.             operation.database_backwards(\\n3987.                 "test_runsql", editor, project_state, new_state\\n3988.             )\\n3989.             self.assertIn("LIKE \\\'%%Ponies%%\\\';", "\\\\n".join(editor.collected_sql))\\n3990.         # Test the database alteration\\n3991.         with connection.schema_editor() as editor:\\n3992.             operation.database_forwards("test_runsql", editor, project_state, new_state)\\n3993.         self.assertTableExists("i_love_ponies")\\n3994.         # Make sure all the SQL was processed\\n3995.         with connection.cursor() as cursor:\\n3996.             cursor.execute("SELECT COUNT(*) FROM i_love_ponies")\\n3997.             self.assertEqual(cursor.fetchall()[0][0], 2)\\n3998.             cursor.execute(\\n3999.                 "SELECT COUNT(*) FROM i_love_ponies WHERE special_thing = \\\'Django\\\'"\\n4000.             )\\n4001.             self.assertEqual(cursor.fetchall()[0][0], 1)\\n4002.             cursor.execute(\\n4003.                 "SELECT COUNT(*) FROM i_love_ponies WHERE special_thing = \\\'Ponies\\\'"\\n4004.             )\\n4005.             self.assertEqual(cursor.fetchall()[0][0], 1)\\n4006.         # And test reversal\\n4007.         self.assertTrue(operation.reversible)\\n4008.         with connection.schema_editor() as editor:\\n4009.             operation.database_backwards(\\n4010.                 "test_runsql", editor, new_state, project_state\\n4011.             )\\n4012.         self.assertTableNotExists("i_love_ponies")\\n4013.         # And deconstruction\\n4014.         definition = operation.deconstruct()\\n4015.         self.assertEqual(definition[0], "RunSQL")\\n4016.         self.assertEqual(definition[1], [])\\n4017.         self.assertEqual(\\n4018.             sorted(definition[2]), ["reverse_sql", "sql", "state_operations"]\\n4019.         )\\n4020.         # And elidable reduction\\n4021.         self.assertIs(False, operation.reduce(operation, []))\\n4022.         elidable_operation = migrations.RunSQL("SELECT 1 FROM void;", elidable=True)\\n4023.         self.assertEqual(elidable_operation.reduce(operation, []), [operation])\\n4024. \\n4025.     def test_run_sql_params(self):\\n4026.         """\\n4027.         #23426 - RunSQL should accept parameters.\\n4028.         """\\n4029.         project_state = self.set_up_test_model("test_runsql")\\n4030.         # Create the operation\\n4031.         operation = migrations.RunSQL(\\n4032.             ["CREATE TABLE i_love_ponies (id int, special_thing varchar(15));"],\\n4033.             ["DROP TABLE i_love_ponies"],\\n4034.         )\\n4035.         param_operation = migrations.RunSQL(\\n4036.             # forwards\\n4037.             (\\n4038.                 "INSERT INTO i_love_ponies (id, special_thing) VALUES (1, \\\'Django\\\');",\\n4039.                 [\\n4040.                     "INSERT INTO i_love_ponies (id, special_thing) VALUES (2, %s);",\\n4041.                     ["Ponies"],\\n4042.                 ],\\n4043.                 (\\n4044.                     "INSERT INTO i_love_ponies (id, special_thing) VALUES (%s, %s);",\\n4045.                     (\\n4046.                         3,\\n4047.                         "Python",\\n4048.                     ),\\n4049.                 ),\\n4050.             ),\\n4051.             # backwards\\n4052.             [\\n4053.                 "DELETE FROM i_love_ponies WHERE special_thing = \\\'Django\\\';",\\n4054.                 ["DELETE FROM i_love_ponies WHERE special_thing = \\\'Ponies\\\';", None],\\n4055.                 (\\n4056.                     "DELETE FROM i_love_ponies WHERE id = %s OR special_thing = %s;",\\n4057.                     [3, "Python"],\\n4058.                 ),\\n4059.             ],\\n4060.         )\\n4061. \\n4062.         # Make sure there\\\'s no table\\n4063.         self.assertTableNotExists("i_love_ponies")\\n4064.         new_state = project_state.clone()\\n4065.         # Test the database alteration\\n4066.         with connection.schema_editor() as editor:\\n4067.             operation.database_forwards("test_runsql", editor, project_state, new_state)\\n4068. \\n4069.         # Test parameter passing\\n4070.         with connection.schema_editor() as editor:\\n4071.             param_operation.database_forwards(\\n4072.                 "test_runsql", editor, project_state, new_state\\n4073.             )\\n4074.         # Make sure all the SQL was processed\\n4075.         with connection.cursor() as cursor:\\n4076.             cursor.execute("SELECT COUNT(*) FROM i_love_ponies")\\n4077.             self.assertEqual(cursor.fetchall()[0][0], 3)\\n4078. \\n4079.         with connection.schema_editor() as editor:\\n4080.             param_operation.database_backwards(\\n4081.                 "test_runsql", editor, new_state, project_state\\n4082.             )\\n4083.         with connection.cursor() as cursor:\\n4084.             cursor.execute("SELECT COUNT(*) FROM i_love_ponies")\\n4085.             self.assertEqual(cursor.fetchall()[0][0], 0)\\n4086. \\n4087.         # And test reversal\\n4088.         with connection.schema_editor() as editor:\\n4089.             operation.database_backwards(\\n4090.                 "test_runsql", editor, new_state, project_state\\n4091.             )\\n4092.         self.assertTableNotExists("i_love_ponies")\\n4093. \\n4094.     def test_run_sql_params_invalid(self):\\n4095.         """\\n4096.         #23426 - RunSQL should fail when a list of statements with an incorrect\\n4097.         number of tuples is given.\\n4098.         """\\n4099.         project_state = self.set_up_test_model("test_runsql")\\n4100.         new_state = project_state.clone()\\n4101.         operation = migrations.RunSQL(\\n4102.             # forwards\\n4103.             [["INSERT INTO foo (bar) VALUES (\\\'buz\\\');"]],\\n4104.             # backwards\\n4105.             (("DELETE FROM foo WHERE bar = \\\'buz\\\';", "invalid", "parameter count"),),\\n4106.         )\\n4107. \\n4108.         with connection.schema_editor() as editor:\\n4109.             with self.assertRaisesMessage(ValueError, "Expected a 2-tuple but got 1"):\\n4110.                 operation.database_forwards(\\n4111.                     "test_runsql", editor, project_state, new_state\\n4112.                 )\\n4113. \\n4114.         with connection.schema_editor() as editor:\\n4115.             with self.assertRaisesMessage(ValueError, "Expected a 2-tuple but got 3"):\\n4116.                 operation.database_backwards(\\n4117.                     "test_runsql", editor, new_state, project_state\\n4118.                 )\\n4119. \\n4120.     def test_run_sql_noop(self):\\n4121.         """\\n4122.         #24098 - Tests no-op RunSQL operations.\\n4123.         """\\n4124.         operation = migrations.RunSQL(migrations.RunSQL.noop, migrations.RunSQL.noop)\\n4125.         with connection.schema_editor() as editor:\\n4126.             operation.database_forwards("test_runsql", editor, None, None)\\n4127.             operation.database_backwards("test_runsql", editor, None, None)\\n4128. \\n4129.     def test_run_sql_add_missing_semicolon_on_collect_sql(self):\\n4130.         project_state = self.set_up_test_model("test_runsql")\\n4131.         new_state = project_state.clone()\\n4132.         tests = [\\n4133.             "INSERT INTO test_runsql_pony (pink, weight) VALUES (1, 1);\\\\n",\\n4134.             "INSERT INTO test_runsql_pony (pink, weight) VALUES (1, 1)\\\\n",\\n4135.         ]\\n4136.         for sql in tests:\\n4137.             with self.subTest(sql=sql):\\n4138.                 operation = migrations.RunSQL(sql, migrations.RunPython.noop)\\n4139.                 with connection.schema_editor(collect_sql=True) as editor:\\n4140.                     operation.database_forwards(\\n4141.                         "test_runsql", editor, project_state, new_state\\n4142.                     )\\n4143.                     collected_sql = "\\\\n".join(editor.collected_sql)\\n4144.                     self.assertEqual(collected_sql.count(";"), 1)\\n4145. \\n4146.     def test_run_python(self):\\n4147.         """\\n4148.         Tests the RunPython operation\\n4149.         """\\n4150. \\n4151.         project_state = self.set_up_test_model("test_runpython", mti_model=True)\\n4152. \\n4153.         # Create the operation\\n4154.         def inner_method(models, schema_editor):\\n4155.             Pony = models.get_model("test_runpython", "Pony")\\n4156.             Pony.objects.create(pink=1, weight=3.55)\\n4157.             Pony.objects.create(weight=5)\\n4158. \\n4159.         def inner_method_reverse(models, schema_editor):\\n4160.             Pony = models.get_model("test_runpython", "Pony")\\n4161.             Pony.objects.filter(pink=1, weight=3.55).delete()\\n4162.             Pony.objects.filter(weight=5).delete()\\n4163. \\n4164.         operation = migrations.RunPython(\\n4165.             inner_method, reverse_code=inner_method_reverse\\n4166.         )\\n4167.         self.assertEqual(operation.describe(), "Raw Python operation")\\n4168.         # Test the state alteration does nothing\\n4169.         new_state = project_state.clone()\\n4170.         operation.state_forwards("test_runpython", new_state)\\n4171.         self.assertEqual(new_state, project_state)\\n4172.         # Test the database alteration\\n4173.         self.assertEqual(\\n4174.             project_state.apps.get_model("test_runpython", "Pony").objects.count(), 0\\n4175.         )\\n4176.         with connection.schema_editor() as editor:\\n4177.             operation.database_forwards(\\n4178.                 "test_runpython", editor, project_state, new_state\\n4179.             )\\n4180.         self.assertEqual(\\n4181.             project_state.apps.get_model("test_runpython", "Pony").objects.count(), 2\\n4182.         )\\n4183.         # Now test reversal\\n4184.         self.assertTrue(operation.reversible)\\n4185.         with connection.schema_editor() as editor:\\n4186.             operation.database_backwards(\\n4187.                 "test_runpython", editor, project_state, new_state\\n4188.             )\\n4189.         self.assertEqual(\\n4190.             project_state.apps.get_model("test_runpython", "Pony").objects.count(), 0\\n4191.         )\\n4192.         # Now test we can\\\'t use a string\\n4193.         with self.assertRaisesMessage(\\n4194.             ValueError, "RunPython must be supplied with a callable"\\n4195.         ):\\n4196.             migrations.RunPython("print \\\'ahahaha\\\'")\\n4197.         # And deconstruction\\n4198.         definition = operation.deconstruct()\\n4199.         self.assertEqual(definition[0], "RunPython")\\n4200.         self.assertEqual(definition[1], [])\\n4201.         self.assertEqual(sorted(definition[2]), ["code", "reverse_code"])\\n4202. \\n4203.         # Also test reversal fails, with an operation identical to above but\\n4204.         # without reverse_code set.\\n4205.         no_reverse_operation = migrations.RunPython(inner_method)\\n4206.         self.assertFalse(no_reverse_operation.reversible)\\n4207.         with connection.schema_editor() as editor:\\n4208.             no_reverse_operation.database_forwards(\\n4209.                 "test_runpython", editor, project_state, new_state\\n4210.             )\\n4211.             with self.assertRaises(NotImplementedError):\\n4212.                 no_reverse_operation.database_backwards(\\n4213.                     "test_runpython", editor, new_state, project_state\\n4214.                 )\\n4215.         self.assertEqual(\\n4216.             project_state.apps.get_model("test_runpython", "Pony").objects.count(), 2\\n4217.         )\\n4218. \\n4219.         def create_ponies(models, schema_editor):\\n4220.             Pony = models.get_model("test_runpython", "Pony")\\n4221.             pony1 = Pony.objects.create(pink=1, weight=3.55)\\n4222.             self.assertIsNot(pony1.pk, None)\\n4223.             pony2 = Pony.objects.create(weight=5)\\n4224.             self.assertIsNot(pony2.pk, None)\\n4225.             self.assertNotEqual(pony1.pk, pony2.pk)\\n4226. \\n4227.         operation = migrations.RunPython(create_ponies)\\n4228.         with connection.schema_editor() as editor:\\n4229.             operation.database_forwards(\\n4230.                 "test_runpython", editor, project_state, new_state\\n4231.             )\\n4232.         self.assertEqual(\\n4233.             project_state.apps.get_model("test_runpython", "Pony").objects.count(), 4\\n4234.         )\\n4235.         # And deconstruction\\n4236.         definition = operation.deconstruct()\\n4237.         self.assertEqual(definition[0], "RunPython")\\n4238.         self.assertEqual(definition[1], [])\\n4239.         self.assertEqual(sorted(definition[2]), ["code"])\\n4240. \\n4241.         def create_shetlandponies(models, schema_editor):\\n4242.             ShetlandPony = models.get_model("test_runpython", "ShetlandPony")\\n4243.             pony1 = ShetlandPony.objects.create(weight=4.0)\\n4244.             self.assertIsNot(pony1.pk, None)\\n4245.             pony2 = ShetlandPony.objects.create(weight=5.0)\\n4246.             self.assertIsNot(pony2.pk, None)\\n4247.             self.assertNotEqual(pony1.pk, pony2.pk)\\n4248. \\n4249.         operation = migrations.RunPython(create_shetlandponies)\\n4250.         with connection.schema_editor() as editor:\\n4251.             operation.database_forwards(\\n4252.                 "test_runpython", editor, project_state, new_state\\n4253.             )\\n4254.         self.assertEqual(\\n4255.             project_state.apps.get_model("test_runpython", "Pony").objects.count(), 6\\n4256.         )\\n4257.         self.assertEqual(\\n4258.             project_state.apps.get_model(\\n4259.                 "test_runpython", "ShetlandPony"\\n4260.             ).objects.count(),\\n4261.             2,\\n4262.         )\\n4263.         # And elidable reduction\\n4264.         self.assertIs(False, operation.reduce(operation, []))\\n4265.         elidable_operation = migrations.RunPython(inner_method, elidable=True)\\n4266.         self.assertEqual(elidable_operation.reduce(operation, []), [operation])\\n4267. \\n4268.     def test_run_python_atomic(self):\\n4269.         """\\n4270.         Tests the RunPython operation correctly handles the "atomic" keyword\\n4271.         """\\n4272.         project_state = self.set_up_test_model("test_runpythonatomic", mti_model=True)\\n4273. \\n4274.         def inner_method(models, schema_editor):\\n4275.             Pony = models.get_model("test_runpythonatomic", "Pony")\\n4276.             Pony.objects.create(pink=1, weight=3.55)\\n4277.             raise ValueError("Adrian hates ponies.")\\n4278. \\n4279.         # Verify atomicity when applying.\\n4280.         atomic_migration = Migration("test", "test_runpythonatomic")\\n4281.         atomic_migration.operations = [\\n4282.             migrations.RunPython(inner_method, reverse_code=inner_method)\\n4283.         ]\\n4284.         non_atomic_migration = Migration("test", "test_runpythonatomic")\\n4285.         non_atomic_migration.operations = [\\n4286.             migrations.RunPython(inner_method, reverse_code=inner_method, atomic=False)\\n4287.         ]\\n4288.         # If we\\\'re a fully-transactional database, both versions should rollback\\n4289.         if connection.features.can_rollback_ddl:\\n4290.             self.assertEqual(\\n4291.                 project_state.apps.get_model(\\n4292.                     "test_runpythonatomic", "Pony"\\n4293.                 ).objects.count(),\\n4294.                 0,\\n4295.             )\\n4296.             with self.assertRaises(ValueError):\\n4297.                 with connection.schema_editor() as editor:\\n4298.                     atomic_migration.apply(project_state, editor)\\n4299.             self.assertEqual(\\n4300.                 project_state.apps.get_model(\\n4301.                     "test_runpythonatomic", "Pony"\\n4302.                 ).objects.count(),\\n4303.                 0,\\n4304.             )\\n4305.             with self.assertRaises(ValueError):\\n4306.                 with connection.schema_editor() as editor:\\n4307.                     non_atomic_migration.apply(project_state, editor)\\n4308.             self.assertEqual(\\n4309.                 project_state.apps.get_model(\\n4310.                     "test_runpythonatomic", "Pony"\\n4311.                 ).objects.count(),\\n4312.                 0,\\n4313.             )\\n4314.         # Otherwise, the non-atomic operation should leave a row there\\n4315.         else:\\n4316.             self.assertEqual(\\n4317.                 project_state.apps.get_model(\\n4318.                     "test_runpythonatomic", "Pony"\\n4319.                 ).objects.count(),\\n4320.                 0,\\n4321.             )\\n4322.             with self.assertRaises(ValueError):\\n4323.                 with connection.schema_editor() as editor:\\n4324.                     atomic_migration.apply(project_state, editor)\\n4325.             self.assertEqual(\\n4326.                 project_state.apps.get_model(\\n4327.                     "test_runpythonatomic", "Pony"\\n4328.                 ).objects.count(),\\n4329.                 0,\\n4330.             )\\n4331.             with self.assertRaises(ValueError):\\n4332.                 with connection.schema_editor() as editor:\\n4333.                     non_atomic_migration.apply(project_state, editor)\\n4334.             self.assertEqual(\\n4335.                 project_state.apps.get_model(\\n4336.                     "test_runpythonatomic", "Pony"\\n4337.                 ).objects.count(),\\n4338.                 1,\\n4339.             )\\n4340.         # Reset object count to zero and verify atomicity when unapplying.\\n4341.         project_state.apps.get_model(\\n4342.             "test_runpythonatomic", "Pony"\\n4343.         ).objects.all().delete()\\n4344.         # On a fully-transactional database, both versions rollback.\\n4345.         if connection.features.can_rollback_ddl:\\n4346.             self.assertEqual(\\n4347.                 project_state.apps.get_model(\\n4348.                     "test_runpythonatomic", "Pony"\\n4349.                 ).objects.count(),\\n4350.                 0,\\n4351.             )\\n4352.             with self.assertRaises(ValueError):\\n4353.                 with connection.schema_editor() as editor:\\n4354.                     atomic_migration.unapply(project_state, editor)\\n4355.             self.assertEqual(\\n4356.                 project_state.apps.get_model(\\n4357.                     "test_runpythonatomic", "Pony"\\n4358.                 ).objects.count(),\\n4359.                 0,\\n4360.             )\\n4361.             with self.assertRaises(ValueError):\\n4362.                 with connection.schema_editor() as editor:\\n4363.                     non_atomic_migration.unapply(project_state, editor)\\n4364.             self.assertEqual(\\n4365.                 project_state.apps.get_model(\\n4366.                     "test_runpythonatomic", "Pony"\\n4367.                 ).objects.count(),\\n4368.                 0,\\n4369.             )\\n4370.         # Otherwise, the non-atomic operation leaves a row there.\\n4371.         else:\\n4372.             self.assertEqual(\\n4373.                 project_state.apps.get_model(\\n4374.                     "test_runpythonatomic", "Pony"\\n4375.                 ).objects.count(),\\n4376.                 0,\\n4377.             )\\n4378.             with self.assertRaises(ValueError):\\n4379.                 with connection.schema_editor() as editor:\\n4380.                     atomic_migration.unapply(project_state, editor)\\n4381.             self.assertEqual(\\n4382.                 project_state.apps.get_model(\\n4383.                     "test_runpythonatomic", "Pony"\\n4384.                 ).objects.count(),\\n4385.                 0,\\n4386.             )\\n4387.             with self.assertRaises(ValueError):\\n4388.                 with connection.schema_editor() as editor:\\n4389.                     non_atomic_migration.unapply(project_state, editor)\\n4390.             self.assertEqual(\\n4391.                 project_state.apps.get_model(\\n4392.                     "test_runpythonatomic", "Pony"\\n4393.                 ).objects.count(),\\n4394.                 1,\\n4395.             )\\n4396.         # Verify deconstruction.\\n4397.         definition = non_atomic_migration.operations[0].deconstruct()\\n4398.         self.assertEqual(definition[0], "RunPython")\\n4399.         self.assertEqual(definition[1], [])\\n4400.         self.assertEqual(sorted(definition[2]), ["atomic", "code", "reverse_code"])\\n4401. \\n4402.     def test_run_python_related_assignment(self):\\n4403.         """\\n4404.         #24282 - Model changes to a FK reverse side update the model\\n4405.         on the FK side as well.\\n4406.         """\\n4407. \\n4408.         def inner_method(models, schema_editor):\\n4409.             Author = models.get_model("test_authors", "Author")\\n4410.             Book = models.get_model("test_books", "Book")\\n4411.             author = Author.objects.create(name="Hemingway")\\n4412.             Book.objects.create(title="Old Man and The Sea", author=author)\\n4413. \\n4414.         create_author = migrations.CreateModel(\\n4415.             "Author",\\n4416.             [\\n4417.                 ("id", models.AutoField(primary_key=True)),\\n4418.                 ("name", models.CharField(max_length=100)),\\n4419.             ],\\n4420.             options={},\\n4421.         )\\n4422.         create_book = migrations.CreateModel(\\n4423.             "Book",\\n4424.             [\\n4425.                 ("id", models.AutoField(primary_key=True)),\\n4426.                 ("title", models.CharField(max_length=100)),\\n4427.                 ("author", models.ForeignKey("test_authors.Author", models.CASCADE)),\\n4428.             ],\\n4429.             options={},\\n4430.         )\\n4431.         add_hometown = migrations.AddField(\\n4432.             "Author",\\n4433.             "hometown",\\n4434.             models.CharField(max_length=100),\\n4435.         )\\n4436.         create_old_man = migrations.RunPython(inner_method, inner_method)\\n4437. \\n4438.         project_state = ProjectState()\\n4439.         new_state = project_state.clone()\\n4440.         with connection.schema_editor() as editor:\\n4441.             create_author.state_forwards("test_authors", new_state)\\n4442.             create_author.database_forwards(\\n4443.                 "test_authors", editor, project_state, new_state\\n4444.             )\\n4445.         project_state = new_state\\n4446.         new_state = new_state.clone()\\n4447.         with connection.schema_editor() as editor:\\n4448.             create_book.state_forwards("test_books", new_state)\\n4449.             create_book.database_forwards(\\n4450.                 "test_books", editor, project_state, new_state\\n4451.             )\\n4452.         project_state = new_state\\n4453.         new_state = new_state.clone()\\n4454.         with connection.schema_editor() as editor:\\n4455.             add_hometown.state_forwards("test_authors", new_state)\\n4456.             add_hometown.database_forwards(\\n4457.                 "test_authors", editor, project_state, new_state\\n4458.             )\\n4459.         project_state = new_state\\n4460.         new_state = new_state.clone()\\n4461.         with connection.schema_editor() as editor:\\n4462.             create_old_man.state_forwards("test_books", new_state)\\n4463.             create_old_man.database_forwards(\\n4464.                 "test_books", editor, project_state, new_state\\n4465.             )\\n4466. \\n4467.     def test_model_with_bigautofield(self):\\n4468.         """\\n4469.         A model with BigAutoField can be created.\\n4470.         """\\n4471. \\n4472.         def create_data(models, schema_editor):\\n4473.             Author = models.get_model("test_author", "Author")\\n4474.             Book = models.get_model("test_book", "Book")\\n4475.             author1 = Author.objects.create(name="Hemingway")\\n4476.             Book.objects.create(title="Old Man and The Sea", author=author1)\\n4477.             Book.objects.create(id=2**33, title="A farewell to arms", author=author1)\\n4478. \\n4479.             author2 = Author.objects.create(id=2**33, name="Remarque")\\n4480.             Book.objects.create(title="All quiet on the western front", author=author2)\\n4481.             Book.objects.create(title="Arc de Triomphe", author=author2)\\n4482. \\n4483.         create_author = migrations.CreateModel(\\n4484.             "Author",\\n4485.             [\\n4486.                 ("id", models.BigAutoField(primary_key=True)),\\n4487.                 ("name", models.CharField(max_length=100)),\\n4488.             ],\\n4489.             options={},\\n4490.         )\\n4491.         create_book = migrations.CreateModel(\\n4492.             "Book",\\n4493.             [\\n4494.                 ("id", models.BigAutoField(primary_key=True)),\\n4495.                 ("title", models.CharField(max_length=100)),\\n4496.                 (\\n4497.                     "author",\\n4498.                     models.ForeignKey(\\n4499.                         to="test_author.Author", on_delete=models.CASCADE\\n4500.                     ),\\n4501.                 ),\\n4502.             ],\\n4503.             options={},\\n4504.         )\\n4505.         fill_data = migrations.RunPython(create_data)\\n4506. \\n4507.         project_state = ProjectState()\\n4508.         new_state = project_state.clone()\\n4509.         with connection.schema_editor() as editor:\\n4510.             create_author.state_forwards("test_author", new_state)\\n4511.             create_author.database_forwards(\\n4512.                 "test_author", editor, project_state, new_state\\n4513.             )\\n4514. \\n4515.         project_state = new_state\\n4516.         new_state = new_state.clone()\\n4517.         with connection.schema_editor() as editor:\\n4518.             create_book.state_forwards("test_book", new_state)\\n4519.             create_book.database_forwards("test_book", editor, project_state, new_state)\\n4520. \\n4521.         project_state = new_state\\n4522.         new_state = new_state.clone()\\n4523.         with connection.schema_editor() as editor:\\n4524.             fill_data.state_forwards("fill_data", new_state)\\n4525.             fill_data.database_forwards("fill_data", editor, project_state, new_state)\\n4526. \\n4527.     def _test_autofield_foreignfield_growth(\\n4528.         self, source_field, target_field, target_value\\n4529.     ):\\n4530.         """\\n4531.         A field may be migrated in the following ways:\\n4532. \\n4533.         - AutoField to BigAutoField\\n4534.         - SmallAutoField to AutoField\\n4535.         - SmallAutoField to BigAutoField\\n4536.         """\\n4537. \\n4538.         def create_initial_data(models, schema_editor):\\n4539.             Article = models.get_model("test_article", "Article")\\n4540.             Blog = models.get_model("test_blog", "Blog")\\n4541.             blog = Blog.objects.create(name="web development done right")\\n4542.             Article.objects.create(name="Frameworks", blog=blog)\\n4543.             Article.objects.create(name="Programming Languages", blog=blog)\\n4544. \\n4545.         def create_big_data(models, schema_editor):\\n4546.             Article = models.get_model("test_article", "Article")\\n4547.             Blog = models.get_model("test_blog", "Blog")\\n4548.             blog2 = Blog.objects.create(name="Frameworks", id=target_value)\\n4549.             Article.objects.create(name="Django", blog=blog2)\\n4550.             Article.objects.create(id=target_value, name="Django2", blog=blog2)\\n4551. \\n4552.         create_blog = migrations.CreateModel(\\n4553.             "Blog",\\n4554.             [\\n4555.                 ("id", source_field(primary_key=True)),\\n4556.                 ("name", models.CharField(max_length=100)),\\n4557.             ],\\n4558.             options={},\\n4559.         )\\n4560.         create_article = migrations.CreateModel(\\n4561.             "Article",\\n4562.             [\\n4563.                 ("id", source_field(primary_key=True)),\\n4564.                 (\\n4565.                     "blog",\\n4566.                     models.ForeignKey(to="test_blog.Blog", on_delete=models.CASCADE),\\n4567.                 ),\\n4568.                 ("name", models.CharField(max_length=100)),\\n4569.                 ("data", models.TextField(default="")),\\n4570.             ],\\n4571.             options={},\\n4572.         )\\n4573.         fill_initial_data = migrations.RunPython(\\n4574.             create_initial_data, create_initial_data\\n4575.         )\\n4576.         fill_big_data = migrations.RunPython(create_big_data, create_big_data)\\n4577. \\n4578.         grow_article_id = migrations.AlterField(\\n4579.             "Article", "id", target_field(primary_key=True)\\n4580.         )\\n4581.         grow_blog_id = migrations.AlterField(\\n4582.             "Blog", "id", target_field(primary_key=True)\\n4583.         )\\n4584. \\n4585.         project_state = ProjectState()\\n4586.         new_state = project_state.clone()\\n4587.         with connection.schema_editor() as editor:\\n4588.             create_blog.state_forwards("test_blog", new_state)\\n4589.             create_blog.database_forwards("test_blog", editor, project_state, new_state)\\n4590. \\n4591.         project_state = new_state\\n4592.         new_state = new_state.clone()\\n4593.         with connection.schema_editor() as editor:\\n4594.             create_article.state_forwards("test_article", new_state)\\n4595.             create_article.database_forwards(\\n4596.                 "test_article", editor, project_state, new_state\\n4597.             )\\n4598. \\n4599.         project_state = new_state\\n4600.         new_state = new_state.clone()\\n4601.         with connection.schema_editor() as editor:\\n4602.             fill_initial_data.state_forwards("fill_initial_data", new_state)\\n4603.             fill_initial_data.database_forwards(\\n4604.                 "fill_initial_data", editor, project_state, new_state\\n4605.             )\\n4606. \\n4607.         project_state = new_state\\n4608.         new_state = new_state.clone()\\n4609.         with connection.schema_editor() as editor:\\n4610.             grow_article_id.state_forwards("test_article", new_state)\\n4611.             grow_article_id.database_forwards(\\n4612.                 "test_article", editor, project_state, new_state\\n4613.             )\\n4614. \\n4615.         state = new_state.clone()\\n4616.         article = state.apps.get_model("test_article.Article")\\n4617.         self.assertIsInstance(article._meta.pk, target_field)\\n4618. \\n4619.         project_state = new_state\\n4620.         new_state = new_state.clone()\\n4621.         with connection.schema_editor() as editor:\\n4622.             grow_blog_id.state_forwards("test_blog", new_state)\\n4623.             grow_blog_id.database_forwards(\\n4624.                 "test_blog", editor, project_state, new_state\\n4625.             )\\n4626. \\n4627.         state = new_state.clone()\\n4628.         blog = state.apps.get_model("test_blog.Blog")\\n4629.         self.assertIsInstance(blog._meta.pk, target_field)\\n4630. \\n4631.         project_state = new_state\\n4632.         new_state = new_state.clone()\\n4633.         with connection.schema_editor() as editor:\\n4634.             fill_big_data.state_forwards("fill_big_data", new_state)\\n4635.             fill_big_data.database_forwards(\\n4636.                 "fill_big_data", editor, project_state, new_state\\n4637.             )\\n4638. \\n4639.     def test_autofield__bigautofield_foreignfield_growth(self):\\n4640.         """A field may be migrated from AutoField to BigAutoField."""\\n4641.         self._test_autofield_foreignfield_growth(\\n4642.             models.AutoField,\\n4643.             models.BigAutoField,\\n4644.             2**33,\\n4645.         )\\n4646. \\n4647.     def test_smallfield_autofield_foreignfield_growth(self):\\n4648.         """A field may be migrated from SmallAutoField to AutoField."""\\n4649.         self._test_autofield_foreignfield_growth(\\n4650.             models.SmallAutoField,\\n4651.             models.AutoField,\\n4652.             2**22,\\n4653.         )\\n4654. \\n4655.     def test_smallfield_bigautofield_foreignfield_growth(self):\\n4656.         """A field may be migrated from SmallAutoField to BigAutoField."""\\n4657.         self._test_autofield_foreignfield_growth(\\n4658.             models.SmallAutoField,\\n4659.             models.BigAutoField,\\n4660.             2**33,\\n4661.         )\\n4662. \\n4663.     def test_run_python_noop(self):\\n4664.         """\\n4665.         #24098 - Tests no-op RunPython operations.\\n4666.         """\\n4667.         project_state = ProjectState()\\n4668.         new_state = project_state.clone()\\n4669.         operation = migrations.RunPython(\\n4670.             migrations.RunPython.noop, migrations.RunPython.noop\\n4671.         )\\n4672.         with connection.schema_editor() as editor:\\n4673.             operation.database_forwards(\\n4674.                 "test_runpython", editor, project_state, new_state\\n4675.             )\\n4676.             operation.database_backwards(\\n4677.                 "test_runpython", editor, new_state, project_state\\n4678.             )\\n4679. \\n4680.     def test_separate_database_and_state(self):\\n4681.         """\\n4682.         Tests the SeparateDatabaseAndState operation.\\n4683.         """\\n4684.         project_state = self.set_up_test_model("test_separatedatabaseandstate")\\n4685.         # Create the operation\\n4686.         database_operation = migrations.RunSQL(\\n4687.             "CREATE TABLE i_love_ponies (id int, special_thing int);",\\n4688.             "DROP TABLE i_love_ponies;",\\n4689.         )\\n4690.         state_operation = migrations.CreateModel(\\n4691.             "SomethingElse", [("id", models.AutoField(primary_key=True))]\\n4692.         )\\n4693.         operation = migrations.SeparateDatabaseAndState(\\n4694.             state_operations=[state_operation], database_operations=[database_operation]\\n4695.         )\\n4696.         self.assertEqual(\\n4697.             operation.describe(), "Custom state/database change combination"\\n4698.         )\\n4699.         # Test the state alteration\\n4700.         new_state = project_state.clone()\\n4701.         operation.state_forwards("test_separatedatabaseandstate", new_state)\\n4702.         self.assertEqual(\\n4703.             len(\\n4704.                 new_state.models[\\n4705.                     "test_separatedatabaseandstate", "somethingelse"\\n4706.                 ].fields\\n4707.             ),\\n4708.             1,\\n4709.         )\\n4710.         # Make sure there\\\'s no table\\n4711.         self.assertTableNotExists("i_love_ponies")\\n4712.         # Test the database alteration\\n4713.         with connection.schema_editor() as editor:\\n4714.             operation.database_forwards(\\n4715.                 "test_separatedatabaseandstate", editor, project_state, new_state\\n4716.             )\\n4717.         self.assertTableExists("i_love_ponies")\\n4718.         # And test reversal\\n4719.         self.assertTrue(operation.reversible)\\n4720.         with connection.schema_editor() as editor:\\n4721.             operation.database_backwards(\\n4722.                 "test_separatedatabaseandstate", editor, new_state, project_state\\n4723.             )\\n4724.         self.assertTableNotExists("i_love_ponies")\\n4725.         # And deconstruction\\n4726.         definition = operation.deconstruct()\\n4727.         self.assertEqual(definition[0], "SeparateDatabaseAndState")\\n4728.         self.assertEqual(definition[1], [])\\n4729.         self.assertEqual(\\n4730.             sorted(definition[2]), ["database_operations", "state_operations"]\\n4731.         )\\n4732. \\n4733.     def test_separate_database_and_state2(self):\\n4734.         """\\n4735.         A complex SeparateDatabaseAndState operation: Multiple operations both\\n4736.         for state and database. Verify the state dependencies within each list\\n4737.         and that state ops don\\\'t affect the database.\\n4738.         """\\n4739.         app_label = "test_separatedatabaseandstate2"\\n4740.         project_state = self.set_up_test_model(app_label)\\n4741.         # Create the operation\\n4742.         database_operations = [\\n4743.             migrations.CreateModel(\\n4744.                 "ILovePonies",\\n4745.                 [("id", models.AutoField(primary_key=True))],\\n4746.                 options={"db_table": "iloveponies"},\\n4747.             ),\\n4748.             migrations.CreateModel(\\n4749.                 "ILoveMorePonies",\\n4750.                 # We use IntegerField and not AutoField because\\n4751.                 # the model is going to be deleted immediately\\n4752.                 # and with an AutoField this fails on Oracle\\n4753.                 [("id", models.IntegerField(primary_key=True))],\\n4754.                 options={"db_table": "ilovemoreponies"},\\n4755.             ),\\n4756.             migrations.DeleteModel("ILoveMorePonies"),\\n4757.             migrations.CreateModel(\\n4758.                 "ILoveEvenMorePonies",\\n4759.                 [("id", models.AutoField(primary_key=True))],\\n4760.                 options={"db_table": "iloveevenmoreponies"},\\n4761.             ),\\n4762.         ]\\n4763.         state_operations = [\\n4764.             migrations.CreateModel(\\n4765.                 "SomethingElse",\\n4766.                 [("id", models.AutoField(primary_key=True))],\\n4767.                 options={"db_table": "somethingelse"},\\n4768.             ),\\n4769.             migrations.DeleteModel("SomethingElse"),\\n4770.             migrations.CreateModel(\\n4771.                 "SomethingCompletelyDifferent",\\n4772.                 [("id", models.AutoField(primary_key=True))],\\n4773.                 options={"db_table": "somethingcompletelydifferent"},\\n4774.             ),\\n4775.         ]\\n4776.         operation = migrations.SeparateDatabaseAndState(\\n4777.             state_operations=state_operations,\\n4778.             database_operations=database_operations,\\n4779.         )\\n4780.         # Test the state alteration\\n4781.         new_state = project_state.clone()\\n4782.         operation.state_forwards(app_label, new_state)\\n4783. \\n4784.         def assertModelsAndTables(after_db):\\n4785.             # Tables and models exist, or don\\\'t, as they should:\\n4786.             self.assertNotIn((app_label, "somethingelse"), new_state.models)\\n4787.             self.assertEqual(\\n4788.                 len(new_state.models[app_label, "somethingcompletelydifferent"].fields),\\n4789.                 1,\\n4790.             )\\n4791.             self.assertNotIn((app_label, "iloveponiesonies"), new_state.models)\\n4792.             self.assertNotIn((app_label, "ilovemoreponies"), new_state.models)\\n4793.             self.assertNotIn((app_label, "iloveevenmoreponies"), new_state.models)\\n4794.             self.assertTableNotExists("somethingelse")\\n4795.             self.assertTableNotExists("somethingcompletelydifferent")\\n4796.             self.assertTableNotExists("ilovemoreponies")\\n4797.             if after_db:\\n4798.                 self.assertTableExists("iloveponies")\\n4799.                 self.assertTableExists("iloveevenmoreponies")\\n4800.             else:\\n4801.                 self.assertTableNotExists("iloveponies")\\n4802.                 self.assertTableNotExists("iloveevenmoreponies")\\n4803. \\n4804.         assertModelsAndTables(after_db=False)\\n4805.         # Test the database alteration\\n4806.         with connection.schema_editor() as editor:\\n4807.             operation.database_forwards(app_label, editor, project_state, new_state)\\n4808.         assertModelsAndTables(after_db=True)\\n4809.         # And test reversal\\n4810.         self.assertTrue(operation.reversible)\\n4811.         with connection.schema_editor() as editor:\\n4812.             operation.database_backwards(app_label, editor, new_state, project_state)\\n4813.         assertModelsAndTables(after_db=False)\', \'start_line_number\': 19, \'end_line_number\': 4813}\n\n{\'relative_path\': \'tests/schema/tests.py\', \'content\': \'1. import datetime\\n2. import itertools\\n3. import unittest\\n4. from copy import copy\\n5. from unittest import mock\\n6. \\n7. from django.core.exceptions import FieldError\\n8. from django.core.management.color import no_style\\n9. from django.db import (\\n10.     DatabaseError,\\n11.     DataError,\\n12.     IntegrityError,\\n13.     OperationalError,\\n14.     connection,\\n15. )\\n16. from django.db.models import (\\n17.     CASCADE,\\n18.     PROTECT,\\n19.     AutoField,\\n20.     BigAutoField,\\n21.     BigIntegerField,\\n22.     BinaryField,\\n23.     BooleanField,\\n24.     CharField,\\n25.     CheckConstraint,\\n26.     DateField,\\n27.     DateTimeField,\\n28.     DecimalField,\\n29.     DurationField,\\n30.     F,\\n31.     FloatField,\\n32.     ForeignKey,\\n33.     ForeignObject,\\n34.     Index,\\n35.     IntegerField,\\n36.     JSONField,\\n37.     ManyToManyField,\\n38.     Model,\\n39.     OneToOneField,\\n40.     OrderBy,\\n41.     PositiveIntegerField,\\n42.     Q,\\n43.     SlugField,\\n44.     SmallAutoField,\\n45.     SmallIntegerField,\\n46.     TextField,\\n47.     TimeField,\\n48.     UniqueConstraint,\\n49.     UUIDField,\\n50.     Value,\\n51. )\\n52. from django.db.models.fields.json import KeyTextTransform\\n53. from django.db.models.functions import Abs, Cast, Collate, Lower, Random, Upper\\n54. from django.db.models.indexes import IndexExpression\\n55. from django.db.transaction import TransactionManagementError, atomic\\n56. from django.test import TransactionTestCase, skipIfDBFeature, skipUnlessDBFeature\\n57. from django.test.utils import CaptureQueriesContext, isolate_apps, register_lookup\\n58. \\n59. from .fields import CustomManyToManyField, InheritedManyToManyField, MediumBlobField\\n60. from .models import (\\n61.     Author,\\n62.     AuthorCharFieldWithIndex,\\n63.     AuthorTextFieldWithIndex,\\n64.     AuthorWithDefaultHeight,\\n65.     AuthorWithEvenLongerName,\\n66.     AuthorWithIndexedName,\\n67.     AuthorWithIndexedNameAndBirthday,\\n68.     AuthorWithUniqueName,\\n69.     AuthorWithUniqueNameAndBirthday,\\n70.     Book,\\n71.     BookForeignObj,\\n72.     BookWeak,\\n73.     BookWithLongName,\\n74.     BookWithO2O,\\n75.     BookWithoutAuthor,\\n76.     BookWithSlug,\\n77.     IntegerPK,\\n78.     Node,\\n79.     Note,\\n80.     NoteRename,\\n81.     Tag,\\n82.     TagIndexed,\\n83.     TagM2MTest,\\n84.     TagUniqueRename,\\n85.     Thing,\\n86.     UniqueTest,\\n87.     new_apps,\\n88. )\\n89. \\n90. \', \'start_line_number\': 1, \'end_line_number\': 90}\n\n{\'relative_path\': \'tests/schema/tests.py\', \'content\': \'91. class SchemaTests(TransactionTestCase):\\n92.     """\\n93.     Tests for the schema-alteration code.\\n94. \\n95.     Be aware that these tests are more liable than most to false results,\\n96.     as sometimes the code to check if a test has worked is almost as complex\\n97.     as the code it is testing.\\n98.     """\\n99. \\n100.     available_apps = []\\n101. \\n102.     models = [\\n103.         Author,\\n104.         AuthorCharFieldWithIndex,\\n105.         AuthorTextFieldWithIndex,\\n106.         AuthorWithDefaultHeight,\\n107.         AuthorWithEvenLongerName,\\n108.         Book,\\n109.         BookWeak,\\n110.         BookWithLongName,\\n111.         BookWithO2O,\\n112.         BookWithSlug,\\n113.         IntegerPK,\\n114.         Node,\\n115.         Note,\\n116.         Tag,\\n117.         TagIndexed,\\n118.         TagM2MTest,\\n119.         TagUniqueRename,\\n120.         Thing,\\n121.         UniqueTest,\\n122.     ]\\n123. \\n124.     # Utility functions\\n125. \\n126.     def setUp(self):\\n127.         # local_models should contain test dependent model classes that will be\\n128.         # automatically removed from the app cache on test tear down.\\n129.         self.local_models = []\\n130.         # isolated_local_models contains models that are in test methods\\n131.         # decorated with @isolate_apps.\\n132.         self.isolated_local_models = []\\n133. \\n134.     def tearDown(self):\\n135.         # Delete any tables made for our models\\n136.         self.delete_tables()\\n137.         new_apps.clear_cache()\\n138.         for model in new_apps.get_models():\\n139.             model._meta._expire_cache()\\n140.         if "schema" in new_apps.all_models:\\n141.             for model in self.local_models:\\n142.                 for many_to_many in model._meta.many_to_many:\\n143.                     through = many_to_many.remote_field.through\\n144.                     if through and through._meta.auto_created:\\n145.                         del new_apps.all_models["schema"][through._meta.model_name]\\n146.                 del new_apps.all_models["schema"][model._meta.model_name]\\n147.         if self.isolated_local_models:\\n148.             with connection.schema_editor() as editor:\\n149.                 for model in self.isolated_local_models:\\n150.                     editor.delete_model(model)\\n151. \\n152.     def delete_tables(self):\\n153.         "Deletes all model tables for our models for a clean test environment"\\n154.         converter = connection.introspection.identifier_converter\\n155.         with connection.schema_editor() as editor:\\n156.             connection.disable_constraint_checking()\\n157.             table_names = connection.introspection.table_names()\\n158.             if connection.features.ignores_table_name_case:\\n159.                 table_names = [table_name.lower() for table_name in table_names]\\n160.             for model in itertools.chain(SchemaTests.models, self.local_models):\\n161.                 tbl = converter(model._meta.db_table)\\n162.                 if connection.features.ignores_table_name_case:\\n163.                     tbl = tbl.lower()\\n164.                 if tbl in table_names:\\n165.                     editor.delete_model(model)\\n166.                     table_names.remove(tbl)\\n167.             connection.enable_constraint_checking()\\n168. \\n169.     def column_classes(self, model):\\n170.         with connection.cursor() as cursor:\\n171.             columns = {\\n172.                 d[0]: (connection.introspection.get_field_type(d[1], d), d)\\n173.                 for d in connection.introspection.get_table_description(\\n174.                     cursor,\\n175.                     model._meta.db_table,\\n176.                 )\\n177.             }\\n178.         # SQLite has a different format for field_type\\n179.         for name, (type, desc) in columns.items():\\n180.             if isinstance(type, tuple):\\n181.                 columns[name] = (type[0], desc)\\n182.         return columns\\n183. \\n184.     def get_primary_key(self, table):\\n185.         with connection.cursor() as cursor:\\n186.             return connection.introspection.get_primary_key_column(cursor, table)\\n187. \\n188.     def get_indexes(self, table):\\n189.         """\\n190.         Get the indexes on the table using a new cursor.\\n191.         """\\n192.         with connection.cursor() as cursor:\\n193.             return [\\n194.                 c["columns"][0]\\n195.                 for c in connection.introspection.get_constraints(\\n196.                     cursor, table\\n197.                 ).values()\\n198.                 if c["index"] and len(c["columns"]) == 1\\n199.             ]\\n200. \\n201.     def get_uniques(self, table):\\n202.         with connection.cursor() as cursor:\\n203.             return [\\n204.                 c["columns"][0]\\n205.                 for c in connection.introspection.get_constraints(\\n206.                     cursor, table\\n207.                 ).values()\\n208.                 if c["unique"] and len(c["columns"]) == 1\\n209.             ]\\n210. \\n211.     def get_constraints(self, table):\\n212.         """\\n213.         Get the constraints on a table using a new cursor.\\n214.         """\\n215.         with connection.cursor() as cursor:\\n216.             return connection.introspection.get_constraints(cursor, table)\\n217. \\n218.     def get_constraints_for_column(self, model, column_name):\\n219.         constraints = self.get_constraints(model._meta.db_table)\\n220.         constraints_for_column = []\\n221.         for name, details in constraints.items():\\n222.             if details["columns"] == [column_name]:\\n223.                 constraints_for_column.append(name)\\n224.         return sorted(constraints_for_column)\\n225. \\n226.     def check_added_field_default(\\n227.         self,\\n228.         schema_editor,\\n229.         model,\\n230.         field,\\n231.         field_name,\\n232.         expected_default,\\n233.         cast_function=None,\\n234.     ):\\n235.         with connection.cursor() as cursor:\\n236.             schema_editor.add_field(model, field)\\n237.             cursor.execute(\\n238.                 "SELECT {} FROM {};".format(field_name, model._meta.db_table)\\n239.             )\\n240.             database_default = cursor.fetchall()[0][0]\\n241.             if cast_function and type(database_default) != type(expected_default):\\n242.                 database_default = cast_function(database_default)\\n243.             self.assertEqual(database_default, expected_default)\\n244. \\n245.     def get_constraints_count(self, table, column, fk_to):\\n246.         """\\n247.         Return a dict with keys \\\'fks\\\', \\\'uniques, and \\\'indexes\\\' indicating the\\n248.         number of foreign keys, unique constraints, and indexes on\\n249.         `table`.`column`. The `fk_to` argument is a 2-tuple specifying the\\n250.         expected foreign key relationship\\\'s (table, column).\\n251.         """\\n252.         with connection.cursor() as cursor:\\n253.             constraints = connection.introspection.get_constraints(cursor, table)\\n254.         counts = {"fks": 0, "uniques": 0, "indexes": 0}\\n255.         for c in constraints.values():\\n256.             if c["columns"] == [column]:\\n257.                 if c["foreign_key"] == fk_to:\\n258.                     counts["fks"] += 1\\n259.                 if c["unique"]:\\n260.                     counts["uniques"] += 1\\n261.                 elif c["index"]:\\n262.                     counts["indexes"] += 1\\n263.         return counts\\n264. \\n265.     def get_column_collation(self, table, column):\\n266.         with connection.cursor() as cursor:\\n267.             return next(\\n268.                 f.collation\\n269.                 for f in connection.introspection.get_table_description(cursor, table)\\n270.                 if f.name == column\\n271.             )\\n272. \\n273.     def assertIndexOrder(self, table, index, order):\\n274.         constraints = self.get_constraints(table)\\n275.         self.assertIn(index, constraints)\\n276.         index_orders = constraints[index]["orders"]\\n277.         self.assertTrue(\\n278.             all(val == expected for val, expected in zip(index_orders, order))\\n279.         )\\n280. \\n281.     def assertForeignKeyExists(self, model, column, expected_fk_table, field="id"):\\n282.         """\\n283.         Fail if the FK constraint on `model.Meta.db_table`.`column` to\\n284.         `expected_fk_table`.id doesn\\\'t exist.\\n285.         """\\n286.         constraints = self.get_constraints(model._meta.db_table)\\n287.         constraint_fk = None\\n288.         for details in constraints.values():\\n289.             if details["columns"] == [column] and details["foreign_key"]:\\n290.                 constraint_fk = details["foreign_key"]\\n291.                 break\\n292.         self.assertEqual(constraint_fk, (expected_fk_table, field))\\n293. \\n294.     def assertForeignKeyNotExists(self, model, column, expected_fk_table):\\n295.         with self.assertRaises(AssertionError):\\n296.             self.assertForeignKeyExists(model, column, expected_fk_table)\\n297. \\n298.     # Tests\\n299.     def test_creation_deletion(self):\\n300.         """\\n301.         Tries creating a model\\\'s table, and then deleting it.\\n302.         """\\n303.         with connection.schema_editor() as editor:\\n304.             # Create the table\\n305.             editor.create_model(Author)\\n306.             # The table is there\\n307.             list(Author.objects.all())\\n308.             # Clean up that table\\n309.             editor.delete_model(Author)\\n310.             # No deferred SQL should be left over.\\n311.             self.assertEqual(editor.deferred_sql, [])\\n312.         # The table is gone\\n313.         with self.assertRaises(DatabaseError):\\n314.             list(Author.objects.all())\\n315. \\n316.     @skipUnlessDBFeature("supports_foreign_keys")\\n317.     def test_fk(self):\\n318.         "Creating tables out of FK order, then repointing, works"\\n319.         # Create the table\\n320.         with connection.schema_editor() as editor:\\n321.             editor.create_model(Book)\\n322.             editor.create_model(Author)\\n323.             editor.create_model(Tag)\\n324.         # Initial tables are there\\n325.         list(Author.objects.all())\\n326.         list(Book.objects.all())\\n327.         # Make sure the FK constraint is present\\n328.         with self.assertRaises(IntegrityError):\\n329.             Book.objects.create(\\n330.                 author_id=1,\\n331.                 title="Much Ado About Foreign Keys",\\n332.                 pub_date=datetime.datetime.now(),\\n333.             )\\n334.         # Repoint the FK constraint\\n335.         old_field = Book._meta.get_field("author")\\n336.         new_field = ForeignKey(Tag, CASCADE)\\n337.         new_field.set_attributes_from_name("author")\\n338.         with connection.schema_editor() as editor:\\n339.             editor.alter_field(Book, old_field, new_field, strict=True)\\n340.         self.assertForeignKeyExists(Book, "author_id", "schema_tag")\\n341. \\n342.     @skipUnlessDBFeature("can_create_inline_fk")\\n343.     def test_inline_fk(self):\\n344.         # Create some tables.\\n345.         with connection.schema_editor() as editor:\\n346.             editor.create_model(Author)\\n347.             editor.create_model(Book)\\n348.             editor.create_model(Note)\\n349.         self.assertForeignKeyNotExists(Note, "book_id", "schema_book")\\n350.         # Add a foreign key from one to the other.\\n351.         with connection.schema_editor() as editor:\\n352.             new_field = ForeignKey(Book, CASCADE)\\n353.             new_field.set_attributes_from_name("book")\\n354.             editor.add_field(Note, new_field)\\n355.         self.assertForeignKeyExists(Note, "book_id", "schema_book")\\n356.         # Creating a FK field with a constraint uses a single statement without\\n357.         # a deferred ALTER TABLE.\\n358.         self.assertFalse(\\n359.             [\\n360.                 sql\\n361.                 for sql in (str(statement) for statement in editor.deferred_sql)\\n362.                 if sql.startswith("ALTER TABLE") and "ADD CONSTRAINT" in sql\\n363.             ]\\n364.         )\\n365. \\n366.     @skipUnlessDBFeature("can_create_inline_fk")\\n367.     def test_add_inline_fk_update_data(self):\\n368.         with connection.schema_editor() as editor:\\n369.             editor.create_model(Node)\\n370.         # Add an inline foreign key and update data in the same transaction.\\n371.         new_field = ForeignKey(Node, CASCADE, related_name="new_fk", null=True)\\n372.         new_field.set_attributes_from_name("new_parent_fk")\\n373.         parent = Node.objects.create()\\n374.         with connection.schema_editor() as editor:\\n375.             editor.add_field(Node, new_field)\\n376.             editor.execute("UPDATE schema_node SET new_parent_fk_id = %s;", [parent.pk])\\n377.         assertIndex = (\\n378.             self.assertIn\\n379.             if connection.features.indexes_foreign_keys\\n380.             else self.assertNotIn\\n381.         )\\n382.         assertIndex("new_parent_fk_id", self.get_indexes(Node._meta.db_table))\\n383. \\n384.     @skipUnlessDBFeature(\\n385.         "can_create_inline_fk",\\n386.         "allows_multiple_constraints_on_same_fields",\\n387.     )\\n388.     @isolate_apps("schema")\\n389.     def test_add_inline_fk_index_update_data(self):\\n390.         class Node(Model):\\n391.             class Meta:\\n392.                 app_label = "schema"\\n393. \\n394.         with connection.schema_editor() as editor:\\n395.             editor.create_model(Node)\\n396.         # Add an inline foreign key, update data, and an index in the same\\n397.         # transaction.\\n398.         new_field = ForeignKey(Node, CASCADE, related_name="new_fk", null=True)\\n399.         new_field.set_attributes_from_name("new_parent_fk")\\n400.         parent = Node.objects.create()\\n401.         with connection.schema_editor() as editor:\\n402.             editor.add_field(Node, new_field)\\n403.             Node._meta.add_field(new_field)\\n404.             editor.execute("UPDATE schema_node SET new_parent_fk_id = %s;", [parent.pk])\\n405.             editor.add_index(\\n406.                 Node, Index(fields=["new_parent_fk"], name="new_parent_inline_fk_idx")\\n407.             )\\n408.         self.assertIn("new_parent_fk_id", self.get_indexes(Node._meta.db_table))\\n409. \\n410.     @skipUnlessDBFeature("supports_foreign_keys")\\n411.     def test_char_field_with_db_index_to_fk(self):\\n412.         # Create the table\\n413.         with connection.schema_editor() as editor:\\n414.             editor.create_model(Author)\\n415.             editor.create_model(AuthorCharFieldWithIndex)\\n416.         # Change CharField to FK\\n417.         old_field = AuthorCharFieldWithIndex._meta.get_field("char_field")\\n418.         new_field = ForeignKey(Author, CASCADE, blank=True)\\n419.         new_field.set_attributes_from_name("char_field")\\n420.         with connection.schema_editor() as editor:\\n421.             editor.alter_field(\\n422.                 AuthorCharFieldWithIndex, old_field, new_field, strict=True\\n423.             )\\n424.         self.assertForeignKeyExists(\\n425.             AuthorCharFieldWithIndex, "char_field_id", "schema_author"\\n426.         )\\n427. \\n428.     @skipUnlessDBFeature("supports_foreign_keys")\\n429.     @skipUnlessDBFeature("supports_index_on_text_field")\\n430.     def test_text_field_with_db_index_to_fk(self):\\n431.         # Create the table\\n432.         with connection.schema_editor() as editor:\\n433.             editor.create_model(Author)\\n434.             editor.create_model(AuthorTextFieldWithIndex)\\n435.         # Change TextField to FK\\n436.         old_field = AuthorTextFieldWithIndex._meta.get_field("text_field")\\n437.         new_field = ForeignKey(Author, CASCADE, blank=True)\\n438.         new_field.set_attributes_from_name("text_field")\\n439.         with connection.schema_editor() as editor:\\n440.             editor.alter_field(\\n441.                 AuthorTextFieldWithIndex, old_field, new_field, strict=True\\n442.             )\\n443.         self.assertForeignKeyExists(\\n444.             AuthorTextFieldWithIndex, "text_field_id", "schema_author"\\n445.         )\\n446. \\n447.     @isolate_apps("schema")\\n448.     def test_char_field_pk_to_auto_field(self):\\n449.         class Foo(Model):\\n450.             id = CharField(max_length=255, primary_key=True)\\n451. \\n452.             class Meta:\\n453.                 app_label = "schema"\\n454. \\n455.         with connection.schema_editor() as editor:\\n456.             editor.create_model(Foo)\\n457.         self.isolated_local_models = [Foo]\\n458.         old_field = Foo._meta.get_field("id")\\n459.         new_field = AutoField(primary_key=True)\\n460.         new_field.set_attributes_from_name("id")\\n461.         new_field.model = Foo\\n462.         with connection.schema_editor() as editor:\\n463.             editor.alter_field(Foo, old_field, new_field, strict=True)\\n464. \\n465.     @skipUnlessDBFeature("supports_foreign_keys")\\n466.     def test_fk_to_proxy(self):\\n467.         "Creating a FK to a proxy model creates database constraints."\\n468. \\n469.         class AuthorProxy(Author):\\n470.             class Meta:\\n471.                 app_label = "schema"\\n472.                 apps = new_apps\\n473.                 proxy = True\\n474. \\n475.         class AuthorRef(Model):\\n476.             author = ForeignKey(AuthorProxy, on_delete=CASCADE)\\n477. \\n478.             class Meta:\\n479.                 app_label = "schema"\\n480.                 apps = new_apps\\n481. \\n482.         self.local_models = [AuthorProxy, AuthorRef]\\n483. \\n484.         # Create the table\\n485.         with connection.schema_editor() as editor:\\n486.             editor.create_model(Author)\\n487.             editor.create_model(AuthorRef)\\n488.         self.assertForeignKeyExists(AuthorRef, "author_id", "schema_author")\\n489. \\n490.     @skipUnlessDBFeature("supports_foreign_keys")\\n491.     def test_fk_db_constraint(self):\\n492.         "The db_constraint parameter is respected"\\n493.         # Create the table\\n494.         with connection.schema_editor() as editor:\\n495.             editor.create_model(Tag)\\n496.             editor.create_model(Author)\\n497.             editor.create_model(BookWeak)\\n498.         # Initial tables are there\\n499.         list(Author.objects.all())\\n500.         list(Tag.objects.all())\\n501.         list(BookWeak.objects.all())\\n502.         self.assertForeignKeyNotExists(BookWeak, "author_id", "schema_author")\\n503.         # Make a db_constraint=False FK\\n504.         new_field = ForeignKey(Tag, CASCADE, db_constraint=False)\\n505.         new_field.set_attributes_from_name("tag")\\n506.         with connection.schema_editor() as editor:\\n507.             editor.add_field(Author, new_field)\\n508.         self.assertForeignKeyNotExists(Author, "tag_id", "schema_tag")\\n509.         # Alter to one with a constraint\\n510.         new_field2 = ForeignKey(Tag, CASCADE)\\n511.         new_field2.set_attributes_from_name("tag")\\n512.         with connection.schema_editor() as editor:\\n513.             editor.alter_field(Author, new_field, new_field2, strict=True)\\n514.         self.assertForeignKeyExists(Author, "tag_id", "schema_tag")\\n515.         # Alter to one without a constraint again\\n516.         new_field2 = ForeignKey(Tag, CASCADE)\\n517.         new_field2.set_attributes_from_name("tag")\\n518.         with connection.schema_editor() as editor:\\n519.             editor.alter_field(Author, new_field2, new_field, strict=True)\\n520.         self.assertForeignKeyNotExists(Author, "tag_id", "schema_tag")\\n521. \\n522.     @isolate_apps("schema")\\n523.     def test_no_db_constraint_added_during_primary_key_change(self):\\n524.         """\\n525.         When a primary key that\\\'s pointed to by a ForeignKey with\\n526.         db_constraint=False is altered, a foreign key constraint isn\\\'t added.\\n527.         """\\n528. \\n529.         class Author(Model):\\n530.             class Meta:\\n531.                 app_label = "schema"\\n532. \\n533.         class BookWeak(Model):\\n534.             author = ForeignKey(Author, CASCADE, db_constraint=False)\\n535. \\n536.             class Meta:\\n537.                 app_label = "schema"\\n538. \\n539.         with connection.schema_editor() as editor:\\n540.             editor.create_model(Author)\\n541.             editor.create_model(BookWeak)\\n542.         self.assertForeignKeyNotExists(BookWeak, "author_id", "schema_author")\\n543.         old_field = Author._meta.get_field("id")\\n544.         new_field = BigAutoField(primary_key=True)\\n545.         new_field.model = Author\\n546.         new_field.set_attributes_from_name("id")\\n547.         # @isolate_apps() and inner models are needed to have the model\\n548.         # relations populated, otherwise this doesn\\\'t act as a regression test.\\n549.         self.assertEqual(len(new_field.model._meta.related_objects), 1)\\n550.         with connection.schema_editor() as editor:\\n551.             editor.alter_field(Author, old_field, new_field, strict=True)\\n552.         self.assertForeignKeyNotExists(BookWeak, "author_id", "schema_author")\\n553. \\n554.     def _test_m2m_db_constraint(self, M2MFieldClass):\\n555.         class LocalAuthorWithM2M(Model):\\n556.             name = CharField(max_length=255)\\n557. \\n558.             class Meta:\\n559.                 app_label = "schema"\\n560.                 apps = new_apps\\n561. \\n562.         self.local_models = [LocalAuthorWithM2M]\\n563. \\n564.         # Create the table\\n565.         with connection.schema_editor() as editor:\\n566.             editor.create_model(Tag)\\n567.             editor.create_model(LocalAuthorWithM2M)\\n568.         # Initial tables are there\\n569.         list(LocalAuthorWithM2M.objects.all())\\n570.         list(Tag.objects.all())\\n571.         # Make a db_constraint=False FK\\n572.         new_field = M2MFieldClass(Tag, related_name="authors", db_constraint=False)\\n573.         new_field.contribute_to_class(LocalAuthorWithM2M, "tags")\\n574.         # Add the field\\n575.         with connection.schema_editor() as editor:\\n576.             editor.add_field(LocalAuthorWithM2M, new_field)\\n577.         self.assertForeignKeyNotExists(\\n578.             new_field.remote_field.through, "tag_id", "schema_tag"\\n579.         )\\n580. \\n581.     @skipUnlessDBFeature("supports_foreign_keys")\\n582.     def test_m2m_db_constraint(self):\\n583.         self._test_m2m_db_constraint(ManyToManyField)\\n584. \\n585.     @skipUnlessDBFeature("supports_foreign_keys")\\n586.     def test_m2m_db_constraint_custom(self):\\n587.         self._test_m2m_db_constraint(CustomManyToManyField)\\n588. \\n589.     @skipUnlessDBFeature("supports_foreign_keys")\\n590.     def test_m2m_db_constraint_inherited(self):\\n591.         self._test_m2m_db_constraint(InheritedManyToManyField)\\n592. \\n593.     def test_add_field(self):\\n594.         """\\n595.         Tests adding fields to models\\n596.         """\\n597.         # Create the table\\n598.         with connection.schema_editor() as editor:\\n599.             editor.create_model(Author)\\n600.         # Ensure there\\\'s no age field\\n601.         columns = self.column_classes(Author)\\n602.         self.assertNotIn("age", columns)\\n603.         # Add the new field\\n604.         new_field = IntegerField(null=True)\\n605.         new_field.set_attributes_from_name("age")\\n606.         with CaptureQueriesContext(\\n607.             connection\\n608.         ) as ctx, connection.schema_editor() as editor:\\n609.             editor.add_field(Author, new_field)\\n610.         drop_default_sql = editor.sql_alter_column_no_default % {\\n611.             "column": editor.quote_name(new_field.name),\\n612.         }\\n613.         self.assertFalse(\\n614.             any(drop_default_sql in query["sql"] for query in ctx.captured_queries)\\n615.         )\\n616.         # Table is not rebuilt.\\n617.         self.assertIs(\\n618.             any("CREATE TABLE" in query["sql"] for query in ctx.captured_queries), False\\n619.         )\\n620.         self.assertIs(\\n621.             any("DROP TABLE" in query["sql"] for query in ctx.captured_queries), False\\n622.         )\\n623.         columns = self.column_classes(Author)\\n624.         self.assertEqual(\\n625.             columns["age"][0],\\n626.             connection.features.introspected_field_types["IntegerField"],\\n627.         )\\n628.         self.assertTrue(columns["age"][1][6])\\n629. \\n630.     def test_add_field_remove_field(self):\\n631.         """\\n632.         Adding a field and removing it removes all deferred sql referring to it.\\n633.         """\\n634.         with connection.schema_editor() as editor:\\n635.             # Create a table with a unique constraint on the slug field.\\n636.             editor.create_model(Tag)\\n637.             # Remove the slug column.\\n638.             editor.remove_field(Tag, Tag._meta.get_field("slug"))\\n639.         self.assertEqual(editor.deferred_sql, [])\\n640. \\n641.     def test_add_field_temp_default(self):\\n642.         """\\n643.         Tests adding fields to models with a temporary default\\n644.         """\\n645.         # Create the table\\n646.         with connection.schema_editor() as editor:\\n647.             editor.create_model(Author)\\n648.         # Ensure there\\\'s no age field\\n649.         columns = self.column_classes(Author)\\n650.         self.assertNotIn("age", columns)\\n651.         # Add some rows of data\\n652.         Author.objects.create(name="Andrew", height=30)\\n653.         Author.objects.create(name="Andrea")\\n654.         # Add a not-null field\\n655.         new_field = CharField(max_length=30, default="Godwin")\\n656.         new_field.set_attributes_from_name("surname")\\n657.         with connection.schema_editor() as editor:\\n658.             editor.add_field(Author, new_field)\\n659.         columns = self.column_classes(Author)\\n660.         self.assertEqual(\\n661.             columns["surname"][0],\\n662.             connection.features.introspected_field_types["CharField"],\\n663.         )\\n664.         self.assertEqual(\\n665.             columns["surname"][1][6],\\n666.             connection.features.interprets_empty_strings_as_nulls,\\n667.         )\\n668. \\n669.     def test_add_field_temp_default_boolean(self):\\n670.         """\\n671.         Tests adding fields to models with a temporary default where\\n672.         the default is False. (#21783)\\n673.         """\\n674.         # Create the table\\n675.         with connection.schema_editor() as editor:\\n676.             editor.create_model(Author)\\n677.         # Ensure there\\\'s no age field\\n678.         columns = self.column_classes(Author)\\n679.         self.assertNotIn("age", columns)\\n680.         # Add some rows of data\\n681.         Author.objects.create(name="Andrew", height=30)\\n682.         Author.objects.create(name="Andrea")\\n683.         # Add a not-null field\\n684.         new_field = BooleanField(default=False)\\n685.         new_field.set_attributes_from_name("awesome")\\n686.         with connection.schema_editor() as editor:\\n687.             editor.add_field(Author, new_field)\\n688.         columns = self.column_classes(Author)\\n689.         # BooleanField are stored as TINYINT(1) on MySQL.\\n690.         field_type = columns["awesome"][0]\\n691.         self.assertEqual(\\n692.             field_type, connection.features.introspected_field_types["BooleanField"]\\n693.         )\\n694. \\n695.     def test_add_field_default_transform(self):\\n696.         """\\n697.         Tests adding fields to models with a default that is not directly\\n698.         valid in the database (#22581)\\n699.         """\\n700. \\n701.         class TestTransformField(IntegerField):\\n702. \\n703.             # Weird field that saves the count of items in its value\\n704.             def get_default(self):\\n705.                 return self.default\\n706. \\n707.             def get_prep_value(self, value):\\n708.                 if value is None:\\n709.                     return 0\\n710.                 return len(value)\\n711. \\n712.         # Create the table\\n713.         with connection.schema_editor() as editor:\\n714.             editor.create_model(Author)\\n715.         # Add some rows of data\\n716.         Author.objects.create(name="Andrew", height=30)\\n717.         Author.objects.create(name="Andrea")\\n718.         # Add the field with a default it needs to cast (to string in this case)\\n719.         new_field = TestTransformField(default={1: 2})\\n720.         new_field.set_attributes_from_name("thing")\\n721.         with connection.schema_editor() as editor:\\n722.             editor.add_field(Author, new_field)\\n723.         # Ensure the field is there\\n724.         columns = self.column_classes(Author)\\n725.         field_type, field_info = columns["thing"]\\n726.         self.assertEqual(\\n727.             field_type, connection.features.introspected_field_types["IntegerField"]\\n728.         )\\n729.         # Make sure the values were transformed correctly\\n730.         self.assertEqual(Author.objects.extra(where=["thing = 1"]).count(), 2)\\n731. \\n732.     def test_add_field_o2o_nullable(self):\\n733.         with connection.schema_editor() as editor:\\n734.             editor.create_model(Author)\\n735.             editor.create_model(Note)\\n736.         new_field = OneToOneField(Note, CASCADE, null=True)\\n737.         new_field.set_attributes_from_name("note")\\n738.         with connection.schema_editor() as editor:\\n739.             editor.add_field(Author, new_field)\\n740.         columns = self.column_classes(Author)\\n741.         self.assertIn("note_id", columns)\\n742.         self.assertTrue(columns["note_id"][1][6])\\n743. \\n744.     def test_add_field_binary(self):\\n745.         """\\n746.         Tests binary fields get a sane default (#22851)\\n747.         """\\n748.         # Create the table\\n749.         with connection.schema_editor() as editor:\\n750.             editor.create_model(Author)\\n751.         # Add the new field\\n752.         new_field = BinaryField(blank=True)\\n753.         new_field.set_attributes_from_name("bits")\\n754.         with connection.schema_editor() as editor:\\n755.             editor.add_field(Author, new_field)\\n756.         columns = self.column_classes(Author)\\n757.         # MySQL annoyingly uses the same backend, so it\\\'ll come back as one of\\n758.         # these two types.\\n759.         self.assertIn(columns["bits"][0], ("BinaryField", "TextField"))\\n760. \\n761.     def test_add_field_durationfield_with_default(self):\\n762.         with connection.schema_editor() as editor:\\n763.             editor.create_model(Author)\\n764.         new_field = DurationField(default=datetime.timedelta(minutes=10))\\n765.         new_field.set_attributes_from_name("duration")\\n766.         with connection.schema_editor() as editor:\\n767.             editor.add_field(Author, new_field)\\n768.         columns = self.column_classes(Author)\\n769.         self.assertEqual(\\n770.             columns["duration"][0],\\n771.             connection.features.introspected_field_types["DurationField"],\\n772.         )\\n773. \\n774.     @unittest.skipUnless(connection.vendor == "mysql", "MySQL specific")\\n775.     def test_add_binaryfield_mediumblob(self):\\n776.         """\\n777.         Test adding a custom-sized binary field on MySQL (#24846).\\n778.         """\\n779.         # Create the table\\n780.         with connection.schema_editor() as editor:\\n781.             editor.create_model(Author)\\n782.         # Add the new field with default\\n783.         new_field = MediumBlobField(blank=True, default=b"123")\\n784.         new_field.set_attributes_from_name("bits")\\n785.         with connection.schema_editor() as editor:\\n786.             editor.add_field(Author, new_field)\\n787.         columns = self.column_classes(Author)\\n788.         # Introspection treats BLOBs as TextFields\\n789.         self.assertEqual(columns["bits"][0], "TextField")\\n790. \\n791.     def test_remove_field(self):\\n792.         with connection.schema_editor() as editor:\\n793.             editor.create_model(Author)\\n794.             with CaptureQueriesContext(connection) as ctx:\\n795.                 editor.remove_field(Author, Author._meta.get_field("name"))\\n796.         columns = self.column_classes(Author)\\n797.         self.assertNotIn("name", columns)\\n798.         if getattr(connection.features, "can_alter_table_drop_column", True):\\n799.             # Table is not rebuilt.\\n800.             self.assertIs(\\n801.                 any("CREATE TABLE" in query["sql"] for query in ctx.captured_queries),\\n802.                 False,\\n803.             )\\n804.             self.assertIs(\\n805.                 any("DROP TABLE" in query["sql"] for query in ctx.captured_queries),\\n806.                 False,\\n807.             )\\n808. \\n809.     def test_alter(self):\\n810.         """\\n811.         Tests simple altering of fields\\n812.         """\\n813.         # Create the table\\n814.         with connection.schema_editor() as editor:\\n815.             editor.create_model(Author)\\n816.         # Ensure the field is right to begin with\\n817.         columns = self.column_classes(Author)\\n818.         self.assertEqual(\\n819.             columns["name"][0],\\n820.             connection.features.introspected_field_types["CharField"],\\n821.         )\\n822.         self.assertEqual(\\n823.             bool(columns["name"][1][6]),\\n824.             bool(connection.features.interprets_empty_strings_as_nulls),\\n825.         )\\n826.         # Alter the name field to a TextField\\n827.         old_field = Author._meta.get_field("name")\\n828.         new_field = TextField(null=True)\\n829.         new_field.set_attributes_from_name("name")\\n830.         with connection.schema_editor() as editor:\\n831.             editor.alter_field(Author, old_field, new_field, strict=True)\\n832.         columns = self.column_classes(Author)\\n833.         self.assertEqual(columns["name"][0], "TextField")\\n834.         self.assertTrue(columns["name"][1][6])\\n835.         # Change nullability again\\n836.         new_field2 = TextField(null=False)\\n837.         new_field2.set_attributes_from_name("name")\\n838.         with connection.schema_editor() as editor:\\n839.             editor.alter_field(Author, new_field, new_field2, strict=True)\\n840.         columns = self.column_classes(Author)\\n841.         self.assertEqual(columns["name"][0], "TextField")\\n842.         self.assertEqual(\\n843.             bool(columns["name"][1][6]),\\n844.             bool(connection.features.interprets_empty_strings_as_nulls),\\n845.         )\\n846. \\n847.     def test_alter_auto_field_to_integer_field(self):\\n848.         # Create the table\\n849.         with connection.schema_editor() as editor:\\n850.             editor.create_model(Author)\\n851.         # Change AutoField to IntegerField\\n852.         old_field = Author._meta.get_field("id")\\n853.         new_field = IntegerField(primary_key=True)\\n854.         new_field.set_attributes_from_name("id")\\n855.         new_field.model = Author\\n856.         with connection.schema_editor() as editor:\\n857.             editor.alter_field(Author, old_field, new_field, strict=True)\\n858.         # Now that ID is an IntegerField, the database raises an error if it\\n859.         # isn\\\'t provided.\\n860.         if not connection.features.supports_unspecified_pk:\\n861.             with self.assertRaises(DatabaseError):\\n862.                 Author.objects.create()\\n863. \\n864.     def test_alter_auto_field_to_char_field(self):\\n865.         # Create the table\\n866.         with connection.schema_editor() as editor:\\n867.             editor.create_model(Author)\\n868.         # Change AutoField to CharField\\n869.         old_field = Author._meta.get_field("id")\\n870.         new_field = CharField(primary_key=True, max_length=50)\\n871.         new_field.set_attributes_from_name("id")\\n872.         new_field.model = Author\\n873.         with connection.schema_editor() as editor:\\n874.             editor.alter_field(Author, old_field, new_field, strict=True)\\n875. \\n876.     @isolate_apps("schema")\\n877.     def test_alter_auto_field_quoted_db_column(self):\\n878.         class Foo(Model):\\n879.             id = AutoField(primary_key=True, db_column=\\\'"quoted_id"\\\')\\n880. \\n881.             class Meta:\\n882.                 app_label = "schema"\\n883. \\n884.         with connection.schema_editor() as editor:\\n885.             editor.create_model(Foo)\\n886.         self.isolated_local_models = [Foo]\\n887.         old_field = Foo._meta.get_field("id")\\n888.         new_field = BigAutoField(primary_key=True)\\n889.         new_field.model = Foo\\n890.         new_field.db_column = \\\'"quoted_id"\\\'\\n891.         new_field.set_attributes_from_name("id")\\n892.         with connection.schema_editor() as editor:\\n893.             editor.alter_field(Foo, old_field, new_field, strict=True)\\n894.         Foo.objects.create()\\n895. \\n896.     def test_alter_not_unique_field_to_primary_key(self):\\n897.         # Create the table.\\n898.         with connection.schema_editor() as editor:\\n899.             editor.create_model(Author)\\n900.         # Change UUIDField to primary key.\\n901.         old_field = Author._meta.get_field("uuid")\\n902.         new_field = UUIDField(primary_key=True)\\n903.         new_field.set_attributes_from_name("uuid")\\n904.         new_field.model = Author\\n905.         with connection.schema_editor() as editor:\\n906.             editor.remove_field(Author, Author._meta.get_field("id"))\\n907.             editor.alter_field(Author, old_field, new_field, strict=True)\\n908.         # Redundant unique constraint is not added.\\n909.         count = self.get_constraints_count(\\n910.             Author._meta.db_table,\\n911.             Author._meta.get_field("uuid").column,\\n912.             None,\\n913.         )\\n914.         self.assertLessEqual(count["uniques"], 1)\\n915. \\n916.     @isolate_apps("schema")\\n917.     def test_alter_primary_key_quoted_db_table(self):\\n918.         class Foo(Model):\\n919.             class Meta:\\n920.                 app_label = "schema"\\n921.                 db_table = \\\'"foo"\\\'\\n922. \\n923.         with connection.schema_editor() as editor:\\n924.             editor.create_model(Foo)\\n925.         self.isolated_local_models = [Foo]\\n926.         old_field = Foo._meta.get_field("id")\\n927.         new_field = BigAutoField(primary_key=True)\\n928.         new_field.model = Foo\\n929.         new_field.set_attributes_from_name("id")\\n930.         with connection.schema_editor() as editor:\\n931.             editor.alter_field(Foo, old_field, new_field, strict=True)\\n932.         Foo.objects.create()\\n933. \\n934.     def test_alter_text_field(self):\\n935.         # Regression for "BLOB/TEXT column \\\'info\\\' can\\\'t have a default value")\\n936.         # on MySQL.\\n937.         # Create the table\\n938.         with connection.schema_editor() as editor:\\n939.             editor.create_model(Note)\\n940.         old_field = Note._meta.get_field("info")\\n941.         new_field = TextField(blank=True)\\n942.         new_field.set_attributes_from_name("info")\\n943.         with connection.schema_editor() as editor:\\n944.             editor.alter_field(Note, old_field, new_field, strict=True)\\n945. \\n946.     def test_alter_text_field_to_not_null_with_default_value(self):\\n947.         with connection.schema_editor() as editor:\\n948.             editor.create_model(Note)\\n949.         old_field = Note._meta.get_field("address")\\n950.         new_field = TextField(blank=True, default="", null=False)\\n951.         new_field.set_attributes_from_name("address")\\n952.         with connection.schema_editor() as editor:\\n953.             editor.alter_field(Note, old_field, new_field, strict=True)\\n954. \\n955.     @skipUnlessDBFeature("can_defer_constraint_checks", "can_rollback_ddl")\\n956.     def test_alter_fk_checks_deferred_constraints(self):\\n957.         """\\n958.         #25492 - Altering a foreign key\\\'s structure and data in the same\\n959.         transaction.\\n960.         """\\n961.         with connection.schema_editor() as editor:\\n962.             editor.create_model(Node)\\n963.         old_field = Node._meta.get_field("parent")\\n964.         new_field = ForeignKey(Node, CASCADE)\\n965.         new_field.set_attributes_from_name("parent")\\n966.         parent = Node.objects.create()\\n967.         with connection.schema_editor() as editor:\\n968.             # Update the parent FK to create a deferred constraint check.\\n969.             Node.objects.update(parent=parent)\\n970.             editor.alter_field(Node, old_field, new_field, strict=True)\\n971. \\n972.     def test_alter_text_field_to_date_field(self):\\n973.         """\\n974.         #25002 - Test conversion of text field to date field.\\n975.         """\\n976.         with connection.schema_editor() as editor:\\n977.             editor.create_model(Note)\\n978.         Note.objects.create(info="1988-05-05")\\n979.         old_field = Note._meta.get_field("info")\\n980.         new_field = DateField(blank=True)\\n981.         new_field.set_attributes_from_name("info")\\n982.         with connection.schema_editor() as editor:\\n983.             editor.alter_field(Note, old_field, new_field, strict=True)\\n984.         # Make sure the field isn\\\'t nullable\\n985.         columns = self.column_classes(Note)\\n986.         self.assertFalse(columns["info"][1][6])\\n987. \\n988.     def test_alter_text_field_to_datetime_field(self):\\n989.         """\\n990.         #25002 - Test conversion of text field to datetime field.\\n991.         """\\n992.         with connection.schema_editor() as editor:\\n993.             editor.create_model(Note)\\n994.         Note.objects.create(info="1988-05-05 3:16:17.4567")\\n995.         old_field = Note._meta.get_field("info")\\n996.         new_field = DateTimeField(blank=True)\\n997.         new_field.set_attributes_from_name("info")\\n998.         with connection.schema_editor() as editor:\\n999.             editor.alter_field(Note, old_field, new_field, strict=True)\\n1000.         # Make sure the field isn\\\'t nullable\\n1001.         columns = self.column_classes(Note)\\n1002.         self.assertFalse(columns["info"][1][6])\\n1003. \\n1004.     def test_alter_text_field_to_time_field(self):\\n1005.         """\\n1006.         #25002 - Test conversion of text field to time field.\\n1007.         """\\n1008.         with connection.schema_editor() as editor:\\n1009.             editor.create_model(Note)\\n1010.         Note.objects.create(info="3:16:17.4567")\\n1011.         old_field = Note._meta.get_field("info")\\n1012.         new_field = TimeField(blank=True)\\n1013.         new_field.set_attributes_from_name("info")\\n1014.         with connection.schema_editor() as editor:\\n1015.             editor.alter_field(Note, old_field, new_field, strict=True)\\n1016.         # Make sure the field isn\\\'t nullable\\n1017.         columns = self.column_classes(Note)\\n1018.         self.assertFalse(columns["info"][1][6])\\n1019. \\n1020.     @skipIfDBFeature("interprets_empty_strings_as_nulls")\\n1021.     def test_alter_textual_field_keep_null_status(self):\\n1022.         """\\n1023.         Changing a field type shouldn\\\'t affect the not null status.\\n1024.         """\\n1025.         with connection.schema_editor() as editor:\\n1026.             editor.create_model(Note)\\n1027.         with self.assertRaises(IntegrityError):\\n1028.             Note.objects.create(info=None)\\n1029.         old_field = Note._meta.get_field("info")\\n1030.         new_field = CharField(max_length=50)\\n1031.         new_field.set_attributes_from_name("info")\\n1032.         with connection.schema_editor() as editor:\\n1033.             editor.alter_field(Note, old_field, new_field, strict=True)\\n1034.         with self.assertRaises(IntegrityError):\\n1035.             Note.objects.create(info=None)\\n1036. \\n1037.     @skipUnlessDBFeature("interprets_empty_strings_as_nulls")\\n1038.     def test_alter_textual_field_not_null_to_null(self):\\n1039.         """\\n1040.         Nullability for textual fields is preserved on databases that\\n1041.         interpret empty strings as NULLs.\\n1042.         """\\n1043.         with connection.schema_editor() as editor:\\n1044.             editor.create_model(Author)\\n1045.         columns = self.column_classes(Author)\\n1046.         # Field is nullable.\\n1047.         self.assertTrue(columns["uuid"][1][6])\\n1048.         # Change to NOT NULL.\\n1049.         old_field = Author._meta.get_field("uuid")\\n1050.         new_field = SlugField(null=False, blank=True)\\n1051.         new_field.set_attributes_from_name("uuid")\\n1052.         with connection.schema_editor() as editor:\\n1053.             editor.alter_field(Author, old_field, new_field, strict=True)\\n1054.         columns = self.column_classes(Author)\\n1055.         # Nullability is preserved.\\n1056.         self.assertTrue(columns["uuid"][1][6])\\n1057. \\n1058.     def test_alter_numeric_field_keep_null_status(self):\\n1059.         """\\n1060.         Changing a field type shouldn\\\'t affect the not null status.\\n1061.         """\\n1062.         with connection.schema_editor() as editor:\\n1063.             editor.create_model(UniqueTest)\\n1064.         with self.assertRaises(IntegrityError):\\n1065.             UniqueTest.objects.create(year=None, slug="aaa")\\n1066.         old_field = UniqueTest._meta.get_field("year")\\n1067.         new_field = BigIntegerField()\\n1068.         new_field.set_attributes_from_name("year")\\n1069.         with connection.schema_editor() as editor:\\n1070.             editor.alter_field(UniqueTest, old_field, new_field, strict=True)\\n1071.         with self.assertRaises(IntegrityError):\\n1072.             UniqueTest.objects.create(year=None, slug="bbb")\\n1073. \\n1074.     def test_alter_null_to_not_null(self):\\n1075.         """\\n1076.         #23609 - Tests handling of default values when altering from NULL to NOT NULL.\\n1077.         """\\n1078.         # Create the table\\n1079.         with connection.schema_editor() as editor:\\n1080.             editor.create_model(Author)\\n1081.         # Ensure the field is right to begin with\\n1082.         columns = self.column_classes(Author)\\n1083.         self.assertTrue(columns["height"][1][6])\\n1084.         # Create some test data\\n1085.         Author.objects.create(name="Not null author", height=12)\\n1086.         Author.objects.create(name="Null author")\\n1087.         # Verify null value\\n1088.         self.assertEqual(Author.objects.get(name="Not null author").height, 12)\\n1089.         self.assertIsNone(Author.objects.get(name="Null author").height)\\n1090.         # Alter the height field to NOT NULL with default\\n1091.         old_field = Author._meta.get_field("height")\\n1092.         new_field = PositiveIntegerField(default=42)\\n1093.         new_field.set_attributes_from_name("height")\\n1094.         with connection.schema_editor() as editor:\\n1095.             editor.alter_field(Author, old_field, new_field, strict=True)\\n1096.         columns = self.column_classes(Author)\\n1097.         self.assertFalse(columns["height"][1][6])\\n1098.         # Verify default value\\n1099.         self.assertEqual(Author.objects.get(name="Not null author").height, 12)\\n1100.         self.assertEqual(Author.objects.get(name="Null author").height, 42)\\n1101. \\n1102.     def test_alter_charfield_to_null(self):\\n1103.         """\\n1104.         #24307 - Should skip an alter statement on databases with\\n1105.         interprets_empty_strings_as_nulls when changing a CharField to null.\\n1106.         """\\n1107.         # Create the table\\n1108.         with connection.schema_editor() as editor:\\n1109.             editor.create_model(Author)\\n1110.         # Change the CharField to null\\n1111.         old_field = Author._meta.get_field("name")\\n1112.         new_field = copy(old_field)\\n1113.         new_field.null = True\\n1114.         with connection.schema_editor() as editor:\\n1115.             editor.alter_field(Author, old_field, new_field, strict=True)\\n1116. \\n1117.     @unittest.skipUnless(connection.vendor == "postgresql", "PostgreSQL specific")\\n1118.     def test_alter_char_field_decrease_length(self):\\n1119.         # Create the table.\\n1120.         with connection.schema_editor() as editor:\\n1121.             editor.create_model(Author)\\n1122.         Author.objects.create(name="x" * 255)\\n1123.         # Change max_length of CharField.\\n1124.         old_field = Author._meta.get_field("name")\\n1125.         new_field = CharField(max_length=254)\\n1126.         new_field.set_attributes_from_name("name")\\n1127.         with connection.schema_editor() as editor:\\n1128.             msg = "value too long for type character varying(254)"\\n1129.             with self.assertRaisesMessage(DataError, msg):\\n1130.                 editor.alter_field(Author, old_field, new_field, strict=True)\\n1131. \\n1132.     @unittest.skipUnless(connection.vendor == "postgresql", "PostgreSQL specific")\\n1133.     def test_alter_field_with_custom_db_type(self):\\n1134.         from django.contrib.postgres.fields import ArrayField\\n1135. \\n1136.         class Foo(Model):\\n1137.             field = ArrayField(CharField(max_length=255))\\n1138. \\n1139.             class Meta:\\n1140.                 app_label = "schema"\\n1141. \\n1142.         with connection.schema_editor() as editor:\\n1143.             editor.create_model(Foo)\\n1144.         self.isolated_local_models = [Foo]\\n1145.         old_field = Foo._meta.get_field("field")\\n1146.         new_field = ArrayField(CharField(max_length=16))\\n1147.         new_field.set_attributes_from_name("field")\\n1148.         new_field.model = Foo\\n1149.         with connection.schema_editor() as editor:\\n1150.             editor.alter_field(Foo, old_field, new_field, strict=True)\\n1151. \\n1152.     @isolate_apps("schema")\\n1153.     @unittest.skipUnless(connection.vendor == "postgresql", "PostgreSQL specific")\\n1154.     def test_alter_array_field_decrease_base_field_length(self):\\n1155.         from django.contrib.postgres.fields import ArrayField\\n1156. \\n1157.         class ArrayModel(Model):\\n1158.             field = ArrayField(CharField(max_length=16))\\n1159. \\n1160.             class Meta:\\n1161.                 app_label = "schema"\\n1162. \\n1163.         with connection.schema_editor() as editor:\\n1164.             editor.create_model(ArrayModel)\\n1165.         self.isolated_local_models = [ArrayModel]\\n1166.         ArrayModel.objects.create(field=["x" * 16])\\n1167.         old_field = ArrayModel._meta.get_field("field")\\n1168.         new_field = ArrayField(CharField(max_length=15))\\n1169.         new_field.set_attributes_from_name("field")\\n1170.         new_field.model = ArrayModel\\n1171.         with connection.schema_editor() as editor:\\n1172.             msg = "value too long for type character varying(15)"\\n1173.             with self.assertRaisesMessage(DataError, msg):\\n1174.                 editor.alter_field(ArrayModel, old_field, new_field, strict=True)\\n1175. \\n1176.     @isolate_apps("schema")\\n1177.     @unittest.skipUnless(connection.vendor == "postgresql", "PostgreSQL specific")\\n1178.     def test_alter_array_field_decrease_nested_base_field_length(self):\\n1179.         from django.contrib.postgres.fields import ArrayField\\n1180. \\n1181.         class ArrayModel(Model):\\n1182.             field = ArrayField(ArrayField(CharField(max_length=16)))\\n1183. \\n1184.             class Meta:\\n1185.                 app_label = "schema"\\n1186. \\n1187.         with connection.schema_editor() as editor:\\n1188.             editor.create_model(ArrayModel)\\n1189.         self.isolated_local_models = [ArrayModel]\\n1190.         ArrayModel.objects.create(field=[["x" * 16]])\\n1191.         old_field = ArrayModel._meta.get_field("field")\\n1192.         new_field = ArrayField(ArrayField(CharField(max_length=15)))\\n1193.         new_field.set_attributes_from_name("field")\\n1194.         new_field.model = ArrayModel\\n1195.         with connection.schema_editor() as editor:\\n1196.             msg = "value too long for type character varying(15)"\\n1197.             with self.assertRaisesMessage(DataError, msg):\\n1198.                 editor.alter_field(ArrayModel, old_field, new_field, strict=True)\\n1199. \\n1200.     def test_alter_textfield_to_null(self):\\n1201.         """\\n1202.         #24307 - Should skip an alter statement on databases with\\n1203.         interprets_empty_strings_as_nulls when changing a TextField to null.\\n1204.         """\\n1205.         # Create the table\\n1206.         with connection.schema_editor() as editor:\\n1207.             editor.create_model(Note)\\n1208.         # Change the TextField to null\\n1209.         old_field = Note._meta.get_field("info")\\n1210.         new_field = copy(old_field)\\n1211.         new_field.null = True\\n1212.         with connection.schema_editor() as editor:\\n1213.             editor.alter_field(Note, old_field, new_field, strict=True)\\n1214. \\n1215.     def test_alter_null_to_not_null_keeping_default(self):\\n1216.         """\\n1217.         #23738 - Can change a nullable field with default to non-nullable\\n1218.         with the same default.\\n1219.         """\\n1220.         # Create the table\\n1221.         with connection.schema_editor() as editor:\\n1222.             editor.create_model(AuthorWithDefaultHeight)\\n1223.         # Ensure the field is right to begin with\\n1224.         columns = self.column_classes(AuthorWithDefaultHeight)\\n1225.         self.assertTrue(columns["height"][1][6])\\n1226.         # Alter the height field to NOT NULL keeping the previous default\\n1227.         old_field = AuthorWithDefaultHeight._meta.get_field("height")\\n1228.         new_field = PositiveIntegerField(default=42)\\n1229.         new_field.set_attributes_from_name("height")\\n1230.         with connection.schema_editor() as editor:\\n1231.             editor.alter_field(\\n1232.                 AuthorWithDefaultHeight, old_field, new_field, strict=True\\n1233.             )\\n1234.         columns = self.column_classes(AuthorWithDefaultHeight)\\n1235.         self.assertFalse(columns["height"][1][6])\\n1236. \\n1237.     @skipUnlessDBFeature("supports_foreign_keys")\\n1238.     def test_alter_fk(self):\\n1239.         """\\n1240.         Tests altering of FKs\\n1241.         """\\n1242.         # Create the table\\n1243.         with connection.schema_editor() as editor:\\n1244.             editor.create_model(Author)\\n1245.             editor.create_model(Book)\\n1246.         # Ensure the field is right to begin with\\n1247.         columns = self.column_classes(Book)\\n1248.         self.assertEqual(\\n1249.             columns["author_id"][0],\\n1250.             connection.features.introspected_field_types["IntegerField"],\\n1251.         )\\n1252.         self.assertForeignKeyExists(Book, "author_id", "schema_author")\\n1253.         # Alter the FK\\n1254.         old_field = Book._meta.get_field("author")\\n1255.         new_field = ForeignKey(Author, CASCADE, editable=False)\\n1256.         new_field.set_attributes_from_name("author")\\n1257.         with connection.schema_editor() as editor:\\n1258.             editor.alter_field(Book, old_field, new_field, strict=True)\\n1259.         columns = self.column_classes(Book)\\n1260.         self.assertEqual(\\n1261.             columns["author_id"][0],\\n1262.             connection.features.introspected_field_types["IntegerField"],\\n1263.         )\\n1264.         self.assertForeignKeyExists(Book, "author_id", "schema_author")\\n1265. \\n1266.     @skipUnlessDBFeature("supports_foreign_keys")\\n1267.     def test_alter_to_fk(self):\\n1268.         """\\n1269.         #24447 - Tests adding a FK constraint for an existing column\\n1270.         """\\n1271. \\n1272.         class LocalBook(Model):\\n1273.             author = IntegerField()\\n1274.             title = CharField(max_length=100, db_index=True)\\n1275.             pub_date = DateTimeField()\\n1276. \\n1277.             class Meta:\\n1278.                 app_label = "schema"\\n1279.                 apps = new_apps\\n1280. \\n1281.         self.local_models = [LocalBook]\\n1282. \\n1283.         # Create the tables\\n1284.         with connection.schema_editor() as editor:\\n1285.             editor.create_model(Author)\\n1286.             editor.create_model(LocalBook)\\n1287.         # Ensure no FK constraint exists\\n1288.         constraints = self.get_constraints(LocalBook._meta.db_table)\\n1289.         for details in constraints.values():\\n1290.             if details["foreign_key"]:\\n1291.                 self.fail(\\n1292.                     "Found an unexpected FK constraint to %s" % details["columns"]\\n1293.                 )\\n1294.         old_field = LocalBook._meta.get_field("author")\\n1295.         new_field = ForeignKey(Author, CASCADE)\\n1296.         new_field.set_attributes_from_name("author")\\n1297.         with connection.schema_editor() as editor:\\n1298.             editor.alter_field(LocalBook, old_field, new_field, strict=True)\\n1299.         self.assertForeignKeyExists(LocalBook, "author_id", "schema_author")\\n1300. \\n1301.     @skipUnlessDBFeature("supports_foreign_keys")\\n1302.     def test_alter_o2o_to_fk(self):\\n1303.         """\\n1304.         #24163 - Tests altering of OneToOneField to ForeignKey\\n1305.         """\\n1306.         # Create the table\\n1307.         with connection.schema_editor() as editor:\\n1308.             editor.create_model(Author)\\n1309.             editor.create_model(BookWithO2O)\\n1310.         # Ensure the field is right to begin with\\n1311.         columns = self.column_classes(BookWithO2O)\\n1312.         self.assertEqual(\\n1313.             columns["author_id"][0],\\n1314.             connection.features.introspected_field_types["IntegerField"],\\n1315.         )\\n1316.         # Ensure the field is unique\\n1317.         author = Author.objects.create(name="Joe")\\n1318.         BookWithO2O.objects.create(\\n1319.             author=author, title="Django 1", pub_date=datetime.datetime.now()\\n1320.         )\\n1321.         with self.assertRaises(IntegrityError):\\n1322.             BookWithO2O.objects.create(\\n1323.                 author=author, title="Django 2", pub_date=datetime.datetime.now()\\n1324.             )\\n1325.         BookWithO2O.objects.all().delete()\\n1326.         self.assertForeignKeyExists(BookWithO2O, "author_id", "schema_author")\\n1327.         # Alter the OneToOneField to ForeignKey\\n1328.         old_field = BookWithO2O._meta.get_field("author")\\n1329.         new_field = ForeignKey(Author, CASCADE)\\n1330.         new_field.set_attributes_from_name("author")\\n1331.         with connection.schema_editor() as editor:\\n1332.             editor.alter_field(BookWithO2O, old_field, new_field, strict=True)\\n1333.         columns = self.column_classes(Book)\\n1334.         self.assertEqual(\\n1335.             columns["author_id"][0],\\n1336.             connection.features.introspected_field_types["IntegerField"],\\n1337.         )\\n1338.         # Ensure the field is not unique anymore\\n1339.         Book.objects.create(\\n1340.             author=author, title="Django 1", pub_date=datetime.datetime.now()\\n1341.         )\\n1342.         Book.objects.create(\\n1343.             author=author, title="Django 2", pub_date=datetime.datetime.now()\\n1344.         )\\n1345.         self.assertForeignKeyExists(Book, "author_id", "schema_author")\\n1346. \\n1347.     @skipUnlessDBFeature("supports_foreign_keys")\\n1348.     def test_alter_fk_to_o2o(self):\\n1349.         """\\n1350.         #24163 - Tests altering of ForeignKey to OneToOneField\\n1351.         """\\n1352.         # Create the table\\n1353.         with connection.schema_editor() as editor:\\n1354.             editor.create_model(Author)\\n1355.             editor.create_model(Book)\\n1356.         # Ensure the field is right to begin with\\n1357.         columns = self.column_classes(Book)\\n1358.         self.assertEqual(\\n1359.             columns["author_id"][0],\\n1360.             connection.features.introspected_field_types["IntegerField"],\\n1361.         )\\n1362.         # Ensure the field is not unique\\n1363.         author = Author.objects.create(name="Joe")\\n1364.         Book.objects.create(\\n1365.             author=author, title="Django 1", pub_date=datetime.datetime.now()\\n1366.         )\\n1367.         Book.objects.create(\\n1368.             author=author, title="Django 2", pub_date=datetime.datetime.now()\\n1369.         )\\n1370.         Book.objects.all().delete()\\n1371.         self.assertForeignKeyExists(Book, "author_id", "schema_author")\\n1372.         # Alter the ForeignKey to OneToOneField\\n1373.         old_field = Book._meta.get_field("author")\\n1374.         new_field = OneToOneField(Author, CASCADE)\\n1375.         new_field.set_attributes_from_name("author")\\n1376.         with connection.schema_editor() as editor:\\n1377.             editor.alter_field(Book, old_field, new_field, strict=True)\\n1378.         columns = self.column_classes(BookWithO2O)\\n1379.         self.assertEqual(\\n1380.             columns["author_id"][0],\\n1381.             connection.features.introspected_field_types["IntegerField"],\\n1382.         )\\n1383.         # Ensure the field is unique now\\n1384.         BookWithO2O.objects.create(\\n1385.             author=author, title="Django 1", pub_date=datetime.datetime.now()\\n1386.         )\\n1387.         with self.assertRaises(IntegrityError):\\n1388.             BookWithO2O.objects.create(\\n1389.                 author=author, title="Django 2", pub_date=datetime.datetime.now()\\n1390.             )\\n1391.         self.assertForeignKeyExists(BookWithO2O, "author_id", "schema_author")\\n1392. \\n1393.     def test_alter_field_fk_to_o2o(self):\\n1394.         with connection.schema_editor() as editor:\\n1395.             editor.create_model(Author)\\n1396.             editor.create_model(Book)\\n1397.         expected_fks = 1 if connection.features.supports_foreign_keys else 0\\n1398.         expected_indexes = 1 if connection.features.indexes_foreign_keys else 0\\n1399. \\n1400.         # Check the index is right to begin with.\\n1401.         counts = self.get_constraints_count(\\n1402.             Book._meta.db_table,\\n1403.             Book._meta.get_field("author").column,\\n1404.             (Author._meta.db_table, Author._meta.pk.column),\\n1405.         )\\n1406.         self.assertEqual(\\n1407.             counts,\\n1408.             {"fks": expected_fks, "uniques": 0, "indexes": expected_indexes},\\n1409.         )\\n1410. \\n1411.         old_field = Book._meta.get_field("author")\\n1412.         new_field = OneToOneField(Author, CASCADE)\\n1413.         new_field.set_attributes_from_name("author")\\n1414.         with connection.schema_editor() as editor:\\n1415.             editor.alter_field(Book, old_field, new_field, strict=True)\\n1416. \\n1417.         counts = self.get_constraints_count(\\n1418.             Book._meta.db_table,\\n1419.             Book._meta.get_field("author").column,\\n1420.             (Author._meta.db_table, Author._meta.pk.column),\\n1421.         )\\n1422.         # The index on ForeignKey is replaced with a unique constraint for\\n1423.         # OneToOneField.\\n1424.         self.assertEqual(counts, {"fks": expected_fks, "uniques": 1, "indexes": 0})\\n1425. \\n1426.     def test_alter_field_fk_keeps_index(self):\\n1427.         with connection.schema_editor() as editor:\\n1428.             editor.create_model(Author)\\n1429.             editor.create_model(Book)\\n1430.         expected_fks = 1 if connection.features.supports_foreign_keys else 0\\n1431.         expected_indexes = 1 if connection.features.indexes_foreign_keys else 0\\n1432. \\n1433.         # Check the index is right to begin with.\\n1434.         counts = self.get_constraints_count(\\n1435.             Book._meta.db_table,\\n1436.             Book._meta.get_field("author").column,\\n1437.             (Author._meta.db_table, Author._meta.pk.column),\\n1438.         )\\n1439.         self.assertEqual(\\n1440.             counts,\\n1441.             {"fks": expected_fks, "uniques": 0, "indexes": expected_indexes},\\n1442.         )\\n1443. \\n1444.         old_field = Book._meta.get_field("author")\\n1445.         # on_delete changed from CASCADE.\\n1446.         new_field = ForeignKey(Author, PROTECT)\\n1447.         new_field.set_attributes_from_name("author")\\n1448.         with connection.schema_editor() as editor:\\n1449.             editor.alter_field(Book, old_field, new_field, strict=True)\\n1450. \\n1451.         counts = self.get_constraints_count(\\n1452.             Book._meta.db_table,\\n1453.             Book._meta.get_field("author").column,\\n1454.             (Author._meta.db_table, Author._meta.pk.column),\\n1455.         )\\n1456.         # The index remains.\\n1457.         self.assertEqual(\\n1458.             counts,\\n1459.             {"fks": expected_fks, "uniques": 0, "indexes": expected_indexes},\\n1460.         )\\n1461. \\n1462.     def test_alter_field_o2o_to_fk(self):\\n1463.         with connection.schema_editor() as editor:\\n1464.             editor.create_model(Author)\\n1465.             editor.create_model(BookWithO2O)\\n1466.         expected_fks = 1 if connection.features.supports_foreign_keys else 0\\n1467. \\n1468.         # Check the unique constraint is right to begin with.\\n1469.         counts = self.get_constraints_count(\\n1470.             BookWithO2O._meta.db_table,\\n1471.             BookWithO2O._meta.get_field("author").column,\\n1472.             (Author._meta.db_table, Author._meta.pk.column),\\n1473.         )\\n1474.         self.assertEqual(counts, {"fks": expected_fks, "uniques": 1, "indexes": 0})\\n1475. \\n1476.         old_field = BookWithO2O._meta.get_field("author")\\n1477.         new_field = ForeignKey(Author, CASCADE)\\n1478.         new_field.set_attributes_from_name("author")\\n1479.         with connection.schema_editor() as editor:\\n1480.             editor.alter_field(BookWithO2O, old_field, new_field, strict=True)\\n1481. \\n1482.         counts = self.get_constraints_count(\\n1483.             BookWithO2O._meta.db_table,\\n1484.             BookWithO2O._meta.get_field("author").column,\\n1485.             (Author._meta.db_table, Author._meta.pk.column),\\n1486.         )\\n1487.         # The unique constraint on OneToOneField is replaced with an index for\\n1488.         # ForeignKey.\\n1489.         self.assertEqual(counts, {"fks": expected_fks, "uniques": 0, "indexes": 1})\\n1490. \\n1491.     def test_alter_field_o2o_keeps_unique(self):\\n1492.         with connection.schema_editor() as editor:\\n1493.             editor.create_model(Author)\\n1494.             editor.create_model(BookWithO2O)\\n1495.         expected_fks = 1 if connection.features.supports_foreign_keys else 0\\n1496. \\n1497.         # Check the unique constraint is right to begin with.\\n1498.         counts = self.get_constraints_count(\\n1499.             BookWithO2O._meta.db_table,\\n1500.             BookWithO2O._meta.get_field("author").column,\\n1501.             (Author._meta.db_table, Author._meta.pk.column),\\n1502.         )\\n1503.         self.assertEqual(counts, {"fks": expected_fks, "uniques": 1, "indexes": 0})\\n1504. \\n1505.         old_field = BookWithO2O._meta.get_field("author")\\n1506.         # on_delete changed from CASCADE.\\n1507.         new_field = OneToOneField(Author, PROTECT)\\n1508.         new_field.set_attributes_from_name("author")\\n1509.         with connection.schema_editor() as editor:\\n1510.             editor.alter_field(BookWithO2O, old_field, new_field, strict=True)\\n1511. \\n1512.         counts = self.get_constraints_count(\\n1513.             BookWithO2O._meta.db_table,\\n1514.             BookWithO2O._meta.get_field("author").column,\\n1515.             (Author._meta.db_table, Author._meta.pk.column),\\n1516.         )\\n1517.         # The unique constraint remains.\\n1518.         self.assertEqual(counts, {"fks": expected_fks, "uniques": 1, "indexes": 0})\\n1519. \\n1520.     @skipUnlessDBFeature("ignores_table_name_case")\\n1521.     def test_alter_db_table_case(self):\\n1522.         # Create the table\\n1523.         with connection.schema_editor() as editor:\\n1524.             editor.create_model(Author)\\n1525.         # Alter the case of the table\\n1526.         old_table_name = Author._meta.db_table\\n1527.         with connection.schema_editor() as editor:\\n1528.             editor.alter_db_table(Author, old_table_name, old_table_name.upper())\\n1529. \\n1530.     def test_alter_implicit_id_to_explicit(self):\\n1531.         """\\n1532.         Should be able to convert an implicit "id" field to an explicit "id"\\n1533.         primary key field.\\n1534.         """\\n1535.         with connection.schema_editor() as editor:\\n1536.             editor.create_model(Author)\\n1537. \\n1538.         old_field = Author._meta.get_field("id")\\n1539.         new_field = AutoField(primary_key=True)\\n1540.         new_field.set_attributes_from_name("id")\\n1541.         new_field.model = Author\\n1542.         with connection.schema_editor() as editor:\\n1543.             editor.alter_field(Author, old_field, new_field, strict=True)\\n1544.         # This will fail if DROP DEFAULT is inadvertently executed on this\\n1545.         # field which drops the id sequence, at least on PostgreSQL.\\n1546.         Author.objects.create(name="Foo")\\n1547.         Author.objects.create(name="Bar")\\n1548. \\n1549.     def test_alter_autofield_pk_to_bigautofield_pk_sequence_owner(self):\\n1550.         """\\n1551.         Converting an implicit PK to BigAutoField(primary_key=True) should keep\\n1552.         a sequence owner on PostgreSQL.\\n1553.         """\\n1554.         with connection.schema_editor() as editor:\\n1555.             editor.create_model(Author)\\n1556.         old_field = Author._meta.get_field("id")\\n1557.         new_field = BigAutoField(primary_key=True)\\n1558.         new_field.set_attributes_from_name("id")\\n1559.         new_field.model = Author\\n1560.         with connection.schema_editor() as editor:\\n1561.             editor.alter_field(Author, old_field, new_field, strict=True)\\n1562. \\n1563.         Author.objects.create(name="Foo", pk=1)\\n1564.         with connection.cursor() as cursor:\\n1565.             sequence_reset_sqls = connection.ops.sequence_reset_sql(\\n1566.                 no_style(), [Author]\\n1567.             )\\n1568.             if sequence_reset_sqls:\\n1569.                 cursor.execute(sequence_reset_sqls[0])\\n1570.         # Fail on PostgreSQL if sequence is missing an owner.\\n1571.         self.assertIsNotNone(Author.objects.create(name="Bar"))\\n1572. \\n1573.     def test_alter_autofield_pk_to_smallautofield_pk_sequence_owner(self):\\n1574.         """\\n1575.         Converting an implicit PK to SmallAutoField(primary_key=True) should\\n1576.         keep a sequence owner on PostgreSQL.\\n1577.         """\\n1578.         with connection.schema_editor() as editor:\\n1579.             editor.create_model(Author)\\n1580.         old_field = Author._meta.get_field("id")\\n1581.         new_field = SmallAutoField(primary_key=True)\\n1582.         new_field.set_attributes_from_name("id")\\n1583.         new_field.model = Author\\n1584.         with connection.schema_editor() as editor:\\n1585.             editor.alter_field(Author, old_field, new_field, strict=True)\\n1586. \\n1587.         Author.objects.create(name="Foo", pk=1)\\n1588.         with connection.cursor() as cursor:\\n1589.             sequence_reset_sqls = connection.ops.sequence_reset_sql(\\n1590.                 no_style(), [Author]\\n1591.             )\\n1592.             if sequence_reset_sqls:\\n1593.                 cursor.execute(sequence_reset_sqls[0])\\n1594.         # Fail on PostgreSQL if sequence is missing an owner.\\n1595.         self.assertIsNotNone(Author.objects.create(name="Bar"))\\n1596. \\n1597.     def test_alter_int_pk_to_autofield_pk(self):\\n1598.         """\\n1599.         Should be able to rename an IntegerField(primary_key=True) to\\n1600.         AutoField(primary_key=True).\\n1601.         """\\n1602.         with connection.schema_editor() as editor:\\n1603.             editor.create_model(IntegerPK)\\n1604. \\n1605.         old_field = IntegerPK._meta.get_field("i")\\n1606.         new_field = AutoField(primary_key=True)\\n1607.         new_field.model = IntegerPK\\n1608.         new_field.set_attributes_from_name("i")\\n1609. \\n1610.         with connection.schema_editor() as editor:\\n1611.             editor.alter_field(IntegerPK, old_field, new_field, strict=True)\\n1612. \\n1613.         # A model representing the updated model.\\n1614.         class IntegerPKToAutoField(Model):\\n1615.             i = AutoField(primary_key=True)\\n1616.             j = IntegerField(unique=True)\\n1617. \\n1618.             class Meta:\\n1619.                 app_label = "schema"\\n1620.                 apps = new_apps\\n1621.                 db_table = IntegerPK._meta.db_table\\n1622. \\n1623.         # An id (i) is generated by the database.\\n1624.         obj = IntegerPKToAutoField.objects.create(j=1)\\n1625.         self.assertIsNotNone(obj.i)\\n1626. \\n1627.     def test_alter_int_pk_to_bigautofield_pk(self):\\n1628.         """\\n1629.         Should be able to rename an IntegerField(primary_key=True) to\\n1630.         BigAutoField(primary_key=True).\\n1631.         """\\n1632.         with connection.schema_editor() as editor:\\n1633.             editor.create_model(IntegerPK)\\n1634. \\n1635.         old_field = IntegerPK._meta.get_field("i")\\n1636.         new_field = BigAutoField(primary_key=True)\\n1637.         new_field.model = IntegerPK\\n1638.         new_field.set_attributes_from_name("i")\\n1639. \\n1640.         with connection.schema_editor() as editor:\\n1641.             editor.alter_field(IntegerPK, old_field, new_field, strict=True)\\n1642. \\n1643.         # A model representing the updated model.\\n1644.         class IntegerPKToBigAutoField(Model):\\n1645.             i = BigAutoField(primary_key=True)\\n1646.             j = IntegerField(unique=True)\\n1647. \\n1648.             class Meta:\\n1649.                 app_label = "schema"\\n1650.                 apps = new_apps\\n1651.                 db_table = IntegerPK._meta.db_table\\n1652. \\n1653.         # An id (i) is generated by the database.\\n1654.         obj = IntegerPKToBigAutoField.objects.create(j=1)\\n1655.         self.assertIsNotNone(obj.i)\\n1656. \\n1657.     @isolate_apps("schema")\\n1658.     def test_alter_smallint_pk_to_smallautofield_pk(self):\\n1659.         """\\n1660.         Should be able to rename an SmallIntegerField(primary_key=True) to\\n1661.         SmallAutoField(primary_key=True).\\n1662.         """\\n1663. \\n1664.         class SmallIntegerPK(Model):\\n1665.             i = SmallIntegerField(primary_key=True)\\n1666. \\n1667.             class Meta:\\n1668.                 app_label = "schema"\\n1669. \\n1670.         with connection.schema_editor() as editor:\\n1671.             editor.create_model(SmallIntegerPK)\\n1672.         self.isolated_local_models = [SmallIntegerPK]\\n1673.         old_field = SmallIntegerPK._meta.get_field("i")\\n1674.         new_field = SmallAutoField(primary_key=True)\\n1675.         new_field.model = SmallIntegerPK\\n1676.         new_field.set_attributes_from_name("i")\\n1677.         with connection.schema_editor() as editor:\\n1678.             editor.alter_field(SmallIntegerPK, old_field, new_field, strict=True)\\n1679. \\n1680.     def test_alter_int_pk_to_int_unique(self):\\n1681.         """\\n1682.         Should be able to rename an IntegerField(primary_key=True) to\\n1683.         IntegerField(unique=True).\\n1684.         """\\n1685.         with connection.schema_editor() as editor:\\n1686.             editor.create_model(IntegerPK)\\n1687.         # Delete the old PK\\n1688.         old_field = IntegerPK._meta.get_field("i")\\n1689.         new_field = IntegerField(unique=True)\\n1690.         new_field.model = IntegerPK\\n1691.         new_field.set_attributes_from_name("i")\\n1692.         with connection.schema_editor() as editor:\\n1693.             editor.alter_field(IntegerPK, old_field, new_field, strict=True)\\n1694.         # The primary key constraint is gone. Result depends on database:\\n1695.         # \\\'id\\\' for SQLite, None for others (must not be \\\'i\\\').\\n1696.         self.assertIn(self.get_primary_key(IntegerPK._meta.db_table), ("id", None))\\n1697. \\n1698.         # Set up a model class as it currently stands. The original IntegerPK\\n1699.         # class is now out of date and some backends make use of the whole\\n1700.         # model class when modifying a field (such as sqlite3 when remaking a\\n1701.         # table) so an outdated model class leads to incorrect results.\\n1702.         class Transitional(Model):\\n1703.             i = IntegerField(unique=True)\\n1704.             j = IntegerField(unique=True)\\n1705. \\n1706.             class Meta:\\n1707.                 app_label = "schema"\\n1708.                 apps = new_apps\\n1709.                 db_table = "INTEGERPK"\\n1710. \\n1711.         # model requires a new PK\\n1712.         old_field = Transitional._meta.get_field("j")\\n1713.         new_field = IntegerField(primary_key=True)\\n1714.         new_field.model = Transitional\\n1715.         new_field.set_attributes_from_name("j")\\n1716. \\n1717.         with connection.schema_editor() as editor:\\n1718.             editor.alter_field(Transitional, old_field, new_field, strict=True)\\n1719. \\n1720.         # Create a model class representing the updated model.\\n1721.         class IntegerUnique(Model):\\n1722.             i = IntegerField(unique=True)\\n1723.             j = IntegerField(primary_key=True)\\n1724. \\n1725.             class Meta:\\n1726.                 app_label = "schema"\\n1727.                 apps = new_apps\\n1728.                 db_table = "INTEGERPK"\\n1729. \\n1730.         # Ensure unique constraint works.\\n1731.         IntegerUnique.objects.create(i=1, j=1)\\n1732.         with self.assertRaises(IntegrityError):\\n1733.             IntegerUnique.objects.create(i=1, j=2)\\n1734. \\n1735.     def test_rename(self):\\n1736.         """\\n1737.         Tests simple altering of fields\\n1738.         """\\n1739.         # Create the table\\n1740.         with connection.schema_editor() as editor:\\n1741.             editor.create_model(Author)\\n1742.         # Ensure the field is right to begin with\\n1743.         columns = self.column_classes(Author)\\n1744.         self.assertEqual(\\n1745.             columns["name"][0],\\n1746.             connection.features.introspected_field_types["CharField"],\\n1747.         )\\n1748.         self.assertNotIn("display_name", columns)\\n1749.         # Alter the name field\\\'s name\\n1750.         old_field = Author._meta.get_field("name")\\n1751.         new_field = CharField(max_length=254)\\n1752.         new_field.set_attributes_from_name("display_name")\\n1753.         with connection.schema_editor() as editor:\\n1754.             editor.alter_field(Author, old_field, new_field, strict=True)\\n1755.         columns = self.column_classes(Author)\\n1756.         self.assertEqual(\\n1757.             columns["display_name"][0],\\n1758.             connection.features.introspected_field_types["CharField"],\\n1759.         )\\n1760.         self.assertNotIn("name", columns)\\n1761. \\n1762.     @isolate_apps("schema")\\n1763.     def test_rename_referenced_field(self):\\n1764.         class Author(Model):\\n1765.             name = CharField(max_length=255, unique=True)\\n1766. \\n1767.             class Meta:\\n1768.                 app_label = "schema"\\n1769. \\n1770.         class Book(Model):\\n1771.             author = ForeignKey(Author, CASCADE, to_field="name")\\n1772. \\n1773.             class Meta:\\n1774.                 app_label = "schema"\\n1775. \\n1776.         with connection.schema_editor() as editor:\\n1777.             editor.create_model(Author)\\n1778.             editor.create_model(Book)\\n1779.         new_field = CharField(max_length=255, unique=True)\\n1780.         new_field.set_attributes_from_name("renamed")\\n1781.         with connection.schema_editor(\\n1782.             atomic=connection.features.supports_atomic_references_rename\\n1783.         ) as editor:\\n1784.             editor.alter_field(Author, Author._meta.get_field("name"), new_field)\\n1785.         # Ensure the foreign key reference was updated.\\n1786.         self.assertForeignKeyExists(Book, "author_id", "schema_author", "renamed")\\n1787. \\n1788.     @skipIfDBFeature("interprets_empty_strings_as_nulls")\\n1789.     def test_rename_keep_null_status(self):\\n1790.         """\\n1791.         Renaming a field shouldn\\\'t affect the not null status.\\n1792.         """\\n1793.         with connection.schema_editor() as editor:\\n1794.             editor.create_model(Note)\\n1795.         with self.assertRaises(IntegrityError):\\n1796.             Note.objects.create(info=None)\\n1797.         old_field = Note._meta.get_field("info")\\n1798.         new_field = TextField()\\n1799.         new_field.set_attributes_from_name("detail_info")\\n1800.         with connection.schema_editor() as editor:\\n1801.             editor.alter_field(Note, old_field, new_field, strict=True)\\n1802.         columns = self.column_classes(Note)\\n1803.         self.assertEqual(columns["detail_info"][0], "TextField")\\n1804.         self.assertNotIn("info", columns)\\n1805.         with self.assertRaises(IntegrityError):\\n1806.             NoteRename.objects.create(detail_info=None)\\n1807. \\n1808.     def _test_m2m_create(self, M2MFieldClass):\\n1809.         """\\n1810.         Tests M2M fields on models during creation\\n1811.         """\\n1812. \\n1813.         class LocalBookWithM2M(Model):\\n1814.             author = ForeignKey(Author, CASCADE)\\n1815.             title = CharField(max_length=100, db_index=True)\\n1816.             pub_date = DateTimeField()\\n1817.             tags = M2MFieldClass("TagM2MTest", related_name="books")\\n1818. \\n1819.             class Meta:\\n1820.                 app_label = "schema"\\n1821.                 apps = new_apps\\n1822. \\n1823.         self.local_models = [LocalBookWithM2M]\\n1824.         # Create the tables\\n1825.         with connection.schema_editor() as editor:\\n1826.             editor.create_model(Author)\\n1827.             editor.create_model(TagM2MTest)\\n1828.             editor.create_model(LocalBookWithM2M)\\n1829.         # Ensure there is now an m2m table there\\n1830.         columns = self.column_classes(\\n1831.             LocalBookWithM2M._meta.get_field("tags").remote_field.through\\n1832.         )\\n1833.         self.assertEqual(\\n1834.             columns["tagm2mtest_id"][0],\\n1835.             connection.features.introspected_field_types["IntegerField"],\\n1836.         )\\n1837. \\n1838.     def test_m2m_create(self):\\n1839.         self._test_m2m_create(ManyToManyField)\\n1840. \\n1841.     def test_m2m_create_custom(self):\\n1842.         self._test_m2m_create(CustomManyToManyField)\\n1843. \\n1844.     def test_m2m_create_inherited(self):\\n1845.         self._test_m2m_create(InheritedManyToManyField)\\n1846. \\n1847.     def _test_m2m_create_through(self, M2MFieldClass):\\n1848.         """\\n1849.         Tests M2M fields on models during creation with through models\\n1850.         """\\n1851. \\n1852.         class LocalTagThrough(Model):\\n1853.             book = ForeignKey("schema.LocalBookWithM2MThrough", CASCADE)\\n1854.             tag = ForeignKey("schema.TagM2MTest", CASCADE)\\n1855. \\n1856.             class Meta:\\n1857.                 app_label = "schema"\\n1858.                 apps = new_apps\\n1859. \\n1860.         class LocalBookWithM2MThrough(Model):\\n1861.             tags = M2MFieldClass(\\n1862.                 "TagM2MTest", related_name="books", through=LocalTagThrough\\n1863.             )\\n1864. \\n1865.             class Meta:\\n1866.                 app_label = "schema"\\n1867.                 apps = new_apps\\n1868. \\n1869.         self.local_models = [LocalTagThrough, LocalBookWithM2MThrough]\\n1870. \\n1871.         # Create the tables\\n1872.         with connection.schema_editor() as editor:\\n1873.             editor.create_model(LocalTagThrough)\\n1874.             editor.create_model(TagM2MTest)\\n1875.             editor.create_model(LocalBookWithM2MThrough)\\n1876.         # Ensure there is now an m2m table there\\n1877.         columns = self.column_classes(LocalTagThrough)\\n1878.         self.assertEqual(\\n1879.             columns["book_id"][0],\\n1880.             connection.features.introspected_field_types["IntegerField"],\\n1881.         )\\n1882.         self.assertEqual(\\n1883.             columns["tag_id"][0],\\n1884.             connection.features.introspected_field_types["IntegerField"],\\n1885.         )\\n1886. \\n1887.     def test_m2m_create_through(self):\\n1888.         self._test_m2m_create_through(ManyToManyField)\\n1889. \\n1890.     def test_m2m_create_through_custom(self):\\n1891.         self._test_m2m_create_through(CustomManyToManyField)\\n1892. \\n1893.     def test_m2m_create_through_inherited(self):\\n1894.         self._test_m2m_create_through(InheritedManyToManyField)\\n1895. \\n1896.     def test_m2m_through_remove(self):\\n1897.         class LocalAuthorNoteThrough(Model):\\n1898.             book = ForeignKey("schema.Author", CASCADE)\\n1899.             tag = ForeignKey("self", CASCADE)\\n1900. \\n1901.             class Meta:\\n1902.                 app_label = "schema"\\n1903.                 apps = new_apps\\n1904. \\n1905.         class LocalNoteWithM2MThrough(Model):\\n1906.             authors = ManyToManyField("schema.Author", through=LocalAuthorNoteThrough)\\n1907. \\n1908.             class Meta:\\n1909.                 app_label = "schema"\\n1910.                 apps = new_apps\\n1911. \\n1912.         self.local_models = [LocalAuthorNoteThrough, LocalNoteWithM2MThrough]\\n1913.         # Create the tables.\\n1914.         with connection.schema_editor() as editor:\\n1915.             editor.create_model(Author)\\n1916.             editor.create_model(LocalAuthorNoteThrough)\\n1917.             editor.create_model(LocalNoteWithM2MThrough)\\n1918.         # Remove the through parameter.\\n1919.         old_field = LocalNoteWithM2MThrough._meta.get_field("authors")\\n1920.         new_field = ManyToManyField("Author")\\n1921.         new_field.set_attributes_from_name("authors")\\n1922.         msg = (\\n1923.             f"Cannot alter field {old_field} into {new_field} - they are not "\\n1924.             f"compatible types (you cannot alter to or from M2M fields, or add or "\\n1925.             f"remove through= on M2M fields)"\\n1926.         )\\n1927.         with connection.schema_editor() as editor:\\n1928.             with self.assertRaisesMessage(ValueError, msg):\\n1929.                 editor.alter_field(LocalNoteWithM2MThrough, old_field, new_field)\\n1930. \\n1931.     def _test_m2m(self, M2MFieldClass):\\n1932.         """\\n1933.         Tests adding/removing M2M fields on models\\n1934.         """\\n1935. \\n1936.         class LocalAuthorWithM2M(Model):\\n1937.             name = CharField(max_length=255)\\n1938. \\n1939.             class Meta:\\n1940.                 app_label = "schema"\\n1941.                 apps = new_apps\\n1942. \\n1943.         self.local_models = [LocalAuthorWithM2M]\\n1944. \\n1945.         # Create the tables\\n1946.         with connection.schema_editor() as editor:\\n1947.             editor.create_model(LocalAuthorWithM2M)\\n1948.             editor.create_model(TagM2MTest)\\n1949.         # Create an M2M field\\n1950.         new_field = M2MFieldClass("schema.TagM2MTest", related_name="authors")\\n1951.         new_field.contribute_to_class(LocalAuthorWithM2M, "tags")\\n1952.         # Ensure there\\\'s no m2m table there\\n1953.         with self.assertRaises(DatabaseError):\\n1954.             self.column_classes(new_field.remote_field.through)\\n1955.         # Add the field\\n1956.         with connection.schema_editor() as editor:\\n1957.             editor.add_field(LocalAuthorWithM2M, new_field)\\n1958.         # Ensure there is now an m2m table there\\n1959.         columns = self.column_classes(new_field.remote_field.through)\\n1960.         self.assertEqual(\\n1961.             columns["tagm2mtest_id"][0],\\n1962.             connection.features.introspected_field_types["IntegerField"],\\n1963.         )\\n1964. \\n1965.         # "Alter" the field. This should not rename the DB table to itself.\\n1966.         with connection.schema_editor() as editor:\\n1967.             editor.alter_field(LocalAuthorWithM2M, new_field, new_field, strict=True)\\n1968. \\n1969.         # Remove the M2M table again\\n1970.         with connection.schema_editor() as editor:\\n1971.             editor.remove_field(LocalAuthorWithM2M, new_field)\\n1972.         # Ensure there\\\'s no m2m table there\\n1973.         with self.assertRaises(DatabaseError):\\n1974.             self.column_classes(new_field.remote_field.through)\\n1975. \\n1976.         # Make sure the model state is coherent with the table one now that\\n1977.         # we\\\'ve removed the tags field.\\n1978.         opts = LocalAuthorWithM2M._meta\\n1979.         opts.local_many_to_many.remove(new_field)\\n1980.         del new_apps.all_models["schema"][\\n1981.             new_field.remote_field.through._meta.model_name\\n1982.         ]\\n1983.         opts._expire_cache()\\n1984. \\n1985.     def test_m2m(self):\\n1986.         self._test_m2m(ManyToManyField)\\n1987. \\n1988.     def test_m2m_custom(self):\\n1989.         self._test_m2m(CustomManyToManyField)\\n1990. \\n1991.     def test_m2m_inherited(self):\\n1992.         self._test_m2m(InheritedManyToManyField)\\n1993. \\n1994.     def _test_m2m_through_alter(self, M2MFieldClass):\\n1995.         """\\n1996.         Tests altering M2Ms with explicit through models (should no-op)\\n1997.         """\\n1998. \\n1999.         class LocalAuthorTag(Model):\\n2000.             author = ForeignKey("schema.LocalAuthorWithM2MThrough", CASCADE)\\n2001.             tag = ForeignKey("schema.TagM2MTest", CASCADE)\\n2002. \\n2003.             class Meta:\\n2004.                 app_label = "schema"\\n2005.                 apps = new_apps\\n2006. \\n2007.         class LocalAuthorWithM2MThrough(Model):\\n2008.             name = CharField(max_length=255)\\n2009.             tags = M2MFieldClass(\\n2010.                 "schema.TagM2MTest", related_name="authors", through=LocalAuthorTag\\n2011.             )\\n2012. \\n2013.             class Meta:\\n2014.                 app_label = "schema"\\n2015.                 apps = new_apps\\n2016. \\n2017.         self.local_models = [LocalAuthorTag, LocalAuthorWithM2MThrough]\\n2018. \\n2019.         # Create the tables\\n2020.         with connection.schema_editor() as editor:\\n2021.             editor.create_model(LocalAuthorTag)\\n2022.             editor.create_model(LocalAuthorWithM2MThrough)\\n2023.             editor.create_model(TagM2MTest)\\n2024.         # Ensure the m2m table is there\\n2025.         self.assertEqual(len(self.column_classes(LocalAuthorTag)), 3)\\n2026.         # "Alter" the field\\\'s blankness. This should not actually do anything.\\n2027.         old_field = LocalAuthorWithM2MThrough._meta.get_field("tags")\\n2028.         new_field = M2MFieldClass(\\n2029.             "schema.TagM2MTest", related_name="authors", through=LocalAuthorTag\\n2030.         )\\n2031.         new_field.contribute_to_class(LocalAuthorWithM2MThrough, "tags")\\n2032.         with connection.schema_editor() as editor:\\n2033.             editor.alter_field(\\n2034.                 LocalAuthorWithM2MThrough, old_field, new_field, strict=True\\n2035.             )\\n2036.         # Ensure the m2m table is still there\\n2037.         self.assertEqual(len(self.column_classes(LocalAuthorTag)), 3)\\n2038. \\n2039.     def test_m2m_through_alter(self):\\n2040.         self._test_m2m_through_alter(ManyToManyField)\\n2041. \\n2042.     def test_m2m_through_alter_custom(self):\\n2043.         self._test_m2m_through_alter(CustomManyToManyField)\\n2044. \\n2045.     def test_m2m_through_alter_inherited(self):\\n2046.         self._test_m2m_through_alter(InheritedManyToManyField)\\n2047. \\n2048.     def _test_m2m_repoint(self, M2MFieldClass):\\n2049.         """\\n2050.         Tests repointing M2M fields\\n2051.         """\\n2052. \\n2053.         class LocalBookWithM2M(Model):\\n2054.             author = ForeignKey(Author, CASCADE)\\n2055.             title = CharField(max_length=100, db_index=True)\\n2056.             pub_date = DateTimeField()\\n2057.             tags = M2MFieldClass("TagM2MTest", related_name="books")\\n2058. \\n2059.             class Meta:\\n2060.                 app_label = "schema"\\n2061.                 apps = new_apps\\n2062. \\n2063.         self.local_models = [LocalBookWithM2M]\\n2064.         # Create the tables\\n2065.         with connection.schema_editor() as editor:\\n2066.             editor.create_model(Author)\\n2067.             editor.create_model(LocalBookWithM2M)\\n2068.             editor.create_model(TagM2MTest)\\n2069.             editor.create_model(UniqueTest)\\n2070.         # Ensure the M2M exists and points to TagM2MTest\\n2071.         if connection.features.supports_foreign_keys:\\n2072.             self.assertForeignKeyExists(\\n2073.                 LocalBookWithM2M._meta.get_field("tags").remote_field.through,\\n2074.                 "tagm2mtest_id",\\n2075.                 "schema_tagm2mtest",\\n2076.             )\\n2077.         # Repoint the M2M\\n2078.         old_field = LocalBookWithM2M._meta.get_field("tags")\\n2079.         new_field = M2MFieldClass(UniqueTest)\\n2080.         new_field.contribute_to_class(LocalBookWithM2M, "uniques")\\n2081.         with connection.schema_editor() as editor:\\n2082.             editor.alter_field(LocalBookWithM2M, old_field, new_field, strict=True)\\n2083.         # Ensure old M2M is gone\\n2084.         with self.assertRaises(DatabaseError):\\n2085.             self.column_classes(\\n2086.                 LocalBookWithM2M._meta.get_field("tags").remote_field.through\\n2087.             )\\n2088. \\n2089.         # This model looks like the new model and is used for teardown.\\n2090.         opts = LocalBookWithM2M._meta\\n2091.         opts.local_many_to_many.remove(old_field)\\n2092.         # Ensure the new M2M exists and points to UniqueTest\\n2093.         if connection.features.supports_foreign_keys:\\n2094.             self.assertForeignKeyExists(\\n2095.                 new_field.remote_field.through, "uniquetest_id", "schema_uniquetest"\\n2096.             )\\n2097. \\n2098.     def test_m2m_repoint(self):\\n2099.         self._test_m2m_repoint(ManyToManyField)\\n2100. \\n2101.     def test_m2m_repoint_custom(self):\\n2102.         self._test_m2m_repoint(CustomManyToManyField)\\n2103. \\n2104.     def test_m2m_repoint_inherited(self):\\n2105.         self._test_m2m_repoint(InheritedManyToManyField)\\n2106. \\n2107.     @isolate_apps("schema")\\n2108.     def test_m2m_rename_field_in_target_model(self):\\n2109.         class LocalTagM2MTest(Model):\\n2110.             title = CharField(max_length=255)\\n2111. \\n2112.             class Meta:\\n2113.                 app_label = "schema"\\n2114. \\n2115.         class LocalM2M(Model):\\n2116.             tags = ManyToManyField(LocalTagM2MTest)\\n2117. \\n2118.             class Meta:\\n2119.                 app_label = "schema"\\n2120. \\n2121.         # Create the tables.\\n2122.         with connection.schema_editor() as editor:\\n2123.             editor.create_model(LocalM2M)\\n2124.             editor.create_model(LocalTagM2MTest)\\n2125.         self.isolated_local_models = [LocalM2M, LocalTagM2MTest]\\n2126.         # Ensure the m2m table is there.\\n2127.         self.assertEqual(len(self.column_classes(LocalM2M)), 1)\\n2128.         # Alter a field in LocalTagM2MTest.\\n2129.         old_field = LocalTagM2MTest._meta.get_field("title")\\n2130.         new_field = CharField(max_length=254)\\n2131.         new_field.contribute_to_class(LocalTagM2MTest, "title1")\\n2132.         # @isolate_apps() and inner models are needed to have the model\\n2133.         # relations populated, otherwise this doesn\\\'t act as a regression test.\\n2134.         self.assertEqual(len(new_field.model._meta.related_objects), 1)\\n2135.         with connection.schema_editor() as editor:\\n2136.             editor.alter_field(LocalTagM2MTest, old_field, new_field, strict=True)\\n2137.         # Ensure the m2m table is still there.\\n2138.         self.assertEqual(len(self.column_classes(LocalM2M)), 1)\\n2139. \\n2140.     @skipUnlessDBFeature(\\n2141.         "supports_column_check_constraints", "can_introspect_check_constraints"\\n2142.     )\\n2143.     def test_check_constraints(self):\\n2144.         """\\n2145.         Tests creating/deleting CHECK constraints\\n2146.         """\\n2147.         # Create the tables\\n2148.         with connection.schema_editor() as editor:\\n2149.             editor.create_model(Author)\\n2150.         # Ensure the constraint exists\\n2151.         constraints = self.get_constraints(Author._meta.db_table)\\n2152.         if not any(\\n2153.             details["columns"] == ["height"] and details["check"]\\n2154.             for details in constraints.values()\\n2155.         ):\\n2156.             self.fail("No check constraint for height found")\\n2157.         # Alter the column to remove it\\n2158.         old_field = Author._meta.get_field("height")\\n2159.         new_field = IntegerField(null=True, blank=True)\\n2160.         new_field.set_attributes_from_name("height")\\n2161.         with connection.schema_editor() as editor:\\n2162.             editor.alter_field(Author, old_field, new_field, strict=True)\\n2163.         constraints = self.get_constraints(Author._meta.db_table)\\n2164.         for details in constraints.values():\\n2165.             if details["columns"] == ["height"] and details["check"]:\\n2166.                 self.fail("Check constraint for height found")\\n2167.         # Alter the column to re-add it\\n2168.         new_field2 = Author._meta.get_field("height")\\n2169.         with connection.schema_editor() as editor:\\n2170.             editor.alter_field(Author, new_field, new_field2, strict=True)\\n2171.         constraints = self.get_constraints(Author._meta.db_table)\\n2172.         if not any(\\n2173.             details["columns"] == ["height"] and details["check"]\\n2174.             for details in constraints.values()\\n2175.         ):\\n2176.             self.fail("No check constraint for height found")\\n2177. \\n2178.     @skipUnlessDBFeature(\\n2179.         "supports_column_check_constraints", "can_introspect_check_constraints"\\n2180.     )\\n2181.     @isolate_apps("schema")\\n2182.     def test_check_constraint_timedelta_param(self):\\n2183.         class DurationModel(Model):\\n2184.             duration = DurationField()\\n2185. \\n2186.             class Meta:\\n2187.                 app_label = "schema"\\n2188. \\n2189.         with connection.schema_editor() as editor:\\n2190.             editor.create_model(DurationModel)\\n2191.         self.isolated_local_models = [DurationModel]\\n2192.         constraint_name = "duration_gte_5_minutes"\\n2193.         constraint = CheckConstraint(\\n2194.             check=Q(duration__gt=datetime.timedelta(minutes=5)),\\n2195.             name=constraint_name,\\n2196.         )\\n2197.         DurationModel._meta.constraints = [constraint]\\n2198.         with connection.schema_editor() as editor:\\n2199.             editor.add_constraint(DurationModel, constraint)\\n2200.         constraints = self.get_constraints(DurationModel._meta.db_table)\\n2201.         self.assertIn(constraint_name, constraints)\\n2202.         with self.assertRaises(IntegrityError), atomic():\\n2203.             DurationModel.objects.create(duration=datetime.timedelta(minutes=4))\\n2204.         DurationModel.objects.create(duration=datetime.timedelta(minutes=10))\\n2205. \\n2206.     @skipUnlessDBFeature(\\n2207.         "supports_column_check_constraints", "can_introspect_check_constraints"\\n2208.     )\\n2209.     def test_remove_field_check_does_not_remove_meta_constraints(self):\\n2210.         with connection.schema_editor() as editor:\\n2211.             editor.create_model(Author)\\n2212.         # Add the custom check constraint\\n2213.         constraint = CheckConstraint(\\n2214.             check=Q(height__gte=0), name="author_height_gte_0_check"\\n2215.         )\\n2216.         custom_constraint_name = constraint.name\\n2217.         Author._meta.constraints = [constraint]\\n2218.         with connection.schema_editor() as editor:\\n2219.             editor.add_constraint(Author, constraint)\\n2220.         # Ensure the constraints exist\\n2221.         constraints = self.get_constraints(Author._meta.db_table)\\n2222.         self.assertIn(custom_constraint_name, constraints)\\n2223.         other_constraints = [\\n2224.             name\\n2225.             for name, details in constraints.items()\\n2226.             if details["columns"] == ["height"]\\n2227.             and details["check"]\\n2228.             and name != custom_constraint_name\\n2229.         ]\\n2230.         self.assertEqual(len(other_constraints), 1)\\n2231.         # Alter the column to remove field check\\n2232.         old_field = Author._meta.get_field("height")\\n2233.         new_field = IntegerField(null=True, blank=True)\\n2234.         new_field.set_attributes_from_name("height")\\n2235.         with connection.schema_editor() as editor:\\n2236.             editor.alter_field(Author, old_field, new_field, strict=True)\\n2237.         constraints = self.get_constraints(Author._meta.db_table)\\n2238.         self.assertIn(custom_constraint_name, constraints)\\n2239.         other_constraints = [\\n2240.             name\\n2241.             for name, details in constraints.items()\\n2242.             if details["columns"] == ["height"]\\n2243.             and details["check"]\\n2244.             and name != custom_constraint_name\\n2245.         ]\\n2246.         self.assertEqual(len(other_constraints), 0)\\n2247.         # Alter the column to re-add field check\\n2248.         new_field2 = Author._meta.get_field("height")\\n2249.         with connection.schema_editor() as editor:\\n2250.             editor.alter_field(Author, new_field, new_field2, strict=True)\\n2251.         constraints = self.get_constraints(Author._meta.db_table)\\n2252.         self.assertIn(custom_constraint_name, constraints)\\n2253.         other_constraints = [\\n2254.             name\\n2255.             for name, details in constraints.items()\\n2256.             if details["columns"] == ["height"]\\n2257.             and details["check"]\\n2258.             and name != custom_constraint_name\\n2259.         ]\\n2260.         self.assertEqual(len(other_constraints), 1)\\n2261.         # Drop the check constraint\\n2262.         with connection.schema_editor() as editor:\\n2263.             Author._meta.constraints = []\\n2264.             editor.remove_constraint(Author, constraint)\\n2265. \\n2266.     def test_unique(self):\\n2267.         """\\n2268.         Tests removing and adding unique constraints to a single column.\\n2269.         """\\n2270.         # Create the table\\n2271.         with connection.schema_editor() as editor:\\n2272.             editor.create_model(Tag)\\n2273.         # Ensure the field is unique to begin with\\n2274.         Tag.objects.create(title="foo", slug="foo")\\n2275.         with self.assertRaises(IntegrityError):\\n2276.             Tag.objects.create(title="bar", slug="foo")\\n2277.         Tag.objects.all().delete()\\n2278.         # Alter the slug field to be non-unique\\n2279.         old_field = Tag._meta.get_field("slug")\\n2280.         new_field = SlugField(unique=False)\\n2281.         new_field.set_attributes_from_name("slug")\\n2282.         with connection.schema_editor() as editor:\\n2283.             editor.alter_field(Tag, old_field, new_field, strict=True)\\n2284.         # Ensure the field is no longer unique\\n2285.         Tag.objects.create(title="foo", slug="foo")\\n2286.         Tag.objects.create(title="bar", slug="foo")\\n2287.         Tag.objects.all().delete()\\n2288.         # Alter the slug field to be unique\\n2289.         new_field2 = SlugField(unique=True)\\n2290.         new_field2.set_attributes_from_name("slug")\\n2291.         with connection.schema_editor() as editor:\\n2292.             editor.alter_field(Tag, new_field, new_field2, strict=True)\\n2293.         # Ensure the field is unique again\\n2294.         Tag.objects.create(title="foo", slug="foo")\\n2295.         with self.assertRaises(IntegrityError):\\n2296.             Tag.objects.create(title="bar", slug="foo")\\n2297.         Tag.objects.all().delete()\\n2298.         # Rename the field\\n2299.         new_field3 = SlugField(unique=True)\\n2300.         new_field3.set_attributes_from_name("slug2")\\n2301.         with connection.schema_editor() as editor:\\n2302.             editor.alter_field(Tag, new_field2, new_field3, strict=True)\\n2303.         # Ensure the field is still unique\\n2304.         TagUniqueRename.objects.create(title="foo", slug2="foo")\\n2305.         with self.assertRaises(IntegrityError):\\n2306.             TagUniqueRename.objects.create(title="bar", slug2="foo")\\n2307.         Tag.objects.all().delete()\\n2308. \\n2309.     def test_unique_name_quoting(self):\\n2310.         old_table_name = TagUniqueRename._meta.db_table\\n2311.         try:\\n2312.             with connection.schema_editor() as editor:\\n2313.                 editor.create_model(TagUniqueRename)\\n2314.                 editor.alter_db_table(TagUniqueRename, old_table_name, "unique-table")\\n2315.                 TagUniqueRename._meta.db_table = "unique-table"\\n2316.                 # This fails if the unique index name isn\\\'t quoted.\\n2317.                 editor.alter_unique_together(TagUniqueRename, [], (("title", "slug2"),))\\n2318.         finally:\\n2319.             with connection.schema_editor() as editor:\\n2320.                 editor.delete_model(TagUniqueRename)\\n2321.             TagUniqueRename._meta.db_table = old_table_name\\n2322. \\n2323.     @isolate_apps("schema")\\n2324.     @skipUnlessDBFeature("supports_foreign_keys")\\n2325.     def test_unique_no_unnecessary_fk_drops(self):\\n2326.         """\\n2327.         If AlterField isn\\\'t selective about dropping foreign key constraints\\n2328.         when modifying a field with a unique constraint, the AlterField\\n2329.         incorrectly drops and recreates the Book.author foreign key even though\\n2330.         it doesn\\\'t restrict the field being changed (#29193).\\n2331.         """\\n2332. \\n2333.         class Author(Model):\\n2334.             name = CharField(max_length=254, unique=True)\\n2335. \\n2336.             class Meta:\\n2337.                 app_label = "schema"\\n2338. \\n2339.         class Book(Model):\\n2340.             author = ForeignKey(Author, CASCADE)\\n2341. \\n2342.             class Meta:\\n2343.                 app_label = "schema"\\n2344. \\n2345.         with connection.schema_editor() as editor:\\n2346.             editor.create_model(Author)\\n2347.             editor.create_model(Book)\\n2348.         new_field = CharField(max_length=255, unique=True)\\n2349.         new_field.model = Author\\n2350.         new_field.set_attributes_from_name("name")\\n2351.         with self.assertLogs("django.db.backends.schema", "DEBUG") as cm:\\n2352.             with connection.schema_editor() as editor:\\n2353.                 editor.alter_field(Author, Author._meta.get_field("name"), new_field)\\n2354.         # One SQL statement is executed to alter the field.\\n2355.         self.assertEqual(len(cm.records), 1)\\n2356. \\n2357.     @isolate_apps("schema")\\n2358.     def test_unique_and_reverse_m2m(self):\\n2359.         """\\n2360.         AlterField can modify a unique field when there\\\'s a reverse M2M\\n2361.         relation on the model.\\n2362.         """\\n2363. \\n2364.         class Tag(Model):\\n2365.             title = CharField(max_length=255)\\n2366.             slug = SlugField(unique=True)\\n2367. \\n2368.             class Meta:\\n2369.                 app_label = "schema"\\n2370. \\n2371.         class Book(Model):\\n2372.             tags = ManyToManyField(Tag, related_name="books")\\n2373. \\n2374.             class Meta:\\n2375.                 app_label = "schema"\\n2376. \\n2377.         self.isolated_local_models = [Book._meta.get_field("tags").remote_field.through]\\n2378.         with connection.schema_editor() as editor:\\n2379.             editor.create_model(Tag)\\n2380.             editor.create_model(Book)\\n2381.         new_field = SlugField(max_length=75, unique=True)\\n2382.         new_field.model = Tag\\n2383.         new_field.set_attributes_from_name("slug")\\n2384.         with self.assertLogs("django.db.backends.schema", "DEBUG") as cm:\\n2385.             with connection.schema_editor() as editor:\\n2386.                 editor.alter_field(Tag, Tag._meta.get_field("slug"), new_field)\\n2387.         # One SQL statement is executed to alter the field.\\n2388.         self.assertEqual(len(cm.records), 1)\\n2389.         # Ensure that the field is still unique.\\n2390.         Tag.objects.create(title="foo", slug="foo")\\n2391.         with self.assertRaises(IntegrityError):\\n2392.             Tag.objects.create(title="bar", slug="foo")\\n2393. \\n2394.     @skipUnlessDBFeature("allows_multiple_constraints_on_same_fields")\\n2395.     def test_remove_field_unique_does_not_remove_meta_constraints(self):\\n2396.         with connection.schema_editor() as editor:\\n2397.             editor.create_model(AuthorWithUniqueName)\\n2398.         self.local_models = [AuthorWithUniqueName]\\n2399.         # Add the custom unique constraint\\n2400.         constraint = UniqueConstraint(fields=["name"], name="author_name_uniq")\\n2401.         custom_constraint_name = constraint.name\\n2402.         AuthorWithUniqueName._meta.constraints = [constraint]\\n2403.         with connection.schema_editor() as editor:\\n2404.             editor.add_constraint(AuthorWithUniqueName, constraint)\\n2405.         # Ensure the constraints exist\\n2406.         constraints = self.get_constraints(AuthorWithUniqueName._meta.db_table)\\n2407.         self.assertIn(custom_constraint_name, constraints)\\n2408.         other_constraints = [\\n2409.             name\\n2410.             for name, details in constraints.items()\\n2411.             if details["columns"] == ["name"]\\n2412.             and details["unique"]\\n2413.             and name != custom_constraint_name\\n2414.         ]\\n2415.         self.assertEqual(len(other_constraints), 1)\\n2416.         # Alter the column to remove field uniqueness\\n2417.         old_field = AuthorWithUniqueName._meta.get_field("name")\\n2418.         new_field = CharField(max_length=255)\\n2419.         new_field.set_attributes_from_name("name")\\n2420.         with connection.schema_editor() as editor:\\n2421.             editor.alter_field(AuthorWithUniqueName, old_field, new_field, strict=True)\\n2422.         constraints = self.get_constraints(AuthorWithUniqueName._meta.db_table)\\n2423.         self.assertIn(custom_constraint_name, constraints)\\n2424.         other_constraints = [\\n2425.             name\\n2426.             for name, details in constraints.items()\\n2427.             if details["columns"] == ["name"]\\n2428.             and details["unique"]\\n2429.             and name != custom_constraint_name\\n2430.         ]\\n2431.         self.assertEqual(len(other_constraints), 0)\\n2432.         # Alter the column to re-add field uniqueness\\n2433.         new_field2 = AuthorWithUniqueName._meta.get_field("name")\\n2434.         with connection.schema_editor() as editor:\\n2435.             editor.alter_field(AuthorWithUniqueName, new_field, new_field2, strict=True)\\n2436.         constraints = self.get_constraints(AuthorWithUniqueName._meta.db_table)\\n2437.         self.assertIn(custom_constraint_name, constraints)\\n2438.         other_constraints = [\\n2439.             name\\n2440.             for name, details in constraints.items()\\n2441.             if details["columns"] == ["name"]\\n2442.             and details["unique"]\\n2443.             and name != custom_constraint_name\\n2444.         ]\\n2445.         self.assertEqual(len(other_constraints), 1)\\n2446.         # Drop the unique constraint\\n2447.         with connection.schema_editor() as editor:\\n2448.             AuthorWithUniqueName._meta.constraints = []\\n2449.             editor.remove_constraint(AuthorWithUniqueName, constraint)\\n2450. \\n2451.     def test_unique_together(self):\\n2452.         """\\n2453.         Tests removing and adding unique_together constraints on a model.\\n2454.         """\\n2455.         # Create the table\\n2456.         with connection.schema_editor() as editor:\\n2457.             editor.create_model(UniqueTest)\\n2458.         # Ensure the fields are unique to begin with\\n2459.         UniqueTest.objects.create(year=2012, slug="foo")\\n2460.         UniqueTest.objects.create(year=2011, slug="foo")\\n2461.         UniqueTest.objects.create(year=2011, slug="bar")\\n2462.         with self.assertRaises(IntegrityError):\\n2463.             UniqueTest.objects.create(year=2012, slug="foo")\\n2464.         UniqueTest.objects.all().delete()\\n2465.         # Alter the model to its non-unique-together companion\\n2466.         with connection.schema_editor() as editor:\\n2467.             editor.alter_unique_together(\\n2468.                 UniqueTest, UniqueTest._meta.unique_together, []\\n2469.             )\\n2470.         # Ensure the fields are no longer unique\\n2471.         UniqueTest.objects.create(year=2012, slug="foo")\\n2472.         UniqueTest.objects.create(year=2012, slug="foo")\\n2473.         UniqueTest.objects.all().delete()\\n2474.         # Alter it back\\n2475.         new_field2 = SlugField(unique=True)\\n2476.         new_field2.set_attributes_from_name("slug")\\n2477.         with connection.schema_editor() as editor:\\n2478.             editor.alter_unique_together(\\n2479.                 UniqueTest, [], UniqueTest._meta.unique_together\\n2480.             )\\n2481.         # Ensure the fields are unique again\\n2482.         UniqueTest.objects.create(year=2012, slug="foo")\\n2483.         with self.assertRaises(IntegrityError):\\n2484.             UniqueTest.objects.create(year=2012, slug="foo")\\n2485.         UniqueTest.objects.all().delete()\\n2486. \\n2487.     def test_unique_together_with_fk(self):\\n2488.         """\\n2489.         Tests removing and adding unique_together constraints that include\\n2490.         a foreign key.\\n2491.         """\\n2492.         # Create the table\\n2493.         with connection.schema_editor() as editor:\\n2494.             editor.create_model(Author)\\n2495.             editor.create_model(Book)\\n2496.         # Ensure the fields are unique to begin with\\n2497.         self.assertEqual(Book._meta.unique_together, ())\\n2498.         # Add the unique_together constraint\\n2499.         with connection.schema_editor() as editor:\\n2500.             editor.alter_unique_together(Book, [], [["author", "title"]])\\n2501.         # Alter it back\\n2502.         with connection.schema_editor() as editor:\\n2503.             editor.alter_unique_together(Book, [["author", "title"]], [])\\n2504. \\n2505.     def test_unique_together_with_fk_with_existing_index(self):\\n2506.         """\\n2507.         Tests removing and adding unique_together constraints that include\\n2508.         a foreign key, where the foreign key is added after the model is\\n2509.         created.\\n2510.         """\\n2511.         # Create the tables\\n2512.         with connection.schema_editor() as editor:\\n2513.             editor.create_model(Author)\\n2514.             editor.create_model(BookWithoutAuthor)\\n2515.             new_field = ForeignKey(Author, CASCADE)\\n2516.             new_field.set_attributes_from_name("author")\\n2517.             editor.add_field(BookWithoutAuthor, new_field)\\n2518.         # Ensure the fields aren\\\'t unique to begin with\\n2519.         self.assertEqual(Book._meta.unique_together, ())\\n2520.         # Add the unique_together constraint\\n2521.         with connection.schema_editor() as editor:\\n2522.             editor.alter_unique_together(Book, [], [["author", "title"]])\\n2523.         # Alter it back\\n2524.         with connection.schema_editor() as editor:\\n2525.             editor.alter_unique_together(Book, [["author", "title"]], [])\\n2526. \\n2527.     @skipUnlessDBFeature("allows_multiple_constraints_on_same_fields")\\n2528.     def test_remove_unique_together_does_not_remove_meta_constraints(self):\\n2529.         with connection.schema_editor() as editor:\\n2530.             editor.create_model(AuthorWithUniqueNameAndBirthday)\\n2531.         self.local_models = [AuthorWithUniqueNameAndBirthday]\\n2532.         # Add the custom unique constraint\\n2533.         constraint = UniqueConstraint(\\n2534.             fields=["name", "birthday"], name="author_name_birthday_uniq"\\n2535.         )\\n2536.         custom_constraint_name = constraint.name\\n2537.         AuthorWithUniqueNameAndBirthday._meta.constraints = [constraint]\\n2538.         with connection.schema_editor() as editor:\\n2539.             editor.add_constraint(AuthorWithUniqueNameAndBirthday, constraint)\\n2540.         # Ensure the constraints exist\\n2541.         constraints = self.get_constraints(\\n2542.             AuthorWithUniqueNameAndBirthday._meta.db_table\\n2543.         )\\n2544.         self.assertIn(custom_constraint_name, constraints)\\n2545.         other_constraints = [\\n2546.             name\\n2547.             for name, details in constraints.items()\\n2548.             if details["columns"] == ["name", "birthday"]\\n2549.             and details["unique"]\\n2550.             and name != custom_constraint_name\\n2551.         ]\\n2552.         self.assertEqual(len(other_constraints), 1)\\n2553.         # Remove unique together\\n2554.         unique_together = AuthorWithUniqueNameAndBirthday._meta.unique_together\\n2555.         with connection.schema_editor() as editor:\\n2556.             editor.alter_unique_together(\\n2557.                 AuthorWithUniqueNameAndBirthday, unique_together, []\\n2558.             )\\n2559.         constraints = self.get_constraints(\\n2560.             AuthorWithUniqueNameAndBirthday._meta.db_table\\n2561.         )\\n2562.         self.assertIn(custom_constraint_name, constraints)\\n2563.         other_constraints = [\\n2564.             name\\n2565.             for name, details in constraints.items()\\n2566.             if details["columns"] == ["name", "birthday"]\\n2567.             and details["unique"]\\n2568.             and name != custom_constraint_name\\n2569.         ]\\n2570.         self.assertEqual(len(other_constraints), 0)\\n2571.         # Re-add unique together\\n2572.         with connection.schema_editor() as editor:\\n2573.             editor.alter_unique_together(\\n2574.                 AuthorWithUniqueNameAndBirthday, [], unique_together\\n2575.             )\\n2576.         constraints = self.get_constraints(\\n2577.             AuthorWithUniqueNameAndBirthday._meta.db_table\\n2578.         )\\n2579.         self.assertIn(custom_constraint_name, constraints)\\n2580.         other_constraints = [\\n2581.             name\\n2582.             for name, details in constraints.items()\\n2583.             if details["columns"] == ["name", "birthday"]\\n2584.             and details["unique"]\\n2585.             and name != custom_constraint_name\\n2586.         ]\\n2587.         self.assertEqual(len(other_constraints), 1)\\n2588.         # Drop the unique constraint\\n2589.         with connection.schema_editor() as editor:\\n2590.             AuthorWithUniqueNameAndBirthday._meta.constraints = []\\n2591.             editor.remove_constraint(AuthorWithUniqueNameAndBirthday, constraint)\\n2592. \\n2593.     def test_unique_constraint(self):\\n2594.         with connection.schema_editor() as editor:\\n2595.             editor.create_model(Author)\\n2596.         constraint = UniqueConstraint(fields=["name"], name="name_uq")\\n2597.         # Add constraint.\\n2598.         with connection.schema_editor() as editor:\\n2599.             editor.add_constraint(Author, constraint)\\n2600.             sql = constraint.create_sql(Author, editor)\\n2601.         table = Author._meta.db_table\\n2602.         self.assertIs(sql.references_table(table), True)\\n2603.         self.assertIs(sql.references_column(table, "name"), True)\\n2604.         # Remove constraint.\\n2605.         with connection.schema_editor() as editor:\\n2606.             editor.remove_constraint(Author, constraint)\\n2607.         self.assertNotIn(constraint.name, self.get_constraints(table))\\n2608. \\n2609.     @skipUnlessDBFeature("supports_expression_indexes")\\n2610.     def test_func_unique_constraint(self):\\n2611.         with connection.schema_editor() as editor:\\n2612.             editor.create_model(Author)\\n2613.         constraint = UniqueConstraint(Upper("name").desc(), name="func_upper_uq")\\n2614.         # Add constraint.\\n2615.         with connection.schema_editor() as editor:\\n2616.             editor.add_constraint(Author, constraint)\\n2617.             sql = constraint.create_sql(Author, editor)\\n2618.         table = Author._meta.db_table\\n2619.         constraints = self.get_constraints(table)\\n2620.         if connection.features.supports_index_column_ordering:\\n2621.             self.assertIndexOrder(table, constraint.name, ["DESC"])\\n2622.         self.assertIn(constraint.name, constraints)\\n2623.         self.assertIs(constraints[constraint.name]["unique"], True)\\n2624.         # SQL contains a database function.\\n2625.         self.assertIs(sql.references_column(table, "name"), True)\\n2626.         self.assertIn("UPPER(%s)" % editor.quote_name("name"), str(sql))\\n2627.         # Remove constraint.\\n2628.         with connection.schema_editor() as editor:\\n2629.             editor.remove_constraint(Author, constraint)\\n2630.         self.assertNotIn(constraint.name, self.get_constraints(table))\\n2631. \\n2632.     @skipUnlessDBFeature("supports_expression_indexes")\\n2633.     def test_composite_func_unique_constraint(self):\\n2634.         with connection.schema_editor() as editor:\\n2635.             editor.create_model(Author)\\n2636.             editor.create_model(BookWithSlug)\\n2637.         constraint = UniqueConstraint(\\n2638.             Upper("title"),\\n2639.             Lower("slug"),\\n2640.             name="func_upper_lower_unq",\\n2641.         )\\n2642.         # Add constraint.\\n2643.         with connection.schema_editor() as editor:\\n2644.             editor.add_constraint(BookWithSlug, constraint)\\n2645.             sql = constraint.create_sql(BookWithSlug, editor)\\n2646.         table = BookWithSlug._meta.db_table\\n2647.         constraints = self.get_constraints(table)\\n2648.         self.assertIn(constraint.name, constraints)\\n2649.         self.assertIs(constraints[constraint.name]["unique"], True)\\n2650.         # SQL contains database functions.\\n2651.         self.assertIs(sql.references_column(table, "title"), True)\\n2652.         self.assertIs(sql.references_column(table, "slug"), True)\\n2653.         sql = str(sql)\\n2654.         self.assertIn("UPPER(%s)" % editor.quote_name("title"), sql)\\n2655.         self.assertIn("LOWER(%s)" % editor.quote_name("slug"), sql)\\n2656.         self.assertLess(sql.index("UPPER"), sql.index("LOWER"))\\n2657.         # Remove constraint.\\n2658.         with connection.schema_editor() as editor:\\n2659.             editor.remove_constraint(BookWithSlug, constraint)\\n2660.         self.assertNotIn(constraint.name, self.get_constraints(table))\\n2661. \\n2662.     @skipUnlessDBFeature("supports_expression_indexes")\\n2663.     def test_unique_constraint_field_and_expression(self):\\n2664.         with connection.schema_editor() as editor:\\n2665.             editor.create_model(Author)\\n2666.         constraint = UniqueConstraint(\\n2667.             F("height").desc(),\\n2668.             "uuid",\\n2669.             Lower("name").asc(),\\n2670.             name="func_f_lower_field_unq",\\n2671.         )\\n2672.         # Add constraint.\\n2673.         with connection.schema_editor() as editor:\\n2674.             editor.add_constraint(Author, constraint)\\n2675.             sql = constraint.create_sql(Author, editor)\\n2676.         table = Author._meta.db_table\\n2677.         if connection.features.supports_index_column_ordering:\\n2678.             self.assertIndexOrder(table, constraint.name, ["DESC", "ASC", "ASC"])\\n2679.         constraints = self.get_constraints(table)\\n2680.         self.assertIs(constraints[constraint.name]["unique"], True)\\n2681.         self.assertEqual(len(constraints[constraint.name]["columns"]), 3)\\n2682.         self.assertEqual(constraints[constraint.name]["columns"][1], "uuid")\\n2683.         # SQL contains database functions and columns.\\n2684.         self.assertIs(sql.references_column(table, "height"), True)\\n2685.         self.assertIs(sql.references_column(table, "name"), True)\\n2686.         self.assertIs(sql.references_column(table, "uuid"), True)\\n2687.         self.assertIn("LOWER(%s)" % editor.quote_name("name"), str(sql))\\n2688.         # Remove constraint.\\n2689.         with connection.schema_editor() as editor:\\n2690.             editor.remove_constraint(Author, constraint)\\n2691.         self.assertNotIn(constraint.name, self.get_constraints(table))\\n2692. \\n2693.     @skipUnlessDBFeature("supports_expression_indexes", "supports_partial_indexes")\\n2694.     def test_func_unique_constraint_partial(self):\\n2695.         with connection.schema_editor() as editor:\\n2696.             editor.create_model(Author)\\n2697.         constraint = UniqueConstraint(\\n2698.             Upper("name"),\\n2699.             name="func_upper_cond_weight_uq",\\n2700.             condition=Q(weight__isnull=False),\\n2701.         )\\n2702.         # Add constraint.\\n2703.         with connection.schema_editor() as editor:\\n2704.             editor.add_constraint(Author, constraint)\\n2705.             sql = constraint.create_sql(Author, editor)\\n2706.         table = Author._meta.db_table\\n2707.         constraints = self.get_constraints(table)\\n2708.         self.assertIn(constraint.name, constraints)\\n2709.         self.assertIs(constraints[constraint.name]["unique"], True)\\n2710.         self.assertIs(sql.references_column(table, "name"), True)\\n2711.         self.assertIn("UPPER(%s)" % editor.quote_name("name"), str(sql))\\n2712.         self.assertIn(\\n2713.             "WHERE %s IS NOT NULL" % editor.quote_name("weight"),\\n2714.             str(sql),\\n2715.         )\\n2716.         # Remove constraint.\\n2717.         with connection.schema_editor() as editor:\\n2718.             editor.remove_constraint(Author, constraint)\\n2719.         self.assertNotIn(constraint.name, self.get_constraints(table))\\n2720. \\n2721.     @skipUnlessDBFeature("supports_expression_indexes", "supports_covering_indexes")\\n2722.     def test_func_unique_constraint_covering(self):\\n2723.         with connection.schema_editor() as editor:\\n2724.             editor.create_model(Author)\\n2725.         constraint = UniqueConstraint(\\n2726.             Upper("name"),\\n2727.             name="func_upper_covering_uq",\\n2728.             include=["weight", "height"],\\n2729.         )\\n2730.         # Add constraint.\\n2731.         with connection.schema_editor() as editor:\\n2732.             editor.add_constraint(Author, constraint)\\n2733.             sql = constraint.create_sql(Author, editor)\\n2734.         table = Author._meta.db_table\\n2735.         constraints = self.get_constraints(table)\\n2736.         self.assertIn(constraint.name, constraints)\\n2737.         self.assertIs(constraints[constraint.name]["unique"], True)\\n2738.         self.assertEqual(\\n2739.             constraints[constraint.name]["columns"],\\n2740.             [None, "weight", "height"],\\n2741.         )\\n2742.         self.assertIs(sql.references_column(table, "name"), True)\\n2743.         self.assertIs(sql.references_column(table, "weight"), True)\\n2744.         self.assertIs(sql.references_column(table, "height"), True)\\n2745.         self.assertIn("UPPER(%s)" % editor.quote_name("name"), str(sql))\\n2746.         self.assertIn(\\n2747.             "INCLUDE (%s, %s)"\\n2748.             % (\\n2749.                 editor.quote_name("weight"),\\n2750.                 editor.quote_name("height"),\\n2751.             ),\\n2752.             str(sql),\\n2753.         )\\n2754.         # Remove constraint.\\n2755.         with connection.schema_editor() as editor:\\n2756.             editor.remove_constraint(Author, constraint)\\n2757.         self.assertNotIn(constraint.name, self.get_constraints(table))\\n2758. \\n2759.     @skipUnlessDBFeature("supports_expression_indexes")\\n2760.     def test_func_unique_constraint_lookups(self):\\n2761.         with connection.schema_editor() as editor:\\n2762.             editor.create_model(Author)\\n2763.         with register_lookup(CharField, Lower), register_lookup(IntegerField, Abs):\\n2764.             constraint = UniqueConstraint(\\n2765.                 F("name__lower"),\\n2766.                 F("weight__abs"),\\n2767.                 name="func_lower_abs_lookup_uq",\\n2768.             )\\n2769.             # Add constraint.\\n2770.             with connection.schema_editor() as editor:\\n2771.                 editor.add_constraint(Author, constraint)\\n2772.                 sql = constraint.create_sql(Author, editor)\\n2773.         table = Author._meta.db_table\\n2774.         constraints = self.get_constraints(table)\\n2775.         self.assertIn(constraint.name, constraints)\\n2776.         self.assertIs(constraints[constraint.name]["unique"], True)\\n2777.         # SQL contains columns.\\n2778.         self.assertIs(sql.references_column(table, "name"), True)\\n2779.         self.assertIs(sql.references_column(table, "weight"), True)\\n2780.         # Remove constraint.\\n2781.         with connection.schema_editor() as editor:\\n2782.             editor.remove_constraint(Author, constraint)\\n2783.         self.assertNotIn(constraint.name, self.get_constraints(table))\\n2784. \\n2785.     @skipUnlessDBFeature("supports_expression_indexes")\\n2786.     def test_func_unique_constraint_collate(self):\\n2787.         collation = connection.features.test_collations.get("non_default")\\n2788.         if not collation:\\n2789.             self.skipTest("This backend does not support case-insensitive collations.")\\n2790.         with connection.schema_editor() as editor:\\n2791.             editor.create_model(Author)\\n2792.             editor.create_model(BookWithSlug)\\n2793.         constraint = UniqueConstraint(\\n2794.             Collate(F("title"), collation=collation).desc(),\\n2795.             Collate("slug", collation=collation),\\n2796.             name="func_collate_uq",\\n2797.         )\\n2798.         # Add constraint.\\n2799.         with connection.schema_editor() as editor:\\n2800.             editor.add_constraint(BookWithSlug, constraint)\\n2801.             sql = constraint.create_sql(BookWithSlug, editor)\\n2802.         table = BookWithSlug._meta.db_table\\n2803.         constraints = self.get_constraints(table)\\n2804.         self.assertIn(constraint.name, constraints)\\n2805.         self.assertIs(constraints[constraint.name]["unique"], True)\\n2806.         if connection.features.supports_index_column_ordering:\\n2807.             self.assertIndexOrder(table, constraint.name, ["DESC", "ASC"])\\n2808.         # SQL contains columns and a collation.\\n2809.         self.assertIs(sql.references_column(table, "title"), True)\\n2810.         self.assertIs(sql.references_column(table, "slug"), True)\\n2811.         self.assertIn("COLLATE %s" % editor.quote_name(collation), str(sql))\\n2812.         # Remove constraint.\\n2813.         with connection.schema_editor() as editor:\\n2814.             editor.remove_constraint(BookWithSlug, constraint)\\n2815.         self.assertNotIn(constraint.name, self.get_constraints(table))\\n2816. \\n2817.     @skipIfDBFeature("supports_expression_indexes")\\n2818.     def test_func_unique_constraint_unsupported(self):\\n2819.         # UniqueConstraint is ignored on databases that don\\\'t support indexes on\\n2820.         # expressions.\\n2821.         with connection.schema_editor() as editor:\\n2822.             editor.create_model(Author)\\n2823.         constraint = UniqueConstraint(F("name"), name="func_name_uq")\\n2824.         with connection.schema_editor() as editor, self.assertNumQueries(0):\\n2825.             self.assertIsNone(editor.add_constraint(Author, constraint))\\n2826.             self.assertIsNone(editor.remove_constraint(Author, constraint))\\n2827. \\n2828.     @skipUnlessDBFeature("supports_expression_indexes")\\n2829.     def test_func_unique_constraint_nonexistent_field(self):\\n2830.         constraint = UniqueConstraint(Lower("nonexistent"), name="func_nonexistent_uq")\\n2831.         msg = (\\n2832.             "Cannot resolve keyword \\\'nonexistent\\\' into field. Choices are: "\\n2833.             "height, id, name, uuid, weight"\\n2834.         )\\n2835.         with self.assertRaisesMessage(FieldError, msg):\\n2836.             with connection.schema_editor() as editor:\\n2837.                 editor.add_constraint(Author, constraint)\\n2838. \\n2839.     @skipUnlessDBFeature("supports_expression_indexes")\\n2840.     def test_func_unique_constraint_nondeterministic(self):\\n2841.         with connection.schema_editor() as editor:\\n2842.             editor.create_model(Author)\\n2843.         constraint = UniqueConstraint(Random(), name="func_random_uq")\\n2844.         with connection.schema_editor() as editor:\\n2845.             with self.assertRaises(DatabaseError):\\n2846.                 editor.add_constraint(Author, constraint)\\n2847. \\n2848.     def test_index_together(self):\\n2849.         """\\n2850.         Tests removing and adding index_together constraints on a model.\\n2851.         """\\n2852.         # Create the table\\n2853.         with connection.schema_editor() as editor:\\n2854.             editor.create_model(Tag)\\n2855.         # Ensure there\\\'s no index on the year/slug columns first\\n2856.         self.assertIs(\\n2857.             any(\\n2858.                 c["index"]\\n2859.                 for c in self.get_constraints("schema_tag").values()\\n2860.                 if c["columns"] == ["slug", "title"]\\n2861.             ),\\n2862.             False,\\n2863.         )\\n2864.         # Alter the model to add an index\\n2865.         with connection.schema_editor() as editor:\\n2866.             editor.alter_index_together(Tag, [], [("slug", "title")])\\n2867.         # Ensure there is now an index\\n2868.         self.assertIs(\\n2869.             any(\\n2870.                 c["index"]\\n2871.                 for c in self.get_constraints("schema_tag").values()\\n2872.                 if c["columns"] == ["slug", "title"]\\n2873.             ),\\n2874.             True,\\n2875.         )\\n2876.         # Alter it back\\n2877.         new_field2 = SlugField(unique=True)\\n2878.         new_field2.set_attributes_from_name("slug")\\n2879.         with connection.schema_editor() as editor:\\n2880.             editor.alter_index_together(Tag, [("slug", "title")], [])\\n2881.         # Ensure there\\\'s no index\\n2882.         self.assertIs(\\n2883.             any(\\n2884.                 c["index"]\\n2885.                 for c in self.get_constraints("schema_tag").values()\\n2886.                 if c["columns"] == ["slug", "title"]\\n2887.             ),\\n2888.             False,\\n2889.         )\\n2890. \\n2891.     def test_index_together_with_fk(self):\\n2892.         """\\n2893.         Tests removing and adding index_together constraints that include\\n2894.         a foreign key.\\n2895.         """\\n2896.         # Create the table\\n2897.         with connection.schema_editor() as editor:\\n2898.             editor.create_model(Author)\\n2899.             editor.create_model(Book)\\n2900.         # Ensure the fields are unique to begin with\\n2901.         self.assertEqual(Book._meta.index_together, ())\\n2902.         # Add the unique_together constraint\\n2903.         with connection.schema_editor() as editor:\\n2904.             editor.alter_index_together(Book, [], [["author", "title"]])\\n2905.         # Alter it back\\n2906.         with connection.schema_editor() as editor:\\n2907.             editor.alter_index_together(Book, [["author", "title"]], [])\\n2908. \\n2909.     def test_create_index_together(self):\\n2910.         """\\n2911.         Tests creating models with index_together already defined\\n2912.         """\\n2913.         # Create the table\\n2914.         with connection.schema_editor() as editor:\\n2915.             editor.create_model(TagIndexed)\\n2916.         # Ensure there is an index\\n2917.         self.assertIs(\\n2918.             any(\\n2919.                 c["index"]\\n2920.                 for c in self.get_constraints("schema_tagindexed").values()\\n2921.                 if c["columns"] == ["slug", "title"]\\n2922.             ),\\n2923.             True,\\n2924.         )\\n2925. \\n2926.     @skipUnlessDBFeature("allows_multiple_constraints_on_same_fields")\\n2927.     def test_remove_index_together_does_not_remove_meta_indexes(self):\\n2928.         with connection.schema_editor() as editor:\\n2929.             editor.create_model(AuthorWithIndexedNameAndBirthday)\\n2930.         self.local_models = [AuthorWithIndexedNameAndBirthday]\\n2931.         # Add the custom index\\n2932.         index = Index(fields=["name", "birthday"], name="author_name_birthday_idx")\\n2933.         custom_index_name = index.name\\n2934.         AuthorWithIndexedNameAndBirthday._meta.indexes = [index]\\n2935.         with connection.schema_editor() as editor:\\n2936.             editor.add_index(AuthorWithIndexedNameAndBirthday, index)\\n2937.         # Ensure the indexes exist\\n2938.         constraints = self.get_constraints(\\n2939.             AuthorWithIndexedNameAndBirthday._meta.db_table\\n2940.         )\\n2941.         self.assertIn(custom_index_name, constraints)\\n2942.         other_constraints = [\\n2943.             name\\n2944.             for name, details in constraints.items()\\n2945.             if details["columns"] == ["name", "birthday"]\\n2946.             and details["index"]\\n2947.             and name != custom_index_name\\n2948.         ]\\n2949.         self.assertEqual(len(other_constraints), 1)\\n2950.         # Remove index together\\n2951.         index_together = AuthorWithIndexedNameAndBirthday._meta.index_together\\n2952.         with connection.schema_editor() as editor:\\n2953.             editor.alter_index_together(\\n2954.                 AuthorWithIndexedNameAndBirthday, index_together, []\\n2955.             )\\n2956.         constraints = self.get_constraints(\\n2957.             AuthorWithIndexedNameAndBirthday._meta.db_table\\n2958.         )\\n2959.         self.assertIn(custom_index_name, constraints)\\n2960.         other_constraints = [\\n2961.             name\\n2962.             for name, details in constraints.items()\\n2963.             if details["columns"] == ["name", "birthday"]\\n2964.             and details["index"]\\n2965.             and name != custom_index_name\\n2966.         ]\\n2967.         self.assertEqual(len(other_constraints), 0)\\n2968.         # Re-add index together\\n2969.         with connection.schema_editor() as editor:\\n2970.             editor.alter_index_together(\\n2971.                 AuthorWithIndexedNameAndBirthday, [], index_together\\n2972.             )\\n2973.         constraints = self.get_constraints(\\n2974.             AuthorWithIndexedNameAndBirthday._meta.db_table\\n2975.         )\\n2976.         self.assertIn(custom_index_name, constraints)\\n2977.         other_constraints = [\\n2978.             name\\n2979.             for name, details in constraints.items()\\n2980.             if details["columns"] == ["name", "birthday"]\\n2981.             and details["index"]\\n2982.             and name != custom_index_name\\n2983.         ]\\n2984.         self.assertEqual(len(other_constraints), 1)\\n2985.         # Drop the index\\n2986.         with connection.schema_editor() as editor:\\n2987.             AuthorWithIndexedNameAndBirthday._meta.indexes = []\\n2988.             editor.remove_index(AuthorWithIndexedNameAndBirthday, index)\\n2989. \\n2990.     @isolate_apps("schema")\\n2991.     def test_db_table(self):\\n2992.         """\\n2993.         Tests renaming of the table\\n2994.         """\\n2995. \\n2996.         class Author(Model):\\n2997.             name = CharField(max_length=255)\\n2998. \\n2999.             class Meta:\\n3000.                 app_label = "schema"\\n3001. \\n3002.         class Book(Model):\\n3003.             author = ForeignKey(Author, CASCADE)\\n3004. \\n3005.             class Meta:\\n3006.                 app_label = "schema"\\n3007. \\n3008.         # Create the table and one referring it.\\n3009.         with connection.schema_editor() as editor:\\n3010.             editor.create_model(Author)\\n3011.             editor.create_model(Book)\\n3012.         # Ensure the table is there to begin with\\n3013.         columns = self.column_classes(Author)\\n3014.         self.assertEqual(\\n3015.             columns["name"][0],\\n3016.             connection.features.introspected_field_types["CharField"],\\n3017.         )\\n3018.         # Alter the table\\n3019.         with connection.schema_editor(\\n3020.             atomic=connection.features.supports_atomic_references_rename\\n3021.         ) as editor:\\n3022.             editor.alter_db_table(Author, "schema_author", "schema_otherauthor")\\n3023.         Author._meta.db_table = "schema_otherauthor"\\n3024.         columns = self.column_classes(Author)\\n3025.         self.assertEqual(\\n3026.             columns["name"][0],\\n3027.             connection.features.introspected_field_types["CharField"],\\n3028.         )\\n3029.         # Ensure the foreign key reference was updated\\n3030.         self.assertForeignKeyExists(Book, "author_id", "schema_otherauthor")\\n3031.         # Alter the table again\\n3032.         with connection.schema_editor(\\n3033.             atomic=connection.features.supports_atomic_references_rename\\n3034.         ) as editor:\\n3035.             editor.alter_db_table(Author, "schema_otherauthor", "schema_author")\\n3036.         # Ensure the table is still there\\n3037.         Author._meta.db_table = "schema_author"\\n3038.         columns = self.column_classes(Author)\\n3039.         self.assertEqual(\\n3040.             columns["name"][0],\\n3041.             connection.features.introspected_field_types["CharField"],\\n3042.         )\\n3043. \\n3044.     def test_add_remove_index(self):\\n3045.         """\\n3046.         Tests index addition and removal\\n3047.         """\\n3048.         # Create the table\\n3049.         with connection.schema_editor() as editor:\\n3050.             editor.create_model(Author)\\n3051.         # Ensure the table is there and has no index\\n3052.         self.assertNotIn("title", self.get_indexes(Author._meta.db_table))\\n3053.         # Add the index\\n3054.         index = Index(fields=["name"], name="author_title_idx")\\n3055.         with connection.schema_editor() as editor:\\n3056.             editor.add_index(Author, index)\\n3057.         self.assertIn("name", self.get_indexes(Author._meta.db_table))\\n3058.         # Drop the index\\n3059.         with connection.schema_editor() as editor:\\n3060.             editor.remove_index(Author, index)\\n3061.         self.assertNotIn("name", self.get_indexes(Author._meta.db_table))\\n3062. \\n3063.     def test_remove_db_index_doesnt_remove_custom_indexes(self):\\n3064.         """\\n3065.         Changing db_index to False doesn\\\'t remove indexes from Meta.indexes.\\n3066.         """\\n3067.         with connection.schema_editor() as editor:\\n3068.             editor.create_model(AuthorWithIndexedName)\\n3069.         self.local_models = [AuthorWithIndexedName]\\n3070.         # Ensure the table has its index\\n3071.         self.assertIn("name", self.get_indexes(AuthorWithIndexedName._meta.db_table))\\n3072. \\n3073.         # Add the custom index\\n3074.         index = Index(fields=["-name"], name="author_name_idx")\\n3075.         author_index_name = index.name\\n3076.         with connection.schema_editor() as editor:\\n3077.             db_index_name = editor._create_index_name(\\n3078.                 table_name=AuthorWithIndexedName._meta.db_table,\\n3079.                 column_names=("name",),\\n3080.             )\\n3081.         try:\\n3082.             AuthorWithIndexedName._meta.indexes = [index]\\n3083.             with connection.schema_editor() as editor:\\n3084.                 editor.add_index(AuthorWithIndexedName, index)\\n3085.             old_constraints = self.get_constraints(AuthorWithIndexedName._meta.db_table)\\n3086.             self.assertIn(author_index_name, old_constraints)\\n3087.             self.assertIn(db_index_name, old_constraints)\\n3088.             # Change name field to db_index=False\\n3089.             old_field = AuthorWithIndexedName._meta.get_field("name")\\n3090.             new_field = CharField(max_length=255)\\n3091.             new_field.set_attributes_from_name("name")\\n3092.             with connection.schema_editor() as editor:\\n3093.                 editor.alter_field(\\n3094.                     AuthorWithIndexedName, old_field, new_field, strict=True\\n3095.                 )\\n3096.             new_constraints = self.get_constraints(AuthorWithIndexedName._meta.db_table)\\n3097.             self.assertNotIn(db_index_name, new_constraints)\\n3098.             # The index from Meta.indexes is still in the database.\\n3099.             self.assertIn(author_index_name, new_constraints)\\n3100.             # Drop the index\\n3101.             with connection.schema_editor() as editor:\\n3102.                 editor.remove_index(AuthorWithIndexedName, index)\\n3103.         finally:\\n3104.             AuthorWithIndexedName._meta.indexes = []\\n3105. \\n3106.     def test_order_index(self):\\n3107.         """\\n3108.         Indexes defined with ordering (ASC/DESC) defined on column\\n3109.         """\\n3110.         with connection.schema_editor() as editor:\\n3111.             editor.create_model(Author)\\n3112.         # The table doesn\\\'t have an index\\n3113.         self.assertNotIn("title", self.get_indexes(Author._meta.db_table))\\n3114.         index_name = "author_name_idx"\\n3115.         # Add the index\\n3116.         index = Index(fields=["name", "-weight"], name=index_name)\\n3117.         with connection.schema_editor() as editor:\\n3118.             editor.add_index(Author, index)\\n3119.         if connection.features.supports_index_column_ordering:\\n3120.             self.assertIndexOrder(Author._meta.db_table, index_name, ["ASC", "DESC"])\\n3121.         # Drop the index\\n3122.         with connection.schema_editor() as editor:\\n3123.             editor.remove_index(Author, index)\\n3124. \\n3125.     def test_indexes(self):\\n3126.         """\\n3127.         Tests creation/altering of indexes\\n3128.         """\\n3129.         # Create the table\\n3130.         with connection.schema_editor() as editor:\\n3131.             editor.create_model(Author)\\n3132.             editor.create_model(Book)\\n3133.         # Ensure the table is there and has the right index\\n3134.         self.assertIn(\\n3135.             "title",\\n3136.             self.get_indexes(Book._meta.db_table),\\n3137.         )\\n3138.         # Alter to remove the index\\n3139.         old_field = Book._meta.get_field("title")\\n3140.         new_field = CharField(max_length=100, db_index=False)\\n3141.         new_field.set_attributes_from_name("title")\\n3142.         with connection.schema_editor() as editor:\\n3143.             editor.alter_field(Book, old_field, new_field, strict=True)\\n3144.         # Ensure the table is there and has no index\\n3145.         self.assertNotIn(\\n3146.             "title",\\n3147.             self.get_indexes(Book._meta.db_table),\\n3148.         )\\n3149.         # Alter to re-add the index\\n3150.         new_field2 = Book._meta.get_field("title")\\n3151.         with connection.schema_editor() as editor:\\n3152.             editor.alter_field(Book, new_field, new_field2, strict=True)\\n3153.         # Ensure the table is there and has the index again\\n3154.         self.assertIn(\\n3155.             "title",\\n3156.             self.get_indexes(Book._meta.db_table),\\n3157.         )\\n3158.         # Add a unique column, verify that creates an implicit index\\n3159.         new_field3 = BookWithSlug._meta.get_field("slug")\\n3160.         with connection.schema_editor() as editor:\\n3161.             editor.add_field(Book, new_field3)\\n3162.         self.assertIn(\\n3163.             "slug",\\n3164.             self.get_uniques(Book._meta.db_table),\\n3165.         )\\n3166.         # Remove the unique, check the index goes with it\\n3167.         new_field4 = CharField(max_length=20, unique=False)\\n3168.         new_field4.set_attributes_from_name("slug")\\n3169.         with connection.schema_editor() as editor:\\n3170.             editor.alter_field(BookWithSlug, new_field3, new_field4, strict=True)\\n3171.         self.assertNotIn(\\n3172.             "slug",\\n3173.             self.get_uniques(Book._meta.db_table),\\n3174.         )\\n3175. \\n3176.     def test_text_field_with_db_index(self):\\n3177.         with connection.schema_editor() as editor:\\n3178.             editor.create_model(AuthorTextFieldWithIndex)\\n3179.         # The text_field index is present if the database supports it.\\n3180.         assertion = (\\n3181.             self.assertIn\\n3182.             if connection.features.supports_index_on_text_field\\n3183.             else self.assertNotIn\\n3184.         )\\n3185.         assertion(\\n3186.             "text_field", self.get_indexes(AuthorTextFieldWithIndex._meta.db_table)\\n3187.         )\\n3188. \\n3189.     def _index_expressions_wrappers(self):\\n3190.         index_expression = IndexExpression()\\n3191.         index_expression.set_wrapper_classes(connection)\\n3192.         return ", ".join(\\n3193.             [\\n3194.                 wrapper_cls.__qualname__\\n3195.                 for wrapper_cls in index_expression.wrapper_classes\\n3196.             ]\\n3197.         )\\n3198. \\n3199.     @skipUnlessDBFeature("supports_expression_indexes")\\n3200.     def test_func_index_multiple_wrapper_references(self):\\n3201.         index = Index(OrderBy(F("name").desc(), descending=True), name="name")\\n3202.         msg = (\\n3203.             "Multiple references to %s can\\\'t be used in an indexed expression."\\n3204.             % self._index_expressions_wrappers()\\n3205.         )\\n3206.         with connection.schema_editor() as editor:\\n3207.             with self.assertRaisesMessage(ValueError, msg):\\n3208.                 editor.add_index(Author, index)\\n3209. \\n3210.     @skipUnlessDBFeature("supports_expression_indexes")\\n3211.     def test_func_index_invalid_topmost_expressions(self):\\n3212.         index = Index(Upper(F("name").desc()), name="name")\\n3213.         msg = (\\n3214.             "%s must be topmost expressions in an indexed expression."\\n3215.             % self._index_expressions_wrappers()\\n3216.         )\\n3217.         with connection.schema_editor() as editor:\\n3218.             with self.assertRaisesMessage(ValueError, msg):\\n3219.                 editor.add_index(Author, index)\\n3220. \\n3221.     @skipUnlessDBFeature("supports_expression_indexes")\\n3222.     def test_func_index(self):\\n3223.         with connection.schema_editor() as editor:\\n3224.             editor.create_model(Author)\\n3225.         index = Index(Lower("name").desc(), name="func_lower_idx")\\n3226.         # Add index.\\n3227.         with connection.schema_editor() as editor:\\n3228.             editor.add_index(Author, index)\\n3229.             sql = index.create_sql(Author, editor)\\n3230.         table = Author._meta.db_table\\n3231.         if connection.features.supports_index_column_ordering:\\n3232.             self.assertIndexOrder(table, index.name, ["DESC"])\\n3233.         # SQL contains a database function.\\n3234.         self.assertIs(sql.references_column(table, "name"), True)\\n3235.         self.assertIn("LOWER(%s)" % editor.quote_name("name"), str(sql))\\n3236.         # Remove index.\\n3237.         with connection.schema_editor() as editor:\\n3238.             editor.remove_index(Author, index)\\n3239.         self.assertNotIn(index.name, self.get_constraints(table))\\n3240. \\n3241.     @skipUnlessDBFeature("supports_expression_indexes")\\n3242.     def test_func_index_f(self):\\n3243.         with connection.schema_editor() as editor:\\n3244.             editor.create_model(Tag)\\n3245.         index = Index("slug", F("title").desc(), name="func_f_idx")\\n3246.         # Add index.\\n3247.         with connection.schema_editor() as editor:\\n3248.             editor.add_index(Tag, index)\\n3249.             sql = index.create_sql(Tag, editor)\\n3250.         table = Tag._meta.db_table\\n3251.         self.assertIn(index.name, self.get_constraints(table))\\n3252.         if connection.features.supports_index_column_ordering:\\n3253.             self.assertIndexOrder(Tag._meta.db_table, index.name, ["ASC", "DESC"])\\n3254.         # SQL contains columns.\\n3255.         self.assertIs(sql.references_column(table, "slug"), True)\\n3256.         self.assertIs(sql.references_column(table, "title"), True)\\n3257.         # Remove index.\\n3258.         with connection.schema_editor() as editor:\\n3259.             editor.remove_index(Tag, index)\\n3260.         self.assertNotIn(index.name, self.get_constraints(table))\\n3261. \\n3262.     @skipUnlessDBFeature("supports_expression_indexes")\\n3263.     def test_func_index_lookups(self):\\n3264.         with connection.schema_editor() as editor:\\n3265.             editor.create_model(Author)\\n3266.         with register_lookup(CharField, Lower), register_lookup(IntegerField, Abs):\\n3267.             index = Index(\\n3268.                 F("name__lower"),\\n3269.                 F("weight__abs"),\\n3270.                 name="func_lower_abs_lookup_idx",\\n3271.             )\\n3272.             # Add index.\\n3273.             with connection.schema_editor() as editor:\\n3274.                 editor.add_index(Author, index)\\n3275.                 sql = index.create_sql(Author, editor)\\n3276.         table = Author._meta.db_table\\n3277.         self.assertIn(index.name, self.get_constraints(table))\\n3278.         # SQL contains columns.\\n3279.         self.assertIs(sql.references_column(table, "name"), True)\\n3280.         self.assertIs(sql.references_column(table, "weight"), True)\\n3281.         # Remove index.\\n3282.         with connection.schema_editor() as editor:\\n3283.             editor.remove_index(Author, index)\\n3284.         self.assertNotIn(index.name, self.get_constraints(table))\\n3285. \\n3286.     @skipUnlessDBFeature("supports_expression_indexes")\\n3287.     def test_composite_func_index(self):\\n3288.         with connection.schema_editor() as editor:\\n3289.             editor.create_model(Author)\\n3290.         index = Index(Lower("name"), Upper("name"), name="func_lower_upper_idx")\\n3291.         # Add index.\\n3292.         with connection.schema_editor() as editor:\\n3293.             editor.add_index(Author, index)\\n3294.             sql = index.create_sql(Author, editor)\\n3295.         table = Author._meta.db_table\\n3296.         self.assertIn(index.name, self.get_constraints(table))\\n3297.         # SQL contains database functions.\\n3298.         self.assertIs(sql.references_column(table, "name"), True)\\n3299.         sql = str(sql)\\n3300.         self.assertIn("LOWER(%s)" % editor.quote_name("name"), sql)\\n3301.         self.assertIn("UPPER(%s)" % editor.quote_name("name"), sql)\\n3302.         self.assertLess(sql.index("LOWER"), sql.index("UPPER"))\\n3303.         # Remove index.\\n3304.         with connection.schema_editor() as editor:\\n3305.             editor.remove_index(Author, index)\\n3306.         self.assertNotIn(index.name, self.get_constraints(table))\\n3307. \\n3308.     @skipUnlessDBFeature("supports_expression_indexes")\\n3309.     def test_composite_func_index_field_and_expression(self):\\n3310.         with connection.schema_editor() as editor:\\n3311.             editor.create_model(Author)\\n3312.             editor.create_model(Book)\\n3313.         index = Index(\\n3314.             F("author").desc(),\\n3315.             Lower("title").asc(),\\n3316.             "pub_date",\\n3317.             name="func_f_lower_field_idx",\\n3318.         )\\n3319.         # Add index.\\n3320.         with connection.schema_editor() as editor:\\n3321.             editor.add_index(Book, index)\\n3322.             sql = index.create_sql(Book, editor)\\n3323.         table = Book._meta.db_table\\n3324.         constraints = self.get_constraints(table)\\n3325.         if connection.features.supports_index_column_ordering:\\n3326.             self.assertIndexOrder(table, index.name, ["DESC", "ASC", "ASC"])\\n3327.         self.assertEqual(len(constraints[index.name]["columns"]), 3)\\n3328.         self.assertEqual(constraints[index.name]["columns"][2], "pub_date")\\n3329.         # SQL contains database functions and columns.\\n3330.         self.assertIs(sql.references_column(table, "author_id"), True)\\n3331.         self.assertIs(sql.references_column(table, "title"), True)\\n3332.         self.assertIs(sql.references_column(table, "pub_date"), True)\\n3333.         self.assertIn("LOWER(%s)" % editor.quote_name("title"), str(sql))\\n3334.         # Remove index.\\n3335.         with connection.schema_editor() as editor:\\n3336.             editor.remove_index(Book, index)\\n3337.         self.assertNotIn(index.name, self.get_constraints(table))\\n3338. \\n3339.     @skipUnlessDBFeature("supports_expression_indexes")\\n3340.     @isolate_apps("schema")\\n3341.     def test_func_index_f_decimalfield(self):\\n3342.         class Node(Model):\\n3343.             value = DecimalField(max_digits=5, decimal_places=2)\\n3344. \\n3345.             class Meta:\\n3346.                 app_label = "schema"\\n3347. \\n3348.         with connection.schema_editor() as editor:\\n3349.             editor.create_model(Node)\\n3350.         index = Index(F("value"), name="func_f_decimalfield_idx")\\n3351.         # Add index.\\n3352.         with connection.schema_editor() as editor:\\n3353.             editor.add_index(Node, index)\\n3354.             sql = index.create_sql(Node, editor)\\n3355.         table = Node._meta.db_table\\n3356.         self.assertIn(index.name, self.get_constraints(table))\\n3357.         self.assertIs(sql.references_column(table, "value"), True)\\n3358.         # SQL doesn\\\'t contain casting.\\n3359.         self.assertNotIn("CAST", str(sql))\\n3360.         # Remove index.\\n3361.         with connection.schema_editor() as editor:\\n3362.             editor.remove_index(Node, index)\\n3363.         self.assertNotIn(index.name, self.get_constraints(table))\\n3364. \\n3365.     @skipUnlessDBFeature("supports_expression_indexes")\\n3366.     def test_func_index_cast(self):\\n3367.         with connection.schema_editor() as editor:\\n3368.             editor.create_model(Author)\\n3369.         index = Index(Cast("weight", FloatField()), name="func_cast_idx")\\n3370.         # Add index.\\n3371.         with connection.schema_editor() as editor:\\n3372.             editor.add_index(Author, index)\\n3373.             sql = index.create_sql(Author, editor)\\n3374.         table = Author._meta.db_table\\n3375.         self.assertIn(index.name, self.get_constraints(table))\\n3376.         self.assertIs(sql.references_column(table, "weight"), True)\\n3377.         # Remove index.\\n3378.         with connection.schema_editor() as editor:\\n3379.             editor.remove_index(Author, index)\\n3380.         self.assertNotIn(index.name, self.get_constraints(table))\\n3381. \\n3382.     @skipUnlessDBFeature("supports_expression_indexes")\\n3383.     def test_func_index_collate(self):\\n3384.         collation = connection.features.test_collations.get("non_default")\\n3385.         if not collation:\\n3386.             self.skipTest("This backend does not support case-insensitive collations.")\\n3387.         with connection.schema_editor() as editor:\\n3388.             editor.create_model(Author)\\n3389.             editor.create_model(BookWithSlug)\\n3390.         index = Index(\\n3391.             Collate(F("title"), collation=collation).desc(),\\n3392.             Collate("slug", collation=collation),\\n3393.             name="func_collate_idx",\\n3394.         )\\n3395.         # Add index.\\n3396.         with connection.schema_editor() as editor:\\n3397.             editor.add_index(BookWithSlug, index)\\n3398.             sql = index.create_sql(BookWithSlug, editor)\\n3399.         table = Book._meta.db_table\\n3400.         self.assertIn(index.name, self.get_constraints(table))\\n3401.         if connection.features.supports_index_column_ordering:\\n3402.             self.assertIndexOrder(table, index.name, ["DESC", "ASC"])\\n3403.         # SQL contains columns and a collation.\\n3404.         self.assertIs(sql.references_column(table, "title"), True)\\n3405.         self.assertIs(sql.references_column(table, "slug"), True)\\n3406.         self.assertIn("COLLATE %s" % editor.quote_name(collation), str(sql))\\n3407.         # Remove index.\\n3408.         with connection.schema_editor() as editor:\\n3409.             editor.remove_index(Book, index)\\n3410.         self.assertNotIn(index.name, self.get_constraints(table))\\n3411. \\n3412.     @skipUnlessDBFeature("supports_expression_indexes")\\n3413.     @skipIfDBFeature("collate_as_index_expression")\\n3414.     def test_func_index_collate_f_ordered(self):\\n3415.         collation = connection.features.test_collations.get("non_default")\\n3416.         if not collation:\\n3417.             self.skipTest("This backend does not support case-insensitive collations.")\\n3418.         with connection.schema_editor() as editor:\\n3419.             editor.create_model(Author)\\n3420.         index = Index(\\n3421.             Collate(F("name").desc(), collation=collation),\\n3422.             name="func_collate_f_desc_idx",\\n3423.         )\\n3424.         # Add index.\\n3425.         with connection.schema_editor() as editor:\\n3426.             editor.add_index(Author, index)\\n3427.             sql = index.create_sql(Author, editor)\\n3428.         table = Author._meta.db_table\\n3429.         self.assertIn(index.name, self.get_constraints(table))\\n3430.         if connection.features.supports_index_column_ordering:\\n3431.             self.assertIndexOrder(table, index.name, ["DESC"])\\n3432.         # SQL contains columns and a collation.\\n3433.         self.assertIs(sql.references_column(table, "name"), True)\\n3434.         self.assertIn("COLLATE %s" % editor.quote_name(collation), str(sql))\\n3435.         # Remove index.\\n3436.         with connection.schema_editor() as editor:\\n3437.             editor.remove_index(Author, index)\\n3438.         self.assertNotIn(index.name, self.get_constraints(table))\\n3439. \\n3440.     @skipUnlessDBFeature("supports_expression_indexes")\\n3441.     def test_func_index_calc(self):\\n3442.         with connection.schema_editor() as editor:\\n3443.             editor.create_model(Author)\\n3444.         index = Index(F("height") / (F("weight") + Value(5)), name="func_calc_idx")\\n3445.         # Add index.\\n3446.         with connection.schema_editor() as editor:\\n3447.             editor.add_index(Author, index)\\n3448.             sql = index.create_sql(Author, editor)\\n3449.         table = Author._meta.db_table\\n3450.         self.assertIn(index.name, self.get_constraints(table))\\n3451.         # SQL contains columns and expressions.\\n3452.         self.assertIs(sql.references_column(table, "height"), True)\\n3453.         self.assertIs(sql.references_column(table, "weight"), True)\\n3454.         sql = str(sql)\\n3455.         self.assertIs(\\n3456.             sql.index(editor.quote_name("height"))\\n3457.             < sql.index("/")\\n3458.             < sql.index(editor.quote_name("weight"))\\n3459.             < sql.index("+")\\n3460.             < sql.index("5"),\\n3461.             True,\\n3462.         )\\n3463.         # Remove index.\\n3464.         with connection.schema_editor() as editor:\\n3465.             editor.remove_index(Author, index)\\n3466.         self.assertNotIn(index.name, self.get_constraints(table))\\n3467. \\n3468.     @skipUnlessDBFeature("supports_expression_indexes", "supports_json_field")\\n3469.     @isolate_apps("schema")\\n3470.     def test_func_index_json_key_transform(self):\\n3471.         class JSONModel(Model):\\n3472.             field = JSONField()\\n3473. \\n3474.             class Meta:\\n3475.                 app_label = "schema"\\n3476. \\n3477.         with connection.schema_editor() as editor:\\n3478.             editor.create_model(JSONModel)\\n3479.         self.isolated_local_models = [JSONModel]\\n3480.         index = Index("field__some_key", name="func_json_key_idx")\\n3481.         with connection.schema_editor() as editor:\\n3482.             editor.add_index(JSONModel, index)\\n3483.             sql = index.create_sql(JSONModel, editor)\\n3484.         table = JSONModel._meta.db_table\\n3485.         self.assertIn(index.name, self.get_constraints(table))\\n3486.         self.assertIs(sql.references_column(table, "field"), True)\\n3487.         with connection.schema_editor() as editor:\\n3488.             editor.remove_index(JSONModel, index)\\n3489.         self.assertNotIn(index.name, self.get_constraints(table))\\n3490. \\n3491.     @skipUnlessDBFeature("supports_expression_indexes", "supports_json_field")\\n3492.     @isolate_apps("schema")\\n3493.     def test_func_index_json_key_transform_cast(self):\\n3494.         class JSONModel(Model):\\n3495.             field = JSONField()\\n3496. \\n3497.             class Meta:\\n3498.                 app_label = "schema"\\n3499. \\n3500.         with connection.schema_editor() as editor:\\n3501.             editor.create_model(JSONModel)\\n3502.         self.isolated_local_models = [JSONModel]\\n3503.         index = Index(\\n3504.             Cast(KeyTextTransform("some_key", "field"), IntegerField()),\\n3505.             name="func_json_key_cast_idx",\\n3506.         )\\n3507.         with connection.schema_editor() as editor:\\n3508.             editor.add_index(JSONModel, index)\\n3509.             sql = index.create_sql(JSONModel, editor)\\n3510.         table = JSONModel._meta.db_table\\n3511.         self.assertIn(index.name, self.get_constraints(table))\\n3512.         self.assertIs(sql.references_column(table, "field"), True)\\n3513.         with connection.schema_editor() as editor:\\n3514.             editor.remove_index(JSONModel, index)\\n3515.         self.assertNotIn(index.name, self.get_constraints(table))\\n3516. \\n3517.     @skipIfDBFeature("supports_expression_indexes")\\n3518.     def test_func_index_unsupported(self):\\n3519.         # Index is ignored on databases that don\\\'t support indexes on\\n3520.         # expressions.\\n3521.         with connection.schema_editor() as editor:\\n3522.             editor.create_model(Author)\\n3523.         index = Index(F("name"), name="random_idx")\\n3524.         with connection.schema_editor() as editor, self.assertNumQueries(0):\\n3525.             self.assertIsNone(editor.add_index(Author, index))\\n3526.             self.assertIsNone(editor.remove_index(Author, index))\\n3527. \\n3528.     @skipUnlessDBFeature("supports_expression_indexes")\\n3529.     def test_func_index_nonexistent_field(self):\\n3530.         index = Index(Lower("nonexistent"), name="func_nonexistent_idx")\\n3531.         msg = (\\n3532.             "Cannot resolve keyword \\\'nonexistent\\\' into field. Choices are: "\\n3533.             "height, id, name, uuid, weight"\\n3534.         )\\n3535.         with self.assertRaisesMessage(FieldError, msg):\\n3536.             with connection.schema_editor() as editor:\\n3537.                 editor.add_index(Author, index)\\n3538. \\n3539.     @skipUnlessDBFeature("supports_expression_indexes")\\n3540.     def test_func_index_nondeterministic(self):\\n3541.         with connection.schema_editor() as editor:\\n3542.             editor.create_model(Author)\\n3543.         index = Index(Random(), name="func_random_idx")\\n3544.         with connection.schema_editor() as editor:\\n3545.             with self.assertRaises(DatabaseError):\\n3546.                 editor.add_index(Author, index)\\n3547. \\n3548.     def test_primary_key(self):\\n3549.         """\\n3550.         Tests altering of the primary key\\n3551.         """\\n3552.         # Create the table\\n3553.         with connection.schema_editor() as editor:\\n3554.             editor.create_model(Tag)\\n3555.         # Ensure the table is there and has the right PK\\n3556.         self.assertEqual(self.get_primary_key(Tag._meta.db_table), "id")\\n3557.         # Alter to change the PK\\n3558.         id_field = Tag._meta.get_field("id")\\n3559.         old_field = Tag._meta.get_field("slug")\\n3560.         new_field = SlugField(primary_key=True)\\n3561.         new_field.set_attributes_from_name("slug")\\n3562.         new_field.model = Tag\\n3563.         with connection.schema_editor() as editor:\\n3564.             editor.remove_field(Tag, id_field)\\n3565.             editor.alter_field(Tag, old_field, new_field)\\n3566.         # Ensure the PK changed\\n3567.         self.assertNotIn(\\n3568.             "id",\\n3569.             self.get_indexes(Tag._meta.db_table),\\n3570.         )\\n3571.         self.assertEqual(self.get_primary_key(Tag._meta.db_table), "slug")\\n3572. \\n3573.     def test_context_manager_exit(self):\\n3574.         """\\n3575.         Ensures transaction is correctly closed when an error occurs\\n3576.         inside a SchemaEditor context.\\n3577.         """\\n3578. \\n3579.         class SomeError(Exception):\\n3580.             pass\\n3581. \\n3582.         try:\\n3583.             with connection.schema_editor():\\n3584.                 raise SomeError\\n3585.         except SomeError:\\n3586.             self.assertFalse(connection.in_atomic_block)\\n3587. \\n3588.     @skipIfDBFeature("can_rollback_ddl")\\n3589.     def test_unsupported_transactional_ddl_disallowed(self):\\n3590.         message = (\\n3591.             "Executing DDL statements while in a transaction on databases "\\n3592.             "that can\\\'t perform a rollback is prohibited."\\n3593.         )\\n3594.         with atomic(), connection.schema_editor() as editor:\\n3595.             with self.assertRaisesMessage(TransactionManagementError, message):\\n3596.                 editor.execute(\\n3597.                     editor.sql_create_table % {"table": "foo", "definition": ""}\\n3598.                 )\\n3599. \\n3600.     @skipUnlessDBFeature("supports_foreign_keys", "indexes_foreign_keys")\\n3601.     def test_foreign_key_index_long_names_regression(self):\\n3602.         """\\n3603.         Regression test for #21497.\\n3604.         Only affects databases that supports foreign keys.\\n3605.         """\\n3606.         # Create the table\\n3607.         with connection.schema_editor() as editor:\\n3608.             editor.create_model(AuthorWithEvenLongerName)\\n3609.             editor.create_model(BookWithLongName)\\n3610.         # Find the properly shortened column name\\n3611.         column_name = connection.ops.quote_name(\\n3612.             "author_foreign_key_with_really_long_field_name_id"\\n3613.         )\\n3614.         column_name = column_name[1:-1].lower()  # unquote, and, for Oracle, un-upcase\\n3615.         # Ensure the table is there and has an index on the column\\n3616.         self.assertIn(\\n3617.             column_name,\\n3618.             self.get_indexes(BookWithLongName._meta.db_table),\\n3619.         )\\n3620. \\n3621.     @skipUnlessDBFeature("supports_foreign_keys")\\n3622.     def test_add_foreign_key_long_names(self):\\n3623.         """\\n3624.         Regression test for #23009.\\n3625.         Only affects databases that supports foreign keys.\\n3626.         """\\n3627.         # Create the initial tables\\n3628.         with connection.schema_editor() as editor:\\n3629.             editor.create_model(AuthorWithEvenLongerName)\\n3630.             editor.create_model(BookWithLongName)\\n3631.         # Add a second FK, this would fail due to long ref name before the fix\\n3632.         new_field = ForeignKey(\\n3633.             AuthorWithEvenLongerName, CASCADE, related_name="something"\\n3634.         )\\n3635.         new_field.set_attributes_from_name(\\n3636.             "author_other_really_long_named_i_mean_so_long_fk"\\n3637.         )\\n3638.         with connection.schema_editor() as editor:\\n3639.             editor.add_field(BookWithLongName, new_field)\\n3640. \\n3641.     @isolate_apps("schema")\\n3642.     @skipUnlessDBFeature("supports_foreign_keys")\\n3643.     def test_add_foreign_key_quoted_db_table(self):\\n3644.         class Author(Model):\\n3645.             class Meta:\\n3646.                 db_table = \\\'"table_author_double_quoted"\\\'\\n3647.                 app_label = "schema"\\n3648. \\n3649.         class Book(Model):\\n3650.             author = ForeignKey(Author, CASCADE)\\n3651. \\n3652.             class Meta:\\n3653.                 app_label = "schema"\\n3654. \\n3655.         with connection.schema_editor() as editor:\\n3656.             editor.create_model(Author)\\n3657.             editor.create_model(Book)\\n3658.         self.isolated_local_models = [Author]\\n3659.         if connection.vendor == "mysql":\\n3660.             self.assertForeignKeyExists(\\n3661.                 Book, "author_id", \\\'"table_author_double_quoted"\\\'\\n3662.             )\\n3663.         else:\\n3664.             self.assertForeignKeyExists(Book, "author_id", "table_author_double_quoted")\\n3665. \\n3666.     def test_add_foreign_object(self):\\n3667.         with connection.schema_editor() as editor:\\n3668.             editor.create_model(BookForeignObj)\\n3669.         self.local_models = [BookForeignObj]\\n3670. \\n3671.         new_field = ForeignObject(\\n3672.             Author, on_delete=CASCADE, from_fields=["author_id"], to_fields=["id"]\\n3673.         )\\n3674.         new_field.set_attributes_from_name("author")\\n3675.         with connection.schema_editor() as editor:\\n3676.             editor.add_field(BookForeignObj, new_field)\\n3677. \\n3678.     def test_creation_deletion_reserved_names(self):\\n3679.         """\\n3680.         Tries creating a model\\\'s table, and then deleting it when it has a\\n3681.         SQL reserved name.\\n3682.         """\\n3683.         # Create the table\\n3684.         with connection.schema_editor() as editor:\\n3685.             try:\\n3686.                 editor.create_model(Thing)\\n3687.             except OperationalError as e:\\n3688.                 self.fail(\\n3689.                     "Errors when applying initial migration for a model "\\n3690.                     "with a table named after an SQL reserved word: %s" % e\\n3691.                 )\\n3692.         # The table is there\\n3693.         list(Thing.objects.all())\\n3694.         # Clean up that table\\n3695.         with connection.schema_editor() as editor:\\n3696.             editor.delete_model(Thing)\\n3697.         # The table is gone\\n3698.         with self.assertRaises(DatabaseError):\\n3699.             list(Thing.objects.all())\\n3700. \\n3701.     def test_remove_constraints_capital_letters(self):\\n3702.         """\\n3703.         #23065 - Constraint names must be quoted if they contain capital letters.\\n3704.         """\\n3705. \\n3706.         def get_field(*args, field_class=IntegerField, **kwargs):\\n3707.             kwargs["db_column"] = "CamelCase"\\n3708.             field = field_class(*args, **kwargs)\\n3709.             field.set_attributes_from_name("CamelCase")\\n3710.             return field\\n3711. \\n3712.         model = Author\\n3713.         field = get_field()\\n3714.         table = model._meta.db_table\\n3715.         column = field.column\\n3716.         identifier_converter = connection.introspection.identifier_converter\\n3717. \\n3718.         with connection.schema_editor() as editor:\\n3719.             editor.create_model(model)\\n3720.             editor.add_field(model, field)\\n3721. \\n3722.             constraint_name = "CamelCaseIndex"\\n3723.             expected_constraint_name = identifier_converter(constraint_name)\\n3724.             editor.execute(\\n3725.                 editor.sql_create_index\\n3726.                 % {\\n3727.                     "table": editor.quote_name(table),\\n3728.                     "name": editor.quote_name(constraint_name),\\n3729.                     "using": "",\\n3730.                     "columns": editor.quote_name(column),\\n3731.                     "extra": "",\\n3732.                     "condition": "",\\n3733.                     "include": "",\\n3734.                 }\\n3735.             )\\n3736.             self.assertIn(\\n3737.                 expected_constraint_name, self.get_constraints(model._meta.db_table)\\n3738.             )\\n3739.             editor.alter_field(model, get_field(db_index=True), field, strict=True)\\n3740.             self.assertNotIn(\\n3741.                 expected_constraint_name, self.get_constraints(model._meta.db_table)\\n3742.             )\\n3743. \\n3744.             constraint_name = "CamelCaseUniqConstraint"\\n3745.             expected_constraint_name = identifier_converter(constraint_name)\\n3746.             editor.execute(editor._create_unique_sql(model, [field], constraint_name))\\n3747.             self.assertIn(\\n3748.                 expected_constraint_name, self.get_constraints(model._meta.db_table)\\n3749.             )\\n3750.             editor.alter_field(model, get_field(unique=True), field, strict=True)\\n3751.             self.assertNotIn(\\n3752.                 expected_constraint_name, self.get_constraints(model._meta.db_table)\\n3753.             )\\n3754. \\n3755.             if editor.sql_create_fk:\\n3756.                 constraint_name = "CamelCaseFKConstraint"\\n3757.                 expected_constraint_name = identifier_converter(constraint_name)\\n3758.                 editor.execute(\\n3759.                     editor.sql_create_fk\\n3760.                     % {\\n3761.                         "table": editor.quote_name(table),\\n3762.                         "name": editor.quote_name(constraint_name),\\n3763.                         "column": editor.quote_name(column),\\n3764.                         "to_table": editor.quote_name(table),\\n3765.                         "to_column": editor.quote_name(model._meta.auto_field.column),\\n3766.                         "deferrable": connection.ops.deferrable_sql(),\\n3767.                     }\\n3768.                 )\\n3769.                 self.assertIn(\\n3770.                     expected_constraint_name, self.get_constraints(model._meta.db_table)\\n3771.                 )\\n3772.                 editor.alter_field(\\n3773.                     model,\\n3774.                     get_field(Author, CASCADE, field_class=ForeignKey),\\n3775.                     field,\\n3776.                     strict=True,\\n3777.                 )\\n3778.                 self.assertNotIn(\\n3779.                     expected_constraint_name, self.get_constraints(model._meta.db_table)\\n3780.                 )\\n3781. \\n3782.     def test_add_field_use_effective_default(self):\\n3783.         """\\n3784.         #23987 - effective_default() should be used as the field default when\\n3785.         adding a new field.\\n3786.         """\\n3787.         # Create the table\\n3788.         with connection.schema_editor() as editor:\\n3789.             editor.create_model(Author)\\n3790.         # Ensure there\\\'s no surname field\\n3791.         columns = self.column_classes(Author)\\n3792.         self.assertNotIn("surname", columns)\\n3793.         # Create a row\\n3794.         Author.objects.create(name="Anonymous1")\\n3795.         # Add new CharField to ensure default will be used from effective_default\\n3796.         new_field = CharField(max_length=15, blank=True)\\n3797.         new_field.set_attributes_from_name("surname")\\n3798.         with connection.schema_editor() as editor:\\n3799.             editor.add_field(Author, new_field)\\n3800.         # Ensure field was added with the right default\\n3801.         with connection.cursor() as cursor:\\n3802.             cursor.execute("SELECT surname FROM schema_author;")\\n3803.             item = cursor.fetchall()[0]\\n3804.             self.assertEqual(\\n3805.                 item[0],\\n3806.                 None if connection.features.interprets_empty_strings_as_nulls else "",\\n3807.             )\\n3808. \\n3809.     def test_add_field_default_dropped(self):\\n3810.         # Create the table\\n3811.         with connection.schema_editor() as editor:\\n3812.             editor.create_model(Author)\\n3813.         # Ensure there\\\'s no surname field\\n3814.         columns = self.column_classes(Author)\\n3815.         self.assertNotIn("surname", columns)\\n3816.         # Create a row\\n3817.         Author.objects.create(name="Anonymous1")\\n3818.         # Add new CharField with a default\\n3819.         new_field = CharField(max_length=15, blank=True, default="surname default")\\n3820.         new_field.set_attributes_from_name("surname")\\n3821.         with connection.schema_editor() as editor:\\n3822.             editor.add_field(Author, new_field)\\n3823.         # Ensure field was added with the right default\\n3824.         with connection.cursor() as cursor:\\n3825.             cursor.execute("SELECT surname FROM schema_author;")\\n3826.             item = cursor.fetchall()[0]\\n3827.             self.assertEqual(item[0], "surname default")\\n3828.             # And that the default is no longer set in the database.\\n3829.             field = next(\\n3830.                 f\\n3831.                 for f in connection.introspection.get_table_description(\\n3832.                     cursor, "schema_author"\\n3833.                 )\\n3834.                 if f.name == "surname"\\n3835.             )\\n3836.             if connection.features.can_introspect_default:\\n3837.                 self.assertIsNone(field.default)\\n3838. \\n3839.     def test_add_field_default_nullable(self):\\n3840.         with connection.schema_editor() as editor:\\n3841.             editor.create_model(Author)\\n3842.         # Add new nullable CharField with a default.\\n3843.         new_field = CharField(max_length=15, blank=True, null=True, default="surname")\\n3844.         new_field.set_attributes_from_name("surname")\\n3845.         with connection.schema_editor() as editor:\\n3846.             editor.add_field(Author, new_field)\\n3847.         Author.objects.create(name="Anonymous1")\\n3848.         with connection.cursor() as cursor:\\n3849.             cursor.execute("SELECT surname FROM schema_author;")\\n3850.             item = cursor.fetchall()[0]\\n3851.             self.assertIsNone(item[0])\\n3852.             field = next(\\n3853.                 f\\n3854.                 for f in connection.introspection.get_table_description(\\n3855.                     cursor,\\n3856.                     "schema_author",\\n3857.                 )\\n3858.                 if f.name == "surname"\\n3859.             )\\n3860.             # Field is still nullable.\\n3861.             self.assertTrue(field.null_ok)\\n3862.             # The database default is no longer set.\\n3863.             if connection.features.can_introspect_default:\\n3864.                 self.assertIn(field.default, ["NULL", None])\\n3865. \\n3866.     def test_add_textfield_default_nullable(self):\\n3867.         with connection.schema_editor() as editor:\\n3868.             editor.create_model(Author)\\n3869.         # Add new nullable TextField with a default.\\n3870.         new_field = TextField(blank=True, null=True, default="text")\\n3871.         new_field.set_attributes_from_name("description")\\n3872.         with connection.schema_editor() as editor:\\n3873.             editor.add_field(Author, new_field)\\n3874.         Author.objects.create(name="Anonymous1")\\n3875.         with connection.cursor() as cursor:\\n3876.             cursor.execute("SELECT description FROM schema_author;")\\n3877.             item = cursor.fetchall()[0]\\n3878.             self.assertIsNone(item[0])\\n3879.             field = next(\\n3880.                 f\\n3881.                 for f in connection.introspection.get_table_description(\\n3882.                     cursor,\\n3883.                     "schema_author",\\n3884.                 )\\n3885.                 if f.name == "description"\\n3886.             )\\n3887.             # Field is still nullable.\\n3888.             self.assertTrue(field.null_ok)\\n3889.             # The database default is no longer set.\\n3890.             if connection.features.can_introspect_default:\\n3891.                 self.assertIn(field.default, ["NULL", None])\\n3892. \\n3893.     def test_alter_field_default_dropped(self):\\n3894.         # Create the table\\n3895.         with connection.schema_editor() as editor:\\n3896.             editor.create_model(Author)\\n3897.         # Create a row\\n3898.         Author.objects.create(name="Anonymous1")\\n3899.         self.assertIsNone(Author.objects.get().height)\\n3900.         old_field = Author._meta.get_field("height")\\n3901.         # The default from the new field is used in updating existing rows.\\n3902.         new_field = IntegerField(blank=True, default=42)\\n3903.         new_field.set_attributes_from_name("height")\\n3904.         with connection.schema_editor() as editor:\\n3905.             editor.alter_field(Author, old_field, new_field, strict=True)\\n3906.         self.assertEqual(Author.objects.get().height, 42)\\n3907.         # The database default should be removed.\\n3908.         with connection.cursor() as cursor:\\n3909.             field = next(\\n3910.                 f\\n3911.                 for f in connection.introspection.get_table_description(\\n3912.                     cursor, "schema_author"\\n3913.                 )\\n3914.                 if f.name == "height"\\n3915.             )\\n3916.             if connection.features.can_introspect_default:\\n3917.                 self.assertIsNone(field.default)\\n3918. \\n3919.     def test_alter_field_default_doesnt_perform_queries(self):\\n3920.         """\\n3921.         No queries are performed if a field default changes and the field\\\'s\\n3922.         not changing from null to non-null.\\n3923.         """\\n3924.         with connection.schema_editor() as editor:\\n3925.             editor.create_model(AuthorWithDefaultHeight)\\n3926.         old_field = AuthorWithDefaultHeight._meta.get_field("height")\\n3927.         new_default = old_field.default * 2\\n3928.         new_field = PositiveIntegerField(null=True, blank=True, default=new_default)\\n3929.         new_field.set_attributes_from_name("height")\\n3930.         with connection.schema_editor() as editor, self.assertNumQueries(0):\\n3931.             editor.alter_field(\\n3932.                 AuthorWithDefaultHeight, old_field, new_field, strict=True\\n3933.             )\\n3934. \\n3935.     @skipUnlessDBFeature("supports_foreign_keys")\\n3936.     def test_alter_field_fk_attributes_noop(self):\\n3937.         """\\n3938.         No queries are performed when changing field attributes that don\\\'t\\n3939.         affect the schema.\\n3940.         """\\n3941.         with connection.schema_editor() as editor:\\n3942.             editor.create_model(Author)\\n3943.             editor.create_model(Book)\\n3944.         old_field = Book._meta.get_field("author")\\n3945.         new_field = ForeignKey(\\n3946.             Author,\\n3947.             blank=True,\\n3948.             editable=False,\\n3949.             error_messages={"invalid": "error message"},\\n3950.             help_text="help text",\\n3951.             limit_choices_to={"limit": "choice"},\\n3952.             on_delete=PROTECT,\\n3953.             related_name="related_name",\\n3954.             related_query_name="related_query_name",\\n3955.             validators=[lambda x: x],\\n3956.             verbose_name="verbose name",\\n3957.         )\\n3958.         new_field.set_attributes_from_name("author")\\n3959.         with connection.schema_editor() as editor, self.assertNumQueries(0):\\n3960.             editor.alter_field(Book, old_field, new_field, strict=True)\\n3961.         with connection.schema_editor() as editor, self.assertNumQueries(0):\\n3962.             editor.alter_field(Book, new_field, old_field, strict=True)\\n3963. \\n3964.     def test_add_textfield_unhashable_default(self):\\n3965.         # Create the table\\n3966.         with connection.schema_editor() as editor:\\n3967.             editor.create_model(Author)\\n3968.         # Create a row\\n3969.         Author.objects.create(name="Anonymous1")\\n3970.         # Create a field that has an unhashable default\\n3971.         new_field = TextField(default={})\\n3972.         new_field.set_attributes_from_name("info")\\n3973.         with connection.schema_editor() as editor:\\n3974.             editor.add_field(Author, new_field)\\n3975. \\n3976.     @unittest.skipUnless(connection.vendor == "postgresql", "PostgreSQL specific")\\n3977.     def test_add_indexed_charfield(self):\\n3978.         field = CharField(max_length=255, db_index=True)\\n3979.         field.set_attributes_from_name("nom_de_plume")\\n3980.         with connection.schema_editor() as editor:\\n3981.             editor.create_model(Author)\\n3982.             editor.add_field(Author, field)\\n3983.         # Should create two indexes; one for like operator.\\n3984.         self.assertEqual(\\n3985.             self.get_constraints_for_column(Author, "nom_de_plume"),\\n3986.             [\\n3987.                 "schema_author_nom_de_plume_7570a851",\\n3988.                 "schema_author_nom_de_plume_7570a851_like",\\n3989.             ],\\n3990.         )\\n3991. \\n3992.     @unittest.skipUnless(connection.vendor == "postgresql", "PostgreSQL specific")\\n3993.     def test_add_unique_charfield(self):\\n3994.         field = CharField(max_length=255, unique=True)\\n3995.         field.set_attributes_from_name("nom_de_plume")\\n3996.         with connection.schema_editor() as editor:\\n3997.             editor.create_model(Author)\\n3998.             editor.add_field(Author, field)\\n3999.         # Should create two indexes; one for like operator.\\n4000.         self.assertEqual(\\n4001.             self.get_constraints_for_column(Author, "nom_de_plume"),\\n4002.             [\\n4003.                 "schema_author_nom_de_plume_7570a851_like",\\n4004.                 "schema_author_nom_de_plume_key",\\n4005.             ],\\n4006.         )\\n4007. \\n4008.     @unittest.skipUnless(connection.vendor == "postgresql", "PostgreSQL specific")\\n4009.     def test_alter_field_add_index_to_charfield(self):\\n4010.         # Create the table and verify no initial indexes.\\n4011.         with connection.schema_editor() as editor:\\n4012.             editor.create_model(Author)\\n4013.         self.assertEqual(self.get_constraints_for_column(Author, "name"), [])\\n4014.         # Alter to add db_index=True and create 2 indexes.\\n4015.         old_field = Author._meta.get_field("name")\\n4016.         new_field = CharField(max_length=255, db_index=True)\\n4017.         new_field.set_attributes_from_name("name")\\n4018.         with connection.schema_editor() as editor:\\n4019.             editor.alter_field(Author, old_field, new_field, strict=True)\\n4020.         self.assertEqual(\\n4021.             self.get_constraints_for_column(Author, "name"),\\n4022.             ["schema_author_name_1fbc5617", "schema_author_name_1fbc5617_like"],\\n4023.         )\\n4024.         # Remove db_index=True to drop both indexes.\\n4025.         with connection.schema_editor() as editor:\\n4026.             editor.alter_field(Author, new_field, old_field, strict=True)\\n4027.         self.assertEqual(self.get_constraints_for_column(Author, "name"), [])\\n4028. \\n4029.     @unittest.skipUnless(connection.vendor == "postgresql", "PostgreSQL specific")\\n4030.     def test_alter_field_add_unique_to_charfield(self):\\n4031.         # Create the table and verify no initial indexes.\\n4032.         with connection.schema_editor() as editor:\\n4033.             editor.create_model(Author)\\n4034.         self.assertEqual(self.get_constraints_for_column(Author, "name"), [])\\n4035.         # Alter to add unique=True and create 2 indexes.\\n4036.         old_field = Author._meta.get_field("name")\\n4037.         new_field = CharField(max_length=255, unique=True)\\n4038.         new_field.set_attributes_from_name("name")\\n4039.         with connection.schema_editor() as editor:\\n4040.             editor.alter_field(Author, old_field, new_field, strict=True)\\n4041.         self.assertEqual(\\n4042.             self.get_constraints_for_column(Author, "name"),\\n4043.             ["schema_author_name_1fbc5617_like", "schema_author_name_1fbc5617_uniq"],\\n4044.         )\\n4045.         # Remove unique=True to drop both indexes.\\n4046.         with connection.schema_editor() as editor:\\n4047.             editor.alter_field(Author, new_field, old_field, strict=True)\\n4048.         self.assertEqual(self.get_constraints_for_column(Author, "name"), [])\\n4049. \\n4050.     @unittest.skipUnless(connection.vendor == "postgresql", "PostgreSQL specific")\\n4051.     def test_alter_field_add_index_to_textfield(self):\\n4052.         # Create the table and verify no initial indexes.\\n4053.         with connection.schema_editor() as editor:\\n4054.             editor.create_model(Note)\\n4055.         self.assertEqual(self.get_constraints_for_column(Note, "info"), [])\\n4056.         # Alter to add db_index=True and create 2 indexes.\\n4057.         old_field = Note._meta.get_field("info")\\n4058.         new_field = TextField(db_index=True)\\n4059.         new_field.set_attributes_from_name("info")\\n4060.         with connection.schema_editor() as editor:\\n4061.             editor.alter_field(Note, old_field, new_field, strict=True)\\n4062.         self.assertEqual(\\n4063.             self.get_constraints_for_column(Note, "info"),\\n4064.             ["schema_note_info_4b0ea695", "schema_note_info_4b0ea695_like"],\\n4065.         )\\n4066.         # Remove db_index=True to drop both indexes.\\n4067.         with connection.schema_editor() as editor:\\n4068.             editor.alter_field(Note, new_field, old_field, strict=True)\\n4069.         self.assertEqual(self.get_constraints_for_column(Note, "info"), [])\\n4070. \\n4071.     @unittest.skipUnless(connection.vendor == "postgresql", "PostgreSQL specific")\\n4072.     def test_alter_field_add_unique_to_charfield_with_db_index(self):\\n4073.         # Create the table and verify initial indexes.\\n4074.         with connection.schema_editor() as editor:\\n4075.             editor.create_model(BookWithoutAuthor)\\n4076.         self.assertEqual(\\n4077.             self.get_constraints_for_column(BookWithoutAuthor, "title"),\\n4078.             ["schema_book_title_2dfb2dff", "schema_book_title_2dfb2dff_like"],\\n4079.         )\\n4080.         # Alter to add unique=True (should replace the index)\\n4081.         old_field = BookWithoutAuthor._meta.get_field("title")\\n4082.         new_field = CharField(max_length=100, db_index=True, unique=True)\\n4083.         new_field.set_attributes_from_name("title")\\n4084.         with connection.schema_editor() as editor:\\n4085.             editor.alter_field(BookWithoutAuthor, old_field, new_field, strict=True)\\n4086.         self.assertEqual(\\n4087.             self.get_constraints_for_column(BookWithoutAuthor, "title"),\\n4088.             ["schema_book_title_2dfb2dff_like", "schema_book_title_2dfb2dff_uniq"],\\n4089.         )\\n4090.         # Alter to remove unique=True (should drop unique index)\\n4091.         new_field2 = CharField(max_length=100, db_index=True)\\n4092.         new_field2.set_attributes_from_name("title")\\n4093.         with connection.schema_editor() as editor:\\n4094.             editor.alter_field(BookWithoutAuthor, new_field, new_field2, strict=True)\\n4095.         self.assertEqual(\\n4096.             self.get_constraints_for_column(BookWithoutAuthor, "title"),\\n4097.             ["schema_book_title_2dfb2dff", "schema_book_title_2dfb2dff_like"],\\n4098.         )\\n4099. \\n4100.     @unittest.skipUnless(connection.vendor == "postgresql", "PostgreSQL specific")\\n4101.     def test_alter_field_remove_unique_and_db_index_from_charfield(self):\\n4102.         # Create the table and verify initial indexes.\\n4103.         with connection.schema_editor() as editor:\\n4104.             editor.create_model(BookWithoutAuthor)\\n4105.         self.assertEqual(\\n4106.             self.get_constraints_for_column(BookWithoutAuthor, "title"),\\n4107.             ["schema_book_title_2dfb2dff", "schema_book_title_2dfb2dff_like"],\\n4108.         )\\n4109.         # Alter to add unique=True (should replace the index)\\n4110.         old_field = BookWithoutAuthor._meta.get_field("title")\\n4111.         new_field = CharField(max_length=100, db_index=True, unique=True)\\n4112.         new_field.set_attributes_from_name("title")\\n4113.         with connection.schema_editor() as editor:\\n4114.             editor.alter_field(BookWithoutAuthor, old_field, new_field, strict=True)\\n4115.         self.assertEqual(\\n4116.             self.get_constraints_for_column(BookWithoutAuthor, "title"),\\n4117.             ["schema_book_title_2dfb2dff_like", "schema_book_title_2dfb2dff_uniq"],\\n4118.         )\\n4119.         # Alter to remove both unique=True and db_index=True (should drop all indexes)\\n4120.         new_field2 = CharField(max_length=100)\\n4121.         new_field2.set_attributes_from_name("title")\\n4122.         with connection.schema_editor() as editor:\\n4123.             editor.alter_field(BookWithoutAuthor, new_field, new_field2, strict=True)\\n4124.         self.assertEqual(\\n4125.             self.get_constraints_for_column(BookWithoutAuthor, "title"), []\\n4126.         )\\n4127. \\n4128.     @unittest.skipUnless(connection.vendor == "postgresql", "PostgreSQL specific")\\n4129.     def test_alter_field_swap_unique_and_db_index_with_charfield(self):\\n4130.         # Create the table and verify initial indexes.\\n4131.         with connection.schema_editor() as editor:\\n4132.             editor.create_model(BookWithoutAuthor)\\n4133.         self.assertEqual(\\n4134.             self.get_constraints_for_column(BookWithoutAuthor, "title"),\\n4135.             ["schema_book_title_2dfb2dff", "schema_book_title_2dfb2dff_like"],\\n4136.         )\\n4137.         # Alter to set unique=True and remove db_index=True (should replace the index)\\n4138.         old_field = BookWithoutAuthor._meta.get_field("title")\\n4139.         new_field = CharField(max_length=100, unique=True)\\n4140.         new_field.set_attributes_from_name("title")\\n4141.         with connection.schema_editor() as editor:\\n4142.             editor.alter_field(BookWithoutAuthor, old_field, new_field, strict=True)\\n4143.         self.assertEqual(\\n4144.             self.get_constraints_for_column(BookWithoutAuthor, "title"),\\n4145.             ["schema_book_title_2dfb2dff_like", "schema_book_title_2dfb2dff_uniq"],\\n4146.         )\\n4147.         # Alter to set db_index=True and remove unique=True (should restore index)\\n4148.         new_field2 = CharField(max_length=100, db_index=True)\\n4149.         new_field2.set_attributes_from_name("title")\\n4150.         with connection.schema_editor() as editor:\\n4151.             editor.alter_field(BookWithoutAuthor, new_field, new_field2, strict=True)\\n4152.         self.assertEqual(\\n4153.             self.get_constraints_for_column(BookWithoutAuthor, "title"),\\n4154.             ["schema_book_title_2dfb2dff", "schema_book_title_2dfb2dff_like"],\\n4155.         )\\n4156. \\n4157.     @unittest.skipUnless(connection.vendor == "postgresql", "PostgreSQL specific")\\n4158.     def test_alter_field_add_db_index_to_charfield_with_unique(self):\\n4159.         # Create the table and verify initial indexes.\\n4160.         with connection.schema_editor() as editor:\\n4161.             editor.create_model(Tag)\\n4162.         self.assertEqual(\\n4163.             self.get_constraints_for_column(Tag, "slug"),\\n4164.             ["schema_tag_slug_2c418ba3_like", "schema_tag_slug_key"],\\n4165.         )\\n4166.         # Alter to add db_index=True\\n4167.         old_field = Tag._meta.get_field("slug")\\n4168.         new_field = SlugField(db_index=True, unique=True)\\n4169.         new_field.set_attributes_from_name("slug")\\n4170.         with connection.schema_editor() as editor:\\n4171.             editor.alter_field(Tag, old_field, new_field, strict=True)\\n4172.         self.assertEqual(\\n4173.             self.get_constraints_for_column(Tag, "slug"),\\n4174.             ["schema_tag_slug_2c418ba3_like", "schema_tag_slug_key"],\\n4175.         )\\n4176.         # Alter to remove db_index=True\\n4177.         new_field2 = SlugField(unique=True)\\n4178.         new_field2.set_attributes_from_name("slug")\\n4179.         with connection.schema_editor() as editor:\\n4180.             editor.alter_field(Tag, new_field, new_field2, strict=True)\\n4181.         self.assertEqual(\\n4182.             self.get_constraints_for_column(Tag, "slug"),\\n4183.             ["schema_tag_slug_2c418ba3_like", "schema_tag_slug_key"],\\n4184.         )\\n4185. \\n4186.     def test_alter_field_add_index_to_integerfield(self):\\n4187.         # Create the table and verify no initial indexes.\\n4188.         with connection.schema_editor() as editor:\\n4189.             editor.create_model(Author)\\n4190.         self.assertEqual(self.get_constraints_for_column(Author, "weight"), [])\\n4191. \\n4192.         # Alter to add db_index=True and create index.\\n4193.         old_field = Author._meta.get_field("weight")\\n4194.         new_field = IntegerField(null=True, db_index=True)\\n4195.         new_field.set_attributes_from_name("weight")\\n4196.         with connection.schema_editor() as editor:\\n4197.             editor.alter_field(Author, old_field, new_field, strict=True)\\n4198.         self.assertEqual(\\n4199.             self.get_constraints_for_column(Author, "weight"),\\n4200.             ["schema_author_weight_587740f9"],\\n4201.         )\\n4202. \\n4203.         # Remove db_index=True to drop index.\\n4204.         with connection.schema_editor() as editor:\\n4205.             editor.alter_field(Author, new_field, old_field, strict=True)\\n4206.         self.assertEqual(self.get_constraints_for_column(Author, "weight"), [])\\n4207. \\n4208.     def test_alter_pk_with_self_referential_field(self):\\n4209.         """\\n4210.         Changing the primary key field name of a model with a self-referential\\n4211.         foreign key (#26384).\\n4212.         """\\n4213.         with connection.schema_editor() as editor:\\n4214.             editor.create_model(Node)\\n4215.         old_field = Node._meta.get_field("node_id")\\n4216.         new_field = AutoField(primary_key=True)\\n4217.         new_field.set_attributes_from_name("id")\\n4218.         with connection.schema_editor() as editor:\\n4219.             editor.alter_field(Node, old_field, new_field, strict=True)\\n4220.         self.assertForeignKeyExists(Node, "parent_id", Node._meta.db_table)\\n4221. \\n4222.     @mock.patch("django.db.backends.base.schema.datetime")\\n4223.     @mock.patch("django.db.backends.base.schema.timezone")\\n4224.     def test_add_datefield_and_datetimefield_use_effective_default(\\n4225.         self, mocked_datetime, mocked_tz\\n4226.     ):\\n4227.         """\\n4228.         effective_default() should be used for DateField, DateTimeField, and\\n4229.         TimeField if auto_now or auto_now_add is set (#25005).\\n4230.         """\\n4231.         now = datetime.datetime(month=1, day=1, year=2000, hour=1, minute=1)\\n4232.         now_tz = datetime.datetime(\\n4233.             month=1, day=1, year=2000, hour=1, minute=1, tzinfo=datetime.timezone.utc\\n4234.         )\\n4235.         mocked_datetime.now = mock.MagicMock(return_value=now)\\n4236.         mocked_tz.now = mock.MagicMock(return_value=now_tz)\\n4237.         # Create the table\\n4238.         with connection.schema_editor() as editor:\\n4239.             editor.create_model(Author)\\n4240.         # Check auto_now/auto_now_add attributes are not defined\\n4241.         columns = self.column_classes(Author)\\n4242.         self.assertNotIn("dob_auto_now", columns)\\n4243.         self.assertNotIn("dob_auto_now_add", columns)\\n4244.         self.assertNotIn("dtob_auto_now", columns)\\n4245.         self.assertNotIn("dtob_auto_now_add", columns)\\n4246.         self.assertNotIn("tob_auto_now", columns)\\n4247.         self.assertNotIn("tob_auto_now_add", columns)\\n4248.         # Create a row\\n4249.         Author.objects.create(name="Anonymous1")\\n4250.         # Ensure fields were added with the correct defaults\\n4251.         dob_auto_now = DateField(auto_now=True)\\n4252.         dob_auto_now.set_attributes_from_name("dob_auto_now")\\n4253.         self.check_added_field_default(\\n4254.             editor,\\n4255.             Author,\\n4256.             dob_auto_now,\\n4257.             "dob_auto_now",\\n4258.             now.date(),\\n4259.             cast_function=lambda x: x.date(),\\n4260.         )\\n4261.         dob_auto_now_add = DateField(auto_now_add=True)\\n4262.         dob_auto_now_add.set_attributes_from_name("dob_auto_now_add")\\n4263.         self.check_added_field_default(\\n4264.             editor,\\n4265.             Author,\\n4266.             dob_auto_now_add,\\n4267.             "dob_auto_now_add",\\n4268.             now.date(),\\n4269.             cast_function=lambda x: x.date(),\\n4270.         )\\n4271.         dtob_auto_now = DateTimeField(auto_now=True)\\n4272.         dtob_auto_now.set_attributes_from_name("dtob_auto_now")\\n4273.         self.check_added_field_default(\\n4274.             editor,\\n4275.             Author,\\n4276.             dtob_auto_now,\\n4277.             "dtob_auto_now",\\n4278.             now,\\n4279.         )\\n4280.         dt_tm_of_birth_auto_now_add = DateTimeField(auto_now_add=True)\\n4281.         dt_tm_of_birth_auto_now_add.set_attributes_from_name("dtob_auto_now_add")\\n4282.         self.check_added_field_default(\\n4283.             editor,\\n4284.             Author,\\n4285.             dt_tm_of_birth_auto_now_add,\\n4286.             "dtob_auto_now_add",\\n4287.             now,\\n4288.         )\\n4289.         tob_auto_now = TimeField(auto_now=True)\\n4290.         tob_auto_now.set_attributes_from_name("tob_auto_now")\\n4291.         self.check_added_field_default(\\n4292.             editor,\\n4293.             Author,\\n4294.             tob_auto_now,\\n4295.             "tob_auto_now",\\n4296.             now.time(),\\n4297.             cast_function=lambda x: x.time(),\\n4298.         )\\n4299.         tob_auto_now_add = TimeField(auto_now_add=True)\\n4300.         tob_auto_now_add.set_attributes_from_name("tob_auto_now_add")\\n4301.         self.check_added_field_default(\\n4302.             editor,\\n4303.             Author,\\n4304.             tob_auto_now_add,\\n4305.             "tob_auto_now_add",\\n4306.             now.time(),\\n4307.             cast_function=lambda x: x.time(),\\n4308.         )\\n4309. \\n4310.     def test_namespaced_db_table_create_index_name(self):\\n4311.         """\\n4312.         Table names are stripped of their namespace/schema before being used to\\n4313.         generate index names.\\n4314.         """\\n4315.         with connection.schema_editor() as editor:\\n4316.             max_name_length = connection.ops.max_name_length() or 200\\n4317.             namespace = "n" * max_name_length\\n4318.             table_name = "t" * max_name_length\\n4319.             namespaced_table_name = \\\'"%s"."%s"\\\' % (namespace, table_name)\\n4320.             self.assertEqual(\\n4321.                 editor._create_index_name(table_name, []),\\n4322.                 editor._create_index_name(namespaced_table_name, []),\\n4323.             )\\n4324. \\n4325.     @unittest.skipUnless(\\n4326.         connection.vendor == "oracle", "Oracle specific db_table syntax"\\n4327.     )\\n4328.     def test_creation_with_db_table_double_quotes(self):\\n4329.         oracle_user = connection.creation._test_database_user()\\n4330. \\n4331.         class Student(Model):\\n4332.             name = CharField(max_length=30)\\n4333. \\n4334.             class Meta:\\n4335.                 app_label = "schema"\\n4336.                 apps = new_apps\\n4337.                 db_table = \\\'"%s"."DJANGO_STUDENT_TABLE"\\\' % oracle_user\\n4338. \\n4339.         class Document(Model):\\n4340.             name = CharField(max_length=30)\\n4341.             students = ManyToManyField(Student)\\n4342. \\n4343.             class Meta:\\n4344.                 app_label = "schema"\\n4345.                 apps = new_apps\\n4346.                 db_table = \\\'"%s"."DJANGO_DOCUMENT_TABLE"\\\' % oracle_user\\n4347. \\n4348.         self.isolated_local_models = [Student, Document]\\n4349. \\n4350.         with connection.schema_editor() as editor:\\n4351.             editor.create_model(Student)\\n4352.             editor.create_model(Document)\\n4353. \\n4354.         doc = Document.objects.create(name="Test Name")\\n4355.         student = Student.objects.create(name="Some man")\\n4356.         doc.students.add(student)\\n4357. \\n4358.     @isolate_apps("schema")\\n4359.     @unittest.skipUnless(\\n4360.         connection.vendor == "postgresql", "PostgreSQL specific db_table syntax."\\n4361.     )\\n4362.     def test_namespaced_db_table_foreign_key_reference(self):\\n4363.         with connection.cursor() as cursor:\\n4364.             cursor.execute("CREATE SCHEMA django_schema_tests")\\n4365. \\n4366.         def delete_schema():\\n4367.             with connection.cursor() as cursor:\\n4368.                 cursor.execute("DROP SCHEMA django_schema_tests CASCADE")\\n4369. \\n4370.         self.addCleanup(delete_schema)\\n4371. \\n4372.         class Author(Model):\\n4373.             class Meta:\\n4374.                 app_label = "schema"\\n4375. \\n4376.         class Book(Model):\\n4377.             class Meta:\\n4378.                 app_label = "schema"\\n4379.                 db_table = \\\'"django_schema_tests"."schema_book"\\\'\\n4380. \\n4381.         author = ForeignKey(Author, CASCADE)\\n4382.         author.set_attributes_from_name("author")\\n4383. \\n4384.         with connection.schema_editor() as editor:\\n4385.             editor.create_model(Author)\\n4386.             editor.create_model(Book)\\n4387.             editor.add_field(Book, author)\\n4388. \\n4389.     def test_rename_table_renames_deferred_sql_references(self):\\n4390.         atomic_rename = connection.features.supports_atomic_references_rename\\n4391.         with connection.schema_editor(atomic=atomic_rename) as editor:\\n4392.             editor.create_model(Author)\\n4393.             editor.create_model(Book)\\n4394.             editor.alter_db_table(Author, "schema_author", "schema_renamed_author")\\n4395.             editor.alter_db_table(Author, "schema_book", "schema_renamed_book")\\n4396.             try:\\n4397.                 self.assertGreater(len(editor.deferred_sql), 0)\\n4398.                 for statement in editor.deferred_sql:\\n4399.                     self.assertIs(statement.references_table("schema_author"), False)\\n4400.                     self.assertIs(statement.references_table("schema_book"), False)\\n4401.             finally:\\n4402.                 editor.alter_db_table(Author, "schema_renamed_author", "schema_author")\\n4403.                 editor.alter_db_table(Author, "schema_renamed_book", "schema_book")\\n4404. \\n4405.     def test_rename_column_renames_deferred_sql_references(self):\\n4406.         with connection.schema_editor() as editor:\\n4407.             editor.create_model(Author)\\n4408.             editor.create_model(Book)\\n4409.             old_title = Book._meta.get_field("title")\\n4410.             new_title = CharField(max_length=100, db_index=True)\\n4411.             new_title.set_attributes_from_name("renamed_title")\\n4412.             editor.alter_field(Book, old_title, new_title)\\n4413.             old_author = Book._meta.get_field("author")\\n4414.             new_author = ForeignKey(Author, CASCADE)\\n4415.             new_author.set_attributes_from_name("renamed_author")\\n4416.             editor.alter_field(Book, old_author, new_author)\\n4417.             self.assertGreater(len(editor.deferred_sql), 0)\\n4418.             for statement in editor.deferred_sql:\\n4419.                 self.assertIs(statement.references_column("book", "title"), False)\\n4420.                 self.assertIs(statement.references_column("book", "author_id"), False)\\n4421. \\n4422.     @isolate_apps("schema")\\n4423.     def test_referenced_field_without_constraint_rename_inside_atomic_block(self):\\n4424.         """\\n4425.         Foreign keys without database level constraint don\\\'t prevent the field\\n4426.         they reference from being renamed in an atomic block.\\n4427.         """\\n4428. \\n4429.         class Foo(Model):\\n4430.             field = CharField(max_length=255, unique=True)\\n4431. \\n4432.             class Meta:\\n4433.                 app_label = "schema"\\n4434. \\n4435.         class Bar(Model):\\n4436.             foo = ForeignKey(Foo, CASCADE, to_field="field", db_constraint=False)\\n4437. \\n4438.             class Meta:\\n4439.                 app_label = "schema"\\n4440. \\n4441.         self.isolated_local_models = [Foo, Bar]\\n4442.         with connection.schema_editor() as editor:\\n4443.             editor.create_model(Foo)\\n4444.             editor.create_model(Bar)\\n4445. \\n4446.         new_field = CharField(max_length=255, unique=True)\\n4447.         new_field.set_attributes_from_name("renamed")\\n4448.         with connection.schema_editor(atomic=True) as editor:\\n4449.             editor.alter_field(Foo, Foo._meta.get_field("field"), new_field)\\n4450. \\n4451.     @isolate_apps("schema")\\n4452.     def test_referenced_table_without_constraint_rename_inside_atomic_block(self):\\n4453.         """\\n4454.         Foreign keys without database level constraint don\\\'t prevent the table\\n4455.         they reference from being renamed in an atomic block.\\n4456.         """\\n4457. \\n4458.         class Foo(Model):\\n4459.             field = CharField(max_length=255, unique=True)\\n4460. \\n4461.             class Meta:\\n4462.                 app_label = "schema"\\n4463. \\n4464.         class Bar(Model):\\n4465.             foo = ForeignKey(Foo, CASCADE, to_field="field", db_constraint=False)\\n4466. \\n4467.             class Meta:\\n4468.                 app_label = "schema"\\n4469. \\n4470.         self.isolated_local_models = [Foo, Bar]\\n4471.         with connection.schema_editor() as editor:\\n4472.             editor.create_model(Foo)\\n4473.             editor.create_model(Bar)\\n4474. \\n4475.         new_field = CharField(max_length=255, unique=True)\\n4476.         new_field.set_attributes_from_name("renamed")\\n4477.         with connection.schema_editor(atomic=True) as editor:\\n4478.             editor.alter_db_table(Foo, Foo._meta.db_table, "renamed_table")\\n4479.         Foo._meta.db_table = "renamed_table"\\n4480. \\n4481.     @isolate_apps("schema")\\n4482.     @skipUnlessDBFeature("supports_collation_on_charfield")\\n4483.     def test_db_collation_charfield(self):\\n4484.         collation = connection.features.test_collations.get("non_default")\\n4485.         if not collation:\\n4486.             self.skipTest("Language collations are not supported.")\\n4487. \\n4488.         class Foo(Model):\\n4489.             field = CharField(max_length=255, db_collation=collation)\\n4490. \\n4491.             class Meta:\\n4492.                 app_label = "schema"\\n4493. \\n4494.         self.isolated_local_models = [Foo]\\n4495.         with connection.schema_editor() as editor:\\n4496.             editor.create_model(Foo)\\n4497. \\n4498.         self.assertEqual(\\n4499.             self.get_column_collation(Foo._meta.db_table, "field"),\\n4500.             collation,\\n4501.         )\\n4502. \\n4503.     @isolate_apps("schema")\\n4504.     @skipUnlessDBFeature("supports_collation_on_textfield")\\n4505.     def test_db_collation_textfield(self):\\n4506.         collation = connection.features.test_collations.get("non_default")\\n4507.         if not collation:\\n4508.             self.skipTest("Language collations are not supported.")\\n4509. \\n4510.         class Foo(Model):\\n4511.             field = TextField(db_collation=collation)\\n4512. \\n4513.             class Meta:\\n4514.                 app_label = "schema"\\n4515. \\n4516.         self.isolated_local_models = [Foo]\\n4517.         with connection.schema_editor() as editor:\\n4518.             editor.create_model(Foo)\\n4519. \\n4520.         self.assertEqual(\\n4521.             self.get_column_collation(Foo._meta.db_table, "field"),\\n4522.             collation,\\n4523.         )\\n4524. \\n4525.     @skipUnlessDBFeature("supports_collation_on_charfield")\\n4526.     def test_add_field_db_collation(self):\\n4527.         collation = connection.features.test_collations.get("non_default")\\n4528.         if not collation:\\n4529.             self.skipTest("Language collations are not supported.")\\n4530. \\n4531.         with connection.schema_editor() as editor:\\n4532.             editor.create_model(Author)\\n4533. \\n4534.         new_field = CharField(max_length=255, db_collation=collation)\\n4535.         new_field.set_attributes_from_name("alias")\\n4536.         with connection.schema_editor() as editor:\\n4537.             editor.add_field(Author, new_field)\\n4538.         columns = self.column_classes(Author)\\n4539.         self.assertEqual(\\n4540.             columns["alias"][0],\\n4541.             connection.features.introspected_field_types["CharField"],\\n4542.         )\\n4543.         self.assertEqual(columns["alias"][1][8], collation)\\n4544. \\n4545.     @skipUnlessDBFeature("supports_collation_on_charfield")\\n4546.     def test_alter_field_db_collation(self):\\n4547.         collation = connection.features.test_collations.get("non_default")\\n4548.         if not collation:\\n4549.             self.skipTest("Language collations are not supported.")\\n4550. \\n4551.         with connection.schema_editor() as editor:\\n4552.             editor.create_model(Author)\\n4553. \\n4554.         old_field = Author._meta.get_field("name")\\n4555.         new_field = CharField(max_length=255, db_collation=collation)\\n4556.         new_field.set_attributes_from_name("name")\\n4557.         new_field.model = Author\\n4558.         with connection.schema_editor() as editor:\\n4559.             editor.alter_field(Author, old_field, new_field, strict=True)\\n4560.         self.assertEqual(\\n4561.             self.get_column_collation(Author._meta.db_table, "name"),\\n4562.             collation,\\n4563.         )\\n4564.         with connection.schema_editor() as editor:\\n4565.             editor.alter_field(Author, new_field, old_field, strict=True)\\n4566.         self.assertIsNone(self.get_column_collation(Author._meta.db_table, "name"))\\n4567. \\n4568.     @skipUnlessDBFeature("supports_collation_on_charfield")\\n4569.     def test_alter_field_type_and_db_collation(self):\\n4570.         collation = connection.features.test_collations.get("non_default")\\n4571.         if not collation:\\n4572.             self.skipTest("Language collations are not supported.")\\n4573. \\n4574.         with connection.schema_editor() as editor:\\n4575.             editor.create_model(Note)\\n4576. \\n4577.         old_field = Note._meta.get_field("info")\\n4578.         new_field = CharField(max_length=255, db_collation=collation)\\n4579.         new_field.set_attributes_from_name("info")\\n4580.         new_field.model = Note\\n4581.         with connection.schema_editor() as editor:\\n4582.             editor.alter_field(Note, old_field, new_field, strict=True)\\n4583.         columns = self.column_classes(Note)\\n4584.         self.assertEqual(\\n4585.             columns["info"][0],\\n4586.             connection.features.introspected_field_types["CharField"],\\n4587.         )\\n4588.         self.assertEqual(columns["info"][1][8], collation)\\n4589.         with connection.schema_editor() as editor:\\n4590.             editor.alter_field(Note, new_field, old_field, strict=True)\\n4591.         columns = self.column_classes(Note)\\n4592.         self.assertEqual(columns["info"][0], "TextField")\\n4593.         self.assertIsNone(columns["info"][1][8])\\n4594. \\n4595.     @skipUnlessDBFeature(\\n4596.         "supports_collation_on_charfield",\\n4597.         "supports_non_deterministic_collations",\\n4598.     )\\n4599.     def test_ci_cs_db_collation(self):\\n4600.         cs_collation = connection.features.test_collations.get("cs")\\n4601.         ci_collation = connection.features.test_collations.get("ci")\\n4602.         try:\\n4603.             if connection.vendor == "mysql":\\n4604.                 cs_collation = "latin1_general_cs"\\n4605.             elif connection.vendor == "postgresql":\\n4606.                 cs_collation = "en-x-icu"\\n4607.                 with connection.cursor() as cursor:\\n4608.                     cursor.execute(\\n4609.                         "CREATE COLLATION IF NOT EXISTS case_insensitive "\\n4610.                         "(provider = icu, locale = \\\'und-u-ks-level2\\\', "\\n4611.                         "deterministic = false)"\\n4612.                     )\\n4613.                     ci_collation = "case_insensitive"\\n4614.             # Create the table.\\n4615.             with connection.schema_editor() as editor:\\n4616.                 editor.create_model(Author)\\n4617.             # Case-insensitive collation.\\n4618.             old_field = Author._meta.get_field("name")\\n4619.             new_field_ci = CharField(max_length=255, db_collation=ci_collation)\\n4620.             new_field_ci.set_attributes_from_name("name")\\n4621.             new_field_ci.model = Author\\n4622.             with connection.schema_editor() as editor:\\n4623.                 editor.alter_field(Author, old_field, new_field_ci, strict=True)\\n4624.             Author.objects.create(name="ANDREW")\\n4625.             self.assertIs(Author.objects.filter(name="Andrew").exists(), True)\\n4626.             # Case-sensitive collation.\\n4627.             new_field_cs = CharField(max_length=255, db_collation=cs_collation)\\n4628.             new_field_cs.set_attributes_from_name("name")\\n4629.             new_field_cs.model = Author\\n4630.             with connection.schema_editor() as editor:\\n4631.                 editor.alter_field(Author, new_field_ci, new_field_cs, strict=True)\\n4632.             self.assertIs(Author.objects.filter(name="Andrew").exists(), False)\\n4633.         finally:\\n4634.             if connection.vendor == "postgresql":\\n4635.                 with connection.cursor() as cursor:\\n4636.                     cursor.execute("DROP COLLATION IF EXISTS case_insensitive")\', \'start_line_number\': 91, \'end_line_number\': 4636}\n\nNow generate the complete self-contained test case that reproduces the bug with the same error/exception.\n' additional_kwargs={} response_metadata={}
2025-12-20 22:49:04,040 - thread-130546535499456.app.lang_graph.nodes.bug_reproducing_write_node - DEBUG - content='```python\nfrom django.db import connection, migrations, models\nfrom django.test.utils import CaptureQueriesContext\n\nfrom .test_base import OperationTestBase\n\n\nclass OperationTests(OperationTestBase):\n    def test_alter_field_choices_noop(self):\n        """\n        Changing the \'choices\' option on a field should be a no-op.\n        """\n        app_label = "test_alter_field_choices_noop"\n        project_state = self.set_up_test_model(app_label)\n        operation = migrations.AlterField(\n            "Pony",\n            "pink",\n            models.IntegerField(default=3, choices=[(1, "hot pink")]),\n        )\n\n        new_state = project_state.clone()\n        operation.state_forwards(app_label, new_state)\n\n        # Test that the database alteration is a no-op.\n        with connection.schema_editor() as editor:\n            with self.assertNumQueries(0):\n                operation.database_forwards(\n                    app_label, editor, project_state, new_state\n                )\n\n```' additional_kwargs={} response_metadata={'is_blocked': False, 'safety_ratings': [], 'citation_metadata': {'citations': [{'start_index': 8345, 'end_index': 8612, 'uri': 'https://docs.djangoproject.com/en/3.2/_modules/django/db/backends/base/schema/', 'title': '', 'license_': ''}, {'start_index': 14969, 'end_index': 15452, 'uri': 'https://github.com/django/django', 'license_': 'Python-2.0', 'title': ''}, {'start_index': 17035, 'end_index': 17518, 'uri': 'https://github.com/django/django', 'license_': 'Python-2.0', 'title': ''}]}, 'usage_metadata': {'prompt_token_count': 208199, 'candidates_token_count': 256, 'total_token_count': 214288, 'prompt_tokens_details': [{'modality': 1, 'token_count': 208199}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 256}], 'thoughts_token_count': 5833, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -3.3262691497802734, 'model_name': 'gemini-2.5-pro'} id='run--bc0145b6-4508-4a77-abe7-10d9532149ab-0' usage_metadata={'input_tokens': 208199, 'output_tokens': 256, 'total_tokens': 214288, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 5833}}
2025-12-20 22:49:16,997 - thread-130546535499456.app.lang_graph.nodes.bug_reproducing_file_node - DEBUG - content='' additional_kwargs={'function_call': {'name': 'create_file', 'arguments': '{"content": "from django.db import connection, migrations, models\\nfrom django.test.utils import CaptureQueriesContext\\n\\nfrom .test_base import OperationTestBase\\n\\n\\nclass OperationTests(OperationTestBase):\\n    def test_alter_field_choices_noop(self):\\n        \\"\\"\\"\\n        Changing the \'choices\' option on a field should be a no-op.\\n        \\"\\"\\"\\n        app_label = \\"test_alter_field_choices_noop\\"\\n        project_state = self.set_up_test_model(app_label)\\n        operation = migrations.AlterField(\\n            \\"Pony\\",\\n            \\"pink\\",\\n            models.IntegerField(default=3, choices=[(1, \\"hot pink\\")]),\\n        )\\n\\n        new_state = project_state.clone()\\n        operation.state_forwards(app_label, new_state)\\n\\n        # Test that the database alteration is a no-op.\\n        with connection.schema_editor() as editor:\\n            with self.assertNumQueries(0):\\n                operation.database_forwards(\\n                    app_label, editor, project_state, new_state\\n                )\\n", "relative_path": "tests/migrations/test_operations.py"}'}, '__gemini_function_call_thought_signatures__': {'0fa7e710-b88d-4762-bea2-844e67fde3c7': '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'}} response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 574, 'candidates_token_count': 268, 'total_token_count': 2043, 'prompt_tokens_details': [{'modality': 1, 'token_count': 574}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 268}], 'thoughts_token_count': 1201, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.7650703316304222, 'model_name': 'gemini-2.5-pro'} id='run--ba4b8009-2c40-4d62-932a-0ae1acbbff8b-0' tool_calls=[{'name': 'create_file', 'args': {'content': 'from django.db import connection, migrations, models\nfrom django.test.utils import CaptureQueriesContext\n\nfrom .test_base import OperationTestBase\n\n\nclass OperationTests(OperationTestBase):\n    def test_alter_field_choices_noop(self):\n        """\n        Changing the \'choices\' option on a field should be a no-op.\n        """\n        app_label = "test_alter_field_choices_noop"\n        project_state = self.set_up_test_model(app_label)\n        operation = migrations.AlterField(\n            "Pony",\n            "pink",\n            models.IntegerField(default=3, choices=[(1, "hot pink")]),\n        )\n\n        new_state = project_state.clone()\n        operation.state_forwards(app_label, new_state)\n\n        # Test that the database alteration is a no-op.\n        with connection.schema_editor() as editor:\n            with self.assertNumQueries(0):\n                operation.database_forwards(\n                    app_label, editor, project_state, new_state\n                )\n', 'relative_path': 'tests/migrations/test_operations.py'}, 'id': '0fa7e710-b88d-4762-bea2-844e67fde3c7', 'type': 'tool_call'}] usage_metadata={'input_tokens': 574, 'output_tokens': 268, 'total_tokens': 2043, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1201}}
2025-12-20 22:49:20,784 - thread-130546535499456.app.lang_graph.nodes.bug_reproducing_file_node - DEBUG - content='' additional_kwargs={'function_call': {'name': 'create_file', 'arguments': '{"content": "from django.db import connection, migrations, models\\nfrom django.test.utils import CaptureQueriesContext\\n\\nfrom .test_base import OperationTestBase\\n\\n\\nclass OperationTests(OperationTestBase):\\n    def test_alter_field_choices_noop(self):\\n        \\"\\"\\"\\n        Changing the \'choices\' option on a field should be a no-op.\\n        \\"\\"\\"\\n        app_label = \\"test_alter_field_choices_noop\\"\\n        project_state = self.set_up_test_model(app_label)\\n        operation = migrations.AlterField(\\n            \\"Pony\\",\\n            \\"pink\\",\\n            models.IntegerField(default=3, choices=[(1, \\"hot pink\\")]),\\n        )\\n\\n        new_state = project_state.clone()\\n        operation.state_forwards(app_label, new_state)\\n\\n        # Test that the database alteration is a no-op.\\n        with connection.schema_editor() as editor:\\n            with self.assertNumQueries(0):\\n                operation.database_forwards(\\n                    app_label, editor, project_state, new_state\\n                )\\n", "relative_path": "tests/migrations/test_alter_field.py"}'}, '__gemini_function_call_thought_signatures__': {'f2ed4153-e6e4-484a-8ba3-4a9b26a2b50a': 'CooEAY89a18Ct/OEci75PLFcIGKJmw28kKKp9upSMeOE4g25OEeTNO95u6AkNdwluWFLX5f+vguMMiIzf7A9EWxS+9mcyLOQ6Ik4/xxDF7+bH2dWrRqrydI01ObZcIyTRMeM841UnfV0KtZewIvuX0+AsDiwwD/1BT8RDbGafunMtU0sDl96UdwxmRhMYk/IO6kcmhkezKavGKAo1wKkuHOVSQqlpv94OBA3OUR5lS0IIS5QdSkgqwC8dhU0frxxw4W4hHyzn/d369vg262MpLnlNGV3JtNkFQmzbj7qYS5Ps9Pg9lM9ygGiq5WdCLypxvW2nnCHOCe23BJBIP6lN8Kaogq+HJpYdAWoaUOXHsOcA31szZ81rP9UfVR9owmEBc/IQCdg6chTDNxN86/fun5cXhvouhk0rCbyioFx3kgtbgXig89qb0P7lN6ilmdP0RC0kTFt3GGY8X2Yq5EbGb2YjNtvh3CHe5R36ekBHmALj8K1uwO0HpXU3U6+2fv8agwRqB6nbBd4v4r4XPPSfjH7/MlzyI45Ze00Zr4SNRqBtE3AMAHR4ZkHSxNvsrOF6og98XhDOSISHUP8lxrQsYTvAn0zOgJ/Zf5vt6vj3c1dgvPCBEAqD9GSb3BrF32sQFdXQrHJ4kTDQXKe5fW581ybakfM2h5eQhx1o6M8JUEmarUNm9XwGez7g5f2'}} response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 860, 'candidates_token_count': 270, 'total_token_count': 1263, 'prompt_tokens_details': [{'modality': 1, 'token_count': 2061}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 270}], 'thoughts_token_count': 133, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.03860590193006727, 'model_name': 'gemini-2.5-pro'} id='run--245e5fed-68db-4855-bb38-57bb8dea93c5-0' tool_calls=[{'name': 'create_file', 'args': {'content': 'from django.db import connection, migrations, models\nfrom django.test.utils import CaptureQueriesContext\n\nfrom .test_base import OperationTestBase\n\n\nclass OperationTests(OperationTestBase):\n    def test_alter_field_choices_noop(self):\n        """\n        Changing the \'choices\' option on a field should be a no-op.\n        """\n        app_label = "test_alter_field_choices_noop"\n        project_state = self.set_up_test_model(app_label)\n        operation = migrations.AlterField(\n            "Pony",\n            "pink",\n            models.IntegerField(default=3, choices=[(1, "hot pink")]),\n        )\n\n        new_state = project_state.clone()\n        operation.state_forwards(app_label, new_state)\n\n        # Test that the database alteration is a no-op.\n        with connection.schema_editor() as editor:\n            with self.assertNumQueries(0):\n                operation.database_forwards(\n                    app_label, editor, project_state, new_state\n                )\n', 'relative_path': 'tests/migrations/test_alter_field.py'}, 'id': 'f2ed4153-e6e4-484a-8ba3-4a9b26a2b50a', 'type': 'tool_call'}] usage_metadata={'input_tokens': 860, 'output_tokens': 270, 'total_tokens': 1263, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 133}}
2025-12-20 22:49:21,502 - thread-130546535499456.app.lang_graph.nodes.bug_reproducing_file_node - DEBUG - content='tests/migrations/test_alter_field.py\n' additional_kwargs={} response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 1151, 'candidates_token_count': 12, 'total_token_count': 1163, 'cached_content_token_count': 1648, 'prompt_tokens_details': [{'modality': 1, 'token_count': 2485}], 'cache_tokens_details': [{'modality': 1, 'token_count': 1648}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 12}], 'thoughts_token_count': 0}, 'finish_reason': 'STOP', 'avg_logprobs': -0.026571641365687054, 'model_name': 'gemini-2.5-pro'} id='run--5858b3f8-466f-498f-8151-28187903608a-0' usage_metadata={'input_tokens': 1151, 'output_tokens': 12, 'total_tokens': 1163, 'input_token_details': {'cache_read': 1648}}
2025-12-20 22:49:22,121 - thread-130546535499456.app.lang_graph.nodes.git_diff_node - INFO - Generated patch:
diff --git a/tests/migrations/test_alter_field.py b/tests/migrations/test_alter_field.py
new file mode 100644
index 0000000000..45446bc2a4
--- /dev/null
+++ b/tests/migrations/test_alter_field.py
@@ -0,0 +1,28 @@
+from django.db import connection, migrations, models
+from django.test.utils import CaptureQueriesContext
+
+from .test_base import OperationTestBase
+
+
+class OperationTests(OperationTestBase):
+    def test_alter_field_choices_noop(self):
+        """
+        Changing the 'choices' option on a field should be a no-op.
+        """
+        app_label = "test_alter_field_choices_noop"
+        project_state = self.set_up_test_model(app_label)
+        operation = migrations.AlterField(
+            "Pony",
+            "pink",
+            models.IntegerField(default=3, choices=[(1, "hot pink")]),
+        )
+
+        new_state = project_state.clone()
+        operation.state_forwards(app_label, new_state)
+
+        # Test that the database alteration is a no-op.
+        with connection.schema_editor() as editor:
+            with self.assertNumQueries(0):
+                operation.database_forwards(
+                    app_label, editor, project_state, new_state
+                )

2025-12-20 22:49:22,122 - thread-130546535499456.app.lang_graph.nodes.update_container_node - INFO - Copy over all updated files to the container
2025-12-20 22:49:22,245 - thread-130546535499456.app.docker.base_container - INFO - Resetting git repository in the container
2025-12-20 22:49:22,245 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: git reset --hard
2025-12-20 22:49:23,602 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-20 22:49:23,602 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:

Updating files:  79% (5258/6622)
Updating files:  80% (5298/6622)
Updating files:  81% (5364/6622)
Updating files:  82% (5431/6622)
Updating files:  83% (5497/6622)
Updating files:  84% (5563/6622)
Updating files:  85% (5629/6622)
Updating files:  86% (5695/6622)
Updating files:  87% (5762/6622)
Updating files:  88% (5828/6622)
Updating files:  89% (5894/6622)
Updating files:  90% (5960/6622)
Updating files:  91% (6027/6622)
Updating files:  92% (6093/6622)
Updating files:  93% (6159/6622)
Updating files:  94% (6225/6622)
Updating files:  95% (6291/6622)
Updating files:  96% (6358/6622)
Updating files:  97% (6424/6622)
Updating files:  98% (6490/6622)
Updating files:  99% (6556/6622)
Updating files: 100% (6622/6622)
Updating files: 100% (6622/6622), done.
HEAD is now at 6991880109 Refs #31617 -- Added an id for helptext in admin forms.

2025-12-20 22:49:23,602 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: git clean -fd
2025-12-20 22:49:23,775 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-20 22:49:23,775 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:


2025-12-20 22:49:23,775 - thread-130546535499456.app.docker.base_container - INFO - Updating files in the container after edits.
2025-12-20 22:49:23,775 - thread-130546535499456.app.docker.base_container - INFO - Creating directory tests/migrations in the container
2025-12-20 22:49:23,775 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: mkdir -p tests/migrations
2025-12-20 22:49:23,877 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-20 22:49:23,877 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:


2025-12-20 22:49:23,877 - thread-130546535499456.app.docker.base_container - INFO - Updating tests/migrations/test_alter_field.py in the container
2025-12-20 22:49:23,884 - thread-130546535499456.app.docker.base_container - INFO - Files updated successfully
2025-12-20 22:49:26,300 - thread-130546535499456.app.lang_graph.nodes.bug_reproducing_execute_node - DEBUG - content='' additional_kwargs={'function_call': {'name': 'run_command', 'arguments': '{"command": "./manage.py test tests.migrations.test_alter_field"}'}, '__gemini_function_call_thought_signatures__': {'906bc9d5-5e9d-4b4a-a33a-8ec08c2fd235': 'Cv4FAY89a1+PIhAb2TkXswTVnvKxN+6Filgwn3HawA8+Gu1Fi1PbYdeOb6GDbW+UB2BteSfUgqgh73wD5kb3uq6da6nRc+qqBvJSynzXDwWJKsB80eaHcRA3Hj11LFhLOvvynIY5e0AD4rU7hT40/v92VtaB4WhowKhD99E9Z8Mk6a1hzv2QcewJZkOXZebDr2Qeawfyfqas44+VfgOmNUArcFO+47wjts3jf3dz1LcdpNDfLozd0qG6DmIQgHh+l/zUICvDAiJ+PVxI3ZuyS7sDDt0W/PehKa0O4+gssrUQdRbVpXrLM1NNQcm9qQ2hsQXIFGV1fZ+/NmjcXC1dR4kqHrEi7pNrHkVHsZIGfJacg2dLxZn+eGoBk4iXYRNooAQ+JUmFLYDYfmNT5ytc/mw0OMQxM9M+OBsRr5Firrzh7n4L/vgcKi0iOn5XuE4VOJ1Y46vFXbj6xuyOoXPvf+RQWqHggIpV6EC1TMMEiZeCXIqDBrTW78zs8KGV3AqS5AMiHXSDRhWlLnDC4RnA9tQAC7fShsgRQmZW/Xq3oBhWGBqxoN7yL2isOQDB2SXqybEMdYWmUwqRPyEWoiJbBRH89aXk4dxgNd/7iCp3/9TqMyPNQIi0mHeMnLGzz5smoHRcherM3CcMms5bWlBGuRDV1HitNZTShmlim9GGCChHZvztXF1j/UskSbFJjT6iv2/8qNLZ7rZ09Jy6mtug4VsBnTOUrx3VXFMPq00S6NARjYzL1J5cncDq7MvQF0RQ/ISlszP2Auj1Jzau4Pu4r79hLC/7M9op0qWAvGoZvgAglRJ4L0TWblfkHKHCQ7pE5n2H7taguGWOai1moGN7J6PmsIBz0JHgvAzX4fm5EtAii8QvOOy0PmQHRjOoSa9te8E9wGO7FvGlIYm1cfBT7a3yiLocMe3hhc/v+415jzLtQGcMXvR6QC7NNU9XdYFjua/7Yy0k0NTU+9ssIiI+qpzwPL4iOU91JovA1vY1BSVY58kSZLx/27xF04PPXdpA2Q=='}} response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 307, 'candidates_token_count': 18, 'total_token_count': 506, 'prompt_tokens_details': [{'modality': 1, 'token_count': 307}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 18}], 'thoughts_token_count': 181, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.6861103905571831, 'model_name': 'gemini-2.5-pro'} id='run--cdb0c750-38a3-4bd8-92a6-0dd9c145ab1e-0' tool_calls=[{'name': 'run_command', 'args': {'command': './manage.py test tests.migrations.test_alter_field'}, 'id': '906bc9d5-5e9d-4b4a-a33a-8ec08c2fd235', 'type': 'tool_call'}] usage_metadata={'input_tokens': 307, 'output_tokens': 18, 'total_tokens': 506, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 181}}
2025-12-20 22:49:26,303 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: ./manage.py test tests.migrations.test_alter_field
2025-12-20 22:49:26,404 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 127
2025-12-20 22:49:26,404 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:
./manage.py test tests.migrations.test_alte
er_field

bash: ./manage.py: No such file or directory

2025-12-20 22:49:27,826 - thread-130546535499456.app.lang_graph.nodes.bug_reproducing_execute_node - DEBUG - content='' additional_kwargs={'function_call': {'name': 'run_command', 'arguments': '{"command": "ls -F"}'}, '__gemini_function_call_thought_signatures__': {'d94ee4e9-18f5-40bd-be8e-e2f11219c150': 'CugCAY89a186sDrjfo9s4wuaIQdOqz0BfYhx88fkXrbbH7+dUZzgltHcBNkNEa4/7IaBvbRbGXyBmkJhe+Xo0ct/kxYB292+5tKWQTmdAkJSpbGMmDoQ23E8G9sMPdTaaSpp/C5qAfOPoaO32/psBqHzx2IdarqvY2blLw4+CvDxVs8Hg4505CFGEOq0SQOL8wCixanaMcAenM8nHpiVTkmWmatX7iE4BJ0xpVhEc0dA7LciL8ySHYT2xGR97RNAPMgYtFhtknWYijJhrLD6a5CXLtfoITM7PWYXtlih2lIFDlfriCQr2vbO+Z3rQ4Hq8xdgN44JH3KVo9Dx+1/glBC+PblZEFCxCie3iey49q6YajW1cILOVrLsdetu7j4aAcAQl4Ek82yknsukYFh+NdSZtcldTHCs+3W2Vi28dNb+q4rBNYtGGZMIJCfsBMWoo5UAOmrNsX/YsVSifqIqkB0vubnA3uZB9jvA'}} response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 359, 'candidates_token_count': 7, 'total_token_count': 444, 'prompt_tokens_details': [{'modality': 1, 'token_count': 540}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 7}], 'thoughts_token_count': 78, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.9199622699192592, 'model_name': 'gemini-2.5-pro'} id='run--60cd3d5b-d845-4e42-9495-1df257dbe11f-0' tool_calls=[{'name': 'run_command', 'args': {'command': 'ls -F'}, 'id': 'd94ee4e9-18f5-40bd-be8e-e2f11219c150', 'type': 'tool_call'}] usage_metadata={'input_tokens': 359, 'output_tokens': 7, 'total_tokens': 444, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 78}}
2025-12-20 22:49:27,829 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: ls -F
2025-12-20 22:49:27,930 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-20 22:49:27,931 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:

AUTHORS		  INSTALL	  README.rst  js_tests/       setup.cfg
CONTRIBUTING.rst  LICENSE	  django/     package.json    setup.py
Django.egg-info/  LICENSE.python  docs/       pyproject.toml  tests/
Gruntfile.js	  MANIFEST.in	  extras/     scripts/	      tox.ini

2025-12-20 22:49:30,855 - thread-130546535499456.app.lang_graph.nodes.bug_reproducing_execute_node - DEBUG - content='' additional_kwargs={'function_call': {'name': 'run_command', 'arguments': '{"command": "ls -F django/"}'}, '__gemini_function_call_thought_signatures__': {'4e3f9d6b-34e3-4e8e-b935-1d205d1b0fe6': 'CvQCAY89a198AE+r8VEksFcFwY83gX0bpE32rbwpi4ztN3S4mgyP7X7sOFDAaSznh9GrQLxmWyCr3/V2I8H5NU3SmhA2hToU7IPFv+QjjUspygjP9izgvGJ/ijs5/SB1MWoEa2lAPjgW7SdaYLgjhhRshDHYhYIo/OJTBm6s8YNAKx9lRgZMzEASEzXu6lYNfpvEYlMutcUIraSeXBo5uB2JszTkO3sbYxjHWuyuZxYiJJ1hsy4Gj3h0V3XPhASbUsXDB2PmrPUFaP47ElXmWFs1MIK3z5yChiPKQzRfegS4DCdQKcJ86QvrsH8Zv/ZWxd7AtNJ91QXzdIr7/+n458hLkCJL7QtpgwHRlvSQXu7GlWEmF4w1twuJoM4TSpOiflMJ6HP28GFEIUdytI1YPtVNtAVfyc/Zq9V5td00GRsXDrQON35KL9mGmLY0kfsGCePWvryRKc7XqiC4qYiohs3DtLKO47ERk5xbJ39PKPFg7GyamJlE'}} response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 454, 'candidates_token_count': 9, 'total_token_count': 591, 'prompt_tokens_details': [{'modality': 1, 'token_count': 713}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 9}], 'thoughts_token_count': 128, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -1.9950421651204426, 'model_name': 'gemini-2.5-pro'} id='run--0984f361-f67a-4375-ae24-ef834b7c4537-0' tool_calls=[{'name': 'run_command', 'args': {'command': 'ls -F django/'}, 'id': '4e3f9d6b-34e3-4e8e-b935-1d205d1b0fe6', 'type': 'tool_call'}] usage_metadata={'input_tokens': 454, 'output_tokens': 9, 'total_tokens': 591, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 128}}
2025-12-20 22:49:30,858 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: ls -F django/
2025-12-20 22:49:30,959 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-20 22:49:30,959 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:

__init__.py   apps/	core/	   forms/	shortcuts.py   test/   views/
__main__.py   conf/	db/	   http/	template/      urls/
__pycache__/  contrib/	dispatch/  middleware/	templatetags/  utils/

2025-12-20 22:49:32,931 - thread-130546535499456.app.lang_graph.nodes.bug_reproducing_execute_node - DEBUG - content='' additional_kwargs={'function_call': {'name': 'run_command', 'arguments': '{"command": "ls -F tests/"}'}, '__gemini_function_call_thought_signatures__': {'59c9268f-1438-486f-a78e-b04f5811b1c7': 'Cn0Bjz1rX+/rC/+rSXoCmPl+yLHq4PcKt0QgsfU/3VTg5oWk7ezecXBKfD8d5ohD6imkHLUW4od0v5qVrs/SZ8GM4Jg/tbkrR4W2uZeYZQ1bddUxGGLap6kkKnCJER699LaeXJSz6pCCRY1cNmr8j4jHypLSqi9vsGQzBozO3Q=='}} response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 536, 'candidates_token_count': 9, 'total_token_count': 578, 'prompt_tokens_details': [{'modality': 1, 'token_count': 888}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 9}], 'thoughts_token_count': 33, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.34650810559590656, 'model_name': 'gemini-2.5-pro'} id='run--03fa7b81-1eb1-415e-90a9-6bacd19026b2-0' tool_calls=[{'name': 'run_command', 'args': {'command': 'ls -F tests/'}, 'id': '59c9268f-1438-486f-a78e-b04f5811b1c7', 'type': 'tool_call'}] usage_metadata={'input_tokens': 536, 'output_tokens': 9, 'total_tokens': 578, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 33}}
2025-12-20 22:49:32,934 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: ls -F tests/
2025-12-20 22:49:33,035 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-20 22:49:33,036 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:

README.rst		      m2o_recursive/
absolute_url_overrides/       mail/
admin_autodiscover/	      managers_regress/
admin_changelist/	      many_to_many/
admin_checks/		      many_to_one/
admin_custom_urls/	      many_to_one_null/
admin_default_site/	      max_lengths/
admin_docs/		      messages_tests/
admin_filters/		      middleware/
admin_inlines/		      middleware_exceptions/
admin_ordering/		      migrate_signals/
admin_registration/	      migration_test_data_persistence/
admin_scripts/		      migrations/
admin_utils/		      migrations2/
admin_views/		      model_enums/
admin_widgets/		      model_fields/
aggregation/		      model_forms/
aggregation_regress/	      model_formsets/
annotations/		      model_formsets_regress/
app_loading/		      model_indexes/
apps/			      model_inheritance/
asgi/			      model_inheritance_regress/
async/			      model_meta/
auth_tests/		      model_options/
backends/		      model_package/
base/			      model_regress/
bash_completion/	      model_utils/
basic/			      modeladmin/
builtin_server/		      multiple_database/
bulk_create/		      mutually_referential/
cache/			      nested_foreign_keys/
check_framework/	      no_models/
conditional_processing/       null_fk/
constraints/		      null_fk_ordering/
contenttypes_tests/	      null_queries/
context_processors/	      one_to_one/
csrf_tests/		      or_lookups/
custom_columns/		      order_with_respect_to/
custom_lookups/		      ordering/
custom_managers/	      pagination/
custom_methods/		      postgres_tests/
custom_migration_operations/  prefetch_related/
custom_pk/		      project_template/
datatypes/		      properties/
dates/			      proxy_model_inheritance/
datetimes/		      proxy_models/
db_functions/		      queries/
db_typecasts/		      queryset_pickle/
db_utils/		      raw_query/
dbshell/		      redirects_tests/
decorators/		      requests/
defer/			      requirements/
defer_regress/		      reserved_names/
delete/			      resolve_url/
delete_regress/		      responses/
deprecation/		      reverse_lookup/
dispatch/		      runtests.py*
distinct_on_fields/	      save_delete_hooks/
empty/			      schema/
empty_models/		      select_for_update/
expressions/		      select_related/
expressions_case/	      select_related_onetoone/
expressions_window/	      select_related_regress/
extra_regress/		      serializers/
field_deconstruction/	      servers/
field_defaults/		      sessions_tests/
field_subclassing/	      settings_tests/
file_storage/		      shell/
file_uploads/		      shortcuts/
files/			      signals/
filtered_relation/	      signed_cookies_tests/
fixtures/		      signing/
fixtures_model_package/       sitemaps_tests/
fixtures_regress/	      sites_framework/
flatpages_tests/	      sites_tests/
force_insert_update/	      staticfiles_tests/
foreign_object/		      str/
forms_tests/		      string_lookup/
from_db_value/		      swappable_models/
generic_inline_admin/	      syndication_tests/
generic_relations/	      template_backends/
generic_relations_regress/    template_loader/
generic_views/		      template_tests/
get_earliest_or_latest/       templates/
get_object_or_404/	      test_client/
get_or_create/		      test_client_regress/
gis_tests/		      test_exceptions/
handlers/		      test_runner/
httpwrappers/		      test_runner_apps/
humanize_tests/		      test_sqlite.py
i18n/			      test_utils/
import_error_package/	      timezones/
indexes/		      transaction_hooks/
inline_formsets/	      transactions/
inspectdb/		      unmanaged_models/
introspection/		      update/
invalid_models_tests/	      update_only_fields/
known_related_objects/	      urlpatterns/
logging_tests/		      urlpatterns_reverse/
lookup/			      urls.py
m2m_and_m2o/		      user_commands/
m2m_intermediary/	      utils_tests/
m2m_multiple/		      validation/
m2m_recursive/		      validators/
m2m_regress/		      version/
m2m_signals/		      view_tests/
m2m_through/		      wsgi/
m2m_through_regress/	      xor_lookups/

2025-12-20 22:49:34,702 - thread-130546535499456.app.lang_graph.nodes.bug_reproducing_execute_node - DEBUG - content='' additional_kwargs={'function_call': {'name': 'run_command', 'arguments': '{"command": "./tests/runtests.py tests.migrations.test_alter_field"}'}, '__gemini_function_call_thought_signatures__': {'ef64d35a-2252-4f13-bfff-a0bf556ecfd0': 'CpQCAY89a1+Qt0Ik1ncXjvCbhszr1tPksMKxC84zodnOu6MnSr1PiqomwyjgCuTzDvCTSX1LMpI09b/Xac5QV+kZ9mjZwrL7tYGRaEhoutc1RirPHHhOLlIh7dKSRBrFUUkgV2m/PJ2p3Q0/gu77FjDcMmU7qNWvIA5MANUON8FJ5JgwxRkjFWEWM5HGmg2BAPEcXgEZjo8dXSz3VXAVIZ6fveL0HLSiq4t6b8LSAa+csWPyQzeIkjKDp2CXs+cj+nxHmM8nkC5HAmswA5hkMtvuo1Hl41/mDgBqtGhfa/yeWv2gpitibMqjasYM3BtgGjb537f1e08EHrATNFLK8AGJ708D4/A6UsEuFUowRYniG7fV/WPG'}} response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 1779, 'candidates_token_count': 20, 'total_token_count': 1868, 'prompt_tokens_details': [{'modality': 1, 'token_count': 2164}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 20}], 'thoughts_token_count': 69, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.33644273281097414, 'model_name': 'gemini-2.5-pro'} id='run--ede48103-9e25-4666-aaf4-8d2be367554b-0' tool_calls=[{'name': 'run_command', 'args': {'command': './tests/runtests.py tests.migrations.test_alter_field'}, 'id': 'ef64d35a-2252-4f13-bfff-a0bf556ecfd0', 'type': 'tool_call'}] usage_metadata={'input_tokens': 1779, 'output_tokens': 20, 'total_tokens': 1868, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 69}}
2025-12-20 22:49:34,705 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: ./tests/runtests.py tests.migrations.test_alter_field
2025-12-20 22:49:36,377 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 1
2025-12-20 22:49:36,377 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:
./tests/runtests.py tests.migrations.test_a
alter_field

Testing against Django installed in '/testbed/django' with up to 32 processes
Found 1 test(s).
Creating test database for alias 'default'...
Creating test database for alias 'other'...
System check identified no issues (0 silenced).
E
======================================================================
ERROR: test_alter_field_choices_noop (tests.migrations.test_alter_field.OperationTests)
Changing the 'choices' option on a field should be a no-op.
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/testbed/django/test/testcases.py", line 407, in _setup_and_call
    self._pre_setup()
  File "/testbed/django/test/testcases.py", line 1182, in _pre_setup
    apps.set_available_apps(self.available_apps)
  File "/testbed/django/apps/registry.py", line 321, in set_available_apps
    raise ValueError(
ValueError: Available apps isn't a subset of installed apps, extra apps: migrations

----------------------------------------------------------------------
Ran 0 tests in 0.001s

FAILED (errors=1)
Destroying test database for alias 'default'...
Destroying test database for alias 'other'...

2025-12-20 22:49:38,233 - thread-130546535499456.app.lang_graph.nodes.bug_reproducing_execute_node - DEBUG - content='' additional_kwargs={'function_call': {'name': 'run_command', 'arguments': '{"command": "./tests/runtests.py --help"}'}, '__gemini_function_call_thought_signatures__': {'2a4277f0-6269-48a9-b047-ba93118f7330': 'CsQDAY89a18W5LVIy9nfB84iaCQKnjeztglw3p+TKIU+Uiv++tYchXy91nEs92u0Prs3SPXcekTXx+cges/kh5++eTRNMwaNVL01RCKy5OAx60OvsowqZHKmLoD1MQiwznM6G0wb4O5GWoCLgWzhlhyrutbDYrW7O5Ht0kyBNpGHleMSUQFF5qCZl1Hjj+OsULggkOwC5ku6sUfM5A5hguQ6+0j7EJkOXfucvSW9id+Gf5AkDxns30nQSw11u0Q+flKTNuaCkb/s+/WZf+DiP6W6ryJNNfjWGqBJ7Ujo7CERBQwRNo+V/h9PqyrYiVSrlwq8AYhDq0EOI4iiCCiLzkKgL4IlW033WyEhHgYXNi+TQC3m3cvp8iUGo4Qqsu/I1D1mJIlXCzf4LmDwu0hslGeE4nSOVYgHWqchgunquCeogUI5Oqmzujr6v2EovkYHAzlKFnx7Kd2C+MQWE+7T0ev2jwldnLp4m0TnKH9cTaegUyWJGP9q5BkZ+EnfqrpRczxAXPMiMASbHdtmFwrV+1Lmj1lHPddjFy3hdkhllqYNiE4KdpLcQpDbd1phFYlDyuOTuBhXwB/VSRyQChqftpMJPimsoqw='}} response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 2120, 'candidates_token_count': 13, 'total_token_count': 2233, 'prompt_tokens_details': [{'modality': 1, 'token_count': 2574}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 13}], 'thoughts_token_count': 100, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.7213332102848933, 'model_name': 'gemini-2.5-pro'} id='run--3dbea0b8-732b-49a9-b3c8-09c40533e2bc-0' tool_calls=[{'name': 'run_command', 'args': {'command': './tests/runtests.py --help'}, 'id': '2a4277f0-6269-48a9-b047-ba93118f7330', 'type': 'tool_call'}] usage_metadata={'input_tokens': 2120, 'output_tokens': 13, 'total_tokens': 2233, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 100}}
2025-12-20 22:49:38,236 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: ./tests/runtests.py --help
2025-12-20 22:49:38,728 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-20 22:49:38,728 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:

usage: runtests.py [-h] [-v {0,1,2,3}] [--noinput] [--failfast] [--keepdb]
                   [--settings SETTINGS] [--bisect BISECT] [--pair PAIR]
                   [--shuffle [SEED]] [--reverse] [--selenium BROWSERS]
                   [--headless] [--selenium-hub SELENIUM_HUB]
                   [--external-host EXTERNAL_HOST] [--debug-sql]
                   [--parallel [N]] [--tag TAGS] [--exclude-tag EXCLUDE_TAGS]
                   [--start-after START_AFTER] [--start-at START_AT] [--pdb]
                   [-b] [--timing] [-k TEST_NAME_PATTERNS]
                   [module ...]

Run the Django test suite.

positional arguments:
  module                Optional path(s) to test modules; e.g. "i18n" or
                        "i18n.tests.TranslationTests.test_lazy_objects".

optional arguments:
  -h, --help            show this help message and exit
  -v {0,1,2,3}, --verbosity {0,1,2,3}
                        Verbosity level; 0=minimal output, 1=normal output,
                        2=all output
  --noinput             Tells Django to NOT prompt the user for input of any
                        kind.
  --failfast            Tells Django to stop running the test suite after
                        first failed test.
  --keepdb              Tells Django to preserve the test database between
                        runs.
  --settings SETTINGS   Python path to settings module, e.g.
                        "myproject.settings". If this isn't provided, either
                        the DJANGO_SETTINGS_MODULE environment variable or
                        "test_sqlite" will be used.
  --bisect BISECT       Bisect the test suite to discover a test that causes a
                        test failure when combined with the named test.
  --pair PAIR           Run the test suite in pairs with the named test to
                        find problem pairs.
  --shuffle [SEED]      Shuffle the order of test cases to help check that
                        tests are properly isolated.
  --reverse             Sort test suites and test cases in opposite order to
                        debug test side effects not apparent with normal
                        execution lineup.
  --selenium BROWSERS   A comma-separated list of browsers to run the Selenium
                        tests against.
  --headless            Run selenium tests in headless mode, if the browser
                        supports the option.
  --selenium-hub SELENIUM_HUB
                        A URL for a selenium hub instance to use in
                        combination with --selenium.
  --external-host EXTERNAL_HOST
                        The external host that can be reached by the selenium
                        hub instance when running Selenium tests via Selenium
                        Hub.
  --debug-sql           Turn on the SQL query logger within tests.
  --parallel [N]        Run tests using up to N parallel processes. Use the
                        value "auto" to run one test process for each
                        processor core.
  --tag TAGS            Run only tests with the specified tags. Can be used
                        multiple times.
  --exclude-tag EXCLUDE_TAGS
                        Do not run tests with the specified tag. Can be used
                        multiple times.
  --start-after START_AFTER
                        Run tests starting after the specified top-level
                        module.
  --start-at START_AT   Run tests starting at the specified top-level module.
  --pdb                 Runs the PDB debugger on error or failure.
  -b, --buffer          Discard output of passing tests.
  --timing              Output timings, including database set up and total
                        run time.
  -k TEST_NAME_PATTERNS
                        Only run test methods and classes matching test name
                        pattern. Same as unittest -k option. Can be used
                        multiple times.

2025-12-20 22:49:41,206 - thread-130546535499456.app.lang_graph.nodes.bug_reproducing_execute_node - DEBUG - content='' additional_kwargs={'function_call': {'name': 'run_command', 'arguments': '{"command": "cat tests/test_sqlite.py"}'}, '__gemini_function_call_thought_signatures__': {'ef0553dd-6307-45c5-9bb0-7ca5474396ef': 'Co8DAY89a1/VcrZnjJYzFctYJd9Fk0jvFG02KqEe9sIYW9zS55MWmdx/dj44H3IpjOKMWQEKUMUid9Ss82uuT+HUZXQvo9aIZYIrMP1tEJZ5AR86Vo/4tfn6dKMFBbYX7rgYrxJRLHGotYVDyNEos1MdiZg/z5EL5+FSs4Ue4MT8VnY0St9yz4usPwnmweAyLF4IoStnculR8TI7++9V5cJWpPsmQUHVRKVIuW6jQw14GS4TMCT1K46/CFlvds+MXw8Xu2tHcql8ZdjJvB+2T9SRHLjuHkPemR+6u+JfFYw6A2CfRwMj7OgtwE1MwknQaYn98mXhRo88mr9YfGHdCukLtZkE9u6tD7Ymtu9QVGELdoA3JQsJD1almSWPQYo4F85RP/uwUp6s/emMLskLe+1gJs8+EYNuoO1/JqfMqOoUvC028FRdqFPYholFQtHPJD+v13ZQar4IOOXA3dHDlf7FcEMyN2HubFRx6UOJD+//frEevbx2ApmrTv9s7kSYAQ9hg8ow7TCst9Ikz/0on9l9'}} response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 3011, 'candidates_token_count': 12, 'total_token_count': 3178, 'cached_content_token_count': 1630, 'prompt_tokens_details': [{'modality': 1, 'token_count': 3565}], 'cache_tokens_details': [{'modality': 1, 'token_count': 1630}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 12}], 'thoughts_token_count': 155}, 'finish_reason': 'STOP', 'avg_logprobs': -1.9029313723246257, 'model_name': 'gemini-2.5-pro'} id='run--62be260a-4128-49c8-89db-4b314c247803-0' tool_calls=[{'name': 'run_command', 'args': {'command': 'cat tests/test_sqlite.py'}, 'id': 'ef0553dd-6307-45c5-9bb0-7ca5474396ef', 'type': 'tool_call'}] usage_metadata={'input_tokens': 3011, 'output_tokens': 12, 'total_tokens': 3178, 'input_token_details': {'cache_read': 1630}, 'output_token_details': {'reasoning': 155}}
2025-12-20 22:49:41,209 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: cat tests/test_sqlite.py
2025-12-20 22:49:41,311 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-20 22:49:41,311 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:

# This is an example test settings file for use with the Django test suite.
#
# The 'sqlite3' backend requires only the ENGINE setting (an in-
# memory database will be used). All other backends will require a
# NAME and potentially authentication information. See the
# following section in the docs for more information:
#
# https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/unit-tests/
#
# The different databases that Django supports behave differently in certain
# situations, so it is recommended to run the test suite against as many
# database backends as possible.  You may want to create a separate settings
# file for each of the backends you test against.

DATABASES = {
    "default": {
        "ENGINE": "django.db.backends.sqlite3",
    },
    "other": {
        "ENGINE": "django.db.backends.sqlite3",
    },
}

SECRET_KEY = "django_tests_secret_key"

# Use a fast hasher to speed up tests.
PASSWORD_HASHERS = [
    "django.contrib.auth.hashers.MD5PasswordHasher",
]

DEFAULT_AUTO_FIELD = "django.db.models.AutoField"

USE_TZ = False

2025-12-20 22:49:43,861 - thread-130546535499456.app.lang_graph.nodes.bug_reproducing_execute_node - DEBUG - content='' additional_kwargs={'function_call': {'name': 'run_command', 'arguments': '{"command": "grep -r \'INSTALLED_APPS\' tests/"}'}, '__gemini_function_call_thought_signatures__': {'9ee879eb-731a-4d13-9782-b9df742248bf': 'Cu8GAY89a19Hq/f6/SahZdN4j/t54Ty8ZD+htXvueyw9xXD4wYUorIs7Tso9yrfl3qdSH58IFAo2xXVFG1bujPJ8nF20qOIN/Qqdxf6OsB8+8aamrJdKXfC9vf+B0VeR0WgIDwFXCYWK1MItEfAjguOKSK/zZ1wEtyxIcf9RlZbyFxn+DAkJ7mmICPv7NCqZQ192KBHL4Tmq4684OHzcuhkoeLMrloabNazP8d6zSSImSs7TCKw33BJ+Op8Cp3scwRF4JRf7sVPOXq0c54QKvIlSQvjylz94/EQZLXLQYq3MeQr/LfekW2sWC8fMVTlZgxdgSvUfXKV+9F7VbgWUwGvEm/tlABKQ8CPNesyNrN79tyXpwE/vhVspOgA0awF1Oa+NhbaKuHMtBA7CA7KQVFSjcvBFfAv59G4sc6KyusbzMBYTcwb2pC5cC/FjBm8esUBoNtqxJ4GLLSfBUsXlX3YcmfXzU1Owf6W17x+5Z1TJ2yoGLMZgdynnocAd4YUvYwMWvlFd/+YSNr/EHB3asiJV/BPKRTIAokgntULzYEQZxTZ+GUJNYxAnphsLlAG0lnUOWSzQCLQrSifAmcEPEizdYDqg36BLHAq5gzbAzTFDd1dg5cEgxoWxpsL07qYIW+u8kp2tMzx7k4JyZHS9GjOpthMJG40KjGmVb0gwP7OVWWY8PrUw9m6swaKOhAu4x0Hr2f36hsdpXgosZGyoIPTnt01z9LzHU/BB0kC8TF090EMRCIDH3npdwAxtBXrSL06tN1zdWFvdRaVz1lz8iqo5J9v7RqPYkumE+uSkwHyFVz6LCCS1FUutpZ7DCMOufHEAQ8LHZvVj9zmI1RcFYnIrNuL2ThAFFSMabd9JvIXQ9Clocgw9nolZgzSuVBgs/jLpSKnDQlpxvo8DwmrgIXLr5jp+N2fR3cUwygl8WQYTLRQoLgjiht0WCOIQS+lCyj+ax3/T4/YzLwVvx/jwpR8sdWJOD3njs/UkgZg4BDWpXUkjJBicX9Rmq85fvpi8Tnh1xBqKwkUTS4iRPc8mnho5MaXnqm2s8rvaDsM5BnK1w27ZJeEJvyGZ+RxHQqcIQ6hsDk7l/vNonlo7PHYdRPm5Zl0Z2H2V9rOVrXEr3574Ab8UJQrBpuWIpXxwJb6NA4YNZErYcf7996Q44BEWuoGq'}} response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 3319, 'candidates_token_count': 15, 'total_token_count': 3552, 'cached_content_token_count': 3289, 'prompt_tokens_details': [{'modality': 1, 'token_count': 3966}], 'cache_tokens_details': [{'modality': 1, 'token_count': 3289}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 15}], 'thoughts_token_count': 218}, 'finish_reason': 'STOP', 'avg_logprobs': -1.4772037506103515, 'model_name': 'gemini-2.5-pro'} id='run--1aa256bf-efac-4d8a-9e23-3596cf852a43-0' tool_calls=[{'name': 'run_command', 'args': {'command': "grep -r 'INSTALLED_APPS' tests/"}, 'id': '9ee879eb-731a-4d13-9782-b9df742248bf', 'type': 'tool_call'}] usage_metadata={'input_tokens': 3319, 'output_tokens': 15, 'total_tokens': 3552, 'input_token_details': {'cache_read': 3289}, 'output_token_details': {'reasoning': 218}}
2025-12-20 22:49:43,863 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: grep -r 'INSTALLED_APPS' tests/
2025-12-20 22:49:43,986 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-20 22:49:43,987 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:

tests/admin_views/tests.py:    INSTALLED_APPS={"append": ["django.contrib.admindocs", "django.contrib.flatpages"]}
tests/messages_tests/base.py:        INSTALLED_APPS={"remove": "django.contrib.messages"},
tests/messages_tests/base.py:        INSTALLED_APPS={"remove": "django.contrib.messages"},
tests/asgi/tests.py:    @modify_settings(INSTALLED_APPS={"append": "django.contrib.staticfiles"})
tests/app_loading/tests.py:            with self.settings(INSTALLED_APPS=["app_with_models"]):
tests/app_loading/tests.py:            with self.settings(INSTALLED_APPS=["app_no_models"]):
tests/app_loading/tests.py:            with self.settings(INSTALLED_APPS=["omelet.app_with_models"]):
tests/app_loading/tests.py:            with self.settings(INSTALLED_APPS=["omelet.app_no_models"]):
tests/app_loading/tests.py:                with self.settings(INSTALLED_APPS=["broken_app"]):
tests/i18n/tests.py:        INSTALLED_APPS=["i18n.loading_app"],
tests/i18n/tests.py:        with self.modify_settings(INSTALLED_APPS={"append": "i18n.resolution"}):
tests/i18n/tests.py:                INSTALLED_APPS={"remove": "django.contrib.admin.apps.SimpleAdminConfig"}
tests/i18n/tests.py:        with self.settings(INSTALLED_APPS=["i18n.resolution"]):
tests/i18n/tests.py:@override_settings(INSTALLED_APPS=["i18n.territorial_fallback"])
tests/i18n/tests.py:        with self.settings(INSTALLED_APPS=["tests.i18n.sampleproject"]):
tests/i18n/tests.py:        with self.settings(INSTALLED_APPS=["django.contrib.admin"]):
tests/check_framework/test_templates.py:        INSTALLED_APPS=[
tests/check_framework/test_templates.py:        INSTALLED_APPS=["check_framework.template_test_apps.same_tags_app_1"]
tests/check_framework/test_templates.py:        INSTALLED_APPS=[
tests/check_framework/test_security.py:        INSTALLED_APPS=["django.contrib.sessions"],
tests/check_framework/test_security.py:        in INSTALLED_APPS.
tests/check_framework/test_security.py:        INSTALLED_APPS=["django.contrib.sessions"],
tests/check_framework/test_security.py:        INSTALLED_APPS=[],
tests/check_framework/test_security.py:        INSTALLED_APPS=["django.contrib.sessions"],
tests/check_framework/test_security.py:        INSTALLED_APPS=["django.contrib.sessions"],
tests/check_framework/test_security.py:        INSTALLED_APPS=["django.contrib.sessions"],
tests/check_framework/test_security.py:        is in INSTALLED_APPS.
tests/check_framework/test_security.py:        INSTALLED_APPS=["django.contrib.sessions"],
tests/check_framework/test_security.py:        INSTALLED_APPS=[],
tests/check_framework/test_security.py:        INSTALLED_APPS=["django.contrib.sessions"],
tests/check_framework/test_security.py:        INSTALLED_APPS=["django.contrib.sessions"],
tests/check_framework/test_model_checks.py:    @modify_settings(INSTALLED_APPS={"append": "basic"})
tests/check_framework/test_model_checks.py:    @modify_settings(INSTALLED_APPS={"append": "basic"})
tests/check_framework/test_model_checks.py:    @modify_settings(INSTALLED_APPS={"append": "basic"})
tests/check_framework/test_model_checks.py:    @modify_settings(INSTALLED_APPS={"append": "basic"})
tests/check_framework/test_model_checks.py:    @modify_settings(INSTALLED_APPS={"append": "basic"})
tests/check_framework/test_model_checks.py:    @modify_settings(INSTALLED_APPS={"append": "basic"})
tests/apps/tests.py:SOME_INSTALLED_APPS = [
tests/apps/tests.py:SOME_INSTALLED_APPS_NAMES = [
tests/apps/tests.py:] + SOME_INSTALLED_APPS[2:]
tests/apps/tests.py:        Tests when INSTALLED_APPS contains an incorrect app config.
tests/apps/tests.py:            with self.settings(INSTALLED_APPS=["apps.apps.BadConfig"]):
tests/apps/tests.py:        Tests when INSTALLED_APPS contains a class that isn't an app config.
tests/apps/tests.py:            with self.settings(INSTALLED_APPS=["apps.apps.NotAConfig"]):
tests/apps/tests.py:        Tests when INSTALLED_APPS contains an app that doesn't exist, either
tests/apps/tests.py:            with self.settings(INSTALLED_APPS=["there is no such app"]):
tests/apps/tests.py:            with self.settings(INSTALLED_APPS=["apps.apps.NoSuchApp"]):
tests/apps/tests.py:            with self.settings(INSTALLED_APPS=["apps.NoSuchConfig"]):
tests/apps/tests.py:            with self.settings(INSTALLED_APPS=["apps.apps.NoSuchConfig"]):
tests/apps/tests.py:        with self.settings(INSTALLED_APPS=["apps.no_config_app"]):
tests/apps/tests.py:        with self.settings(INSTALLED_APPS=["apps.one_config_app"]):
tests/apps/tests.py:        with self.settings(INSTALLED_APPS=["apps.two_configs_app"]):
tests/apps/tests.py:            with self.settings(INSTALLED_APPS=["apps.two_default_configs_app"]):
tests/apps/tests.py:        with self.settings(INSTALLED_APPS=["apps.two_configs_one_default_app"]):
tests/apps/tests.py:    @override_settings(INSTALLED_APPS=SOME_INSTALLED_APPS)
tests/apps/tests.py:            [app_config.name for app_config in app_configs], SOME_INSTALLED_APPS_NAMES
tests/apps/tests.py:    @override_settings(INSTALLED_APPS=SOME_INSTALLED_APPS)
tests/apps/tests.py:    @override_settings(INSTALLED_APPS=SOME_INSTALLED_APPS)
tests/apps/tests.py:    @override_settings(INSTALLED_APPS=SOME_INSTALLED_APPS)
tests/apps/tests.py:    @override_settings(INSTALLED_APPS=["apps.apps.RelabeledAppsConfig"])
tests/apps/tests.py:            with self.settings(INSTALLED_APPS=["apps.apps.PlainAppsConfig", "apps"]):
tests/apps/tests.py:                INSTALLED_APPS=["apps.apps.RelabeledAppsConfig", "apps"]
tests/apps/tests.py:            with self.settings(INSTALLED_APPS=["import_error_package"]):
tests/apps/tests.py:        INSTALLED_APPS=["apps.apps.ModelPKAppsConfig"],
tests/apps/tests.py:        INSTALLED_APPS=["apps.apps.PlainAppsConfig"],
tests/apps/tests.py:            with self.settings(INSTALLED_APPS=["nsapp"]):
tests/apps/tests.py:                with self.settings(INSTALLED_APPS=["nsapp"]):
tests/apps/tests.py:            with self.settings(INSTALLED_APPS=["nsapp.apps.NSAppConfig"]):
tests/test_runner/test_discover_runner.py:        with self.modify_settings(INSTALLED_APPS={"append": sub_app}):
tests/migrations/test_loader.py:    @modify_settings(INSTALLED_APPS={"append": "basic"})
tests/migrations/test_loader.py:    @modify_settings(INSTALLED_APPS={"append": "migrations2"})
tests/migrations/test_loader.py:    @modify_settings(INSTALLED_APPS={"append": "migrations2"})
tests/migrations/test_loader.py:        INSTALLED_APPS=["migrations.migrations_test_apps.migrated_app"],
tests/migrations/test_loader.py:        INSTALLED_APPS=["migrations.migrations_test_apps.migrated_app"],
tests/migrations/test_loader.py:        INSTALLED_APPS=["migrations.migrations_test_apps.migrated_app"],
tests/migrations/test_loader.py:        INSTALLED_APPS={
tests/migrations/test_loader.py:        INSTALLED_APPS={
tests/migrations/test_loader.py:        INSTALLED_APPS=["migrations"],
tests/migrations/test_loader.py:        INSTALLED_APPS=["migrations"],
tests/migrations/test_loader.py:        INSTALLED_APPS={
tests/migrations/test_commands.py:        INSTALLED_APPS=[
tests/migrations/test_commands.py:        INSTALLED_APPS=[
tests/migrations/test_commands.py:        INSTALLED_APPS=["migrations.migrations_test_apps.unmigrated_app"]
tests/migrations/test_commands.py:        INSTALLED_APPS=[
tests/migrations/test_commands.py:        INSTALLED_APPS=[
tests/migrations/test_commands.py:        INSTALLED_APPS=["migrations.migrations_test_apps.unmigrated_app"]
tests/migrations/test_commands.py:        INSTALLED_APPS=[
tests/migrations/test_commands.py:        INSTALLED_APPS=["migrations.migrations_test_apps.unmigrated_app_syncdb"]
tests/migrations/test_commands.py:        INSTALLED_APPS=[
tests/migrations/test_commands.py:        INSTALLED_APPS=[
tests/migrations/test_commands.py:    @override_settings(INSTALLED_APPS=["migrations", "migrations2"])
tests/migrations/test_commands.py:        INSTALLED_APPS=[
tests/migrations/test_commands.py:        INSTALLED_APPS=[
tests/migrations/test_commands.py:        INSTALLED_APPS=[
tests/migrations/test_commands.py:    app. 'django.contrib.auth' must be in INSTALLED_APPS for some of these
tests/migrations/test_commands.py:        INSTALLED_APPS=["migrations.migrations_test_apps.unmigrated_app_simple"],
tests/migrations/test_writer.py:            with self.modify_settings(INSTALLED_APPS={"append": app}):
tests/migrations/test_questioner.py:        INSTALLED_APPS=["migrations"],
tests/migrations/test_executor.py:@modify_settings(INSTALLED_APPS={"append": "migrations2"})
tests/migrations/test_executor.py:        INSTALLED_APPS=[
tests/migrations/test_executor.py:        INSTALLED_APPS=[
tests/migrations/test_executor.py:        INSTALLED_APPS=[
tests/migrations/test_executor.py:        INSTALLED_APPS=[
tests/staticfiles_tests/settings.py:    "INSTALLED_APPS": [
tests/staticfiles_tests/settings.py:    # contrib.auth isn't in INSTALLED_APPS.
tests/staticfiles_tests/test_forms.py:    INSTALLED_APPS=("django.contrib.staticfiles",),
tests/staticfiles_tests/test_liveserver.py:    @modify_settings(INSTALLED_APPS={"append": "staticfiles_tests.apps.test"})
tests/staticfiles_tests/test_management.py:    INSTALLED_APPS=[
tests/staticfiles_tests/test_management.py:            INSTALLED_APPS={"prepend": "staticfiles_test_app"},
tests/settings_tests/tests.py:        "INSTALLED_APPS",
tests/backends/base/test_creation.py:@override_settings(INSTALLED_APPS=["backends.base.app_unmigrated"])
tests/gis_tests/geoapp/test_feeds.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.sites"})
tests/gis_tests/geoapp/test_sitemaps.py:    INSTALLED_APPS={"append": ["django.contrib.sites", "django.contrib.sitemaps"]}
tests/gis_tests/inspectapp/tests.py:    INSTALLED_APPS={"append": "django.contrib.gis"},
tests/sites_tests/tests.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.sites"})
tests/sites_tests/tests.py:        with self.modify_settings(INSTALLED_APPS={"remove": "django.contrib.sites"}):
tests/sites_tests/tests.py:        with self.modify_settings(INSTALLED_APPS={"remove": "django.contrib.sites"}):
tests/sites_tests/tests.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.sites"})
tests/forms_tests/tests/test_renderers.py:        """Can find a custom template in INSTALLED_APPS."""
tests/utils_tests/test_module_loading.py:@modify_settings(INSTALLED_APPS={"append": "utils_tests.test_module"})
tests/proxy_model_inheritance/tests.py:            with self.modify_settings(INSTALLED_APPS={"append": ["app1", "app2"]}):
tests/admin_default_site/tests.py:    INSTALLED_APPS=[
tests/template_backends/test_django.py:    @override_settings(INSTALLED_APPS=["template_backends.apps.good"])
tests/template_backends/test_django.py:    @override_settings(INSTALLED_APPS=["template_backends.apps.importerror"])
tests/humanize_tests/tests.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.humanize"})
tests/user_commands/tests.py:    INSTALLED_APPS=[
tests/user_commands/tests.py:            with self.settings(INSTALLED_APPS=["commandegg"]):
tests/contenttypes_tests/test_management.py:@modify_settings(INSTALLED_APPS={"append": ["empty_models", "no_models"]})
tests/contenttypes_tests/test_management.py:    @modify_settings(INSTALLED_APPS={"remove": ["empty_models"]})
tests/contenttypes_tests/test_management.py:    @modify_settings(INSTALLED_APPS={"remove": ["empty_models"]})
tests/contenttypes_tests/test_checks.py:        INSTALLED_APPS=[
tests/contenttypes_tests/test_views.py:        with self.modify_settings(INSTALLED_APPS={"append": "django.contrib.sites"}):
tests/contenttypes_tests/test_views.py:        with self.modify_settings(INSTALLED_APPS={"remove": "django.contrib.sites"}):
tests/flatpages_tests/test_forms.py:@modify_settings(INSTALLED_APPS={"append": ["django.contrib.flatpages"]})
tests/flatpages_tests/test_forms.py:        # INSTALLED_APPS
tests/flatpages_tests/test_sitemaps.py:    INSTALLED_APPS={
tests/flatpages_tests/test_csrf.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.flatpages"})
tests/flatpages_tests/test_middleware.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.flatpages"})
tests/flatpages_tests/test_middleware.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.flatpages"})
tests/flatpages_tests/test_views.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.flatpages"})
tests/flatpages_tests/test_views.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.flatpages"})
tests/admin_checks/tests.py:    INSTALLED_APPS=[
tests/admin_checks/tests.py:    @override_settings(INSTALLED_APPS=["django.contrib.admin"])
tests/admin_checks/tests.py:                "INSTALLED_APPS in order to use the admin application.",
tests/admin_checks/tests.py:                "'django.contrib.auth' must be in INSTALLED_APPS in order "
tests/admin_checks/tests.py:                "'django.contrib.messages' must be in INSTALLED_APPS in order "
tests/admin_docs/tests.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.admindocs"})
tests/admin_docs/tests.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.admindocs"})
tests/admin_docs/test_views.py:    @modify_settings(INSTALLED_APPS={"remove": "django.contrib.sites"})
tests/redirects_tests/tests.py:    @modify_settings(INSTALLED_APPS={"remove": "django.contrib.sites"})
tests/admin_scripts/tests.py:            settings_file.write("INSTALLED_APPS = %s\n" % apps)
tests/admin_scripts/tests.py:        self.write_settings("settings.py", sdict={"BAD_VAR": "INSTALLED_APPS.crash"})
tests/admin_scripts/tests.py:        """check reports an error on a nonexistent app in INSTALLED_APPS."""
tests/admin_scripts/tests.py:    @override_settings(INSTALLED_APPS=["admin_scripts.app_waiting_migration"])
tests/admin_scripts/tests.py:        INSTALLED_APPS=[
tests/admin_scripts/tests.py:        Apps listed first in INSTALLED_APPS have precedence.
tests/admin_scripts/tests.py:            INSTALLED_APPS=[
tests/admin_scripts/tests.py:            INSTALLED_APPS=[
tests/postgres_tests/test_introspection.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.postgres"})
tests/postgres_tests/test_trigram.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.postgres"})
tests/postgres_tests/test_bulk_update.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.postgres"})
tests/postgres_tests/test_apps.py:        with modify_settings(INSTALLED_APPS={"append": "django.contrib.postgres"}):
tests/postgres_tests/test_apps.py:        with self.modify_settings(INSTALLED_APPS={"append": "django.contrib.postgres"}):
tests/postgres_tests/integration_settings.py:INSTALLED_APPS = [
tests/postgres_tests/test_array.py:    @modify_settings(INSTALLED_APPS={"append": "django.contrib.postgres"})
tests/postgres_tests/test_unaccent.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.postgres"})
tests/postgres_tests/test_constraints.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.postgres"})
tests/postgres_tests/test_constraints.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.postgres"})
tests/postgres_tests/test_constraints.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.postgres"})
tests/postgres_tests/test_search.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.postgres"})
tests/postgres_tests/test_search.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.postgres"})
tests/postgres_tests/test_search.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.postgres"})
tests/postgres_tests/test_search.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.postgres"})
tests/postgres_tests/test_search.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.postgres"})
tests/postgres_tests/test_citext.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.postgres"})
tests/postgres_tests/test_operations.py:@modify_settings(INSTALLED_APPS={"append": "migrations"})
tests/postgres_tests/test_operations.py:@modify_settings(INSTALLED_APPS={"append": "migrations"})
tests/postgres_tests/test_operations.py:@modify_settings(INSTALLED_APPS={"append": "migrations"})
tests/postgres_tests/test_operations.py:@modify_settings(INSTALLED_APPS={"append": "migrations"})
tests/postgres_tests/test_indexes.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.postgres"})
tests/postgres_tests/__init__.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.postgres"})
tests/test_client/tests.py:        INSTALLED_APPS=["django.contrib.auth"],
tests/test_client/tests.py:        INSTALLED_APPS=[],
tests/test_client/tests.py:        INSTALLED_APPS=[],
tests/runtests.py:ALWAYS_INSTALLED_APPS = [
tests/runtests.py:# Need to add the associated contrib app to INSTALLED_APPS in some cases to
tests/runtests.py:# and isn't in an application in INSTALLED_APPS."
tests/runtests.py:        "INSTALLED_APPS": settings.INSTALLED_APPS,
tests/runtests.py:    settings.INSTALLED_APPS = ALWAYS_INSTALLED_APPS
tests/runtests.py:    # Load all the ALWAYS_INSTALLED_APPS.
tests/runtests.py:    # Add contrib.gis to INSTALLED_APPS if needed (rather than requiring
tests/runtests.py:    # @override_settings(INSTALLED_APPS=...) on all test cases.
tests/runtests.py:        settings.INSTALLED_APPS.append(app)
tests/runtests.py:    apps.set_installed_apps(settings.INSTALLED_APPS)
tests/sitemaps_tests/test_http.py:    @modify_settings(INSTALLED_APPS={"remove": "django.contrib.sites"})
tests/sitemaps_tests/test_http.py:    @modify_settings(INSTALLED_APPS={"remove": "django.contrib.sites"})
tests/sitemaps_tests/test_utils.py:    @modify_settings(INSTALLED_APPS={"remove": "django.contrib.sites"})
tests/sitemaps_tests/base.py:@modify_settings(INSTALLED_APPS={"append": "django.contrib.sitemaps"})
tests/invalid_models_tests/test_relative_fields.py:    @modify_settings(INSTALLED_APPS={"append": "basic"})
tests/template_tests/syntax_tests/test_static.py:@override_settings(INSTALLED_APPS=[], MEDIA_URL="media/", STATIC_URL="static/")
tests/template_tests/test_loaders.py:    @override_settings(INSTALLED_APPS=["template_tests"])
tests/template_tests/test_loaders.py:    @override_settings(INSTALLED_APPS=[])
tests/template_tests/test_autoreloader.py:    INSTALLED_APPS=["template_tests"],
tests/template_tests/test_autoreloader.py:@override_settings(INSTALLED_APPS=["template_tests"])
tests/view_tests/tests/test_i18n.py:    @modify_settings(INSTALLED_APPS={"append": "view_tests.app0"})
tests/view_tests/tests/test_i18n.py:    @modify_settings(INSTALLED_APPS={"append": ["view_tests.app5"]})
tests/view_tests/tests/test_i18n.py:    @modify_settings(INSTALLED_APPS={"append": ["view_tests.app1", "view_tests.app2"]})
tests/view_tests/tests/test_i18n.py:    @modify_settings(INSTALLED_APPS={"append": ["view_tests.app3", "view_tests.app4"]})
tests/view_tests/tests/test_i18n.py:    @modify_settings(INSTALLED_APPS={"append": ["view_tests.app1", "view_tests.app2"]})

2025-12-20 22:50:26,954 - thread-130546535499456.app.lang_graph.nodes.bug_reproducing_execute_node - DEBUG - content='' additional_kwargs={'function_call': {'name': 'run_command', 'arguments': '{"command": "cat tests/runtests.py"}'}, '__gemini_function_call_thought_signatures__': {'f9691cda-4630-450f-a92f-0822bb021b67': 'Cv8CAY89a18JHZuyPBxW6sFxOU7a7ns/SdMLOtUghHcNBP8b5D80NlmlLv18aFmvuzqJlKNnPpdhH1mSHL/psr9kFwqakYeul15/1mHZQ7CBDwijWZ/ac3wLTgV4TeNd5cBditwJ7UGTfLnZMIL65QiXzKDRV/A1ozIg9XwDar4Iq/Pb5k697T/hrZRu0WbP6AanQjtVbgoOEd81SN4hkjSWA4v2alJrVUf2EPTcV4P804QTLu1YhfGKoEMIVwdsTeXYxzy8PyJ6VEn9tnOt2CZzkbJU4sfMXtKJbbgIw152kYaQYS3/3ExBRylttarierNeNpd186jZS42RSJTkH99r5h4JE6jFosP48SvaVYzRxrxoKNKGHQnQ8pUa4ml2uVbfyohnu+0YBq6fjGpDK/1BdsUoyLrMsSzVSAqdQloMrucNIAc1/UEBcoXX/MjPuCP6OSjIWPq9H2kB2p+2QgH3SE6CZ20yXkFwOQMlBK9mvCF/8vneOM1nlkjLhv94uBY='}} response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 9066, 'candidates_token_count': 11, 'total_token_count': 9900, 'cached_content_token_count': 3589, 'prompt_tokens_details': [{'modality': 1, 'token_count': 9931}], 'cache_tokens_details': [{'modality': 1, 'token_count': 3589}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 11}], 'thoughts_token_count': 823}, 'finish_reason': 'STOP', 'avg_logprobs': -28.309592507102273, 'model_name': 'gemini-2.5-pro'} id='run--a0fe754d-fea1-4a94-8eb1-1aa14075839e-0' tool_calls=[{'name': 'run_command', 'args': {'command': 'cat tests/runtests.py'}, 'id': 'f9691cda-4630-450f-a92f-0822bb021b67', 'type': 'tool_call'}] usage_metadata={'input_tokens': 9066, 'output_tokens': 11, 'total_tokens': 9900, 'input_token_details': {'cache_read': 3589}, 'output_token_details': {'reasoning': 823}}
2025-12-20 22:50:26,957 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: cat tests/runtests.py
2025-12-20 22:50:27,061 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-20 22:50:27,062 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:

#!/usr/bin/env python
import argparse
import atexit
import copy
import gc
import multiprocessing
import os
import shutil
import socket
import subprocess
import sys
import tempfile
import warnings
from functools import partial
from pathlib import Path

try:
    import django
except ImportError as e:
    raise RuntimeError(
        "Django module not found, reference tests/README.rst for instructions."
    ) from e
else:
    from django.apps import apps
    from django.conf import settings
    from django.core.exceptions import ImproperlyConfigured
    from django.db import connection, connections
    from django.test import TestCase, TransactionTestCase
    from django.test.runner import _init_worker, get_max_test_processes, parallel_type
    from django.test.selenium import SeleniumTestCaseBase
    from django.test.utils import NullTimeKeeper, TimeKeeper, get_runner
    from django.utils.deprecation import RemovedInDjango50Warning
    from django.utils.log import DEFAULT_LOGGING

try:
    import MySQLdb
except ImportError:
    pass
else:
    # Ignore informational warnings from QuerySet.explain().
    warnings.filterwarnings("ignore", r"\(1003, *", category=MySQLdb.Warning)

# Make deprecation warnings errors to ensure no usage of deprecated features.
warnings.simplefilter("error", RemovedInDjango50Warning)
# Make resource and runtime warning errors to ensure no usage of error prone
# patterns.
warnings.simplefilter("error", ResourceWarning)
warnings.simplefilter("error", RuntimeWarning)
# Ignore known warnings in test dependencies.
warnings.filterwarnings(
    "ignore", "'U' mode is deprecated", DeprecationWarning, module="docutils.io"
)

# Reduce garbage collection frequency to improve performance. Since CPython
# uses refcounting, garbage collection only collects objects with cyclic
# references, which are a minority, so the garbage collection threshold can be
# larger than the default threshold of 700 allocations + deallocations without
# much increase in memory usage.
gc.set_threshold(100_000)

RUNTESTS_DIR = os.path.abspath(os.path.dirname(__file__))

TEMPLATE_DIR = os.path.join(RUNTESTS_DIR, "templates")

# Create a specific subdirectory for the duration of the test suite.
TMPDIR = tempfile.mkdtemp(prefix="django_")
# Set the TMPDIR environment variable in addition to tempfile.tempdir
# so that children processes inherit it.
tempfile.tempdir = os.environ["TMPDIR"] = TMPDIR

# Removing the temporary TMPDIR.
atexit.register(shutil.rmtree, TMPDIR)


# This is a dict mapping RUNTESTS_DIR subdirectory to subdirectories of that
# directory to skip when searching for test modules.
SUBDIRS_TO_SKIP = {
    "": {"import_error_package", "test_runner_apps"},
    "gis_tests": {"data"},
}

ALWAYS_INSTALLED_APPS = [
    "django.contrib.contenttypes",
    "django.contrib.auth",
    "django.contrib.sites",
    "django.contrib.sessions",
    "django.contrib.messages",
    "django.contrib.admin.apps.SimpleAdminConfig",
    "django.contrib.staticfiles",
]

ALWAYS_MIDDLEWARE = [
    "django.contrib.sessions.middleware.SessionMiddleware",
    "django.middleware.common.CommonMiddleware",
    "django.middleware.csrf.CsrfViewMiddleware",
    "django.contrib.auth.middleware.AuthenticationMiddleware",
    "django.contrib.messages.middleware.MessageMiddleware",
]

# Need to add the associated contrib app to INSTALLED_APPS in some cases to
# avoid "RuntimeError: Model class X doesn't declare an explicit app_label
# and isn't in an application in INSTALLED_APPS."
CONTRIB_TESTS_TO_APPS = {
    "deprecation": ["django.contrib.flatpages", "django.contrib.redirects"],
    "flatpages_tests": ["django.contrib.flatpages"],
    "redirects_tests": ["django.contrib.redirects"],
}


def get_test_modules(gis_enabled):
    """
    Scan the tests directory and yield the names of all test modules.

    The yielded names have either one dotted part like "test_runner" or, in
    the case of GIS tests, two dotted parts like "gis_tests.gdal_tests".
    """
    discovery_dirs = [""]
    if gis_enabled:
        # GIS tests are in nested apps
        discovery_dirs.append("gis_tests")
    else:
        SUBDIRS_TO_SKIP[""].add("gis_tests")

    for dirname in discovery_dirs:
        dirpath = os.path.join(RUNTESTS_DIR, dirname)
        subdirs_to_skip = SUBDIRS_TO_SKIP[dirname]
        with os.scandir(dirpath) as entries:
            for f in entries:
                if (
                    "." in f.name
                    or os.path.basename(f.name) in subdirs_to_skip
                    or f.is_file()
                    or not os.path.exists(os.path.join(f.path, "__init__.py"))
                ):
                    continue
                test_module = f.name
                if dirname:
                    test_module = dirname + "." + test_module
                yield test_module


def get_label_module(label):
    """Return the top-level module part for a test label."""
    path = Path(label)
    if len(path.parts) == 1:
        # Interpret the label as a dotted module name.
        return label.split(".")[0]

    # Otherwise, interpret the label as a path. Check existence first to
    # provide a better error message than relative_to() if it doesn't exist.
    if not path.exists():
        raise RuntimeError(f"Test label path {label} does not exist")
    path = path.resolve()
    rel_path = path.relative_to(RUNTESTS_DIR)
    return rel_path.parts[0]


def get_filtered_test_modules(start_at, start_after, gis_enabled, test_labels=None):
    if test_labels is None:
        test_labels = []
    # Reduce each test label to just the top-level module part.
    label_modules = set()
    for label in test_labels:
        test_module = get_label_module(label)
        label_modules.add(test_module)

    # It would be nice to put this validation earlier but it must come after
    # django.setup() so that connection.features.gis_enabled can be accessed.
    if "gis_tests" in label_modules and not gis_enabled:
        print("Aborting: A GIS database backend is required to run gis_tests.")
        sys.exit(1)

    def _module_match_label(module_name, label):
        # Exact or ancestor match.
        return module_name == label or module_name.startswith(label + ".")

    start_label = start_at or start_after
    for test_module in get_test_modules(gis_enabled):
        if start_label:
            if not _module_match_label(test_module, start_label):
                continue
            start_label = ""
            if not start_at:
                assert start_after
                # Skip the current one before starting.
                continue
        # If the module (or an ancestor) was named on the command line, or
        # no modules were named (i.e., run all), include the test module.
        if not test_labels or any(
            _module_match_label(test_module, label_module)
            for label_module in label_modules
        ):
            yield test_module


def setup_collect_tests(start_at, start_after, test_labels=None):
    state = {
        "INSTALLED_APPS": settings.INSTALLED_APPS,
        "ROOT_URLCONF": getattr(settings, "ROOT_URLCONF", ""),
        "TEMPLATES": settings.TEMPLATES,
        "LANGUAGE_CODE": settings.LANGUAGE_CODE,
        "STATIC_URL": settings.STATIC_URL,
        "STATIC_ROOT": settings.STATIC_ROOT,
        "MIDDLEWARE": settings.MIDDLEWARE,
    }

    # Redirect some settings for the duration of these tests.
    settings.INSTALLED_APPS = ALWAYS_INSTALLED_APPS
    settings.ROOT_URLCONF = "urls"
    settings.STATIC_URL = "static/"
    settings.STATIC_ROOT = os.path.join(TMPDIR, "static")
    settings.TEMPLATES = [
        {
            "BACKEND": "django.template.backends.django.DjangoTemplates",
            "DIRS": [TEMPLATE_DIR],
            "APP_DIRS": True,
            "OPTIONS": {
                "context_processors": [
                    "django.template.context_processors.debug",
                    "django.template.context_processors.request",
                    "django.contrib.auth.context_processors.auth",
                    "django.contrib.messages.context_processors.messages",
                ],
            },
        }
    ]
    settings.LANGUAGE_CODE = "en"
    settings.SITE_ID = 1
    settings.MIDDLEWARE = ALWAYS_MIDDLEWARE
    settings.MIGRATION_MODULES = {
        # This lets us skip creating migrations for the test models as many of
        # them depend on one of the following contrib applications.
        "auth": None,
        "contenttypes": None,
        "sessions": None,
    }
    log_config = copy.deepcopy(DEFAULT_LOGGING)
    # Filter out non-error logging so we don't have to capture it in lots of
    # tests.
    log_config["loggers"]["django"]["level"] = "ERROR"
    settings.LOGGING = log_config
    settings.SILENCED_SYSTEM_CHECKS = [
        "fields.W342",  # ForeignKey(unique=True) -> OneToOneField
    ]

    # Load all the ALWAYS_INSTALLED_APPS.
    django.setup()

    # This flag must be evaluated after django.setup() because otherwise it can
    # raise AppRegistryNotReady when running gis_tests in isolation on some
    # backends (e.g. PostGIS).
    gis_enabled = connection.features.gis_enabled

    test_modules = list(
        get_filtered_test_modules(
            start_at,
            start_after,
            gis_enabled,
            test_labels=test_labels,
        )
    )
    return test_modules, state


def teardown_collect_tests(state):
    # Restore the old settings.
    for key, value in state.items():
        setattr(settings, key, value)


def get_installed():
    return [app_config.name for app_config in apps.get_app_configs()]


# This function should be called only after calling django.setup(),
# since it calls connection.features.gis_enabled.
def get_apps_to_install(test_modules):
    for test_module in test_modules:
        if test_module in CONTRIB_TESTS_TO_APPS:
            yield from CONTRIB_TESTS_TO_APPS[test_module]
        yield test_module

    # Add contrib.gis to INSTALLED_APPS if needed (rather than requiring
    # @override_settings(INSTALLED_APPS=...) on all test cases.
    if connection.features.gis_enabled:
        yield "django.contrib.gis"


def setup_run_tests(verbosity, start_at, start_after, test_labels=None):
    test_modules, state = setup_collect_tests(
        start_at, start_after, test_labels=test_labels
    )

    installed_apps = set(get_installed())
    for app in get_apps_to_install(test_modules):
        if app in installed_apps:
            continue
        if verbosity >= 2:
            print(f"Importing application {app}")
        settings.INSTALLED_APPS.append(app)
        installed_apps.add(app)

    apps.set_installed_apps(settings.INSTALLED_APPS)

    # Force declaring available_apps in TransactionTestCase for faster tests.
    def no_available_apps(self):
        raise Exception(
            "Please define available_apps in TransactionTestCase and its subclasses."
        )

    TransactionTestCase.available_apps = property(no_available_apps)
    TestCase.available_apps = None

    # Set an environment variable that other code may consult to see if
    # Django's own test suite is running.
    os.environ["RUNNING_DJANGOS_TEST_SUITE"] = "true"

    test_labels = test_labels or test_modules
    return test_labels, state


def teardown_run_tests(state):
    teardown_collect_tests(state)
    # Discard the multiprocessing.util finalizer that tries to remove a
    # temporary directory that's already removed by this script's
    # atexit.register(shutil.rmtree, TMPDIR) handler. Prevents
    # FileNotFoundError at the end of a test run (#27890).
    from multiprocessing.util import _finalizer_registry

    _finalizer_registry.pop((-100, 0), None)
    del os.environ["RUNNING_DJANGOS_TEST_SUITE"]


class ActionSelenium(argparse.Action):
    """
    Validate the comma-separated list of requested browsers.
    """

    def __call__(self, parser, namespace, values, option_string=None):
        try:
            import selenium  # NOQA
        except ImportError as e:
            raise ImproperlyConfigured(f"Error loading selenium module: {e}")
        browsers = values.split(",")
        for browser in browsers:
            try:
                SeleniumTestCaseBase.import_webdriver(browser)
            except ImportError:
                raise argparse.ArgumentError(
                    self, "Selenium browser specification '%s' is not valid." % browser
                )
        setattr(namespace, self.dest, browsers)


def django_tests(
    verbosity,
    interactive,
    failfast,
    keepdb,
    reverse,
    test_labels,
    debug_sql,
    parallel,
    tags,
    exclude_tags,
    test_name_patterns,
    start_at,
    start_after,
    pdb,
    buffer,
    timing,
    shuffle,
):
    if parallel in {0, "auto"}:
        max_parallel = get_max_test_processes()
    else:
        max_parallel = parallel

    if verbosity >= 1:
        msg = "Testing against Django installed in '%s'" % os.path.dirname(
            django.__file__
        )
        if max_parallel > 1:
            msg += " with up to %d processes" % max_parallel
        print(msg)

    process_setup_args = (verbosity, start_at, start_after, test_labels)
    test_labels, state = setup_run_tests(*process_setup_args)
    # Run the test suite, including the extra validation tests.
    if not hasattr(settings, "TEST_RUNNER"):
        settings.TEST_RUNNER = "django.test.runner.DiscoverRunner"

    if parallel in {0, "auto"}:
        # This doesn't work before django.setup() on some databases.
        if all(conn.features.can_clone_databases for conn in connections.all()):
            parallel = max_parallel
        else:
            parallel = 1

    TestRunner = get_runner(settings)
    TestRunner.parallel_test_suite.init_worker = partial(
        _init_worker,
        process_setup=setup_run_tests,
        process_setup_args=process_setup_args,
    )
    test_runner = TestRunner(
        verbosity=verbosity,
        interactive=interactive,
        failfast=failfast,
        keepdb=keepdb,
        reverse=reverse,
        debug_sql=debug_sql,
        parallel=parallel,
        tags=tags,
        exclude_tags=exclude_tags,
        test_name_patterns=test_name_patterns,
        pdb=pdb,
        buffer=buffer,
        timing=timing,
        shuffle=shuffle,
    )
    failures = test_runner.run_tests(test_labels)
    teardown_run_tests(state)
    return failures


def collect_test_modules(start_at, start_after):
    test_modules, state = setup_collect_tests(start_at, start_after)
    teardown_collect_tests(state)
    return test_modules


def get_subprocess_args(options):
    subprocess_args = [sys.executable, __file__, "--settings=%s" % options.settings]
    if options.failfast:
        subprocess_args.append("--failfast")
    if options.verbosity:
        subprocess_args.append("--verbosity=%s" % options.verbosity)
    if not options.interactive:
        subprocess_args.append("--noinput")
    if options.tags:
        subprocess_args.append("--tag=%s" % options.tags)
    if options.exclude_tags:
        subprocess_args.append("--exclude_tag=%s" % options.exclude_tags)
    if options.shuffle is not False:
        if options.shuffle is None:
            subprocess_args.append("--shuffle")
        else:
            subprocess_args.append("--shuffle=%s" % options.shuffle)
    return subprocess_args


def bisect_tests(bisection_label, options, test_labels, start_at, start_after):
    if not test_labels:
        test_labels = collect_test_modules(start_at, start_after)

    print("***** Bisecting test suite: %s" % " ".join(test_labels))

    # Make sure the bisection point isn't in the test list
    # Also remove tests that need to be run in specific combinations
    for label in [bisection_label, "model_inheritance_same_model_name"]:
        try:
            test_labels.remove(label)
        except ValueError:
            pass

    subprocess_args = get_subprocess_args(options)

    iteration = 1
    while len(test_labels) > 1:
        midpoint = len(test_labels) // 2
        test_labels_a = test_labels[:midpoint] + [bisection_label]
        test_labels_b = test_labels[midpoint:] + [bisection_label]
        print("***** Pass %da: Running the first half of the test suite" % iteration)
        print("***** Test labels: %s" % " ".join(test_labels_a))
        failures_a = subprocess.run(subprocess_args + test_labels_a)

        print("***** Pass %db: Running the second half of the test suite" % iteration)
        print("***** Test labels: %s" % " ".join(test_labels_b))
        print("")
        failures_b = subprocess.run(subprocess_args + test_labels_b)

        if failures_a.returncode and not failures_b.returncode:
            print("***** Problem found in first half. Bisecting again...")
            iteration += 1
            test_labels = test_labels_a[:-1]
        elif failures_b.returncode and not failures_a.returncode:
            print("***** Problem found in second half. Bisecting again...")
            iteration += 1
            test_labels = test_labels_b[:-1]
        elif failures_a.returncode and failures_b.returncode:
            print("***** Multiple sources of failure found")
            break
        else:
            print("***** No source of failure found... try pair execution (--pair)")
            break

    if len(test_labels) == 1:
        print("***** Source of error: %s" % test_labels[0])


def paired_tests(paired_test, options, test_labels, start_at, start_after):
    if not test_labels:
        test_labels = collect_test_modules(start_at, start_after)

    print("***** Trying paired execution")

    # Make sure the constant member of the pair isn't in the test list
    # Also remove tests that need to be run in specific combinations
    for label in [paired_test, "model_inheritance_same_model_name"]:
        try:
            test_labels.remove(label)
        except ValueError:
            pass

    subprocess_args = get_subprocess_args(options)

    for i, label in enumerate(test_labels):
        print(
            "***** %d of %d: Check test pairing with %s"
            % (i + 1, len(test_labels), label)
        )
        failures = subprocess.call(subprocess_args + [label, paired_test])
        if failures:
            print("***** Found problem pair with %s" % label)
            return

    print("***** No problem pair found")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run the Django test suite.")
    parser.add_argument(
        "modules",
        nargs="*",
        metavar="module",
        help='Optional path(s) to test modules; e.g. "i18n" or '
        '"i18n.tests.TranslationTests.test_lazy_objects".',
    )
    parser.add_argument(
        "-v",
        "--verbosity",
        default=1,
        type=int,
        choices=[0, 1, 2, 3],
        help="Verbosity level; 0=minimal output, 1=normal output, 2=all output",
    )
    parser.add_argument(
        "--noinput",
        action="store_false",
        dest="interactive",
        help="Tells Django to NOT prompt the user for input of any kind.",
    )
    parser.add_argument(
        "--failfast",
        action="store_true",
        help="Tells Django to stop running the test suite after first failed test.",
    )
    parser.add_argument(
        "--keepdb",
        action="store_true",
        help="Tells Django to preserve the test database between runs.",
    )
    parser.add_argument(
        "--settings",
        help='Python path to settings module, e.g. "myproject.settings". If '
        "this isn't provided, either the DJANGO_SETTINGS_MODULE "
        'environment variable or "test_sqlite" will be used.',
    )
    parser.add_argument(
        "--bisect",
        help="Bisect the test suite to discover a test that causes a test "
        "failure when combined with the named test.",
    )
    parser.add_argument(
        "--pair",
        help="Run the test suite in pairs with the named test to find problem pairs.",
    )
    parser.add_argument(
        "--shuffle",
        nargs="?",
        default=False,
        type=int,
        metavar="SEED",
        help=(
            "Shuffle the order of test cases to help check that tests are "
            "properly isolated."
        ),
    )
    parser.add_argument(
        "--reverse",
        action="store_true",
        help="Sort test suites and test cases in opposite order to debug "
        "test side effects not apparent with normal execution lineup.",
    )
    parser.add_argument(
        "--selenium",
        action=ActionSelenium,
        metavar="BROWSERS",
        help="A comma-separated list of browsers to run the Selenium tests against.",
    )
    parser.add_argument(
        "--headless",
        action="store_true",
        help="Run selenium tests in headless mode, if the browser supports the option.",
    )
    parser.add_argument(
        "--selenium-hub",
        help="A URL for a selenium hub instance to use in combination with --selenium.",
    )
    parser.add_argument(
        "--external-host",
        default=socket.gethostname(),
        help=(
            "The external host that can be reached by the selenium hub instance when "
            "running Selenium tests via Selenium Hub."
        ),
    )
    parser.add_argument(
        "--debug-sql",
        action="store_true",
        help="Turn on the SQL query logger within tests.",
    )
    # 0 is converted to "auto" or 1 later on, depending on a method used by
    # multiprocessing to start subprocesses and on the backend support for
    # cloning databases.
    parser.add_argument(
        "--parallel",
        nargs="?",
        const="auto",
        default=0,
        type=parallel_type,
        metavar="N",
        help=(
            'Run tests using up to N parallel processes. Use the value "auto" '
            "to run one test process for each processor core."
        ),
    )
    parser.add_argument(
        "--tag",
        dest="tags",
        action="append",
        help="Run only tests with the specified tags. Can be used multiple times.",
    )
    parser.add_argument(
        "--exclude-tag",
        dest="exclude_tags",
        action="append",
        help="Do not run tests with the specified tag. Can be used multiple times.",
    )
    parser.add_argument(
        "--start-after",
        dest="start_after",
        help="Run tests starting after the specified top-level module.",
    )
    parser.add_argument(
        "--start-at",
        dest="start_at",
        help="Run tests starting at the specified top-level module.",
    )
    parser.add_argument(
        "--pdb", action="store_true", help="Runs the PDB debugger on error or failure."
    )
    parser.add_argument(
        "-b",
        "--buffer",
        action="store_true",
        help="Discard output of passing tests.",
    )
    parser.add_argument(
        "--timing",
        action="store_true",
        help="Output timings, including database set up and total run time.",
    )
    parser.add_argument(
        "-k",
        dest="test_name_patterns",
        action="append",
        help=(
            "Only run test methods and classes matching test name pattern. "
            "Same as unittest -k option. Can be used multiple times."
        ),
    )

    options = parser.parse_args()

    using_selenium_hub = options.selenium and options.selenium_hub
    if options.selenium_hub and not options.selenium:
        parser.error(
            "--selenium-hub and --external-host require --selenium to be used."
        )
    if using_selenium_hub and not options.external_host:
        parser.error("--selenium-hub and --external-host must be used together.")

    # Allow including a trailing slash on app_labels for tab completion convenience
    options.modules = [os.path.normpath(labels) for labels in options.modules]

    mutually_exclusive_options = [
        options.start_at,
        options.start_after,
        options.modules,
    ]
    enabled_module_options = [
        bool(option) for option in mutually_exclusive_options
    ].count(True)
    if enabled_module_options > 1:
        print(
            "Aborting: --start-at, --start-after, and test labels are mutually "
            "exclusive."
        )
        sys.exit(1)
    for opt_name in ["start_at", "start_after"]:
        opt_val = getattr(options, opt_name)
        if opt_val:
            if "." in opt_val:
                print(
                    "Aborting: --%s must be a top-level module."
                    % opt_name.replace("_", "-")
                )
                sys.exit(1)
            setattr(options, opt_name, os.path.normpath(opt_val))
    if options.settings:
        os.environ["DJANGO_SETTINGS_MODULE"] = options.settings
    else:
        os.environ.setdefault("DJANGO_SETTINGS_MODULE", "test_sqlite")
        options.settings = os.environ["DJANGO_SETTINGS_MODULE"]

    if options.selenium:
        if multiprocessing.get_start_method() == "spawn" and options.parallel != 1:
            parser.error(
                "You cannot use --selenium with parallel tests on this system. "
                "Pass --parallel=1 to use --selenium."
            )
        if not options.tags:
            options.tags = ["selenium"]
        elif "selenium" not in options.tags:
            options.tags.append("selenium")
        if options.selenium_hub:
            SeleniumTestCaseBase.selenium_hub = options.selenium_hub
            SeleniumTestCaseBase.external_host = options.external_host
        SeleniumTestCaseBase.headless = options.headless
        SeleniumTestCaseBase.browsers = options.selenium

    if options.bisect:
        bisect_tests(
            options.bisect,
            options,
            options.modules,
            options.start_at,
            options.start_after,
        )
    elif options.pair:
        paired_tests(
            options.pair,
            options,
            options.modules,
            options.start_at,
            options.start_after,
        )
    else:
        time_keeper = TimeKeeper() if options.timing else NullTimeKeeper()
        with time_keeper.timed("Total run"):
            failures = django_tests(
                options.verbosity,
                options.interactive,
                options.failfast,
                options.keepdb,
                options.reverse,
                options.modules,
                options.debug_sql,
                options.parallel,
                options.tags,
                options.exclude_tags,
                getattr(options, "test_name_patterns", None),
                options.start_at,
                options.start_after,
                options.pdb,
                options.buffer,
                options.timing,
                options.shuffle,
            )
        time_keeper.print_results()
        if failures:
            sys.exit(1)

2025-12-20 22:50:35,533 - thread-130546535499456.app.lang_graph.nodes.bug_reproducing_execute_node - DEBUG - content='' additional_kwargs={'function_call': {'name': 'run_command', 'arguments': '{"command": "./tests/runtests.py migrations"}'}, '__gemini_function_call_thought_signatures__': {'e1c9072d-4cbb-4260-b655-0515e61ae847': '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'}} response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 16270, 'candidates_token_count': 12, 'total_token_count': 17167, 'cached_content_token_count': 9164, 'prompt_tokens_details': [{'modality': 1, 'token_count': 17238}], 'cache_tokens_details': [{'modality': 1, 'token_count': 9164}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 12}], 'thoughts_token_count': 885}, 'finish_reason': 'STOP', 'avg_logprobs': -10.43328603108724, 'model_name': 'gemini-2.5-pro'} id='run--c5ccaa4b-15ca-474f-aa01-adb2dd3e9806-0' tool_calls=[{'name': 'run_command', 'args': {'command': './tests/runtests.py migrations'}, 'id': 'e1c9072d-4cbb-4260-b655-0515e61ae847', 'type': 'tool_call'}] usage_metadata={'input_tokens': 16270, 'output_tokens': 12, 'total_tokens': 17167, 'input_token_details': {'cache_read': 9164}, 'output_token_details': {'reasoning': 885}}
2025-12-20 22:50:35,536 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: ./tests/runtests.py migrations
2025-12-20 22:50:45,539 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 1
2025-12-20 22:50:45,539 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:

Testing against Django installed in '/testbed/django' with up to 32 processes
Found 656 test(s).
Creating test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Cloning test database for alias 'default'...
Creating test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
Cloning test database for alias 'other'...
System check identified no issues (0 silenced).
.....................................F.............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................s............................................................................................................
======================================================================
FAIL: test_alter_field_choices_noop (migrations.test_alter_field.OperationTests)
Changing the 'choices' option on a field should be a no-op.
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/opt/miniconda3/envs/testbed/lib/python3.9/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/opt/miniconda3/envs/testbed/lib/python3.9/unittest/case.py", line 592, in run
    self._callTestMethod(testMethod)
  File "/opt/miniconda3/envs/testbed/lib/python3.9/unittest/case.py", line 550, in _callTestMethod
    method()
  File "/testbed/tests/migrations/test_alter_field.py", line 26, in test_alter_field_choices_noop
    operation.database_forwards(
  File "/testbed/django/test/testcases.py", line 98, in __exit__
    self.test_case.assertEqual(
  File "/opt/miniconda3/envs/testbed/lib/python3.9/unittest/case.py", line 837, in assertEqual
    assertion_func(first, second, msg=msg)
  File "/opt/miniconda3/envs/testbed/lib/python3.9/unittest/case.py", line 830, in _baseAssertEqual
    raise self.failureException(msg)
AssertionError: 4 != 0 : 4 queries executed, 0 expected
Captured queries were:
1. CREATE TABLE "new__test_alter_field_choices_noop_pony" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "weight" real NOT NULL, "pink" integer NOT NULL)
2. INSERT INTO "new__test_alter_field_choices_noop_pony" ("id", "weight", "pink") SELECT "id", "weight", "pink" FROM "test_alter_field_choices_noop_pony"
3. DROP TABLE "test_alter_field_choices_noop_pony"
4. ALTER TABLE "new__test_alter_field_choices_noop_pony" RENAME TO "test_alter_field_choices_noop_pony"

----------------------------------------------------------------------
Ran 656 tests in 8.824s

FAILED (failures=1, skipped=1)
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'default'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...
Destroying test database for alias 'other'...

2025-12-20 22:50:48,494 - thread-130546535499456.app.lang_graph.nodes.bug_reproducing_execute_node - DEBUG - content='' additional_kwargs={'function_call': {'name': 'run_command', 'arguments': '{"command": "./tests/runtests.py migrations.test_alter_field"}'}, '__gemini_function_call_thought_signatures__': {'a0cf5e46-27f5-4d41-9c7b-6e54fdea6dd9': 'CsYGAY89a18bqfPPIbKVQArc4FY8GNdMm1wzXJTRiEBAp12FGePqqERJ3f/8U7H3nBKsHP/GE1hASfVl70oHx3cP2P/KKoHBuJrq/DmpThEHvLtZHCt1j/x8TCKX6A+pbBuJuJwyWM/FokJK6Qz922Zrp3qNI0qSSzsO0/TAYIiIBUDEbfhfqNWh8kfjz8nE8sHgr/9MYPDl7SYhPtKN920i5O5kW2xOf/j+a6uWMwe1j53ROsWuc89jJKHn6n8AszcCRDqe75LlhaBXxQFCjVg+zRZPxR8f5DLJop9wxgGQttu3WBdC3y/vCumcC+wpCZCmgZ8RamjsMD5lyAHL+xb/o4DmfiLOodz5/omEQ4HBKN9djuqO63C4TEwbOWR8FnbgXaxxb1xiK/iYoWI1GJaJjiyTQCi/Ewedz4gf6bMxMc23bF98MbpocAzg2Ec4wGpL6u4F6yzQswzb0JBwg75nVzqRIUos+Bfk97BRTm77MxmWJFBw4jjrhEAR+KYuI2cASYaEBKQjrSfKfIHy7aSNlHlBoUES+SQ5wEoU/qWRtXvAkBGCPAAztKXR1MOXPDO2RZ/N+crZUhXGqBuLiErCHZ123/HksNr07gWRF0ikBUvZaWmfXpMBQyX4/DGiGAFM2+srWSjaOfVJxBl88p+uvdRJJxKKmp9G2nO/TBd0Nq8tC6GRay35c/woqAARTzKBAYl4HICwRww23PoaWBZYv3DEA88pJ1glpUKpFcXqSl4N1DkvkfYtIbOnvVprqbpGeBLLlIQWAXKQ7RqxkriZTN3EZIWrhu1gEwjUJ+zdqbZfZirZ0jLvP8nte2Uhn3eggVVqazvNqAf88ZVmuFfVw1mJFzWqE+VBkXQHHVl0tgUEmC9bMJq0MAM0XfDIeNFN5s6vRoM1IOSCPBsWboXBWUmCpyCqfDEeS+MGmGZv6GvtRKxlo9s1q3S6JFi4x15Odb+dfnc/B2OfiU7SERtOb6Vl9a2jAPRhew41hszsQE5ytfEJ6S1RZXAwPqs23FY3ZU2O2AWlEUq1ycwQAGkU/QicW/4acYm4xa0QFlWRoe95nSJC/okqn2NPkPSP5Uvb7+xxqCP+6MyjKXIaOIUJs+rISuHBtg=='}} response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 18224, 'candidates_token_count': 18, 'total_token_count': 18467, 'cached_content_token_count': 16602, 'prompt_tokens_details': [{'modality': 1, 'token_count': 20077}], 'cache_tokens_details': [{'modality': 1, 'token_count': 16602}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 18}], 'thoughts_token_count': 225}, 'finish_reason': 'STOP', 'avg_logprobs': -1.222106721666124, 'model_name': 'gemini-2.5-pro'} id='run--fd9eae2f-736a-4851-af2a-535bc8da1b4b-0' tool_calls=[{'name': 'run_command', 'args': {'command': './tests/runtests.py migrations.test_alter_field'}, 'id': 'a0cf5e46-27f5-4d41-9c7b-6e54fdea6dd9', 'type': 'tool_call'}] usage_metadata={'input_tokens': 18224, 'output_tokens': 18, 'total_tokens': 18467, 'input_token_details': {'cache_read': 16602}, 'output_token_details': {'reasoning': 225}}
2025-12-20 22:50:48,497 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: ./tests/runtests.py migrations.test_alter_field
2025-12-20 22:50:49,383 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 1
2025-12-20 22:50:49,383 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:
./tests/runtests.py migrations.test_alter_f
field

Testing against Django installed in '/testbed/django' with up to 32 processes
Found 1 test(s).
Creating test database for alias 'default'...
Creating test database for alias 'other'...
System check identified no issues (0 silenced).
F
======================================================================
FAIL: test_alter_field_choices_noop (migrations.test_alter_field.OperationTests)
Changing the 'choices' option on a field should be a no-op.
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/testbed/tests/migrations/test_alter_field.py", line 26, in test_alter_field_choices_noop
    operation.database_forwards(
  File "/testbed/django/test/testcases.py", line 98, in __exit__
    self.test_case.assertEqual(
AssertionError: 4 != 0 : 4 queries executed, 0 expected
Captured queries were:
1. CREATE TABLE "new__test_alter_field_choices_noop_pony" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "weight" real NOT NULL, "pink" integer NOT NULL)
2. INSERT INTO "new__test_alter_field_choices_noop_pony" ("id", "weight", "pink") SELECT "id", "weight", "pink" FROM "test_alter_field_choices_noop_pony"
3. DROP TABLE "test_alter_field_choices_noop_pony"
4. ALTER TABLE "new__test_alter_field_choices_noop_pony" RENAME TO "test_alter_field_choices_noop_pony"

----------------------------------------------------------------------
Ran 1 test in 0.016s

FAILED (failures=1)
Destroying test database for alias 'default'...
Destroying test database for alias 'other'...

2025-12-20 22:50:50,245 - thread-130546535499456.app.lang_graph.nodes.bug_reproducing_execute_node - DEBUG - content='' additional_kwargs={} response_metadata={'usage_metadata': {'prompt_token_count': 18681, 'total_token_count': 18681, 'cached_content_token_count': 16613, 'prompt_tokens_details': [{'modality': 1, 'token_count': 18681}], 'cache_tokens_details': [{'modality': 1, 'token_count': 16613}], 'candidates_token_count': 0, 'thoughts_token_count': 0, 'candidates_tokens_details': []}, 'model_name': 'gemini-2.5-pro'} id='run--dfe232fb-013d-4479-8819-d99cacf1957e-0' usage_metadata={'input_tokens': 18681, 'output_tokens': 0, 'total_tokens': 18681, 'input_token_details': {'cache_read': 16613}}
2025-12-20 22:51:03,381 - thread-130546535499456.app.lang_graph.nodes.bug_reproducing_structured_node - DEBUG - reproduced_bug=True reproduced_bug_failure_log='FAIL: test_alter_field_choices_noop (migrations.test_alter_field.OperationTests)\nChanging the \'choices\' option on a field should be a no-op.\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File "/testbed/tests/migrations/test_alter_field.py", line 26, in test_alter_field_choices_noop\n    operation.database_forwards(\n  File "/testbed/django/test/testcases.py", line 98, in __exit__\n    self.test_case.assertEqual(\nAssertionError: 4 != 0 : 4 queries executed, 0 expected\nCaptured queries were:\n1. CREATE TABLE "new__test_alter_field_choices_noop_pony" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "weight" real NOT NULL, "pink" integer NOT NULL)\n2. INSERT INTO "new__test_alter_field_choices_noop_pony" ("id", "weight", "pink") SELECT "id", "weight", "pink" FROM "test_alter_field_choices_noop_pony"\n3. DROP TABLE "test_alter_field_choices_noop_pony"\n4. ALTER TABLE "new__test_alter_field_choices_noop_pony" RENAME TO "test_alter_field_choices_noop_pony"\n\n----------------------------------------------------------------------\nRan 1 test in 0.016s\n\nFAILED (failures=1)' reproduced_bug_commands=['./tests/runtests.py migrations.test_alter_field']
2025-12-20 22:51:03,383 - thread-130546535499456.app.lang_graph.nodes.dual_version_validation_node - INFO - Starting dual-version validation
2025-12-20 22:51:03,383 - thread-130546535499456.app.lang_graph.nodes.dual_version_validation_node - INFO - Running test on old codebase (should fail)
2025-12-20 22:51:03,383 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: ./tests/runtests.py migrations.test_alter_field
2025-12-20 22:51:04,203 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 1
2025-12-20 22:51:04,204 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:
./tests/runtests.py migrations.test_alter_f
field

Testing against Django installed in '/testbed/django' with up to 32 processes
Found 1 test(s).
Creating test database for alias 'default'...
Creating test database for alias 'other'...
System check identified no issues (0 silenced).
F
======================================================================
FAIL: test_alter_field_choices_noop (migrations.test_alter_field.OperationTests)
Changing the 'choices' option on a field should be a no-op.
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/testbed/tests/migrations/test_alter_field.py", line 26, in test_alter_field_choices_noop
    operation.database_forwards(
  File "/testbed/django/test/testcases.py", line 98, in __exit__
    self.test_case.assertEqual(
AssertionError: 4 != 0 : 4 queries executed, 0 expected
Captured queries were:
1. CREATE TABLE "new__test_alter_field_choices_noop_pony" ("id" integer NOT NULL PRIMARY KEY AUTOINCREMENT, "weight" real NOT NULL, "pink" integer NOT NULL)
2. INSERT INTO "new__test_alter_field_choices_noop_pony" ("id", "weight", "pink") SELECT "id", "weight", "pink" FROM "test_alter_field_choices_noop_pony"
3. DROP TABLE "test_alter_field_choices_noop_pony"
4. ALTER TABLE "new__test_alter_field_choices_noop_pony" RENAME TO "test_alter_field_choices_noop_pony"

----------------------------------------------------------------------
Ran 1 test in 0.015s

FAILED (failures=1)
Destroying test database for alias 'default'...
Destroying test database for alias 'other'...

2025-12-20 22:51:04,204 - thread-130546535499456.app.lang_graph.nodes.dual_version_validation_node - INFO - Test failed on old codebase (as expected)
2025-12-20 22:51:04,204 - thread-130546535499456.app.lang_graph.nodes.dual_version_validation_node - INFO - Applying patch to repository
2025-12-20 22:51:04,208 - thread-130546535499456.app.lang_graph.nodes.dual_version_validation_node - INFO - Updating container with patched code
2025-12-20 22:51:04,209 - thread-130546535499456.app.docker.base_container - INFO - Updating files in the container after edits.
2025-12-20 22:51:04,209 - thread-130546535499456.app.docker.base_container - INFO - Creating directory django/db/backends/base in the container
2025-12-20 22:51:04,209 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: mkdir -p django/db/backends/base
2025-12-20 22:51:04,310 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-20 22:51:04,310 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:


2025-12-20 22:51:04,311 - thread-130546535499456.app.docker.base_container - INFO - Updating django/db/backends/base/schema.py in the container
2025-12-20 22:51:04,317 - thread-130546535499456.app.docker.base_container - INFO - Files updated successfully
2025-12-20 22:51:04,317 - thread-130546535499456.app.lang_graph.nodes.dual_version_validation_node - INFO - Running test on patched codebase (should pass)
2025-12-20 22:51:04,317 - thread-130546535499456.app.docker.base_container - DEBUG - Executing command: ./tests/runtests.py migrations.test_alter_field
2025-12-20 22:51:05,157 - thread-130546535499456.app.docker.base_container - DEBUG - Command exit code: 0
2025-12-20 22:51:05,158 - thread-130546535499456.app.docker.base_container - DEBUG - Command output:
./tests/runtests.py migrations.test_alter_f
field

Testing against Django installed in '/testbed/django' with up to 32 processes
Found 1 test(s).
Creating test database for alias 'default'...
Creating test database for alias 'other'...
System check identified no issues (0 silenced).
.
----------------------------------------------------------------------
Ran 1 test in 0.012s

OK
Destroying test database for alias 'default'...
Destroying test database for alias 'other'...

2025-12-20 22:51:05,158 - thread-130546535499456.app.lang_graph.nodes.dual_version_validation_node - INFO - Resetting repository to clean state
2025-12-20 22:51:05,275 - thread-130546535499456.app.lang_graph.nodes.dual_version_validation_node - INFO - Validation PASSED: Test fails on old, passes on new
2025-12-20 22:51:05,277 - thread-130546535499456.app.docker.base_container - INFO - Cleaning up container and temporary files
2025-12-20 22:51:05,277 - thread-130546535499456.app.docker.base_container - INFO - Closing persistent shell...
2025-12-20 22:51:05,377 - thread-130546535499456.app.docker.base_container - INFO - Cleaning up container and temporary files
